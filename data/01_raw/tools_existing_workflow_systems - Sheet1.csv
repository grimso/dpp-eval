name,mapping_from,tool_full_name,Interesting_concepts_thesis,status,Status comment,tool_source,homepage_url,repo_url,publication_url,"tool_origin",company_url,Short Description,type,domain,open_source,license
Arvados,,,yes,Active (2023-04-07),,Existing Workflow systems,https://arvados.org/,https://github.com/arvados/arvados,https://arvados.org/2022/10/05/workflow-data-management/,"Community, Academia",https://arvados.org/community/,"Arvados is an open source platform for
managing, processing, and sharing genomic and other large scientific
and biomedical data",scientific workflow,"genomic, biomedical",yes,
Apache Taverna,,,no,Retired (2020-02-20),,Existing Workflow systems,http://www.taverna.org.uk/,,https://doi.org/10.1093/nar/gkl320,"Non-Profit, Academia",,Taverna is a domain-independent suite of tools used to design and execute data-driven workflows.,scientific workflow,,,
Galaxy,,,yes,Active (2023-04-07),,Existing Workflow systems,https://galaxyproject.org/,https://github.com/galaxyproject/galaxy,https://doi.org/10.1093/nar/gkac247,"Community, Academia",,"Galaxy is an open-source platform for FAIR data analysis that enables users to: 1) Use tools from various domains (that can be plugged into workflows) through its graphical web interface. 2)Run code in interactive environments (RStudio, Jupyter...) along with other tools or workflows. 3)Manage data by sharing and publishing results, workflows, and visualizations. 4)Ensure reproducibility by capturing the necessary information to repeat and understand data analyses.",scientific workflow,bioinformatic workflow,yes,
SHIWA,,,no,Retired (2012),Superseeded by ER-flow,Existing Workflow systems,https://www.shiwa-workflow.eu/,,https://www.doi.org/10.3233/978-1-61499-054-3-109,,,,scientific workflow,,,
ER-flow,,,no,Retired (2014),,,https://tu-dresden.de/zih/forschung/projekte/er_flow,,https://doi.org/10.1109/CCGrid.2014.98,Academia,https://tu-dresden.de/zih,,scientific workflow,,,
Apache Oozie,,,no,Active (2023-04-07),,Existing Workflow systems,https://oozie.apache.org/,https://github.com/apache/oozie,,Non-Profit,,Apache Oozie Workflow Scheduler for Hadoop,Workflow scheduler Hadoop,,yes,
DNANexus,,,-,Active (2023-04-07),,Existing Workflow systems,https://documentation.dnanexus.com/,,,Enterprise,https://www.dnanexus.com/,cloud-based data analysis and management platform for DNA sequence data,,Genomic,proprietary,
BioDT,,BioDatomics,no,Retired,,Existing Workflow systems,https://www.biodatomics.com,,,Enterprise,https://www.biodatomics.com/,BioDatomics provides powerful yet intuitive bioinformatics tools that help genomic and biomedical researchers glean insights from their data more quickly,,Bionformatics,proprietary,
Agave,,,no,not found,404,Existing Workflow systems,http://agaveapi.co/live-docs/,,,,,,,,,
Cyverse,DiscoveryEnvironment,,maybe,Active (2023-04-07),,Existing Workflow systems,https://cyverse.org/discovery-environment,,https://par.nsf.gov/biblio/10108389,"Academia,Non-Profit",https://cyverse.org/about,"An Open Science Workspace for Collaborative Data-driven Discovery - A powerful platform with a simple web interface for managing, sharing, 
running, and visualizing your data, analyses, and results -- where 
data-driven discoveries are made.",hosted scietifix workflow managment,science,yes,
Wings,,Wings Workflows,maybe,Active (2023-04-07),,Existing Workflow systems,https://www.wings-workflows.org,https://github.com/knowledgecaptureanddiscovery/wings,https://doi.org/10.1109/MIS.2010.9,Academia,,"WINGS is a semantic workflow system that assists scientists with the design of computational experiments. A unique feature of WINGS is that its workflow representations incorporate semantic constraints about datasets and workflow components, and are used to create and validate workflows and to generate metadata for new data products. WINGS submits workflows to execution frameworks such as Pegasus and OODT to run workflows at large scale in distributed resources",compuational experiments,science,yes,
Knime,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://www.knime.com/,https://github.com/knime/knime-core,,Academia,https://www.knime.com/open-for-innovation,"KNIME Analytics Platform is an open source software with an intuitive, visual interface that lets you build analyses of any complexity level. Access, blend, analyze, and visualize data, without any coding, or integrate your favorite tools and libraries as needed.",,data anaylsiis,yes,
"make, rake, drake, ant, scons & many others",,,,Active,,Existing Workflow systems,,https://github.com/Factual/drake,,,,,,,,
Snakemake,,,yes,Active (2023-04-07),,Existing Workflow systems,https://snakemake.github.io/,https://github.com/snakemake/snakemake,https://doi.org/10.1093/bioinformatics/bty350,"Non-Profit,Academia",,A framework for reproducible data anylsis,"data anaylsis, workflow managment",data anaylsis,yes,
Bpipe,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://docs.bpipe.org/,https://github.com/ssadedin/bpipe/,https://doi.org/10.1093/bioinformatics/bts167,Academia,,a tool for running and managing bioinformatics pipelines,,bionfirmaticx,yes,
CGAT-Ruffus,Ruffus,,no,Last Release (2020-04-24) - replaced by cgat core,,Existing Workflow systems,https://cgat-core.readthedocs.io/en/latest/,https://github.com/cgat-developers/ruffus,https://doi.org/10.1093/bioinformatics/btt756,Academia,,,workflow managment,bioinformaticx,yes,
CGAT-Core,,,maybe,Active (2023-04-07),,,https://cgat-core.readthedocs.io/en/latest/,https://github.com/cgat-developers/cgat-core,https://doi.org/10.1093/bioinformatics/btt756,Academia,,CGAT-core is a workflow management system that allows users to quickly and reproducibly build scalable data analysis pipelines. CGAT-core is a set of libraries and helper functions used to enable researchers to design and build computational workflows for the analysis of large-scale data-analysis.,workflow managment,data anylsis,yes,
Nextflow,,,yes,Active (2023-04-07),,Existing Workflow systems,https://nextflow.io/,https://github.com/nextflow-io/nextflow,https://www.nature.com/articles/nbt.3820%7B,"Academia,Funded, enterprise platform",,Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages,computational pipelines,Data-driven computational pipelines,yes,
Luigi,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://luigi.readthedocs.io/en/stable/,https://github.com/spotify/luigi,,Enterprise,,"Luigi is a Python package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more.",data batch processing pipelines,data pipeline,yes,
SciLuigi,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://jcheminf.biomedcentral.com/articles/10.1186/s13321-016-0179-6,https://github.com/pharmbio/sciluigi,https://doi.org/10.1186/s13321-016-0179-6,Academia,,"A light-weight wrapper library around Spotify's Luigi workflow library to make writing scientific workflows more fluent, flexible and modular",scientific workflows,science,yes,
Law,Luigi Analysis Workflow,Luigi Analysis Workflow,no,Active (2023-04-07),,Existing Workflow systems,,https://github.com/riga/law,https://indico.cern.ch/event/1097499/contributions/4617472/attachments/2348703/4005689/2021-11-18_law.pdf,Academia,,"Luigi analysis workflow: Use law to build complex and large-scale task workflows. It is build on top of luigi and adds abstractions for run locations, storage locations and software environments. Law strictly disentangles these building blocks and ensures they remain interchangeable and resource-opportunistic.",data workflows,"dada analysis, CERN",yes,
GATK,GATK Queue,,no,Active (2023-04-07),,Existing Workflow systems,https://gatk.broadinstitute.org/hc/en-us,https://github.com/broadinstitute/gatk/,http://www.genome.org/cgi/doi/10.1101/gr.107524.110,Academia,https://www.broadinstitute.org/,"Developed in the Data Sciences Platform at the Broad Institute, the toolkit offers a wide variety of tools with a primary focus on variant discovery and genotyping. Its powerful processing engine and high-performance computing features make it capable of taking on projects of any size.",,genomic,yes,
YABI,,,no,Retired(2016),,Existing Workflow systems,https://yabi.readthedocs.io/en/latest/,https://github.com/muccg/yabi/,https://doi.org/10.1186/1751-0473-7-1,Academia,https://www.murdoch.edu.au/,"Yabi is a 3-tier application stack to provide users with an intuitive, easy to use, abstraction of compute and data environments",online research envrionment,genomic,yes,
Seqware,,,no,Retired(2015),,Existing Workflow systems,https://seqware.github.io/docs/6-pipeline/,https://github.com/SeqWare/seqware,https://doi.org/10.1186/1471-2105-11-S12-S2,Academia,,"The SeqWare Pipeline sub-project is really the heart of the overall SeqWare project. This provides the core functionality of SeqWare; it is workflow developer environment and a series of tools for installing, running, and monitoring workflows.",,genomic,yes,
Pegasus,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://pegasus.isi.edu/,https://github.com/pegasus-isi/pegasus,https://doi.org/10.1016/j.jocs.2020.101200,Academia,https://scitech.group,"Pegasus WMS is a configurable system for mapping and executing scientific workflows over a wide range of computational infrastructures including laptops, campus clusters, supercomputers, grids, and commercial and academic clouds. Pegasus has been used to run workflows with up to 1 million tasks that process tens of terabytes of data at a time.",scientific workflow,science,yes,
Ketrew,,,no,Retired ( 2017),,Existing Workflow systems,http://www.hammerlab.org/docs/ketrew/master/index.html,https://github.com/hammerlab/ketrew,https://doi.org/10.1101/213884,Academia,http://www.hammerlab.org/,,,,yes,
Apache Airflow,,,yes,Active (2023-04-07),,Existing Workflow systems,https://airflow.apache.org/,https://github.com/apache/airflow,,commercial,https://www.airbnb.de/,"Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.",,data processing,yes,
Couler,,,no,Retired ( 2021),,Existing Workflow systems,https://couler-proj.github.io/couler/index.html,https://github.com/couler-proj/couler,,open-source-project,,"Couler aims to provide a unified interface for constructing and managing workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.",,,yes,
Cosmos,,,no,Retired (2022-08),,Existing Workflow systems,https://mizzou-cbmi.github.io/COSMOS2/,https://github.com/Mizzou-CBMI/COSMOS2,https://doi.org/10.1093/bioinformatics/btu385,Academia,,"Cosmos is a python library for creating scientific pipelines that run on a distributed computing cluster. It is primarily designed and used for machine learning and bioinformatics pipelines, but is general enough for any type of distributed computing workflow and is also used in fields such as image processing.",,,yes,
Pinball,,,no,Retired (2019-12-11),,Existing Workflow systems,,https://github.com/pinterest/pinball,,Enterprise,https://engineering.pinterest.com/,Pinball is a scalable workflow manager.,,,yes,
bcbio,,,no,Barley Active,,Existing Workflow systems,bcbio-nextgen.readthedocs.io,https://github.com/bcbio/bcbio-nextgen,http://journal.embnet.org/index.php/embnetjournal/article/download/286/674,Academia,,"A python toolkit providing best-practice pipelines for fully automated high throughput sequencing analysis. You write a high level configuration file specifying your inputs and analysis parameters. This input drives a parallel pipeline that handles distributed execution, idempotent processing restarts and safe transactional steps. The goal is to provide a shared community resource that handles the data processing component of sequencing analysis, providing researchers with more time to focus on the downstream biology.",pipeline sequencing pipeline,genomic,yes,
chronos,,,no,Retired (2018),,Existing Workflow systems,,https://github.com/mesos/chronos,,Enterprise,https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d,Fault tolerant job scheduler for Mesos which handles dependencies and ISO8601 based schedules,job scheduler,,yes,
azkaban,,,no,Active (2023-04-07),,Existing Workflow systems,https://azkaban.github.io/,https://github.com/azkaban/azkaban,https://doi.org/10.1145/2463676.2463707,Enterprise,https://www.linkedin.com/pulse/searching-suitable-hadoop-jobs-scheduler-azkaban-answer-tushar-bhutte/,Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.,hadoob workflow scheduler,"ETL,hadoop",yes,
Apache nifi,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://nifi.apache.org/,https://github.com/apache/nifi,,Agency,,"iFi was built to automate the flow of data between systems. While the term 'dataflow' is used in a variety of contexts, we use it here to mean the automated and managed flow of information between systems.","Dataflow, ETL","Data pipeline, ETL",yes,
flowr,,,no,Active (2023-04-07),,Existing Workflow systems,http://flow-r.github.io/flowr/,https://github.com/flow-r/flowr,,Academia,http://sahilseth.com/software/,"Flowr framework allows you to design and implement complex pipelines, and deploy them on your institution's computing cluster. This has been built keeping in mind the needs of bioinformatics workflows. However, it is easily extendable to any field where a series of steps (shell commands) are to be executed in a (work)flow.",R workflows,-,yes,
Mistral,,,no,Active (2023-04-07),,Existing Workflow systems,https://docs.openstack.org/mistral/latest/user/wf_lang_v2.html,https://github.com/openstack/mistral,,Foundation,,"Workflow Service integrated with OpenStack. This project aims to provide a mechanism to define tasks and workflows in a simple YAML-based language, manage and execute them in a distributed environment.",tasks and workflows,workflow servce ,yes,
Nipype,,,no,Active (2023-04-07),,Existing Workflow systems,https://nipype.readthedocs.io/en/latest/,https://github.com/nipy/nipype,https://doi.org/10.3389/fninf.2011.00013,"Academia, Community",https://nipy.org/,Neuroimaging in Python Pipelines and Interfaces,data porcessing,Neuroimaging,yes,
EndOfDay,End of Day,,no,Retired (2016-04-3),,Existing Workflow systems,,https://github.com/joestubbs/endofday,https://ceur-ws.org/Vol-1871/paper6.pdf,Academia,,Execute pipelines and other workflows of docker containers.,Docker workdlows,,yes,
BioDSL,,,no,Retired (2015-11-18),,Existing Workflow systems,,https://github.com/maasha/BioDSL,,,,"BioDSL (pronounced Biodiesel) is a Domain Specific Language for creating bioinformatic analysis workflows. A workflow may consist of several pipelines and each pipeline consists of a series of steps such as reading in data from a file, processing the data in some way, and writing data to a new file.",analysis workflows,Bionformatik,yes,
BigDataScript,,,no,Active (2023-04-07),,Existing Workflow systems,https://pcingola.github.io/bds/,https://github.com/pcingola/bds,https://doi.org/10.1093/bioinformatics/btu595,Academia,,Develop a data analysis pipelines and run exactly the same script everywhere. No matter how big the computer.,"Data pipelines, Big Data","Scripts, Big Data",yes,
Dagobah,,,no,NoActivity (2015-03-05),,Existing Workflow systems,,https://github.com/thieman/dagobah,,,,"Simple DAG-based job scheduler in Python. Dagobah is a simple dependency-based job scheduler written in Python. 
Dagobah allows you to schedule periodic jobs using Cron syntax. Each job
 then kicks off a series of tasks (subprocesses) in an order defined by a
 dependency graph you can easily draw with click-and-drag in the web 
interface.",Dag Subprocess,job schedukler,yes,
Omics Pipe,Omics Pipe: uses Ruffus,,no,Retired ,404,Existing Workflow systems,https://sulab.org/tools/omics-pipe/,https://bitbucket.org/sulab/omics_pipe,,,,"Omics Pipe is an open-source, modular computational platform that automates best practice multi-omics data analysis pipelines. Omics Pipe can be run from the command-line by providing it with a YAML parameter file specifying your directory structure and software specific parameters or on AWS. This executes a parallel automated pipeline on a Distributed Resource Management system that efficiently handles job resource allocation, monitoring and restarting",multi-omice data analysis pipelines,genomic,yes,
eHive,Ensembl Hive,,no,Active (2023-04-07),,Existing Workflow systems,https://ensembl-hive.readthedocs.io/en/version-2.6/,https://github.com/Ensembl/ensembl-hive,https://doi.org/10.1186/1471-2105-11-240,Academia,https://www.ensembl.org,"eHive is a system for running computation pipelines on distributed computing resources - clusters, farms or grids.","distributed computing, computational pipelines",computational pipeline,yes,
QuickNGS,,,no ,Retired ,404 (http://bifacility.uni-koeln.de/quickngs/web),Existing Workflow systems,https://www.cecad.uni-koeln.de/research/core-facilities/bioinformatics-facility/,,,Academia,https://www.cecad.uni-koeln.de/research/core-facilities/bioinformatics-facility/,"All in one data processing for NExt-Generation Sequencing. QuickNGS,
 a highly scalable NGS analysis system developed in-house (Wagle et al.,
 2015). It currently comprises analysis pipelines for transcriptomics 
(RNA-Seq), epigenomics (ChIP-Seq), whole-genome sequencing (WGS) as well
 as solutions for WGS and whole-exome sequencing (WXS) of tumor samples.
 Integrated analyses of the transcriptome and epigenome will be 
performed if both data types are available. Downstream analyses of the 
results and assistance with the publication process are provided upon 
individual request.",,genomic,yes,
GenePattern,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://www.genepattern.org,https://github.com/genepattern/genepattern-notebook,https://doi.org/10.1038/ng0506-500,Academia,,A Platform for Reproducible bioinformatics,"NOtebooks, genomic tools, anaylsis pipelines, reproducible",bioinformatics,yes,
Chipster,,,no,Retired (2022-03-31),,Existing Workflow systems,https://chipster.csc.fi/,https://github.com/chipster/chipster,https://doi.org/10.1186/1471-2164-12-507,Academia,,"Chipster is a user-friendly analysis software for high-throughput data such as RNA-seq and single-cell RNA-seq. Chipster provides a web interface to over 500 analysis tools, and the actual analysis jobs run on the server side making use of CSC's computing environment.",data analysis,genomic,yes,
The Genome Modeling System,,,no,Retired (2020-01-31),,Existing Workflow systems,https://github.com/genome/gms/wiki,https://github.com/genome/gms,https://doi.org/10.1371/journal.pcbi.1004274,Academia,,"The Genome Institute at Washington University has developed a high-throughput, fault-tolerant analysis information management system called the Genome Modeling System (GMS), capable of executing complex, interdependent, and automated genome analysis pipelines at a massive scale. The GMS framework provides detailed tracking of samples and data coupled with reliable and repeatable analysis pipelines. GMS includes a full system image with software and services, expandable from one workstation to a large compute cluster.","high-throughput, fault-tolerant analysis information management system",genomic,yes,
Cuneiform ,"Cuneiform, A Functional Workflow Language",,maybe,Active (2023-04-07),,Existing Workflow systems,http://www.cuneiform-lang.org/,https://github.com/joergen7/cuneiform,http://ceur-ws.org/Vol-1330/paper-03.pdf,Academia,,"Cuneiform is a large-scale data analysis functional programming language. It is open because it easily integrates foreign tools and libraries, e.g., Python libraries or command line tools. It is general because it has the expressive power of a functional programming language while using the independence of sub-expressions to automatically parallelize programs. Cuneiform uses distributed Erlang to scalably run in cluster and cloud environments.","large-scale, data anylsisi, functional programming, cluster, clozd ",data analysis,yes,
Anvaya,,,no,"?, Retired",,Existing Workflow systems,http://webapp.cabgrid.res.in/biocomp/Anvaya/ANVAYA_Main.html#ANVAYA_WF_SUMM,,https://doi.org/10.1142/S0219720012500060,Academia,https://cdac.in/index.aspx?id=bio_products,"A Workflow-Pipeline is logical connection of commonly used Bioinformatics tools, which are run either in serial mode or in parallel mode, to achieve a scientific target.",workflow,"bioinformatics, genomic",yes,
Makeflow,,,no,Active (2023-04-07),,Existing Workflow systems,http://ccl.cse.nd.edu/software/makeflow/,,http://dx.doi.org/10.1145/2443416.2443417,Academia,,"Makeflow is a workflow engine for large scale distributed computing. It accepts a specification of a large amount of work to be performed, and runs it on remote machines in parallel where possible. In addition, Makeflow is fault-tolerant, so you can use it to coordinate very large tasks that may run for days or weeks in the face of failures. Makeflow is designed to be similar to Make , so if you can write a Makefile, then you can write a Makeflow.","workflow system,  large scale distributed computing, grid, cloud",workflow,yes,
Apache Airavata,,,no,Active (2023-04-07),,Existing Workflow systems,https://airavata.apache.org/,https://github.com/apache/airavata,http://doi.acm.org/10.1145/2110486.2110490,Academia,,"Apache AiravataTM is a software framework that enables you to compose, manage, execute, and monitor large scale applications and workflows on distributed computing resources such as local clusters, supercomputers, computational grids, and computing clouds.","workflow, managment, large, scale, clusters, supercomputers, grids, cloud","workflows, large scale",yes,
PyFlow,,,no,Inactive (2020-07-01),,Existing Workflow systems,,https://github.com/Illumina/pyflow,,commercial,,"pyFlow is a tool to manage tasks in the context of a task dependency graph. It has some similarities to make. pyFlow is not a program – it is a python module, and workflows are defined using pyFlow by writing regular python code with the pyFlow API",workflow,"workflow, task engine",yes,
Cluster Flow,,,no,Inactive (2019-07-08),,Existing Workflow systems,http://clusterflow.io/,https://github.com/ewels/clusterflow,https://doi.org/10.12688%2Ff1000research.10335.2,Academia,,"A simple and flexible bioinformatics pipeline tool. Cluster Flow is a pipelining tool to automate and standardise
bioinformatics analyses on high-performance cluster environments.
It is designed to be easy to use, quick to set up and flexible to configure.","workflows, pipelines, cluster, local",bionformatics,yes,
Unipro Ugene,,,no,Active (2023-04-07),,Existing Workflow systems,http://ugene.net/,https://github.com/ugeneunipro/ugene,https://doi.org/10.1093/bioinformatics/bts091,"Academia, commercial support",,free open-source bioinformatics software,bioinformatic tools,bionformatics,yes,
Cloudslang,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://www.cloudslang.io/#/,https://github.com/CloudSlang/cloud-slang,,commercial,https://www.microfocus.com,"CloudSlang is an open source tool for orchestrating cutting edge technologies. It can orchestrate anything you can imagine in an agentless manner. You can use or customize ready-made YAML based workflows. They are powerful, shareable and human readable. Modernize your IT with CloudSlang.","process based orchestration , agentless, proven, yaml, workflow",orchestration of workflows,yes,
Stacks,,,no,Active (2023-04-07),,Existing Workflow systems,http://catchenlab.life.illinois.edu/stacks/,http://catchenlab.life.illinois.edu/stacks/source/stacks-2.64.tar.gz,https://doi.org/10.1534/g3.111.000240,Academia,,"Stacks is a software pipeline for building loci from short-read sequences, such as those generated on the Illumina platform. Stacks was developed to work with restriction enzyme-based data, such as RAD-seq, for the purpose of building genetic maps and conducting population genomics and phylogeography.",pipeline for RAD-seg data,genomics,yes,
Leaf,,,no,Retired,404 (http://www.francesconapolitano.it/leaf/index.html),Existing Workflow systems,http://www.francesconapolitano.it/leaf/index.html,,,,,,,,-,
omictools,,,no,,,Existing Workflow systems,http://omictools.com/,,,commercial,https://omictools.com/company/,,,,no,
JDL,Job Description Language,Job Description Language,no,,,Existing Workflow systems,https://edms.cern.ch/ui/file/590869/1/WMS-JDL.pdf,,https://edms.cern.ch/ui/file/590869/1/WMS-JDL.pdf,Academia,,"The Job Description Language, JDL, is a high-level, user-oriented language based on Condor classified advertisements for describing jobs and aggregates of jobs such as Direct Acyclic Graphs and Collections","DAGs, jobs",condor,,
YAWL,YAWL yet another workflow language,yet another workflow language,no,Active (2023-04-07),,Existing Workflow systems,https://yawlfoundation.github.io,https://github.com/yawlfoundation/yawl,https://doi.org/10.1016/j.softx.2020.100576,Academia,,"YAWL is a free, open source BPM/Workflow system, based on a concise and powerful modelling language, that handles complex data transformations, and full integration with organizational resources, applications and external Web Services.","BPM, workflow system",BPM,yes,
Triquetrum,,,no,Retired (2021-10-07),,Existing Workflow systems,https://projects.eclipse.org/projects/science.triquetrum,https://github.com/eclipse-archived/triquetrum,https://doi.org/10.1016/j.procs.2016.05.546,Academia,,"Triquetrum delivers an open platform for managing and executing scientific workflows. The goal of Triquetrum is to support a wide range of use cases, ranging from automated processes based on predefined models, to replaying ad-hoc research workflows recorded from a user's actions in a scientific workbench UI. It will allow to define and execute models from personal pipelines with a few steps to massive models with thousands of elements.",scientific workflows,scientific workflows,yes,
Kronos,,,no,Inactive (2016-10-06),,Existing Workflow systems,https://kronos.readthedocs.io/en/latest/,https://github.com/jtaghiyar/kronos,https://doi.org/10.1101/040352,Academia,,A workflow assembler for genome analytics and informatics.,workflow,genome,yes,
qsubsec,,,no,INactive (2019-08-30),,Existing Workflow systems,,https://github.com/alastair-droop/qsubsec,https://doi.org/10.1093/bioinformatics/btv698,Academia,,"qsubsec is a template language for generating script files for submission using the SGE grid system. By using this system, you can separate the logic of your qsub jobs from the data required for a specific run.",git system submission,remplate preporcesser engine,yes,
YesWorkflow,,,no,?,,Existing Workflow systems,,https://github.com/yesworkflow-org/yw-prototypes,https://doi.org/10.2218/ijdc.v10i1.370,Academia,,"YesWorkflow aims to provide a number of the benefits of using a scientific workflow management system without having to rewrite scripts and other scientific software. Rather than reimplement code so that it can be executed and managed by a workflow engine, a YesWorkflow user simply adds special YesWorkflow (YW) comments to existing scripts. These comments declare how data is used and results produced, step by step, by the script. The YesWorkflow tools interpret the YW comments and produce graphical output that reveals the stages of computation and the flow of data in the script.",,scripts,yes,
gwf,,Grid WorkFlow,no,Active (2023-04-07),,Existing Workflow systems,https://gwf.app/,https://github.com/gwforg/gwf,,Academia,https://genome.au.dk/,"gwf is a flexible, pragmatic workflow tool for building and running large, scientific workflows. It runs on Python 3.7+ and is developed at GenomeDK, Aarhus University.",scientific workflow,scientific workflows,yes,
FireWorks,,,maybe,FireWorks,,Existing Workflow systems,https://materialsproject.github.io/fireworks/,https://github.com/materialsproject/fireworks,https://doi.org/10.1002/cpe.3505,Academia,,"FireWorks is a free, open-source code for defining, managing, and executing workflows. Complex workflows can be defined using Python, JSON, or YAML, are stored using MongoDB, and can be monitored through a built-in web interface. Workflow execution can be automated over arbitrary computing resources, including those that have a queueing system. FireWorks has been used to run millions of workflows encompassing tens of millions of CPU-hours across diverse application areas and in long-term production projects over the span of multiple years. An academic paper on FireWorks is also available. For details, see Citing FireWorks","large scale, workflow","workflow, high throughput",yes,
NGLESS,,NGS with less work,no,Active (2023-04-07),,Existing Workflow systems,https://ngless.readthedocs.io/en/latest/,https://github.com/ngless-toolkit/ngless,https://doi.org/10.1186/s40168-019-0684-8,Academia,,NGLess is a domain-specific language for NGS (next-generation sequencing data) processing,prprocessing FastQ files,genomic,yes,
pypipegraph,,,no,INactive ( 2021-07-12),,Existing Workflow systems,,https://github.com/IMTMarburg/pypipegraph,,Academia,https://www.imt.uni-marburg.de/,"pypipegraph: is an MIT-licensed library for constructing a workflow piece by piece and executing just the parts of it that need to be (re-)done. It supports using multiple cores (SMP) and (eventually, alpha code right now) machines (cluster) and is a hybrid between a dependency tracker (think 'make') and a cluster engine.","multicore, workflow pipelines",general purpose,yes,
Cromwell ,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://cromwell.readthedocs.io/en/stable/,https://github.com/broadinstitute/cromwell,,Academia,http://www.broadinstitute.org/,Cromwell is a Workflow Management System geared towards scientific workflows,"workflow managment system, Backends (grids, cloulds, local)",bionformatics,yes,
SUSHI,,,no,Active (2023-04-07),,Existing Workflow systems,,https://github.com/uzh/sushi,,Academia,http://www.uzh.ch/,"USHI can Support Users for SHell script Integration, defined as a recursive acronym as usual, but someone might say after using it, SUSHI is a Super Ultra Special Hyper Incredible system!! SUSHI is an agile and extensible data analysis framework that brings innovative concepts to Next Generation Sequencing bioinformatics. SUSHI lets bioinformaticians wrap open source tools like e.g. bowtie, STAR or GATK into apps and provides natively a commandline and a web interface to run these apps. Users can script their analysis as well as do an analysis-by-clicking in their browser. When running an SUSHI application SUSHI takes care of all aspects of documentation: Input data, parameters, software tools and versions are stored persistently. SUSHI accepts meta-information on samples and processing and lets the user define the meta-information she/he needs in a tabular format. Finally, all results, the associated logs, all parameters and all meta-information is stored in a single, self-contained directory that can be shared with collaborators. Altogether, SUSHI as a framework truly supports collaborative, reproducible data analysis and research. At the Functional Genomics Center, SUSHI is tightly integrated with B-Fabric(fgcz-bfabric.uzh.ch) and shares the authentication, sample information and the storage. The production version of SUSHI is currently being further integrated in the overall B-Fabric framework.",NGS; data anylsis,genomic,yes,
MIRC CTP,Clinical Trial Processor,,no,Inactive,,Existing Workflow systems,https://mircwiki.rsna.org/index.php?title=MIRC_CTP,,,Non-Profit,https://www.rsna.org/about,CTP is a stand-alone program that provides all the processing features of a MIRC site for clinical trials in a highly configurable and extensible application. It connects to FieldCenter applications and can also connect to MIRC sites when necessary,,"image processing, clinical trials",yes,
noodles,,,no,Active (2023-04-07),,Existing Workflow systems,https://nlesc.github.io/noodles/,https://github.com/NLeSC/noodles,,Non-Profit,https://www.esciencecenter.nl/,Noodles is a task-based parallel programming model in Python that offers the same intuitive interface when running complex workflows on your laptop or on large computer clusters.,"workflow, local, distributed",workflow engine,yes,
Swift,Swift & Swift/T,,no,Retired (2015),,Existing Workflow systems,http://swift-lang.org/main/index.php,,https://doi.org/10.1016/j.parco.2011.05.005,Academia,,"A simple tool for fast, easy scripting on big machines.","scripts, parallel, cluster, supercomputing",script deployment,yes,
Consonance,,runs SeqWare & CWL,maybe,Inactive (2018-06-27),,Existing Workflow systems,https://github.com/Consonance/consonance,https://github.com/Consonance/consonance,,Community?,,"Consonance is a cloud orchestration tool for running Docker-based tools and CWL/WDL workflows available at Dockstore on fleets of VMs running in clouds. It allows you to schedule a set of Dockstore job orders, spinning up the necessary VMs on AWS, Microsoft Azure, or OpenStack via the Youxia library for provisioning cloud-based VMs, and then tearing them down after all work is complete.","cloud orchestration, CWL",cloud orchestration,yes,
Dog,,,no,Inactive (2018-08-25),,Existing Workflow systems,,https://github.com/dogtools/dog,,Hackaton,https://slides.com/xsb/dog-dogfile,Dog is a command line application that executes automated tasks.,"yaml, task exeecution, CLI",task orchestration,yes,
Produce,,,maybe,Active (2023-04-07),,Existing Workflow systems,,https://github.com/texttheater/produce,,Community?,,"Produce is an incremental build system for the command line, like Make or redo, but different: it is scriptable in Python and it supports multiple variable parts in file names. This makes it ideal for doing things beyond compiling code, like setting up replicable scientific experiments.","replicable experiments, data processing","data processing, replicable processing",yes,
Loni pipeline,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://pipeline.loni.usc.edu/,-,http://dx.doi.org/10.1016/S1053-8119%2803%2900185-X,Academia,https://www.usc.edu/,"The LONI Pipeline workflow application includes features that allows users to easily describe their executables in a graphical user interface. Instead of manually managing intermediate data in a script, the LONI Pipeline handles the passing of data between programs for you. Once you’ve created a module for use in the LONI Pipeline, you can save it into your personal library and reuse it in other workflows you create by simply dragging and dropping it in.",,"GUI workflows, data processing, ruse pipelines",free,
cpipe,,,no,Inactive (2016-02-26),,Existing Workflow systems,,https://github.com/MelbourneGenomics/cpipe,,Alliance,https://www.melbournegenomics.org.au/about-us/who-we-are,Cpipe is a clinically focused exome sequencing pipeline developed by the Melbourne Genomics Health Alliance. Cpipe offers an industry standard variant calling pipeline with a suite of additional features needed by diagnostic laboratories added on top.,sequencing pipeline,genomic,yes,
AWE,,,maybe,Inactive,,Existing Workflow systems,https://mg-rast.github.io/AWE/,https://github.com/MG-RAST/AWE,https://doi.org/10.1109/BigData.2013.6691723,Academia,,A light weight workflow manager for scientific computing at scale,"CWL, cloud native, CWLProv, RESTFul API, containerization part of reproducible platform",,yes,
Skyport2,,,maybe,Inactive (2019),,Existing Workflow systems,,https://github.com/MG-RAST/Skyport2,,Academia,https://www.mg-rast.org/,"Skyport2 is a RESTful framework for large scale data management and reproducible multi-cloud workflow execution. Scientists and engineers are accustomed to using a different computer systems to accomplish scientific workflow execution. Skyport2 handles data management and execution of CWL workflows across. Data is stored in the RESTful SHOCK object store that handles indexing, subsetting and format conversions. SHOCK is programmable and can be customized to perform additional functions e.g. convert image formats. AWE worker nodes connect to the AWE resource manager and check execute workflows described in the common workflow language.","data managment, reproducible, multi-cloud workflow executuin",workflow execution,yes,
COMPs,(Py)COMPSs,COMP Superscaler,no,Active (2023-04-07),,Existing Workflow systems,https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/,https://github.com/bsc-wdc/compss,"https://doi.org/10.1007/s10723-013-9272-5 , https://doi.org/10.1016/j.softx.2015.10.004",Academia,https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/,"The COMP Superscalar (COMPSs) framework is mainly composed of a task-based programming model which aims to ease the development of parallel applications for distributed infrastructures, such as Clusters, Clouds and containerized platforms, and a runtime system that exploits the inherent parallelism of applications at execution time. The framework is complemented by a set of tools for facilitating the development, execution monitoring and post-mortem performance analysis.","big data, distributed computing, cloud, claster, container","parallel applicaiton, supercomputer",yes,
KLIKO,,,no,INactive (2017),,Existing Workflow systems,,https://github.com/gijzelaerr/kliko,https://doi.org/10.1016/j.ascom.2018.08.003,Academia,,"KLIKO is a specification, validator and parser for the KLIKO Scientific Compute Container specification. It enables a developer of scientific software to structure the input, output and parameters of a dockerized compute task. KLIKO is written in Python.","container, compute tasks",copute container specification,yes,
SoS Workflow,,,yes,Active (2023-04-07),,Existing Workflow systems,https://vatlab.github.io/sos-docs/,https://github.com/vatlab/SoS,https://doi.org/10.1371/journal.pcbi.1006843,Academia,https://vatlab.github.io/sos-docs/,"Script of Scripts (SoS) consists of SoS Notebook, a Jupyter-based polyglot notebook that allows the use of multiple Jupyter kernels in one notebook, and SoS Workflow, a workflow system for the execution of workflows in both process- and outcome-oriented styles. It is designed for data scientists and bioinformatics who routinely work with scripts in different languages such as bash, Python, R, and SAS. This repository contains the SoS Workflow workflow engine.","data analysis, workflow system, Jupyter Notebook, container",data science,yes,
XNAT Pipeline Engine,,,no,Active (2023-04-07),,Existing Workflow systems,https://www.xnat.org/,https://github.com/NrgXnat/xnat-pipeline-engine,https://doi.org/10.1385/NI:5:1:11,Academia,https://nrg.wustl.edu/,"XNAT is an open-source software platform developed to support imaging informatics research. The platform offers permission-controlled storage of imaging and clinical assessment data, as well as support for containerized data processing to support reproducible science. XNAT has been used in hundreds of research projects and clinical trials at institutions around the world.",pipeline,bioinformatics,yes,
Metapipe ,,,no,Inactive (2017),,Existing Workflow systems,,https://github.com/TorkamaniLab/metapipe,,Academia,https://www.scripps.edu/,"Metapipe is a simple command line tool for building and running complex analysis pipelines. If you use a PBS/Torque queue for cluster computing, or if you have complex batch processing that you want simplified, metapipe is the tool for you.","CLI, complex analysis pipeline, cluster computing",pipeline,yes,
OCCAM,,,yes,Active (2023-04-07),,Existing Workflow systems,https://occam.cs.pitt.edu/,https://occam.cs.pitt.edu/system/code,https://doi.org/10.1145/3214239.3214245,Academia,https://occam.cs.pitt.edu/publications,"Occam is a toolset for the preservation of rebuildable software and a self-hostable, federated web portal for composing repeatable workflows. Occam (If you like acronyms, you can say it means the Open Curation of Computation And Metadata) is a project that serve as the catalyst for the tools, education, and community-building needed to bring openness, accountability, comparability, and repeatability to software distribution and scientific digital exploration.","workflows, data association, repeatability, metadata, reproducible, code preservation","workflow, software, reproducible",yes,
Copernicus,,,no,Retired,404,Existing Workflow systems,http://www.copernicus-computing.org,,,,,,,,-,
iRODS Rule Language,,,no,Inactive (2014),,Existing Workflow systems,https://github.com/samuell/irods-cheatsheets/blob/master/irods-rule-lang-full-guide.md,,,Consortium,https://irods.org,,,,yes,
VisTrails,,,yes,Retired(2018),,Existing Workflow systems,https://www.vistrails.org,https://github.com/VisTrails/VisTrails,https://doi.org/10.1109/VISUAL.2005.1532788,Academia,,"VisTrails is an open-source data analysis and visualization tool. It provides a comprehensive provenance infrastructure that maintains detailed history information about the steps followed and data derived in the course of an exploratory task: VisTrails maintains provenance of data products, of the computational processes that derive these products and their executions.","provenance, workflow, data analysis",scientifix workflow managment system,yes,
Bionode,Bionode Watermill,,no,Inactive ,,Existing Workflow systems,,https://github.com/bionode/bionode-watermill,,open-source-community,https://www.bionode.io/,Bionode-watermill is a workflow engine that lets you assemble and run bioinformatic pipelines with ease and less overhead. Bionode-watermill pipelines are essentially node.js scripts in which tasks are the modules that will be assembled in the final pipeline using orchestrators.,"pipeline, workflow engine",bioinformatice,yes,
BIOVIA Pipeline Pilot Overview,,,no,Inactive,,Existing Workflow systems,http://hts.c2b2.columbia.edu/help/docs/ap_help/content/ap_help/intro/aep_overview.htm,,,Academia,,"The BIOVIA Pipeline Pilot Server is an application server that streamlines the integration and analysis of vast amounts of scientific data. Pipeline Pilot makes it easy to create scientific web services that can be used independently or as part of a company’s Service Oriented Architecture (SOA) strategy. Pipeline Pilot provides an agile development environment, fast and secure deployment, minimal maintenance costs, and application extensibility.",,,?,
DAGMan,Dagman A meta-scheduler for HTCondor,A meta-scheduler for HTCondor,no,Active (2023-04-07),,Existing Workflow systems,https://research.cs.wisc.edu/htcondor/dagman/dagman.html,,https://chtc.cs.wisc.edu/research.html,Academia,http://chtc.cs.wisc.edu/,DAGMan (Directed Acyclic Graph Manager) is a meta-scheduler for HTCondor. It manages dependencies between jobs at a higher level than the HTCondor Scheduler.,"workflow, HTCondor, dag",HTCondor,yes,
Unicore,,,no,Active (2023-04-07),,Existing Workflow systems,https://www.unicore.eu/docstore/workflow-7.6.0/workflow-manual.html#wf_dialect,https://github.com/UNICORE-EU/uftp,https://doi.org/10.1007/3-540-44681-8_116,Academia,https://www.unicore.eu/about-unicore/,"UNICORE is unique among federation middleware systems. The UNICORE 
design is based on several guiding principles, that serve as key 
objectives for further enhancements..The UNICORE Workflow System provides advanced workflow processing capabilities using UNICORE Grid resources. Its main components are the Workflow Engine and the Service Orchestrator. While the Workflow Engine provides high-level control constructs (for-each, while, if-then-else, etc), the Service Orchestrator contains a powerful, extensible resource broker, and deals with execution of single UNICORE jobs.","federation, HPC; workflow",super computing,yes,
Toil,,,maybe,Active (2023-04-07),,Existing Workflow systems,http://toil.ucsc-cgl.org/,https://github.com/DataBiosphere/toil,https://doi.org/10.1038/nbt.3772,Academia,http://toil.ucsc-cgl.org/,http://toil.ucsc-cgl.org/,"pipeline, cloud, scalable, CWL",genomic,yes,
Cylc,,,no,Active (2023-04-07),,Existing Workflow systems,https://cylc.github.io/,https://github.com/cylc/cylc-flow,https://doi.org/10.1109/MCSE.2019.2906593,Academia,https://cylc.github.io/,"Cylc is a general purpose workflow engine that also orchestrates cycling systems very efficiently. It is used in production weather, climate, and environmental forecasting on HPC, but is not specialized to those domains.","HPX, workflow",general purpose,yes,
Cloud Compute Cannon,Autodesk Cloud Compute Canon,,no,Inactive,,Existing Workflow systems,,https://github.com/Autodesk/cloud-compute-cannon,,commercial,https://www.autodesk.com/,"Cloud Compute Cannon (CCC) is a stack that provides an HTTP API for running arbitrary compute jobs that run in docker containers. Cloud Compute Cannon is designed to do one thing well: run docker-based 
compute jobs on any cloud provider (or your local machine) reliably, 
with a minimum or user intervention, and scale machines up and down as 
needed. Its feature set is purposefully limited, it is designed to be 
used standalone, or as a component in more complex tools, rather than be
 extended itself.","docker, local, cloud, REST, CLI",general purpose,yes,
Civet,,,no,Retired,404,Existing Workflow systems,,https://github.com/TheJacksonLaboratory/civet,,Non-Profit,https://www.jax.org/,,"CLI pipeline, batch, HPC",,yes,
cumulus,,,no,Inactive (2020-11-23),,Existing Workflow systems,,https://github.com/Kitware/cumulus,https://doi.org/10.1109/PyHPC.2016.005,commercial,https://www.kitware.com/,"The goal of the project is to provide a platform for developing HPC workflows. Cumulus enables running workflows on traditional or on-demand clusters. It provides the ability to create compute resources on cloud computing platforms such as AWS, and then to provision MPI clusters on top of those resources using Ansible.",HPC workflow,HPC workflow,yes,
HIVE,High-performance integrated virtual environment,High-performance integrated virtual environment,no,Active (2023-04-07),,Existing Workflow systems,https://hive.biochemistry.gwu.edu,,https://doi.org/10.1093/database/baw022,Academia,https://hive.biochemistry.gwu.edu/home,"HIVE is a cloud-based environment optimized for the storage and analysis of extra-large data, such as biomedical data, clinical data, next-generation sequencing (NGS) data, mass spectrometry files, and many others. HIVE provides secure web access for authorized users to deposit, retrieve, annotate and compute on Big Data, and to analyze the outcomes using web user interfaces.","storage, anaylsis, big data, ",science,yes,
Cloudgene,,,no,Active (2023-04-07),,Existing Workflow systems,http://www.cloudgene.io/,https://github.com/genepi/cloudgene,https://doi.org/10.1186/1471-2105-13-200,Academia,http://www.cloudgene.io/about,"Cloudgene is a framework to build Software As A Service (SaaS) platforms for data analysis pipelines. By connecting commandline programs, scripts or Hadoop applications to Cloudgene, a powerful web application can be created within minutes. Cloudgene supports the complete workflow including data transfer, program execution and data export.","SAASfor data analysis, PIpeline, build, integrate, deploy, provide, share",genomic,yes,
FASTR,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://fastr.readthedocs.io/en/stable/,https://gitlab.com/radiology/infrastructure/fastr,https://doi.org/10.3389/fict.2016.00015,Academia,,"FASTR is a framework that helps creating workflows of different tools. The workflows created in FASTR are automatically enhanced with flexible data input/output, execution options (local, cluster, etc) and solid provenance.","workflow using different tools, data input, data output, provenance",general purpose,yes,
biomake,,,no,Inactive ,,Existing Workflow systems,,https://github.com/evoldoers/biomake,https://doi.org/10.1093/bioinformatics/btx306,"Academia, Non-Profit",https://www.evoldoers.org/,"This is a make-like utility for managing builds (or analysis workflows) involving multiple dependent files. It supports most of the functionality of GNU Make, along with neat extensions like cluster-based job processing, multiple wildcards per target, MD5 checksums instead of timestamps, and declarative logic programming in Prolog.","make-like, workflows",make-like for analysis workflows,yes,
remake,,,no,Inactive ,,,,https://github.com/richfitz/remake,,Personal?,https://github.com/richfitz,"The idea here is to re-imagine a set of ideas from make but built for R. Rather than having a series of calls to different instances of R (as happens if you run make on R scripts), the idea is to define pieces of a pipeline within an R session. Rather than being language agnostic (like make must be), remake is unapologetically R focussed.","make-like, R ","make-like, workflow",yes,
Zenith,SciFloware,,maybe,Active (2023-04-07),,Existing Workflow systems,https://team.inria.fr/zenith/,,http://www-sop.inria.fr/members/Didier.Parigot/pmwiki/Scifloware/index.php/Main/HomePage,Academia,,"Data-intensive science such as agronomy, astronomy, biology and environmental science must deal with overwhelming amounts of experimental data produced through empirical observation and simulation. Such data must be processed (cleaned, transformed, analyzed) in all kinds of ways in order to draw new conclusions, prove scientific theories and produce knowledge. However, constant progress in scientific observational instruments (e.g. satellites, sensors, loT) and simulation tools (that foster in silico experimentation) creates a huge data overload. Scientific data is very complex, in particular because of heterogeneous methods used for producing data, the uncertainty of captured data, the inherently multi-scale nature (spatial scale, temporal scale) of many sciences and the growing use of imaging (e.g. molecular imaging), resulting in data with hundreds of attributes, dimensions or descriptors. Despite their variety, we can identify common features of scientific data: big data; manipulated through complex, distributed workflows; typically complex, e.g. multidimensional or graph-based; with uncertainty in the data values, e.g., to reflect data capture or observation; important metadata about experiments and their provenance; and mostly append-only (with rare updates).","The three main challenges of scientific data management can be summarized by: (1) scale (big data, big applications); (2) complexity (uncertain, multi-scale data with lots of dimensions), (3) heterogeneity (in particular, data semantics heterogeneity). They are also those of data science, with the goal of making sense of data by combining data management, machine learning, statistics and other disciplines.",scientific data managment,yes,
OpenAlea,,,no,Inactive,,Existing Workflow systems,https://openalea.readthedocs.io/en/latest/,https://github.com/openalea/openalea,https://doi.org/10.1145/2791347.2791365,Academia,,"OpenAlea is an open source project primarily aimed at the plant research community. It is a distributed collaborative effort to develop Python libraries and tools that address the needs of current and future works in Plant Architecture modeling. OpenAlea includes modules to analyse, visualize and model the functioning and growth of plant architectur",,plant research,yes,
COMBUSTI/O,,,no,Inactive,,Existing Workflow systems,,https://github.com/jarlebass/combustio,,Personal,https://github.com/jarlebass/combustio,Abstractions facilitating parallel execution of programs implementing common I/O patterns in a pipelined fashion as workflows in Spark,,spark,yes,
BioCloud,,,no,Retired (2020-01-04),,Existing Workflow systems,,https://github.com/ccwang002/biocloud-server-kai,https://doi.org/10.6342/NTU201601295,Academia,,The ultimate Django server to run NGS analysis pipeline,NGS analysis pipeline,genomic,yes,
Triana,,,no,Retired,404,Existing Workflow systems,http://www.trianacode.org/,,https://link.springer.com/chapter/10.1007/978-1-84628-757-2_20,Academia,,"Triana focuses on supporting services within multiple environments, such as peer-to-peer (P2P) and the Grid, by integrating with various types of middleware toolkits.",,workflow,?,
Kepler,,,no,Retired (2016),,Existing Workflow systems,https://kepler-project.org/,,https://doi.org/10.1002/cpe.994,Academia,,"The Kepler Project is dedicated to furthering and supporting the capabilities, use, and awareness of the free and open source, scientific workflow application, Kepler. Kepler is designed to help scien­tists, analysts, and computer programmers create, execute, and share models and analyses across a broad range of scientific and engineering disciplines. Kepler can operate on data stored in a variety of formats, locally and over the internet, and is an effective environment for integrating disparate software components, such as merging ""R"" scripts with compiled ""C"" code, or facilitating remote, distributed execution of models. Using Kepler's graphical user interface, users simply select and then connect pertinent analytical components and data sources to create a ""scientific workflow""—an executable representation of the steps required to generate results. The Kepler software helps users share and reuse data, workflows, and compo­nents developed by the scientific community to address common needs.",,scientific workflow,yes,
Anduril,,,no,Active (2023-04-07),,Existing Workflow systems,https://anduril.org/site/,https://bitbucket.org/anduril-dev/anduril/src/stable/,https://doi.org/10.1093/bioinformatics/btz133,Academia,,"Anduril is a workflow platform for analyzing large data sets. Anduril provides facilities for analyzing high-thoughput data in biomedical research, and the platform is fully extensible by third parties. Ready-made tools support data visualization, DNA/RNA/ChIP-sequencing, DNA/RNA microarrays, cytometry and image analysis.","big data, HPC,  workflow, container","workflow plaform, big data, genomics",yes,
dgsh,,,no,Active (2023-04-07),,Existing Workflow systems,https://www2.dmst.aueb.gr/dds/sw/dgsh/,https://github.com/dspinellis/dgsh,http://dx.doi.org/10.1109/TC.2017.2695447,Academia,,"The directed graph shell, dgsh, allows the expressive expression of efficient big data set and streams processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell allowing the specification of pipelines with non-linear scatter-gather operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughpu","Big data, streaming, unix, pipeline, multipocessing",shell,yes,
EDGE,EDGE bioinformatics,Empowering the Development of Genomics Expertise,no,Active (2023-04-07),,Existing Workflow systems,https://edge.readthedocs.io/en/latest/,https://github.com/LANL-Bioinformatics/EDGE,https://doi.org/10.1093/nar/gkw1027,Academia,,"EDGE is a highly adaptable bioinformatics platform that allows laboratories to quickly analyze and interpret genomic sequence data. The bioinformatics platform allows users to address a wide range of use cases including assay validation and the characterization of novel biological threats, clinical samples, and complex environmental samples. EDGE is designed to:",,genomic,yes,
Pachyderm,,,yes,Active (2023-04-07),,Existing Workflow systems,https://www.pachyderm.com/,https://github.com/pachyderm/pachyderm,,commercial,https://www.pachyderm.com/,"Pachyderm is cost-effective at scale, enabling data engineering teams to automate complex pipelines with sophisticated data transformations across any type of data. Our unique approach provides parallelized processing of multi-stage, language-agnostic pipelines with data versioning and data lineage tracking. Pachyderm delivers the ultimate CI/CD engine for data.","data pipeline, orchestration, versioning, data lineage","Data engineering, data-driven pipelines",partially,
digdag,,,maybe,Active (2023-04-07),,Existing Workflow systems,https://www.digdag.io/,https://github.com/treasure-data/digdag/,,enterprise,https://www.treasuredata.com/,"Simple, Open Source, Multi-Cloud Workflow Engine. Digdag is a simple tool that helps you to build, run, schedule, and 
monitor complex pipelines of tasks. It handles dependency resolution so 
that tasks run in series or in parallel.

Digdag replaces cron, facilitates IT operations automation, orchestrates
 data engineering tasks, coordinates machine learning pipelines, and 
more.",,"workflow engine, data engineering, pipelines, ML",yes,
Agua / Automated Genomics Utilities Agent,,,,Retired,404,Existing Workflow systems,http://aguadev.org,,,,,,,,,
BioDepot Workflow Builder,,,maybe,Active (2023-04-07),,Existing Workflow systems,,https://github.com/BioDepot/BioDepot-workflow-builder,https://doi.org/10.1016/j.cels.2019.08.007,Academia,http://bime.uw.edu/,"he BioDepot-workflow-builder (Bwb) can be used to build bioinformatics workflows by combining interchangeable and encapsulated widgets, allowing researchers to easily implement and test new algorithms and observe how the outputs differ. Widgets call Docker containers to execute software tools that could potentially be written in a different programming language, require different system configurations and/or be developed by different research groups.","docker, container, workflow, reproducibility",genomic,yes,
IMP,,Integrated Meta-omic Pipeline,maybe,Active (2023-04-07),,Existing Workflow systems,https://r3lab.uni.lu/web/imp/,https://git-r3lab.uni.lu/IMP/imp3,https://doi.org/10.1186/s13059-016-1116-8,Academia,,"IMP, the Integrated Meta-omic Pipeline is a reproducible and modular pipeline for large-scale standardized integrated analysis of coupled metagenomic and metatranscriptomic data. IMP incorporates robust read preprocessing, iterative co-assembly of metagenomic and metatranscriptomic data, analyses of microbial community structure and function as well as genomic signature-based visualizations. As an added functionality, IMP also performs either single-omic metagenomic or metatranscriptomic analyses.","preprocessing, quality control, visualization, workflows, reproducible",genomic,yes,
Butler,,,maybe,Inactive,,Existing Workflow systems,https://butler.readthedocs.io/en/latest/,https://github.com/yakneens/butler,https://doi.org/10.1038/s41587-019-0360-3,Academia,,"Butler is a collection of tools whose goal is to aid researchers in carrying out scientific analyses on a multitude of cloud computing platforms (AWS, Openstack, Google Compute Platform, Azure, and others). Butler is based on many other Open Source projects such as - Apache Airflow, Terraform, Saltstack, Grafana, InfluxDB, PostgreSQL, Celery, Elasticsearch, Consul, and others.","cloud, provisioning, configuration managment, workflow managment, operations management","big data, scientific analysis",yes,
adage / yadage,,,no,Active (2023-04-07),,Existing Workflow systems,https://yadage.readthedocs.io/en/latest/,https://github.com/yadage/adage,,Personal?,,"This is a small experimental package to see how one could describe workflows that are not completely known at definition time. Tasks should be runnable both in a multiprocessing pool, or using a number of celery workers or a IPython cluster.","experimentall, dynamic DAG, workflow","DAG, workflow",yes,
Hi-WAY,,Execution of Scientific Workflows on Hadoop YARN,no,Inactive,,Existing Workflow systems,,https://github.com/marcbux/Hi-WAY,https://doi.org/10.5441/002/edbt.2017.87,Academia,,"The Hi-WAY Workflow ApplicationMaster for YARN provides the means to execute arbitrary scientific workflows on top of Apache Hadoop 2 (YARN). In this context, scientific workflows are directed acyclic graphs (DAGs), in which nodes are black-box tasks (e.g. Bash scripts, Java programs, Python scripts, compiled C++ executables) processing unstructured data (arbitrary files). Edges in the graph represent data dependencies between the tasks. Hi-WAY uses Hadoop's distributed file system HDFS to store the workflow's input, output and intermediate data. Hi-WAY currently supports the workflow languages Cuneiform, Galaxy, and Pegasus DAX, yet can be easily extended to support other workflow languages. A number of general-purpose and more specialized schedulers are provided, which can take into account data locality when assigning tasks to machines to reduce data transfer times during workflow execution. When running workflows, Hi-WAY captures comprehensive provenance information, which can be stored as files in HDFS, as well as in a MySQL or Couchbase database. These provenance traces are evaluated by the scheduler for performance estimation and can be used to re-execute previous workflow runs. The ApplicationMaster has been tested to scale to more than 600 concurrent tasks and is fault-tolerant in that it is able to restart failed tasks. ","DAG, YARN, big data, galaxy, HDFS","YARN, workflow",yes,
OpenMOLE,,,no,Inactive,,Existing Workflow systems,,https://github.com/openmole/openmole,https://doi.org/10.1016/j.future.2013.05.003,Academia,,"It offers tools to run, explore, diagnose and optimize your numerical model, taking advantage of distributed computing environments. With OpenMOLE you can explore your already developed model, in any language (Java, Binary exe, NetLogo, R, SciLab, Python, C++...).","workflow, distributed computing, scalable","dscabale, distributed computing",yes,
Biopet,,,no,Inactive,,Existing Workflow systems,https://github.com/biopet/biopet,https://github.com/biopet/biopet,https://doi.org/10.1109/CCGRID.2017.59,Academia,,"Biopet (Bio Pipeline Execution Toolkit) is the main pipeline development framework of the LUMC Sequencing Analysis Support Core team. It contains our main pipelines and some of the command line tools we develop in-house. It is meant to be used in the main SHARK computing cluster. While usage outside of SHARK is technically possible, some adjustments may need to be made in order to do so.","pipeline development framework, CLI",genomic,yes,
Nephele,,,,Inactive,,Existing Workflow systems,https://nephele.niaid.nih.gov/,https://github.com/niaid/nephele2,https://doi.org/10.1093/bioinformatics/btx617,National-Institute,,"Open, accessible platform for microbial bioinformatics. Overview: Nephele is a cloud platform developed by a team of 
computational biologists and bioinformaticians that also perform 
metagenomics
  analysis in collaboration with researchers at NIAID. It was born from 
the need of making tools and pipelines available to those with
  limited computational resources or those lacking expertise in 
metagenomics pipeline development.
 Pre-process sequencing data, analyze, explore","pipeline, analysisi, exploration, pre-processing",bionformatics,yes,
TOPPAS,,,,URL not found,,Existing Workflow systems,http://www.OpenMS.de/TOPPAS.,,https://doi.org/10.1021/pr300187f,Academia,,TOPPAS: A Graphical Workflow Editor for the Analysis of High-Throughput Proteomics Data,GUI pipeline,proteomics,?,
SBPipe,,,no,Inactive,,Existing Workflow systems,https://sbpipe.readthedocs.io/en/latest/,https://github.com/pdp10/sbpipe,https://doi.org/10.1186/s12918-017-0423-3,Academia,,"SBpipe allows mathematical modellers to automatically repeat the tasks of model simulation and parameter estimation, and extract robustness information from these repeat sequences in a solid and consistent manner, facilitating model development and analysis. SBpipe can run models implemented in COPASI, Python or coded in any other programming language using Python as a wrapper module. Pipelines can run on multicore computers, Sun Grid Engine (SGE), Load Sharing Facility (LSF) clusters, or via Snakemake.","Pipelines, repeated modelling, analysis, multicore, grid engine, snamemake",,yes,
dray,,,no,Retired,Domain taken over,Existing Workflow systems,http://dray.it/,https://github.com/CenturyLinkLabs/dray,,Enterprise,https://www.lumen.com/en-us/about.html,"An engine for managing the execution of container-based workflows. Most common Docker use cases involve using containers for hosting long-running
services. These are things like a web application, database or message queue -- services
that are running continuously, waiting to service requests. Another interesting use case for Docker is to wrap short-lived, single-purpose tasks.
Perhaps it's a Ruby app that needs to be execute periodically or a set of bash scripts
that need to be executed in sequence. Much like the services described above, these things
can be wrapped in a Docker container to provide an isolated execution environment. The only
real difference is that the task containers exit when they've finished their work while the
service containers run until they are explicitly stopped.","container, docker, workflow",container-based workflow engine,yes,
GenomeVIP,,,,Inactive,,Existing Workflow systems,https://genomevip.readthedocs.io/,https://github.com/ding-lab/GenomeVIP,https://doi.org/10.1101/gr.211656.116,Academia,,GenomeVIP is a web platform for performing variant discovery and annotation on Amazon’s Web Service (AWS) cloud or on local high-performance computing clusters.,"AWS, HPC",genomic,,
GridSAM,,,no,Inactive ( 2015),,Existing Workflow systems,https://sourceforge.net/projects/gridsam/,,https://doi.org/10.1007/3-540-45644-9_17,Academia,,GridSAM provides a job submission open standards based web service for submitting computational jobs to many commonly used distributed resource management systems.,distributed computing,"ditributed computing, job submission",yes,
Roddy,,,no,Retired,"No new features will be implemented for Roddy! We will continue to fix bugs occurring with currently existing workflows. On the long run, existing workflows should be migrated to other workflow management systems.",Existing Workflow systems,https://roddy-documentation.readthedocs.io/en/latest/,https://github.com/TheRoddyWMS/Roddy,,National-Institute,https://www.dkfz.de,Roddy is a framework for development and management of workflows on a batch processing cluster. It has been developed at the German Cancer Research Center (DKFZ) in Heidelberg in the eilslabs group and is used by a number of in-house workflows such as the PanCancer Alignment Workflow and the ACEseq workflow. The development is now continued in the Omics IT and Data Management Core Facility (ODCF) at the DKFZ.,,workflow management system,yes,
SciFlo,,,,URL not found,404,Existing Workflow systems,,,,,,,,,,
GNU Guix Workflow Language,,,no,Active,,,https://www.guixwl.org/,https://git.savannah.gnu.org/cgit/gwl.git,https://link.springer.com/protocol/10.1007/978-1-4939-9074-0_24,Academia,,"The Guix Workflow Language (GWL) provides a scientific computing extension to GNU Guix's declarative language for package management for the declaration of scientific workflows. It combines the specification of work units and their relationship to one another with the reproducible software deployment facilities of the functional package manager GNU Guix. A GWL workflow will always run in a reproducible environment that GNU Guix automatically prepares. The GWL extends your Guix installation with a single new sub-command: guix workflow. In the GWL there are two concepts we need to know about: processes and workflows. We describe a computation (running a program, or evaluating a Scheme expression) using a process. A workflow describes how individual processes relate to each other (e.g. process B must run after process A, and process C must run before process A). GWL workflows are executable code. The workflow language is embedded in the powerful general purpose language Guile Scheme, so you can compute arbitrarily complex process and workflow definitions. The GWL supports a classic Lisp syntax as well as a Python-like syntax called Wisp.","workflows, reproducible environment, unix, HPC",scientific omputing,yes,
Porcupine,,,maybe,Retired,This app has been superseded by a web version and is thus no longer under active development,Existing Workflow systems,https://timvanmourik.github.io/Porcupine/,https://github.com/TimVanMourik/Porcupine,https://doi.org/10.1371/journal.pcbi.1006064,Academia,,,"pipeline, reproducible, docker, container","bioinformatics, neuroimaging",yes,
GiraffeTools,Giraffe Tools,,no,,from Porcubine,,https://giraffe.tools/,https://github.com/GiraffeTools/GiraffeTools,,Academia?,,GiraffeTools provides tools for interactive workflow development Currently the main tool is the Porcupine workflow editor. You can visually build a node graph that represents a workflow and read and write it from and to GitHub.,reproducible worklow experiments,workflow,yes,
Parsl,,,no,Active,,Existing Workflow systems,http://parsl-project.org/,https://github.com/Parsl/parsl,https://doi.org/10.1145/3307681.3325400,Academia,,"You can use Parsl just like Python's parallel executors but across multiple cores and nodes. However, the real power of Parsl is in expressing multi-step workflows of functions. Parsl lets you chain functions together and will launch each function as inputs and computing resources are available.","parallel programming, python, multicore, nodes, hpc, multistep, workflow",parallel programming,yes,
ECFLOW ,,,no,Active,,Existing Workflow systems,https://ecflow.readthedocs.io/en/latest/index.html,https://github.com/ecmwf/ecflow,https://www.ecmwf.int/en/elibrary/80527-ecflow,Research-Insitute,https://www.ecmwf.int/,"ecFlow (Workflow primarily for Meteorological Applications) is a client/server workflow package that enables users to run a large number of programs (with dependencies on each other and on time) in a controlled environment. It provides tolerance for hardware and software failures, combined with restart capabilities. It is used at ECMWF to run all our operational suites across a range of platforms.",,wheater forcast,yes,
ophidia,,,no,Active,,Existing Workflow systems,https://ophidia.cmcc.it/,https://github.com/OphidiaBigData/ophidia-analytics-framework,https://doi.org/10.1016/j.procs.2013.05.409,Academia,,"Ophidia is a CMCC Foundation research project addressing big data challenges for eScience. It provides support for data-intensive analysis exploiting advanced parallel computing techniques and smart data distribution methods. It exploits an array-based storage model and a hierarchical storage organisation to partition and distribute multidimensional scientific datasets over multiple nodes. The Ophidia analytics framework can be exploited in different scientific domains (e.g. Climate Change, Earth Sciences, Life Sciences) and with very heterogeneous sets of data","big data, data managment, parallel computing, data mining, data analytics",big data analytics framework,yes,
WebLicht,,,no,Retired,,Existing Workflow systems,https://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/Main_Page,,https://aclanthology.org/P10-4005/,Academia,,"WebLicht is an execution environment for automatic annotation of text corpora. Linguistic tools such as tokenizers, part of speech taggers, and parsers are encapsulated as web services, which can be combined by the user into custom processing chains. The resulting annotations can then be visualized in an appropriate way, such as in a table or tree format.",,"linguistics, text analytics",?,
Gate Cloud ,,,no,Active,,Existing Workflow systems,https://cloud.gate.ac.uk/,,https://doi.org/10.1098/rsta.2012.0071,Academia,https://gate.ac.uk/,"Welcome to GATE Cloud – the home of affordable text analytics solutions from the world-leading open source GATE platform. Collect and/or process documents and social media, using freemium pre-packaged annotation services, or scale out and run your own GATE pipeline on millions of documents. View and export the results in structured formats, or run your own private instance of our highly scalable GATE Mímir semantic search platform.",,text analytics,yes,
SCIPION,,,no,URL not found,404,Existing Workflow systems,,,,,,,,,,
ergatis,,,no,Inactive,,Existing Workflow systems,https://ergatis.sourceforge.net/,https://github.com/jorvis/ergatis,https://doi.org/10.1093/bioinformatics/btq167,Academia,https://www.igs.umaryland.edu/,"Ergatis is a web-based utility that is used to create, run, and monitor reusable computational analysis pipelines. It contains pre-built components for common bioinformatics analysis tasks. These components can be arranged graphically to form highly-configurable pipelines. Each analysis component supports multiple output formats, including the Bioinformatic Sequence Markup 
                Language (BSML). The current implementation includes support for data loading into project databases following the CHADO schema, a highly normalized, community-supported schema for storage of biological annotation data.","reusable computationhal pipelines, analysis tasks via GUI",bioinformatics,,
Tigr workflow,"TIGR ""Workflow""",,no,Retired (2007),,Existing Workflow systems,https://tigr-workflow.sourceforge.net/,,,Non-Profit-Research-Institute,https://en.wikipedia.org/wiki/J._Craig_Venter_Institute,"The Institute for Genomic Research (TIGR) has many process piplelines that need to be created, executed, and monitored on an on-going basis. Each pipeline may include multiple discrete process that can be executed either sequentially or in parallel. To reduce manual intervention, and streamline the process flow, TIGR's Annotation software team has designed a system called Workflow that can be used to build, run, and monitor such process pipelines or workflows.",workflow system,geomic,,
Archivematica,,,maybe,Active,,Existing Workflow systems,https://www.archivematica.org,https://github.com/artefactual/archivematica,https://www.academia.edu/download/30839470/PubDat_191968.pdf#page=145,Enterprise,https://www.artefactual.com/,Archivematica is a free and open-source digital preservation system that is designed to maintain long-term access to digital memory. Archivematica is packaged with the web-based content management system AtoM for access to your digital objects,,digital preservation system,yes,
Martian,,,no,Active,,Existing Workflow systems,https://martian-lang.org/about/,https://github.com/martian-lang/martian,,Enterprise,https://www.10xgenomics.com/,"Martian is a language and framework for developing and executing complex computational pipelines.
    Designed for simplicity: simple to learn, simple to develop, simple to run, and simple to debug.
    Allows combination of software components written in any language into a single pipeline.
    Powerful support for scaling computationally intensive pipelines from standalone machines, to clusters, to cloud.
    Rich support for managing the full lifecycle of complex, compute and data-intensive pipelines","hpc, pipeline, local, cluster",computational pipelines,yes,
BioMAJ,,,maybe,Active,,Existing Workflow systems,http://genouest.github.io/biomaj/,https://github.com/genouest/biomaj,https://doi.org/10.1093%2Fbioinformatics%2Fbtn325,Non-Profit-Research-Institute,https://www.genouest.org/,BioMAJ (BIOlogie Mise A Jour) is a workflow engine dedicated to data synchronization and processing. The Software automates the update cycle and the supervision of the locally mirrored databank repository.,"workflow, data synchronization, DAG, data processing",workflow engine,yes,
Conveyor,,,no,Retired ,,Existing Workflow systems,https://wwww.cebitec.uni-bielefeld.de/conveyor.cebitec.uni-bielefeld.de/,,https://doi.org/10.1093/bioinformatics/btr040,Academia,,,,bioinformatics,yes,
Biopipe ,,,no,URL not found,,Existing Workflow systems,,,,,,,,,yes,
Wildfire,,,no,Retired (2005),,Existing Workflow systems,http://wildfire.bii.a-star.edu.sg/,http://wildfire.bii.a-star.edu.sg/files/wildfire-2.0-src.zip,https://doi.org/10.1186/1471-2105-6-69,Academia,,"Wildfire is a graphical tool for building workflows. It comes preconfigured to use EMBOSS programs as components for Bioinformatics workflows, and is extensible to support other components. 

                                                        GEL is a general-purpose parallel scripting language for describing workflows. It features 
                                                        explicit parallel constructs which allow for efficient parallel execution on clusters, Grids 
                                                        and SMP machines. There is GEL support for Condor Grids, and SGE, PBS and LSF-based 
                                                        clusters. ",,bioinformatics,yes,
BioWBI,,,no,Retired (2004),,Existing Workflow systems,,,http://bioinformatics.hsanmartino.it/bits_library/library/00079.pdf,Academia,,,,bioinformatics,yes,
BioWMS,,,no,Retired (2004),,Existing Workflow systems,,,http://bioinformatics.hsanmartino.it/bits_library/library/00568.pdf,Academia,,,,bioinformatics,yes,
BioMoby,,,no,Retired (2008),,Existing Workflow systems,http://biomoby.open-bio.org/,,https://doi.org/10.1186/1471-2105-7-523,Academia,,,,bioinformatics,yes,
SIBIOS,,,no,Retired (2004),,Existing Workflow systems,,,https://ieeexplore.ieee.org/document/1309094,Academia,,,,bioinformatics,yes,
Ngsane,,,no,Inactive (2016),,Existing Workflow systems,https://github.com/BauerLab/ngsane,https://github.com/BauerLab/ngsane,https://doi.org/10.1093/bioinformatics/btu036,Academia,,NGSANE is a framework for advanced production informatics of Next Generation Sequencing libraries.,,bioinformatics,yes,
Pwrake,,,no,Inactive (2012),,Existing Workflow systems,,https://github.com/misshie/Workflows,https://doi.org/10.1186%2F1756-0500-4-331,Academia,,Pwrake for bioinfomatics workflows using GATK and Dindel,,,yes,
Nesoni,,,no,Retired (2015),"With the closure of the Victorian Bioinformatics Consortium, I anticipate little further development on Nesoni.",Existing Workflow systems,,https://github.com/Victorian-Bioinformatics-Consortium/nesoni,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=f5c76df5b5cb8f74e635cdc9ca3071183c892c48,Consortium,,"Nesoni is a high-throughput sequencing data analysis toolset, which the Victorian Bioinformatics Consortium developed to cope with the flood of Illumina, 454, and SOLiD data being produced.",NGS,"bioinformatics, genomics",yes,
Skam,,,no,Inactive (2005),,Existing Workflow systems,https://skam.sourceforge.net/skam-intro.html,,,,,,,make-like,yes,
TREVA,,Targeted REsequencing Virtual Appliance,no,Inactive (2012),,Existing Workflow systems,http://bioinformatics.petermac.org/treva/,,https://doi.org/10.1371/journal.pone.0095217,Academia,,"TREVA (Targeted REsequencing Virtual Appliance) is a user-friendly virtual appliance, containing complex bioinformatics pipelines that can be installed and setup with minimal efforts. TREVA pipelines support a series of analyses commonly required for targeted resequencing and whole exome sequencing data, including: single-nucleotide and insertion/deletion variant calling, copy number analysis, and cohort-based analyses such as pathway and significantly mutated genes analyses.",,"bioinformatics, genomics",yes,
Eugene,EGene,,no,Active,,Existing Workflow systems,http://eugene.toulouse.inra.fr./,https://github.com/tschiex/eugene,https://doi.org/10.1093/bioinformatics/btu366,Academia,,EuGene: integrative gene finder for eukaryotic and prokaryotic genomes,nextflow pipeline,genomics,yes,
WEP ,,,no,URL not found,,Existing Workflow systems,https://bioinformatics.cineca.it/wep/,,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-S7-S11,Academia,,WEP: a high-performance analysis pipeline for whole-exome data,NGS,genomics,yes,
Microbase,,,no,Inactive (2012),,Existing Workflow systems,https://ico2s.org/software/microbase.html,,https://bitbucket.org/keith_f/microbase/src/master/,Academia,https://ico2s.org/index.html,"As bioinformatics datasets grow ever larger, and analyses become increasingly complex, there is a need for data handling infrastructures to keep pace with developing technology. Large-scale bioinformatics analyses often require the use of multiple software tools, each of which may be computationally intensive. Microbase enables the construction and execution of complex analysis workflows across a cluster of machines. Workflows are not static entities and may be extended with new tools over time, without having to repeat any previously-completed computations. Microbase runs a low-overhead, symmetric compute client to utilise available Grid or Cloud compute resources. A cluster may be expanded or reduced elastically, simply by starting or stopping compute clients. Many bioinformatics analyses can be executed in an embarrassingly parallel fashion, and therefore exploit the inherent parallelism present in large-scale computing environments.",,bionformatics,yes,
esciencecentral,e-Science Central,,no,URL not found,,Existing Workflow systems,https://www.esciencecentral.org/,,https://doi.org/10.1098%2Frsta.2012.0085,Academia,,?,,,,
Cyrille2,,,no,Code not found,,Existing Workflow systems,,,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-96,Academia,,"High-throughput bioinformatics with the Cyrille2 pipeline system. We have developed a generic pipeline system called Cyrille2. The system 
is modular in design and consists of three functionally distinct parts: 
1) a web based, graphical user interface (GUI) that enables a pipeline operator to manage the system; 2) the Scheduler,
 which forms the functional core of the system and which tracks what 
data enters the system and determines what jobs must be scheduled for 
execution, and; 3) the Executor, which searches for scheduled jobs and executes these on a compute cluster.","High-throughput, pipeline, GUI",bioinformatics,yes,
PaPy,,,no,Inactive,,Existing Workflow systems,,https://github.com/mcieslik-mctp/papy,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3051902/,Academia,,"To enable the flexible creation and execution of bioinformatics dataflows, we have written a modular framework for parallel pipelines in Python ('PaPy'). A PaPy workflow is created from re-usable components connected by data-pipes into a directed acyclic graph, which together define nested higher-order map functions. The successive functional transformations of input data are evaluated on flexibly pooled compute resources, either local or remote. Input items are processed in batches of adjustable size, all flowing one to tune the trade-off between parallelism and lazy-evaluation (memory consumption). An add-on module ('NuBio') facilitates the creation of bioinformatics workflows by providing domain specific data-containers (e.g., for biomolecular sequences, alignments, structures) and functionality (e.g., to parse/write standard file formats)","DAG, parallel, distributed, pipeline, flow-based, PYt",bioinformatics,yes,
Jobcenter,,,no,Inactive,,Existing Workflow systems,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3494518/,https://github.com/yeastrc/jobcenter,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3494518/,Academia,,"A client-server application and framework for job management and distributed job execution. JobCenter is a client-server application and framework for job 
management and distributed job execution. The client and server 
components are both written in Java and are cross-platform and 
relatively easy to install. All communication with the server is 
client-driven, which allows worker nodes to run anywhere (even behind 
external firewalls or “in the cloud”) and provides inherent load 
balancing. Adding a worker node to the worker pool is as simple as 
dropping the JobCenter client files onto any computer and performing 
basic configuration, which provides tremendous ease-of-use, flexibility,
 and limitless horizontal scalability. Each worker installation may be 
independently configured, including the types of jobs it is able to run.
 Executed jobs may be written in any language and may include multistep 
workflows.",,distributed job execution,yes,
CoreFlow,,,,Inactive,Demo still running at http://coreflow.mshri.on.ca,Existing Workflow systems,https://pubmed.ncbi.nlm.nih.gov/24503186/,https://github.com/pasculescu/CoreFlow,https://pubmed.ncbi.nlm.nih.gov/24503186/,Academia,,"CoreFlow: a computational platform for integration, analysis and modeling of complex biological data",Computational pipeline; Data analysis; Mass spectrometry; Statistical analysis; Workflow.,bioinformatics,,
dynamic-pipeline,,,,Inactive,,Existing Workflow systems,https://code.google.com/archive/p/dynamic-pipeline/,https://github.com/mairarodrigues/dynamic-pipeline,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-163,Academia,,"The dynamic pipeline project is a graph-based approach towards extensible pipelines. It allows you to compose pipelines on-the-fly, depending on the required functionality. In this way, you don't need to create one specific pipeline for each task that you want to execute. To use the dynamic pipeline system you need only to download the project package and instantiate the Tool Registry with your own tools.","DAG, pipeline",general-purpose pipeline,,
XiP,,,no,Code not found,,Existing Workflow systems,http://xip.hgc.jp/,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530915/,Academia,,"XiP (eXtensible integrative Pipeline) is a flexible, editable and modular environment with a user-friendly interface that does not require previous advanced programming skills to run, construct and edit workflows. XiP allows the construction of workflows by linking components written in both R and Java, the analysis of high-throughput data in grid engine systems and also the development of customized pipelines that can be encapsulated in a package and distributed. XiP already comes with several ready-to-use pipeline flows for the most common genomic and transcriptomic analysis and ∼300 computational components",,"workflow, bioinformatics",yes,
Eoulsan,,,maybe,Active,,Existing Workflow systems,https://www.outils.genomique.biologie.ens.fr/eoulsan/,https://github.com/GenomiqueENS/eoulsan,https://pubmed.ncbi.nlm.nih.gov/22492314/,Academia,,"Eoulsan is a versatile framework based on the Hadoop implementation of the MapReduce algorithm, dedicated to high throughput sequencing data analysis on distributed computers. With Eoulsan, users can easily set up a cloud computing cluster and automate the analysis of several samples at once using various software solutions available. Working either on standalone workstations or cloud computing clusters, Eoulsan provides an integrated and flexible solution for RNA-Seq data analysis of differential expression.","RNA-seq workflow, docker, container","Wokflow engine, NGS, genomics",yes,
CloudDOE,,,no,Inactive,,Existing Workflow systems,http://clouddoe.iis.sinica.edu.tw/,https://github.com/CSCLabTW/CloudDOE,https://doi.org/10.1371/journal.pone.0098146,Academia,,"CloudDOE is a user friendly software package to deploy, operate and extend a
MapReduce-based bioinformatics environment, which is collectively denoted as a
CloudDOE Cloud. A CloudDOE Cloud consists of a Hadoop MapReduce computing
framework and specific bioinformatics data analysis tools, e.g., CloudBurst,
CloudBrush, and CloudRS.","mapreduce, deploy, operate, hadoop","Big Data, mapreduce, bioinformatics",yes,
BioPig,,,no,Inactive,,Existing Workflow systems,,https://github.com/JGI-Bioinformatics/biopig,https://pubmed.ncbi.nlm.nih.gov/24021384/,Academia,,"This project provides additional apache pig commands and data loaders/writers specific
for biologicial sequence data.  Specifically, biopig supports the following functionality:
  - load fasta and fastq files
  - filter sequence data by id, header or size
  - generate kmers
  - wrappers for common external programs (Blast, Newbler, velvet, cap3, etc)

With biopig, users can write data parallel analysis tools that get executed on a hadoop
map/reduce cluster such as NERSC's Magellan or AWS's Elastic MapReduce.","mapreduce, deploy, operate, hadoop","Big Data, map reduce, bioinformatics",yes,
SeqPig,,,no,Inactive,,Existing Workflow systems,,https://github.com/HadoopGenomics/SeqPig,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3866557/,Academia,,"SeqPig is a library of import and export functions for file formats
commonly used in bioinformatics for Apache Pig. Additionally, it
provides a collection of Pig user-defined functions (UDF's) that allow
for processing of aligned and unaligned sequence data. Currently
SeqPig supports BAM/SAM, FastQ and Qseq input and output and FASTA
input. It is built on top of the Hadoop-BAM library. Fore more
information see","Apache Pig, Hadoop","Big Date, bioinformatics",yes,
Zymake,,,no,Inactive,,Existing Workflow systems,http://www-personal.umich.edu/~ebreck/code/zymake/,https://github.com/samuell/zymake-mirror,https://dl.acm.org/doi/10.5555/1622110.1622113,Academia,,"zymake is a high-level language for running complex sets of experiments. The user writes a zymakefile, mostly consisting of parameterized shell commands, and zymake determines the dependency structure and executes the commands in the appropriate order.
make-like semantics, shell-like syntaxexecution order follows dependencies, files are rebuilt only if older than
   files on which they depend, etc.all filenames are inferred by the system a file is determined by a set of key-value pairs, such as ""method=svm""
    or ""number-of-hidden-units=10""simple interpolation syntax: everything apart from whitespace 
  and $(...) is passed untouched to the shell for execution.parallel executio","make, shell, workflow, dependencies, parallel execution","make-like, shell-live",yes,
JMS,,,no,,,Existing Workflow systems,,https://github.com/RUBi-ZA/JMS,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0134273,Academia,,"JMS is a workflow management system and web-based cluster 
front-end for High Performance Computing (HPC) environments. It provides
 an interface to Torque (or similar resource managers) that allows users
 to submit and manage jobs as well as manage, configure and monitor the 
status of their cluster.
In addition to interfacing with a resource manager, JMS 
provides a fully-functional workflow management system that allows users
 to create complex computational pipelines via an easy-to-use, web 
interface. Users can upload their scripts, interface with installed 
programs on their cluster, or both, to build their workflows.
JMS was originally developed for use in the field of 
bioinformatics. It is, however, applicable to any scientific field that 
requires computationally intensive analysis to be performed over a 
cluster. It can also be used to integrate workflows into 3rd party 
websites via it's RESTful web API. JMS is is also a useful tool for 
system administrators who simply want to monitor and manage their 
cluster.","HPC, wfms",workflow managment system,yes,
QIAGEN CLC Genomics Workbench,CLC Genomics Workbench,,no,,,Existing Workflow systems,https://digitalinsights.qiagen.com/products-overview/discovery-insights-portfolio/analysis-and-visualization/qiagen-clc-genomics-workbench/,,,Enterprise,,,"NGS, workflow, data analysis","genomic, NGS",no,
NG6,,,no,Inactive,"Source code not available, homepage lst update in 2018",Existing Workflow systems,https://ng6.toulouse.inra.fr/,,https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-13-462,Academia,,"We describe a user-friendly information system able to manage large sets of sequencing data. It includes, on one hand, a workflow environment already containing pipelines adapted to different input formats (sff, fasta, fastq and qseq), different sequencers (Roche 454, Illumina HiSeq) and various analyses (quality control, assembly, alignment, diversity studies,…) and, on the other hand, a secured web site giving access to the results. The connected user will be able to download raw and processed data and browse through the analysis result statistics. The provided workflows can easily be modified or extended and new ones can be added. Ergatis is used as a workflow building, running and monitoring system. The analyses can be run locally or in a cluster environment using Sun Grid Engine.","NGS, local, cluster, workflow","genomic, NGS",yes,
VIBE,,,no,Inactive,last release from hompage dates back to 2015,Existing Workflow systems,http://www.incogen.com/vibe,,,Enterprise,http://www.incogen.com/about,"
The INCOGEN Visual Integrated Bioinformatics Environment (VIBE) is a state-of-the-art, drag-and-drop analysis workflow management environment. INCOGEN has been recognized as a global leader in the field of workflow management for years and VIBE represents the premier software application in this area.The VIBE system can interface with a variety of environments and high throughput platforms. The rich visualization and data mining environments in combination with the sophisticated server architecture offer the life science researcher a powerful system for data analysis, mining and knowledge discovery. The new VIBE Software Development Kit (SDK) combines the rich and powerful VIBE environment with user-level extensibility. A mass spec case study and white paper are available.","GUI, visual workflow, visualitation, ","workflow management system, bioinformatics",no,
WDL,,Workflow Description Language,yes,Active,,Existing Workflow systems,https://openwdl.org/,https://github.com/openwdl/wdl,,Academia,https://openwdl.org/,"The Workflow Description Language (WDL) is a way to 
specify data processing workflows with a human-readable and writeable 
syntax. WDL makes it straightforward to define complex analysis tasks, 
chain them together in workflows, and parallelize their execution. The 
language makes common patterns simple to express, while also admitting 
uncommon or complicated behavior; and strives to achieve portability not
 only across execution platforms, but also different types of users. 
Whether one is an analyst, a programmer, an operator of a production 
system, or any other sort of user, WDL should be accessible and 
understandable.","workflow, clou,d reproducible,openwdl",data processing workflows,yes,
SciFlow,,,no,,,Existing Workflow systems,,https://github.com/kaizhang/SciFlow,,"Personal,Academia",,"SciFlow is a DSL for building type-safe computational workflows.
SciFlow is implemented based on the Free Arrow and is heavily inspired by the funflow package.
The differences between SciFlow and funflow are:

SciFlow uses Template Haskell to ease the process of 
workflow specification and to allow composition and reuse of defined 
workflows.

SciFlow supports distributed computing thanks to Cloud Haskell!","distributed computing, workflow-managment, cloud, DSLworkflows",sientific workflow managment,yes,
Bioshake,,,no,,,Existing Workflow systems,https://papenfusslab.github.io/bioshake,https://github.com/PapenfussLab/bioshake,https://peerj.com/articles/7223/,Academia,https://papenfusslab.org/,Bioshake is a bioinformatics workflow tool extending the Shake build system.,,bioinformatics workflow,yes,
SciPipe,,,,Active,,Existing Workflow systems,https://scipipe.org/,https://github.com/scipipe/scipipe,https://dx.doi.org/10.1093/gigascience/giz044,Academia,,"When you need to run many commandline programs that depend on each other in complex ways, SciPipe helps by making the process of running these programs flexible, robust and reproducible. SciPipe also lets you restart an interrupted run without over-writing already produced output and produces an audit report of what was run, among many other things.","Flow-based, reproducible, cLI, scripts",scientific workflow,yes,
Kapacitor,Kapacitor / TICKscripts,,partially,Active,,Existing Workflow systems,https://docs.influxdata.com/kapacitor/v1.6/introduction/getting-started/,https://github.com/influxdata/kapacitor,,Enterprise,https://www.influxdata.com/,"Use Kapacitor to import (stream or batch) time series data, and then 
transform, analyze, and act on the data. To get started using Kapacitor,
 use Telegraf to collect system metrics on your local machine and store 
them in InfluxDB. Then, use Kapacitor to process your system data.","time series data, import, transform, analyze, InfluxDB",time series data processing,yes,
AiiDA,,Automated Interactive Infrastructure and Database for Computational Science,yes,Active,,Existing Workflow systems,https://www.aiida.net/,https://github.com/aiidateam/aiida-core,https://doi.org/10.1016/j.commatsci.2015.09.013,Academia,,"AiiDA (www.aiida.net) is a workflow manager for computational science with a strong focus on provenance, performance and extensibility.","Workflow, data provenance, HPC interface, Plugin interface, open science",workflow manager for computational science,yes,
reflow,,,yes,Active,,Existing Workflow systems,,https://github.com/grailbio/reflow,,Enterprise,https://grail.com/,"Reflow is a system for incremental data processing in the cloud. Reflow enables scientists and engineers to compose existing tools (packaged in Docker images) using ordinary programming constructs. Reflow then evaluates these programs in a cloud environment, transparently parallelizing work and memoizing results. Reflow was created at GRAIL to manage our NGS (next generation sequencing) bioinformatics workloads on AWS, but has also been used for many other applications, including model training and ad-hoc data analyses.","pipeline, data-science, data processing",data processing ,yes,
Resolwe,,,no,Active,,Existing Workflow systems,,https://github.com/genialis/resolwe,,Enterprise,https://www.genialis.com/,"Resolwe is an open source dataflow package for Django framework. We envision
Resolwe to follow the Common Workflow Language specification, but the
current implementation does not yet fully support it. Resolwe offers a complete
RESTful API to connect with external resources. A collection of bioinformatics
pipelines is available in Resolwe Bioinformatics.",CWL,dataflow engine,yes,
Yahoo! Pipes,,,no,Retired,,Existing Workflow systems,https://en.wikipedia.org/wiki/Yahoo!_Pipes,,,Enterprise,https://yahoo.com,,"web application, GUI, data mashups",data mashup apps,no,
walrus,,,no,Active,,Existing Workflow systems,,https://github.com/fjukstad/walrus,,Personal,,"walrus is a small tool for executing data analysis pipelines using Docker
containers. It is very simple: walrus reads a pipeline description from either a
JSON or YAML file and starts Docker containers as described in this file. We
have used walrus to develop analysis pipelines for analyzing whome-exome as well
as RNA sequencing datasets.","docker, pipeline, reprouduciblity",data analysis pipeline,yes,
Apache Beam,,,yes,Active,,Existing Workflow systems,https://beam.apache.org/,https://github.com/apache/beam,,Enterprise,https://opensource.google/projects/apachebeam,"Apache Beam is a unified model for defining both batch and streaming data-parallel processing pipelines, as well as a set of language-specific SDKs for constructing pipelines and Runners for executing them on distributed processing backends, including Apache Flink, Apache Spark, Google Cloud Dataflow, and Hazelcast Jet.","big data, pipeline, stream, batch",data processing,yes,
Closha,,,no,retired,Platform URL: Bad Gateeways,Existing Workflow systems,https://closha.kobic.re.kr/,,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2019-3,Academia,,"a cloud-based workflow management system, Closha, to provide fast and cost-effective analysis of massive genomic data. We implemented complex workflows making optimal use of high-performance computing clusters. Closha allows users to create multi-step analyses using drag and drop functionality and to modify the parameters of pipeline tools. Users can also import the Galaxy pipelines into Closha. Closha is a hybrid system that enables users to use both analysis programs providing traditional tools and MapReduce-based big data analysis programs simultaneously in a single pipeline.",cloud-based workflow management system,"genomic, NGS",?,
WopMars,,Workflow Python Manager for Reproducible Science,maybe,Active,,Existing Workflow systems,wopmars.readthedocs.org,https://github.com/aitgon/wopmars,,Personal,,WopMars is a database-driven workflow manager written in python similar to GNU Makefile or Snakemake. The difference is that the definition file of WopMars takes into account input/output SQLITE table defined as python paths to SQLAlchemy model,"workflow management, database, reproducible, science",workflow manager for computational science,yes,
flowing-clj,,,no,Inactive,,Existing Workflow systems,,https://github.com/stain/flowing-clj,,Personal,,A Clojure library for building data-driven workflows (dataflows).,workflow,workflows,yes,
Plumbing,Plumbing and Graph,,no,Active,,Existing Workflow systems,,https://github.com/plumatic/plumbing,,Community?,,"Graph is a simple and declarative way to specify a structured computation, which is easy to analyze, change, compose, and monitor. Here's a simple example of an ordinary function definition, and its Graph equivalent:",,graph,yes,
Labview,,commercial,no,Active,,Existing Workflow systems,https://www.ni.com/en-us/shop/labview.html,,,Enterprise,,"LabVIEW is a graphical programming environment engineers use to develop automated research, validation, and production test systems.",,,no,
MyOpenLab,,,,Retired,"Certificate exipered, spanish",Existing Workflow systems,https://myopenlab.org/inicio/,,,,,,,,no,
Max,Max/MSP,commercial,,Active,,Existing Workflow systems,https://cycling74.com/products/max,,,Enterprise,,Music PIpeline Studio: Max is an infinitely flexible space to create your own interactive software.,,music,no,
NoFlo,,,no,Active,,Existing Workflow systems,https://noflojs.org/,https://github.com/noflo/noflo,,Kickstarter,https://noflojs.org/kickstarter/,Flow-Based Programming for JavaScript,"JavaScript, flow-based programming","JavaScript, flow-based programming",yes,
Flowstone,,commercial,no,Active,,Existing Workflow systems,http://www.dsprobotics.com/flowstone.html,,,Enterprise,,FlowStone uses a combination of graphical and text based programming. Applications are programmed by linking together functional building blocks called components. Events and data then flow between the links as the application executes.,"home automation, camera, devices",robotics,no,
HyperLoom,,,no,Inactive (2018),,Existing Workflow systems,https://loom-it4i.readthedocs.io/en/latest/intro.html,https://code.it4i.cz/ADAS/loom,https://doi.org/10.1145/3183767.3183768,Academia,,"HyperLoom is a platform for defining and executing workflow pipelines in a
distributed environment. HyperLoom aims to be a highly scalable framework
that is able to efficiently execute millions of interconnected tasks on hundreds of computational nodes.","distributed computing, DAG","workflow pipelines, distributed computing",yes,
Dask,,,no,Active,,Existing Workflow systems,https://github.com/dask/dask,https://github.com/dask/dask,,Personal,https://en.wikipedia.org/wiki/Dask_(software),"Python has grown to become the dominant language both in data analytics and general programming. This growth has been fueled by computational libraries like NumPy, pandas, and scikit-learn. However, these packages weren’t designed to scale beyond a single machine. Dask was developed to natively scale these packages and the surrounding ecosystem to multi-core machines and distributed clusters when datasets exceed memory.","Python, scale, parallel, deploy, local, gloud",data analytics,yes,
Stimela,,,no,Active,,Existing Workflow systems,,https://github.com/ratt-ru/Stimela,https://www.semanticscholar.org/paper/Advanced-radio-interferometric-simulation-and-data-Makhathini/cf1f524c876730e6037682cc23e94c3d9d096171,Academia,,A containerized radio interferometry scripting framework,container,astronomy,yes,
JTracker,,,no,Inactive,Certificate of platform expired,Existing Workflow systems,https://jtracker.io/,https://github.com/jtracker-io/jt-cli,,,,"JTracker is a scientific workflow management system. It provides workflow authoring, sharing and execution with full provenance tracking. JTracker system is designed as client-server architecture for distributed compute environments. All jobs are centrally managed by a JTracker server, JTracker executors (the clients) request jobs/tasks from the server and execute them on compute nodes the executors reside.","scienific workflow managment system, authoring, sharing, provenance",scientific workflow management system,yes,
PipelineDog,,,no,Inactive,,Existing Workflow systems,http://pipeline.dog/,https://github.com/zhouanbo/pipelinedog-web,https://doi.org/10.1093/bioinformatics/btx759,Academia,,,"GUI pipeline, expressions, YAML",scientific pipelines,yes,
DALiuGE,,,no,Active,,Existing Workflow systems,https://daliuge.readthedocs.io/en/latest/,https://github.com/ICRAR/daliuge,https://arxiv.org/abs/1702.07617,Academia,,"DALiuGE is a workflow graph development, management and execution framework, specifically designed to support very large scale processing graphs for the reduction of interferometric radio astronomy data sets. DALiuGE has already been used for processing large astronomical datasets in existing radio astronomy projects. It originated from a prototyping activity as part of the SDP Consortium called Data Flow Management System (DFMS). DFMS aimed to prototype the execution framework of the proposed SDP architecture.","workflow graph development, big data",astronomy,yes,
Overseer,,,no,Inactive,,Existing Workflow systems,,https://github.com/framed-data/overseer,,Enterprise,http://framed.io/,"Overseer is a library for building and running data pipelines in Clojure. It allows for defining workflows as a graph (DAG) of dependent tasks, and handles scheduling, execution, and failure handling among a pool of workers.","clojure, pipeline","data pipeline, clojure",yes,
Squonk,,Commercial,no,Active,,Existing Workflow systems,https://squonk.it/,https://github.com/InformaticsMatters/squonk,,Enterprise,https://www.informaticsmatters.com/,"The Squonk Data
 Manager is replacing our original Computational Notebook product. The 
Data Manager provides a novel, easy to use, web based, data centric 
workflow environment in which scientists can execute scientific 
workflows using open source and commercial tools from multiple sources 
such as RDKit, Chemistry Development Kit (CDK), ChemAxon.   It 
provides a scaleable execution environment where you work securely 
within a project team, collaborating on cheminformatics and 
computational chemistry workflows such as virtual screening and fragment
 screening.   You can use the Squonk Data Manager in our 
evaluation site or we can deploy it to a Kubernetes cluster dedicated 
for you own use, such as on AWS.  ",data managment,Drug Discovery,no,
GC3Pie,,,no,Inactive,,Existing Workflow systems,https://gc3pie.readthedocs.io/en/master/,https://github.com/gc3pie/gc3pie,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=928fe23e04ce69e15d2b6f854b96eabaace91a3e,Academia,,"GC3Pie is a python package for executing computational workflows consisting of tasks with complex inter-dependencies. GC3Pie accomplishes this by defining a run-time task list in which a task can only be executed once all upstream task dependencies have been successfullly completed. In contrast to other workflow managers, GC3Pie accplications are written in python and not a markup language. The advantage is that this makes it trivial to write highly complex workflows. GC3Pies roots are in shared-nothing achitectures (server-less for example) and can be configured to use backends such as batch clusters or clouds.","pipelines, grids, cluster, python",computational workflows,yes,
Fractalide,,,maybe,Inactive,,Existing Workflow systems,,https://github.com/fractalide/fractalide,,Enterprise,https://fractalide.com/,"ractalide is a free and open source service programming platform using dataflow graphs. Graph nodes represent computations, while graph edges represent typed data (may also describe tensors) communicated between them. This flexible architecture can be applied to many different computation problems, initially the focus will be Microservices to be expanded out into the Internet of Things.","reproducible, DAG, dataflow,",reproducible workflows,yes,
TOGGLe,,,no,Inactive,Project URL inactive,Existing Workflow systems,"http://toggle.southgreen.fr/,",,https://www.biorxiv.org/content/10.1101/245480v2,Academia,,"Here, we present the new version of TOGGLe (Toolbox for Generic NGS Analyses),
 a simple and highly flexible framework to easily and quickly generate 
pipelines for large-scale second- and third-generation sequencing 
analyses, including multi-sample and multi-threading support. TOGGLe is a
 workflow manager designed to be as effortless as possible to use for 
biologists, so the focus can remain on the analyses. Pipelines are 
easily customizable and supported analyses are reproducible and 
shareable","workflow, NGS, large-scale, pipelines, workflow manager","genomics, NGS",yes,
Askalon,,,no,Retired (2013),Last update 2013-01-10,Existing Workflow systems,https://askalon.org/,,http://www.infosys.tuwien.ac.at/staff/truong/publications/askalon-cpe.pdf,Academia,,"The goal of ASKALON is to simplify the development and optimization of applications that can harness the power of Cloud computing. The ASKALON project crafts a novel environment based on new innovative tools, services, and methodologies to make scientific application development and optimization for real applications and execution on Cloud environments an everyday practice.",,,yes,
Eclipse ICE ,,The Integrated Computational Environment,,Inactive (2017),,Existing Workflow systems,https://www.eclipse.org/ice/,https://gitlab.eclipse.org/eclipse/ice/ice,,Community,,"The Eclipse Integrated Computational Environment (ICE) is
                                                                        a scientific workbench and workflow environment developed to
                                                                        improve the user experience for computational scientists. ICE
                                                                        makes it possible for developers to deploy rich, graphical,
                                                                        interactive capabilities for their science codes and software, 
                                                                        and integrate many different scientific computing technologies 
                                                                        in one common, cross-platform user environment.",,"define problems, run simulations, analyze putputs, arhiece data",,
SAW,Sandia Analysis Workbench,Sandia Analysis Workbench,no,?,"SAW only available to partners, some parts open source, ",Existing Workflow systems,https://www.sandia.gov/saw,,https://www.sandia.gov/app/uploads/sites/95/2021/08/NAFEMS_2015.pdf,"Enterprise, Government",https://www.sandia.gov/about/,"The Sandia Analysis Workbench (SAW) is a family of software applications
 that boost productivity and quality by making modeling and simulation 
easier while enforcing best practices and supporting ubiquitous V&V.
 Capabilities include workflow management, model building, job 
submission, requirements tracking, and simulation data management. The 
Workbench provides a user-friendly graphical interface to Sierra, CTH, 
DAKOTA, and other important Sandia codes.",,analysis workbench,partially,
dispel4py,,,no,INactive,,Existing Workflow systems,,https://github.com/dispel4py/dispel4py,https://doi.org/10.1177/109434201664976,Academia,,"dispel4py is a free and open-source Python library for describing abstract stream-based workflows for distributed data-intensive applications. It enables users to focus on their scientific methods, avoiding distracting details and retaining flexibility over the computing infrastructure they use. It delivers mappings to diverse computing infrastructures, including cloud technologies, HPC architectures and specialised data-intensive machines, to move seamlessly into production with large-scale data loads. The dispel4py system maps workflows dynamically onto multiple enactment systems, such as MPI, STORM and Multiprocessing, without users having to modify their workflows.","abstract workflows, data-intensive, HPC, odcker",,yes,
Jobber,,,,Inactive (2015),,Existing Workflow systems,https://pypi.org/project/Jobber/0.1.4/,,,,,"Jobber is a program that executes pipelines. It manages job dependencies, restarts them if necessary and logs their status in a database. Jobs are persisted in a database and survive system crashes.",,,,
NeatSeq-Flow,,,no,Active,,Existing Workflow systems,https://neatseq-flow.readthedocs.io/en/latest/,https://github.com/bioinfo-core-BGU/neatseq-flow,https://www.biorxiv.org/content/10.1101/173005v1,Academia,,,"ngs, high throughput, bioinformatic workflow, GUI","NGS; workflow, genomics",yes,
S4M,,,no,Inactive (2019),,Existing Workflow systems,https://bitbucket.org/uqokorn/s4m_base/wiki/Home,https://bitbucket.org/uqokorn/s4m_base/src/master/,,,,"S4M is a modular Linux shell-based framework (or wrapper) for shell-invoked programs, allowing for re-use of common toolchains to enable the creation of repeatable workflows.","linux-shell, toolchain","linux-shell, bioinformatics",yes,
Loom,,,maybe,Inactive (2019),,Existing Workflow systems,https://med.stanford.edu/gbsc/pages/loom.html,https://github.com/StanfordBioinformatics/loom,,Academia,https://med.stanford.edu/gbsc/pages/loom.html,"Loom is a platform-independent tool to create, execute, track, and share workflows.","reproducibility, docker, file hash, workflow, local, google cloud, yaml, python",bionformatics,yes,
Watchdog,,,no,Active,,Existing Workflow systems,,https://github.com/klugem/watchdog,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2107-4,Academia,,XML Schema-based bioinformatics workflow system,"workflow, XML, big data",bioinformatics,yes,
phpflo,,,no,Inactive (2017),,Existing Workflow systems,,https://github.com/phpflo/phpflo,,Community?,,PhpFlo is a simple flow-based programming implementation for PH,,"PHP, flow-based programming",yes,
BASTet,,Berkeley Analysis and Storage Toolkit,yes,Active,,Existing Workflow systems,https://openmsi.nersc.gov/openmsi/client/bastet.html,https://github.com/biorack/BASTet,https://doi.org/10.1109/TVCG.2017.2744479,Academia,,"BASTet is a novel framework for shareable and reproducible data analysis that supports standardized data and analysis interfaces, integrated data storage, data provenance, workflow management, and a broad set of integrated tools. BASTet has been motivated by the critical need to enable MSI researchers to share, reuse, reproduce, validate, interpret, and apply common and new analysis methods.","Reproducibility, data storage, wfms, provenance, OpenMSI, tools, analytics",reproducible data analysis,yes,
Tavaxy,,,no,Retired,"demo instance not reachable, last changes in 05-02-2013",Existing Workflow systems,https://www.tavaxy.org/,,https://doi.org/10.1186/1471-2105-13-77,Academia,,"Tavaxy is a pattern based workflow system for the bioinformatics domain, focusing on genome comparison and sequence analysis. The basic motivation behind developing Tavaxy is to enable the design of workflows composed of Taverna and Galaxy sub-workflows in addition to other workflow nodes within a single environment. This provides an easy solution to run parts of the workflow on local infrastructure and other parts remotely through web-services. Tavaxy uses the concept of workflow patterns, which are workflow language constructs, in order to facilitate the design and execution of workflows and to enable the integration of Taverna and Galaxy workflows.","Interface, workflow patterns, Galaxy, cloud",bionformatics,code can be obtained by E-mail,
Ginflow,,,no,Inactive,,Existing Workflow systems,https://ginflow.inria.fr/,https://bitbucket.org/ginflow/hocl-workflow/src/master/,https://ieeexplore.ieee.org/document/7516089,Academia,,GinFlow is a decentralised adaptive workflow engine.,,workflow engine,yes,
SciApps,,,,Inactive,,Existing Workflow systems,https://www.sciapps.org/,https://github.com/warelab/sciapps,https://doi.org/10.1093/bioinformatics/bty439,Academia,,"SciApps
 is a cloud-based platform for building, executing, & sharing 
scientific applications (Apps) and workflows, powered by CyVerse Data 
Store, Texas Advanced Computing Center, and a federated system at Cold 
Spring Harbor Laboratory.","reproducibility, GUI, workflows, Data Managments","scientific workflows, bioinformatics",yes,
Stoa,,Script Tracking for Observational Astronomy,no,Inactive,"github README: ""still in development""",Existing Workflow systems,,https://github.com/petehague/Stoa,,Personal?,,STOA stands for Script Tracking for Observational Astronomy and is a workflow management system primarily designed for large scale production of interferometry data.,"workflow management, big data",astronomy,yes,
Collective Knowledge,,,yes,Active,,Existing Workflow systems,https://cknowledge.org/,https://github.com/mlcommons/ck,https://arxiv.org/abs/2011.01149,Academia,,"We decided to collaborate with the community and MLCommons
to develop a free, open-source and technology-agnostic platform
that can help everyone reproduce, optimize and compare any novel technology
across any rapidly evolving AI models, software, hardware and data(sets)
from different vendors in an automated way via collaborative challenges
and reproducible experiments.","reproducibility, automation, deployment",reproducible data analysis,yes ,
QosCosGrid,,,,Inactive (2019),now news update since January 2019,Existing Workflow systems,http://www.qoscosgrid.org/trac/qcg,,https://link.springer.com/chapter/10.1007/978-3-642-28267-6_4,Academia,,"The QCG middleware (previously known as QosCosGrid) is an integrated 
system offering advanced job and resource management capabilities to 
deliver to end-users supercomputer-like performance and structure. By 
connecting many distributed computing resources together, QCG offers 
highly efficient mapping, execution and monitoring capabilities for 
variety of applications, such as parameter sweep, workflows, MPI or 
hybrid MPI-OpenMP. Thanks to QCG, large-scale applications, multi-scale 
or complex computing models written in Fortran, C, C++ or Java can be 
automatically distributed over a network of computing resources with 
guaranteed QoS. The middleware provides also a set of unique features, 
such as advance reservation and co-allocation of distributed computing 
resources.","big data, middleware, distributed",,,
HTBAC,High-Throughput Binding Affinity Calculator,High-Throughput Binding Affinity Calculator,no,Inactive (2019),,Existing Workflow systems,https://htbac.readthedocs.io/en/latest/,https://github.com/radical-cybertools/htbac,https://arxiv.org/abs/1801.01174,Academia,,High performance bio-simulation framework for running molecular dynamics simulations locally or on supercomputers. Create a workflow for your specific MD requirements and submit jobs to a cluster of choice,,"molecular dynamics, bioinformatics",yes,
BioWorkbench,,,no,Inactive,,Existing Workflow systems,,https://github.com/mmondelli/bioworkbench,https://arxiv.org/abs/1801.03915,Academia,,"This repository contains a Docker recipe for the construction of 
BioWorkbench, a high-performance framework for managing and analyzing 
bioinformatics experiments.","Docker, bioinformatic workflow",bionformatics,yes,
ENVI Task engine,,,,Inactive,URLs not working,Existing Workflow systems,,,,,,"ENVI Py Engine provides a client Python package, named envipyengine, to 
run ENVI analytics provided by ENVI Desktop.
The Python package provides the ability to query for available tasks, 
retrieve task information, and execute tasks on the desktop.",,,no,
Pypeln,Pypeline,,yes,Active,,Existing Workflow systems,https://cgarciae.github.io/pypeln/,https://github.com/cgarciae/pypeln,,Personal,https://github.com/cgarciae,"Pypeln (pronounced as ""pypeline"") is a simple yet powerful Python library for creating concurrent data pipelines.
Main Features
Simple: Pypeln was designed to solve medium data tasks that require parallelism and concurrency where using frameworks like Spark or Dask feels exaggerated or unnatural.Easy-to-use: Pypeln exposes a familiar functional API compatible with regular Python code.Flexible: Pypeln enables you to build pipelines using Processes, Threads and asyncio.Tasks via the exact same API.Fine-grained Control: Pypeln allows you to have control over the memory and cpu resources used at each stage of your pipelines.","concurrent, data pipeline, python, local","data pipelines, python, medium data",yes,
MPipe,,,no,Inactive (2020),,Existing Workflow systems,http://vmlaker.github.io/mpipe/,https://github.com/vmlaker/mpipe,,Personal,,"A tiny Python module that lets you
easily write multi-stage, multiprocess pipeline algorithms.","Python, multiprocess pipeline, local",data pipeline,yes,
czid-dag,idseq-dag,,,Archived,,Existing Workflow systems,,https://github.com/chanzuckerberg/czid-dag,,Non-Profit,https://chanzuckerberg.com/,,,,,
czid-workflows,,,no,Active,,Existing Workflow systems,https://czid.org/,https://github.com/chanzuckerberg/czid-workflows,,Non-Profit,https://chanzuckerberg.com/,"Portable WDL workflows for CZ ID production pipelines
CZ ID is a hypothesis-free global software platform that helps scientists identify pathogens in metagenomic sequencing
data.",,squencing pathogens,yes,
Piper ,,,,Inactive (2017),,Existing Workflow systems,,https://github.com/NationalGenomicsInfrastructure/piper,,"Academia, National-Research",https://github.com/NationalGenomicsInfrastructure/piper,"A genomics pipeline build on top of the GATK Queue framework. Piper builds on the concept of standardized workflows for different next-generation sequencing applications
    ",,bioinformatics,,
Apache OODT,Apache Object Oriented Data Technology,,yes,Active,,Existing Workflow systems,https://oodt.apache.org/,https://github.com/apache/oodt,,National-Insitutue,https://www.jpl.nasa.gov/,"Apache Object Oriented Data Technology (OODT) is the smart 
way to integrate and archive your processes, your data, and its 
metadata. OODT allows you to:
Generate DataProcess DataManage Your DataDistribute Your DataAnalyze Your Data
Allowing for the integration of data, computation, visualization and other components.
OODT also allows for remote execution of jobs on scalable computational infrastructures so that computational and data-
intensive processing can be integrated into OODT’s data processing pipelines using cloud computing and high-performance
computing environments.",,processing pipelines,yes,
CCTools,JX Workflow,,no,Active,,Existing Workflow systems,https://cctools.readthedocs.io/en/latest/about/,https://github.com/cooperative-computing-lab/cctools,,Academia,http://ccl.cse.nd.edu/,"The Cooperating Computing Tools (CCTools) help you to design
and deploy scalable applications that run on hundreds or thousands
of machines at once.  If you have a large quantity of computational
work to accomplish that would take years to complete on your laptop,
then you have come to the right place.",,science and engineering,yes,
ADIOS,The Adaptable IO System,The Adaptable IO System,yes,Active,,Existing Workflow systems,http://csmd.ornl.gov/adios,https://github.com/ornladios/ADIOS2,https://www.sciencedirect.com/science/article/pii/S2352711019302560,Academia,,"The Adaptable IO System (ADIOS) provides a simple, 
flexible way for scientists to describe the data in their code that may 
need to be written, read, or processed outside of the running 
simulation. By providing an external to the code XML file describing the
 various elements, their types, and how you wish to process them this 
run, the routines in the host code (either Fortran or C) can 
transparently change how they process the data.","XML, data descrription","science, data description",yes,
GenPipes,,,yes,Active,,Existing Workflow systems,https://genap.ca/,https://bitbucket.org/mugqic/genpipes/src/master/,,"Academia, Consortium",https://genap.ca/p/help/team,The Genetics and Genomics Analysis Platform (GenAP) is a computing infrastructure and software environment for life science researchers available since 2015. GenAP aims at facilitating the work of researchers and students by offering out of the box Web applications running on an infrastructure currently leveraging Compute Canada Cloud and HPC resources,"Analysis paltofrm, PAAS, Galacy, workflows, data sharing, collaboration","bioinformatics, genomics",yes,
Argo,,,yes,Active,,Existing Workflow systems,https://argoproj.github.io/,https://github.com/argoproj/argo-workflows,,Enterprise,https://www.intuit.com/,"Open source tools for Kubernetes to run workflows, manage clusters, and do GitOps right.","kubernetes, GitOps,CD, workflows, rollouts, events",kubernetes,yes,
reana,,Reusable Analyses,yes,Active,URL outdated,Existing Workflow systems,https://www.reana.io/,https://github.com/reanahub/reana,https://doi.org/10.1051/epjconf/201921406034,Academia,,"Reproducible research data analysis platform 
REANA is a reusable and reproducible research data
analysis platform. It helps researchers to structure their input data, analysis
code, containerised environments and computational workflows so that the
analysis can be instantiated and run on remote compute clouds.
REANA was born to target the use case of particle physics analyses, but is
applicable to any scientific discipline. The system paves the way towards
reusing and reinterpreting preserved data analyses even several years after the
original publication.","data analysis, flexible, workflow, scalable, container, reusable, ",science,yes,
Cuisine Framework,,,no,Inactive (2005),,Existing Workflow systems,https://www.astron.nl/~renting/cuisine.html,,,,,"A pipeline framework for
the WSRT","python, pipelines",pipeline,?,
Niassa,,,no,Not Found (https://github.com/oicr-gsi/niassa),"URL not found, project not found on github only web page doc",Existing Workflow systems,,https://github.com/morgantaschuk/niassa-docs,,,,,,bioinformatics,yes,
pypeFLOW,,,no,Archived,,Existing Workflow systems,,https://github.com/PacificBiosciences/pypeFLOW,,Enterprise,https://www.pacb.com/,a simple lightweight workflow engine for data analysis scripting,pipeline,bioinformatics,yes,
Tiny Cloud Engine,,,no,Not Found,URL: 403,Existing Workflow systems,http://ka.cb.k.u-tokyo.ac.jp/tce/,,,,,,,,?,
Xbowflow,,,no,Inactive,,Existing Workflow systems,,https://github.com/ChrisSuess/Project-Xbow,,Academia,https://github.com/ChrisSuess/Project-Xbow,"Xbowflow provides a workflow system for use with Xbow clusters. Built on top of Dask Distributed**(https://distributed.readthedocs.io/en/latest/), **Xbowflow distributed the different tasks in your workflow across your Xbow
 cluster in as efficient manner as possible, running jobs in parallel 
where appropriate, and keeping data close to compute. In addition it 
provides resilience (e.g., if a worker node fails, the task is re-run 
elsewhere).","cluster, xbow, workflow",workflow,yes,
AdaptiveMD,,,no,Inactive,,Existing Workflow systems,,https://github.com/markovmodel/adaptivemd,,Academia,,"A Python framework to run adaptive MD simulations using Markov State Model (MSM)
analysis on HPC resources.","hpc, molecular dynamics, python framework","molecular dynamics, bioinformatics",yes,
Meshroom,,,no,Active,,Existing Workflow systems,http://alicevision.org/,https://github.com/alicevision/meshroom,https://doi.org/10.1145/3458305.3478443,Non-Profit,https://alicevision.org/#about,"Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework.","3d reconstruction, Photogrammetry",3D Reconstruction Software,yes,
LSST Data Management,,,no,Active,,Existing Workflow systems,https://pipelines.lsst.io/v/daily/index.html,https://github.com/lsst/pipe_base,https://arxiv.org/abs/2206.14941,Non-Profit,https://www.lsst.org/,Pipeline infrastructure code for the Rubin Science Pipelines.,"pipeline, astronomy",astronomy,yes,
prefect,,,yes,Active,,Existing Workflow systems,https://docs.prefect.io/latest/,https://github.com/PrefectHQ/prefect,,Enterprise,https://www.prefect.io/about/company/,"Prefect enables you to build and observe resilient data workflows so 
that you can understand, react to, and recover from unexpected changes. 
It's the easiest way to transform any Python function into a unit of 
work that can be observed and orchestrated. Just bring your Python code,
 sprinkle in a few decorators, and go!","Python, data pipeline, data workflow, scheduling, retry, logging, caching, observaibility",data engineering,yes,
Apache SCXML engine,,,no,Inactive (2015),,Existing Workflow systems,https://commons.apache.org/proper/commons-scxml/guide/scxml-documents.html,,,,,"State Chart XML (SCXML) is a general-purpose event-based state
      machine language that can be used in many ways.",,SCXML,yes,
IceProd,,,no,Active,,Existing Workflow systems,,https://github.com/WIPACrepo/iceprod,https://doi.org/10.1016/j.jpdc.2014.08.001,Academia,,"IceProd is a Python framework for distributed management of batch jobs.
It runs as a layer on top of other batch middleware, such as HTCondor,
and can pool together resources from different batch systems.
The primary purpose is to coordinate and administer many large sets of
jobs at once, keeping a history of the entire job lifecycle.","batch jobs, distributed management, ",physics,yes,
AnADAMA2,,,no,Active,,Existing Workflow systems,https://huttenhower.sph.harvard.edu/anadama2,https://github.com/biobakery/anadama2,https://doi.org/10.1093/bioinformatics/btx754,Academia,,"AnADAMA2 is the next generation of AnADAMA (Another Automated Data 
Analysis Management Application). AnADAMA is a tool to create 
reproducible workflows and execute them efficiently. Tasks can be run 
locally or in a grid computing environment to increase efficiency. 
Essential information from all tasks is recorded, using the default 
logger and command line reporters, to ensure reproducibility. A auto-doc
 feature allows for workflows to generate documentation automatically to
 further ensure reproducibility by capturing the latest essential 
workflow information. AnADAMA2 was architected to be modular allowing 
users to customize the application by subclassing the base grid 
meta-schedulers, reporters, and tracked objects (ie files, executables, 
etc).","metaomics, reproducible, workflow, local, grid, parallel",bioinformatics,yes,
Enso,Luna,,yes,Active,"formerly luna, outdated name and URL on github",Existing Workflow systems,https://enso.org/,https://github.com/enso-org/enso,,Enterprise,https://enso.org/,"Turning your data into knowledge is slow and error-prone. You can’t trust tools that don’t embrace best practices and provide quality assurance. Enso redefines the way you can work with your data: it is interactive, provides intelligent assistance, and was designed on a strong mathematical foundation, so you can always trust the results you get.","reproducible, data processing, visual, textual, multi languages, visualization, workflow",data analysis workflow,yes,
passerelle,,,,Archived,,Existing Workflow systems,https://code.google.com/archive/a/eclipselabs.org/p/passerelle,,https://storage.googleapis.com/google-code-archive-downloads/v2/eclipselabs.org/passerelle/TUAAULT04_talk_paper.pdf,Enterprise,https://code.google.com/archive/a/eclipselabs.org/p/passerelle,"Passerelle is a component-based solution assembly suite, developed and distributed by iSencia Belgium since 2002.
It comes with an execution engine, actor development API, reusable actor libraries and model design IDE.",,,,
Kurator-Akka,,,no,Active,,Existing Workflow systems,,https://github.com/kurator-org/kurator-akka,https://www.slideshare.net/TimothyMcPhillips/data-cleaning-with-the-kurator-toolkit-bridging-the-gap-between-conventional-scripting-and-highperformance-workflow-automation,Academia,https://mbgocs.mobot.org/index.php/tdwg/2015/paper/view/822,The kurator-akka repository hosts source code for the Kurator-Akka workflow engine component of the Kurator workflow automation toolkit. This software toolkit is being developed as part of the Kurator project and is designed to make it easy to develop and run high-performance data cleaning workflows.,"data flows, workflows, data cleaning",data cleaning workflows,yes,
Jug,,,no,Active,,Existing Workflow systems,https://luispedro.org/software/jug/,http://github.com/luispedro/jug,https://doi.org/10.5334/jors.161,Academia,,"It is a light-weight, Python only, distributed computing framework.
Jug allows you to write code that is broken up into tasks and run
different tasks on different processors. You can also think of it as a
lightweight map-reduce type of system, although it\'s a bit more
flexible (and less scalable).
It has two storage backends: One uses the filesystem to communicate
between processes and works correctly over NFS, so you can coordinate
processes on different machines. The other uses a redis database and all
it needs is for different processes to be able to communicate with a
common redis server.
Jug is a pure Python implementation and should work on any platform.
Python 3 is supported (at least 3.3 and greater).","Parallel programming, Python, Memoization , reproducible computation, High performance computing, Data analysis, Computational science",parallel reproducible computation,yes,
Node-RED,,,no,Active,,Existing Workflow systems,https://nodered.org/,https://github.com/node-red/node-red,,Enterprise,https://emerging-technology.co.uk/,"Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.
                It provides a browser-based editor that makes it easy
 to wire together flows using the wide range of nodes in the palette 
that can be deployed to its runtime in a single-click.","JavaScript, Node.js, flow editing, workflows",event-driven applications,yes,
Databolt Flow,,,yes,Active,,Existing Workflow systems,https://www.databolt.tech/index-python-pro.html#flow,https://github.com/d6t/d6tflow,,Enterprise,https://www.databolt.tech/,"For data scientists and data engineers, d6tflow is a python library which makes building complex data science workflows easy, fast and intuitive. It is primarily designed for data scientists to build better models faster.
 For data engineers, it can also be a lightweight alternative and help 
productionize data science models faster. Unlike other data 
pipeline/workflow solutions, d6tflow focuses on managing data science research workflows instead of managing production data pipelines.","Python, data science, data engineering, workflow","data science, data engineering",yes,
dataflows,,,no,Active,,Existing Workflow systems,https://www.dataflows.org,https://github.com/datahq/dataflows,,Enterprise,https://www.datopian.com/,,"data pipeline, data processing, small","data engineering, ETL",yes,
Volcano,,,no,Active,,Existing Workflow systems,https://volcano.sh/en/,https://github.com/volcano-sh/volcano,,Community,https://www.cncf.io/blog/2022/04/07/cloud-native-batch-system-volcano-moves-to-the-cncf-incubator/,"Volcano is a batch system built on Kubernetes. It provides a suite of mechanisms that are commonly required by
many classes of batch & elastic workload including: machine learning/deep learning, bioinformatics/genomics and
other ""big data"" applications. These types of applications typically run on generalized domain frameworks like
TensorFlow, Spark, Ray, PyTorch, MPI, etc, which Volcano integrates with.","cloud native, high.performance, workloads, batch scheduling, kubernetes, big data",cloud native batch system,yes,
DataJoint,,,maybe,Active,URL was outdated,Existing Workflow systems,https://www.datajoint.org/,https://github.com/datajoint/datajoint-python,https://doi.org/10.1101/031658,Academia,,"DataJoint is an open-source framework for programming scientific databases with computational workflows.    
 It provides consistent methods for organizing, populating, computing, and querying data.
DataJoint is for pipeline management based on relational principles.
        ","relational data pipelines, workflow, python, database, scientific",scientific workflow managment,yes,
DIRAC3,,,no,Active,,Existing Workflow systems,https://dirac.ac.uk,,https://doi.org/10.1088/1742-6596/219/6/062029,Academia,,,"HPC, grid computing, physics ","Grid computing, HPC",?,
Frictionless Data Package Pipelines,,,no,Active,,Existing Workflow systems,https://frictionlessdata.io/,https://github.com/frictionlessdata/datapackage-pipelines,https://ui.adsabs.harvard.edu/abs/2017AGUFMIN33C0139S/abstract,Academia,,"datapackage-pipelines is a framework for declarative 
stream-processing of tabular data. It is built upon the concepts and 
tooling of the Frictionless Data project.","pipelines, yaml, declerative",data pipelines,yes,
Orange,,,yes,Actuve,,Existing Workflow systems,https://orangedatamining.com/,https://github.com/biolab/orange3,http://jmlr.org/papers/v14/demsar13a.html,Academia,,"Orange is a data mining and visualization toolbox for novice and expert alike. To explore data with Orange, one requires no programming or in-depth mathematical knowledge.
 We believe that workflow-based data science tools democratize data 
science by hiding complex underlying mechanics and exposing intuitive 
concepts. Anyone who owns data, or is motivated to peek into data, 
should have the means to do so.","data analysis, workflow, visualization, interactive",data mining,yes,
Ensemble Toolkit,,,no,Active,,Existing Workflow systems,https://radicalentk.readthedocs.io/en/latest/entk.html,https://github.com/radical-cybertools/radical.entk,https://arxiv.org/abs/1602.00678v2,Academia,,"The Ensemble Toolkit is a Python library for developing and executing
large-scale ensemble-based workflows. It is being developed by the
RADICAL Research Group at Rutgers University.
Ensemble Toolkit is released under the
MIT License.","DAG, workflow, science, ensemble",ensemble-based workflows,yes,
BioQueue,,,no,,,Existing Workflow systems,https://www.bioqueue.org/,https://github.com/liyao001/BioQueue,https://doi.org/10.1093/bioinformatics/btx403,Academia,,"BioQueue is a researcher-facing bioinformatic platform preferentially to
 improve the efficiency and robustness of analysis in bioinformatics 
research by estimating the system resources required by a particular 
job. At the same time, BioQueue also aims to promote the accessibility 
and reproducibility of data analysis in biomedical research. Implemented
 by Python 3.x, BioQueue can work in both POSIX compatible systems (Linux, Solaris, OS X, etc.) and Windows.","pipelines, data analysis, reproducibility, queue system ",bioinformatics,yes,
mlr3pipelines,,,no,Active,,Existing Workflow systems,https://mlr3pipelines.mlr-org.com/,https://github.com/mlr-org/mlr3pipelines/,https://jmlr.org/papers/v22/21-0281.html,Academia,,"mlr3pipelines is a dataflow programming toolkit for machine learning in R utilising the mlr3
 package. Machine learning workflows can be written as directed “Graphs”
 that represent data flows between preprocessing, model fitting, and 
ensemble learning units in an expressive and intuitive language. Using 
methods from the mlr3tuning package, it is even possible to simultaneously optimize parameters of multiple processing units.
In principle, mlr3pipelines is about defining singular data and model manipulation steps as “PipeOps”:","R, pipelines,",R. pipelines,yes,
Kedro,,,maybe,Active,,Existing Workflow systems,https://kedro.org/,https://github.com/kedro-org/kedro,,Enterprise ,https://www.mckinsey.com/about-us/new-at-mckinsey-blog/meet-kedro-mckinseys-first-open-source-software-tool,"Kedro is an open-source Python framework to create 
reproducible, maintainable, and modular data science code. It uses 
software engineering best practices to help you build production-ready 
data engineering and data science pipelines.
Kedro is hosted by the LF AI & Data Foundation.","Project template, data catalog, pipeline, coding standards, flexible deployment, python",data science ,yes,
DATAVIEW,,,no,Inactive,,Existing Workflow systems,http://www.dataview.org/,https://github.com/shiyonglu/DATAVIEW,,Personal?,,"DATAVIEW (www.dataview.org)
 is a big data workflow management system. It uses Dropbox as the data 
cloud and Amazon EC2 as the compute cloud. It also provides a 
workflow_LocalExecutor for users to run their local machine off the 
cloud. Current research focuses on the 1) performance and cost 
optimization for running workflows in clouds and 2) infrastructual-level
 support on GPU-enabled deep learning workflows. For deep learning 
workflows, it currently supports GPU infrastructures including 1) the 
Local NVIDIA GPU of a PC, 2) GPU Xavier and Nano SoMs (System-on-Module)
 and 3) the Heterogeneous GPU Cluster.","big data, wfms, AWS Ec2, Dropbox",big data workflow management system,yes,
SecDATAVIEW,,,no,Inactive,,Existing Workflow systems,,https://github.com/shiyonglu/SecDATAVIEW,https://dl.acm.org/doi/10.1145/3359789.3359845,Academia,,"SecDATAVIEW is a secure big data workflow management system compatible 
with the heterogeneous computing environment. It leverages 
hardware-assisted TEEs such as Intel SGX and AMD SEV to protect the 
execution of workflows in the untrusted cloud","big data, wfms, secure","big data workflow management system, security",yes,
CERAMICCA,,Cloud Engine Resource for Accelerated Medical Image Computing for Clinical Applications,,Not found,URL not found,Existing Workflow systems,https://ceramicca.ensc.sfu.ca/,,,,,,,,,
uap,,Universal Analysis Pipeline,no,Inactive,,Existing Workflow systems,,https://github.com/yigbt/uap,https://doi.org/10.1186/s12859-019-3219-1,Academia,,"The uap package is a framework to configure, run, and control
large data multi-step analyses.
Its main focus is on the analysis of high-throughput sequencing data.
The aim of this data processing pipeline is to enable robust and straightforward
bioinformatics data evaluation.
It is implemented in Python, runs under GNU/Linux and can be controlled from the
command-line interface.
Although the primary focus is the evaluation of sequencing data, its design
allows for a variety of other applications.","python, data anaylsis, sequencing data, data processing, pipeline",bioinformatics,yes,
signac,,simple data management,yes,Active,,Existing Workflow systems,https://signac.io/,https://github.com/glotzerlab/signac,http://dx.doi.org/10.25080/Majora-4af1f417-016,Academia,,"The signac framework helps users manage and scale file-based workflows, facilitating data reuse, sharing, and reproducibility.
It provides a simple and robust data model to create a 
well-defined indexable storage layout for data and metadata.
This makes it easier to operate on large data spaces, streamlines 
post-processing and analysis and makes data collectively accessible","python, data management, reproducibility, ",data management,yes,
cwltool,,reference implementation of Common Workflow Language,maybe,Active,,Existing Workflow systems,https://www.commonwl.org/,https://github.com/common-workflow-language/cwltool/,https://doi.org/10.6084/m9.figshare.3115156.v1,"Community, Academia",https://www.open-bio.org/wiki/Codefest_2014,"This is the reference implementation of the Common Workflow Language open
standards.  It is intended to be feature complete
and provide comprehensive validation of CWL
files as well as provide other tools related to working with CWL.","workflow, CWL, python",CWL implementation,yes,
CWLEXEC,,CWL executor for IBM Spectrum LSF clusters,no,Inactive,,Existing Workflow systems,,https://github.com/yuch7/cwlexec,,Enterprise,https://github.com/IBMSpectrumComputing/cwlexec/releases,"cwlexec implements running CWL (Common 
Workflow Language) workflows on IBM Spectrum LSF. It is written in Java 
and tested for Java 8, with the following features:
Tight integration with IBM® Spectrum LSFLeverages LSF features (such as native container support)Implements CWL draft-3 and v1.0 with a few exceptions 
(SoftwareRequirement, include directive, remote location in 
File/Directory specification)","Java, CWL, IBM Spectrum LFS",CWL implementation,yes,
drmr,,A tool for submitting pipeline scripts to distributed resource managers,no,Inactive (2017),,Existing Workflow systems,https://drmr.readthedocs.io/en/latest/,https://github.com/ParkerLab/drmr/,,Academia,http://theparkerlab.org/,Drmr (pronounced ‘drummer’) lets you write computational pipelines in simple shell scripts. It’s designed to work with common distributed resource management (DRM) systems (Slurm and PBS so far).,"pipeline, python, scripts, DRM",pipeline,yes,
autosubmit,,Autosubmit is a Python software to manage complicated workflows on HPC platforms,no,Active,Based on documentation,Existing Workflow systems,https://www.bsc.es/research-and-development/software-and-apps/software-list/autosubmit,https://autosubmit.readthedocs.io/en/latest/,https://doi.org/10.1109/HPCSim.2016.7568429,Academia,,"Autosubmit is a Python-based workflow manager to create, manage and 
monitor complex tasks involving different substeps, such as scientific 
computational experiments. These workflows may involve multiple 
computing systems for their completion, from HPCs to post-processing 
clusters or workstations. Autosubmit can orchestrate all the tasks 
integrating the workflow by managing their dependencies, interfacing 
with all the platforms involved, and handling eventual errors.","workflow, HPC, workflow manager, python",scientific workflow,yes,
JUDI,,"Software Pipeline, Just Do It",,Inactive (2021),,Existing Workflow systems,https://pyjudi.readthedocs.io/en/latest/,https://github.com/ncbi/JUDI,https://doi.org/10.1093/bioinformatics/btz956,Academia,,"JUDI simplifies building and executing a software pipeline
under different parameter settings by automating an efficient execution
of the pipeline across the settings.","pipeline, parameter settings, decouple files tasks from parameter",bioinformatics,,
Sumatra,,,maybe,Inactive,beta status,Existing Workflow systems,https://neuralensemble.org/sumatra/,https://github.com/open-research/sumatra,http://doi.ieeecomputersociety.org/10.1109/MCSE.2012.41,Academia,,"Sumatra is a tool for managing and tracking projects based on numerical
simulation and/or analysis, with the aim of supporting reproducible research.
It can be thought of as an automated electronic lab notebook for computational
projects.
It consists of:
a command-line interface, smt, for launching simulations/analyses with
automatic recording of information about the experiment, annotating these
records, linking to data files, etc.a web interface with a built-in web-server, smtweb, for browsing and
annotating simulation/analysis results.a Python API, on which smt and smtweb are based, that can be used in your own
scripts in place of using smt, or could be integrated into a GUI-based
application.
Sumatra is currently beta code, and should be used with caution and frequent
backups of your records.","reproducible, documentation, procenance, ","reproducible research, ''automated electronic lab notebook'' for simulation/analysis projects",yes,
Netflix Conductor,,Conductor is a platform created by Netflix to orchestrate workflows that span across microservices.,no,Active,,Existing Workflow systems,https://conductor.netflix.com/,https://github.com/Netflix/conductor,,Enterprise,http://netflix.com/,Conductor is a platform created by Netflix to orchestrate workflows that span across microservices.,"workflow engine, orchestration, scalable, microservices, workflow management",microservice orchestration engine,yes,
PipEngine,,An ultra light YAML-based pipeline execution engine,no,Inactive (2017),,Existing Workflow systems,,https://github.com/fstrozzi/bioruby-pipengine,https://doi.org/10.21105/joss.00341,Academia,,"A simple launcher for complex biological pipelines.
PipEngine will generate runnable shell scripts, already 
configured for the PBS/Torque job scheduler, for each sample in the 
pipeline. It allows to run a complete pipeline or just a single step 
depending on the needs.
PipEngine is best suited for NGS pipelines, but it can be 
used for any kind of pipeline that can be runned on a job scheduling 
system and which is ""sample"" centric, i.e. you have from one side a list
 of samples with their corresponding raw data, and from the other side a
 pipeline that you would like to apply to them.
PipEngine was developed to combine the typical flexibility
 and portability of shell scripts, with the concept of pipeline 
templates that can be easily applied on different input data to 
reproduce scientific results. The overall improvement over Makefiles or 
customised ad-hoc shell scripts is better readability of the pipelines 
using the YAML format, especially for people with no coding experience, 
the automated scripts generation which allows adding extra 
functionalities like error controls and logging directly into script 
jobs, and an enforced separation between the description of input data 
and the pipeline template, which improves clarity and reusability of 
analysis protocols.","pipelines, YAML, shell scripts","bioinformatics, ngs",yes,
MyQueue,,,no,Active,,Existing Workflow systems,https://myqueue.readthedocs.io/en/latest/,https://gitlab.com/myqueue/myqueue,https://doi.org/10.21105/joss.01844,Academia,,MyQueue is a frontend for SLURM/PBS/LSF that makes handling of tasks easy. It has a command-line interface called mq with a number of Sub-commands and a Python interface for managing Workflows. Simple to set up: no system administrator or database required.,"submit, jobs, cluster, hpc, task status",task sumission,yes,
drake R package,,An R-focused pipeline toolkit for reproducibility and high-performance computing,yes,Active,superseded by targets (2021),Existing Workflow systems,,https://github.com/ropensci/drake,https://doi.org/10.21105/joss.00550,Academia,,"Data analysis can be slow. A round of scientific computation can take
several minutes, hours, or even days to complete. After it finishes, if
you update your code or data, your hard-earned results may no longer be
valid. How much of that valuable output can you keep, and how much do
you need to update? How much runtime must you endure all over again?
For projects in R, the drake package can help. It analyzes your
workflow, skips steps with
up-to-date results, and orchestrates the rest with optional distributed
computing. At the end,
drake provides evidence that your results match the underlying code
and data, which increases your ability to trust your research.","R, pipeline, data anaylsis, HPC, reproducibility",reproducible pipeline and hpc,yes,
targets,targets R package,Function-oriented Make-like declarative workflows for R,yes,Active,,Existing Workflow systems,https://docs.ropensci.org/targets/,https://github.com/ropensci/targets,https://doi.org/10.21105/joss.02959,Academia,,"Pipeline tools coordinate the pieces of computationally demanding analysis projects. The targets package is a Make-like
 pipeline tool for statistics and data science in R. The package skips 
costly runtime for tasks that are already up to date, orchestrates the 
necessary computation with implicit parallel computing, and abstracts 
files as R objects. If all the current output matches the current 
upstream code and data, then the whole pipeline is up to date, and the 
results are more trustworthy than otherwise.","R, data analysis, pipeline, orchestration, reproduciblity",reproducible pipeline and hpc,yes,
MaDaTS,,Managing Data on Tiered Storage for Scientific Workflows,no,Inactive,,Existing Workflow systems,,https://github.com/dghoshal-lbl/madats,https://10.0.82.113/joss.00830,Academia,,"MaDaTS provides an integrated data management and workflow execution
framework on multi-tiered storage systems. Users of MaDaTS can execute
a workflow by either specifying the workflow stages in a YAML description
file, or use the API to manage workflows and associated data. Some examples
of specifying the workflow description and using the API are provided in
the examples/ directory.
The MaDaTS API provides simple data abstractions for managing workflow and
data on multi-tiered storage. It takes a data-driven approach to executing
workflows, where a workflow is mapped to a Virtual Data Space (VDS) consisting
of virtual data objects. A user simply creates a VDS and adds virtual data
objects to the VDS, and MaDaTS takes care of all the necessary data movements
and bindings to seamlessly manage a workflow and associated data across multiple
storage tiers.","data management, workflow, YAML, API, multi-tiered storage","data management, workflow",yes,
Cadence,,Fault-Tolerant Stateful Code Platform,maybe,Active,,Existing Workflow systems,https://cadenceworkflow.io/,https://github.com/uber/cadence,,Enterprise,https://www.uber.com/de/blog/engineering/,"Cadence is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous long-running business logic in a scalable and resilient way",,orchestration engine,yes,
Merlin,,Machine Learning for HPC Workflows,maybe,Active,,Existing Workflow systems,https://merlin.readthedocs.io/en/latest/,https://github.com/LLNL/merlin,https://doi.org/10.1016/j.future.2022.01.024,Academia,,"Merlin is a tool for running machine learning based workflows. The goal of Merlin is to make it easy to build, run, and process the kinds of large scale HPC workflows needed for cognitive simulation.","workflow, ML, HPC, big data, distributed task queuing","ML workflow, HPC workflows",yes,
Janis,,portable workflow specification to run reproducible across different compute platforms,maybe,Active,WIP,Existing Workflow systems,https://janis.readthedocs.io/en/latest/index.html,https://github.com/PMCC-BioinformaticsCore/janis,https://zenodo.org/record/4427231,Academia,,"Janis gives you an API to build computational workflows and will generate a workflow description in CWL (v1.2) and WDL (version development). By using Janis, you get type-safety, portability and reproducibility across all of your execution environments.",,workflow,yes,
AlphaSQL,,Integrated Type and Schema Check and Parallelization for SQL file set mainly for BigQuery,no,Active,,Existing Workflow systems,,https://github.com/Matts966/alphasql,,Personal,,"AlphaSQL provides Automatic Parallelization for sets of SQL files and integrated Type/Scheme Checker to eliminate syntax, type and schema errors from your datawarehouse.","Docker, Dependency analysis, datawarehouse, workflow, schema check","SQL, DataWarehouse",yes,
Zeebe,,Workflow Engine for Microservices Orchestration,no,Active,,Existing Workflow systems,https://camunda.com/platform/zeebe/,https://github.com/camunda/zeebe,,Enterprise,https://camunda.com/,"Zeebe’s cloud-native design provides the performance, resilience, and security enterprises need to future-proof their process orchestration efforts.","workflow, microservice, orchestration, bpmn, workflow engine","workflow engine, microservice orchestration",yes,
durabletask,,Durable Task Framework allows users to write long running persistent workflows in C# using the async/await capabilities,no,Active,,Existing Workflow systems,,https://github.com/Azure/durabletask,,Enterprise,https://azure.microsoft.com,"The Durable Task Framework (DTFx) is a library that allows users to write long running persistent workflows (referred to as orchestrations)
 in C# using simple async/await coding constructs. It is used heavily 
within various teams at Microsoft to reliably orchestrate long running 
provisioning, monitoring, and management operations. The orchestrations 
scale out linearly by simply adding more worker machines. This framework
 is also used to power the serverless Durable Functions extension of Azure Functions.","persistent workflows, C#, orchestration, microsoft","C# workflow orchestration, operations",yes,
Illumina State Language,,,,"URL moved, proprietary ",URL moved,Existing Workflow systems,,,,Enterprise,,,,,,
Moteur,,"a data intensive
service-based workflow
engine",,"Not Found, outdated","Moteur package not found, presentation from 2006",Existing Workflow systems,,,http://www.i3s.unice.fr/~johan/publis/MOTEUR-poster-A4.pdf,Academia,,,,scientific workflow engine,,
SimStack,,,no,Active,,Existing Workflow systems,http://www.simstack.de/,,https://doi.org/10.3389/fmats.2022.877597,Academia,,"SimStack facilitates the efficient implementation, adoption and 
execution of complex and extensive simulation workflows and enables fast
 uptake of modeling techniques for advanced functional and nanomaterials
 by industry. Within a few hours, complex modeling solutions are 
rendered into easy-to-use, market-ready software products for multiscale
 modeling solutions",commercial,material science,no,
Maestro Workflow Conductor,,A tool to easily orchestrate general computational workflows both locally and on supercomputers,yes,Active,,Existing Workflow systems,https://maestrowf.readthedocs.io/en/latest/,https://github.com/LLNL/maestrowf,,National-Institute,https://software.llnl.gov/,"Maestro gives an easy path to automating and orchestrating your 
workflows, building upon your existing shell and batch (HPC scheduled 
scripts/tasks) script tasks to layer on parameterization, task 
dependencies, and output isolation.  Additionally, Maestro's workflow 
specification layer enables documenting those scripts and their 
interdependencies if chaining them together, and makes them more 
repeatable and shareable for enhanced collaboration with your peers.","llnl, reproducibility, shareability, self-documenting, consistent, repeatability, workflows, hpc, local",scientific workflow,yes,
HyWare,,,yes,The workflow engine is currently being implemented in the context of D4Science so as to make it available to all e-infrastructures it supports,,Existing Workflow systems,,,https://doi.org/10.1007/s41060-020-00237-x,Academia,,"Research e-infrastructures are “systems of systems,” patchworks of 
resources such as tools and services, which change over time to address 
the evolving needs of the scientific process. In such environments, 
researchers carry out their scientific process in terms of sequences of 
actions that mainly include invocation of web services, user interaction
 with web applications, user download and use of shared software 
libraries/tools. The resulting workflows are intended to generate new 
research products (articles, datasets, methods, etc.) out of existing 
ones. Sharing a digital and executable representation of such workflows 
with other scientists would enforce Open Science publishing principles 
of “reproducibility of science” and “transparent assessment of science.”
 This work presents HyWare, a language and execution platform capable of
 representing scientific processes in highly heterogeneous research 
e-infrastructures in terms of so-called hybrid workflows. Hybrid 
workflows can express sequences of “manually executable actions,” i.e., 
formal descriptions guiding users to repeat a reasoning, protocol or 
manual procedure, and “machine-executable actions,” i.e., encoding of 
the automated execution of one (or more) web services. An HyWare 
execution platform enables scientists to (i) create and share workflows out of a given action set (as defined by the users to match e-infrastructure needs) and (ii)
 execute hybrid workflows making sure input/output of the actions flow 
properly across manual and automated actions. The HyWare language and 
platform can be implemented as an extension of well-known workflow 
languages and platforms.","open science, workflow, reproducibility",scientific research,?,
HyperFlow,,a scientific workflow management system,maybe,Active,,Existing Workflow systems,,https://github.com/hyperflow-wms/hyperflow,https://www.sciencedirect.com/science/article/abs/pii/S0167739X15002770,Academia,,"This paper presents HyperFlow: a model of computation, programming approach and enactment engine
 for scientific workflows. Workflow programming in HyperFlow combines 
a simple declarative description of the workflow structure with 
low-level implementation of workflow activities in a mainstream scripting language.
 The aim of this approach is to increase the programming productivity of
 workflow developers who are skilled programmers and desire 
a programming experience similar to the one offered by a mature 
programming ecosystem. Combining a declarative description with 
low-level programming enables elimination of shim nodes from the 
workflow graph, considerably simplifying workflow implementations. The workflow description is based on a formal model of computation
 (Process Networks) and is characterized by a simple and concise syntax,
 utilizing just three key abstractions—processes, signals and functions.
 Yet it is sufficient for expressing complex workflow patterns in a simple way. The adopted model of computation implemented in the HyperFlow workflow engine
 enables fully distributed and decentralized workflow enactment. The 
paper describes HyperFlow from the perspective of its workflow 
programming capabilities, the adopted model of computation, as well as 
the enactment engine,
 in particular its distributed workflow enactment capability. The 
provenance model and logging features are also presented. Several 
workflow examples derived from other workflow systems and reimplemented in HyperFlow are extensively discussed.",,scientific workflows,yes,
BRANE Framework,,Programmable Orchestration of Applications and Networking,,"Archived, handed over to https://github.com/epi-project/brane",URL not found (https://onnovalkering.github.io/brane/),Existing Workflow systems,,https://github.com/onnovalkering/brane,https://ieeexplore.ieee.org/document/9582292,Academia,,,,,,
BRANE Framework,,Programmable Orchestration of Applications and Networking,no,,,Existing Workflow systems,https://wiki.enablingpersonalizedinterventions.nl/,https://github.com/epi-project/brane,,,,,,orchestration,yes,
ApolloWF,,APplication Orchestration and runtime framework for Leveraging the edge-cLoud cOntinuum,no,Inactive,"Apollo is a research project first and foremost, and is still in early development.",Existing Workflow systems,,https://apollowf.github.io/,https://doi.org/10.1145/3452413.3464793,Academia,,"Apollo, a runtime system for serverless function compositions distributed across the cloud-edge-IoT continuum. Apollo's modular design enables a fine-grained decomposition of the runtime implementation(scheduling, data transmission, etc.) of the application, so that each of the numerous implementation decisions can be optimized separately, fully exploiting the potential for the optimization of the overall performance and costs. Apollo features (a) a flexible model of the application and the available resources and (b) an implementation process based on a large set of independent agents. This flexible structure enables distributing not only the processing, but the implementation process itself across a large number of resources, each running an independent Apollo instance. The ability to flexibly determine the placement of implementation actions opens up new optimization opportunities, while at the same time providing access to greater computing power for optimizing challenging decisions such as task scheduling and the placement and routing of data.",,orchestration,yes,
IS-EPOS Platform,,,,,,Existing Workflow systems,,,https://doi.org/10.1109/WORKS51914.2020.00009,Academia,,,,,,
pyinvoke,,,no,Active,,Existing Workflow systems,https://www.pyinvoke.org/,https://github.com/pyinvoke/invoke,,Personal?,,"Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented
subprocesses and organizing executable Python code into CLI-invokable tasks. It
draws inspiration from various sources (make/rake, Fabric 1.x, etc) to
arrive at a powerful & clean feature set","python, scripts, CLI tasks",pythonic task execution,yes,
Compi ,,framework for portable computational pipelines,maybe,Active,,Existing Workflow systems,http://sing-group.org/compi/,https://github.com/sing-group/compi,https://doi.org/10.7717/peerj-cs.593,Academia,,"
                    Compi is an extremely simple application framework for portable computational pipelines. A computational
                        pipeline can be seen as a set of processing steps that run one after one (or ocassionally in parallel
                        if they are independent).
                    
                    There are many fields where computational pipelines constitute the main architecture of applications,
                        such as big data analysis or bioinformatics.
                    Many pipelines combine third party tools along with custom made processes, conforming the final pipeline.
                        Compi is the framework helping you to create the final - and portable - application.
                ","XML, lagnuage agnostic, protable, docker, CLI, parallelization, pipeline, reproducible","portable, reproducible pipeline",yes,
TriggerFlow,,Event-based Orchestration of Serverless Workflows,no,Archived,,Existing Workflow systems,,https://github.com/triggerflow/triggerflow,https://arxiv.org/abs/2006.08654,Academia,,"Triggerflow is a scalable, extensible and serverless in design platform for event-based orchestration of serverless workflows.","serverless, workflow, orchestration, event-based, cloud-native","workflow, orhcestration",yes,
Google Cloud Workflows,,Orchestrate and automate Google Cloud and HTTP-based API services with serverless workflows,,Active,,Existing Workflow systems,https://cloud.google.com/workflows/docs,,,Enterprise,https://cloud.google.com/workflows/docs,,gcp,"orchestration, workflow",no,
PanDA Workflow Management System,,,no,Active,,Existing Workflow systems,https://github.com/PanDAWMS,https://github.com/PanDAWMS/pandawms,https://www.epj-conferences.org/articles/epjconf/abs/2016/03/epjconf_mmcp2016_01003/epjconf_mmcp2016_01003.html,Academia,,"Modern experiments collect peta-scale volumes of data and utilize vast, 
geographically distributed computing infrastructure that serves 
thousands of scientists around the world. Requirements for rapid, near 
real-time data processing, fast analysis cycles and need to run massive 
detector simulations to support data analysis pose special premium on 
efficient use of available computational resources. A sophisticated 
Workload Management System (WMS) is needed to coordinate the 
distribution and processing of data and jobs in such environment. The 
ATLAS experiment at CERN uses PanDA (Production and Data Analysis) 
Workload Management System for managing the workflow for all data 
processing on over 150 data centers. While PanDAcurrently uses more than
 250,000 cores with a peak performance of 0.3 petaFLOPS, it runs around 2
 million jobs per day on hundreds of Grid sites and serving thousands of
 ATLAS users. In 2017 about 1.5 exabytes of data were processed with 
PanDA.In 2012 BigPanDA project project was started with aim to introduce
 new types of computing resources into ATLAS computing infrastructure, 
but also to offering PanDA features to different data-intensive 
applications for projects and experiments outside of ATLAS and 
High-Energy and Nuclear Physics. In this article we will present 
accomplishments and discuss possible directions for future work.","Big data, hpc","CERN, high energy physics",yes,
Harvester,,,no,Active,,Existing Workflow systems,,https://github.com/HSF/harvester,https://doi.org/10.1051/epjconf/201921403030,Academia,,Harvester is a resource-facing service between WFMS and the collection of pilots. It is a lightweight stateless service running on a VObox or an edge node of HPC centers to provide a uniform view for various resources.,"resource managment, monitor operations, provisioning","high energy physics, CERN",yes,
BD-Processor,,,no,Active,,Existing Workflow systems,https://big-data-processor.github.io/bdp-document/,https://github.com/big-data-processor/bd-processor/,,Academia,https://www.cgu.edu.tw,"The BD-Processor (BDP) is a general-purpose web-based data workbench that focuses not only on workflow management but also interactive data analysis. 
Our goal is to provide a full-featured data science workbench.","wfms, data analysis, data workbench",data science,yes,
redun,,yet another redundant workflow engine,yes,Active,,Existing Workflow systems,https://insitro.github.io/redun/,https://github.com/insitro/redun,,Enterprise,https://insitro.com/,"redun aims to be a more expressive and efficient 
workflow framework, built on top of the popular Python programming 
language. It takes the somewhat contrarian view that writing dataflows 
directly is unnecessarily restrictive, and by doing so we lose 
abstractions we have come to rely on in most modern high-level languages
 (control flow, composability, recursion, high order functions, etc). 
redun's key insight is that workflows can be expressed as lazy expressions, which are then evaluated by a scheduler that performs automatic parallelization, caching, and data provenance logging.","Python, workflow, DAG, caching, incremental computing, data lineage, provenance",workflow engine,yes,
pyiron,,The materials science IDE,no,Active,,Existing Workflow systems,https://pyiron.org/,https://github.com/pyiron/pyiron,https://doi.org/10.1016/j.commatsci.2018.07.043,Academia,,"pyiron is an integrated development environment for implementing, testing, and running simulations in computational materials science. It combines several tools in a common platform","reproducibility, local, HPC, workflow",material science,yes,
looper,,pipeline submitting engine,no,Active,,Existing Workflow systems,,https://github.com/pepkit/looper,,Academia,https://databio.org/software/,"Looper is a job submitting engine. Looper deploys arbitrary shell commands for each sample in a standard PEP project.
 You can think of looper as providing a single user interface to 
running, monitoring, and managing all of your sample-intensive research 
projects the same way, regardless of data type or pipeline used.","pipeline, submission engine, job submission",bioinformatics,yes,
dagster,,"An orchestration platform for the development, production, and observation of data assets.",yes,Active,,Existing Workflow systems,https://dagster.io/,https://github.com/dagster-io/dagster,,Enterprise,https://www.elementl.com,"Dagster is a cloud-native data pipeline 
orchestrator for the whole development lifecycle, with integrated 
lineage and observability, a declarative programming model, and 
best-in-class testability.
It is designed for developing and maintaining data assets, such as tables, data sets, machine learning models, and reports.
With Dagster, you declare—as Python functions—the data 
assets that you want to build. Dagster then helps you run your functions
 at the right time and keep your assets up-to-date.","python, workflow, metadata, orchestration, etl, data pipelines, mlops",data engineering,yes,
StackStorm,,"StackStorm (aka ""IFTTT for Ops"") is event-driven automation for auto-remediation, incident responses, troubleshooting, deployments, and more for DevOps and SREs.",no,,,Existing Workflow systems,https://stackstorm.com/,https://github.com/StackStorm/st2,,Enterprise,https://en.wikipedia.org/wiki/StackStorm,"StackStorm is a platform for integration and automation 
across services and tools. It ties together your existing infrastructure
 and application environment so you can more easily automate that 
environment -- with a particular focus on taking actions in response to 
events.
StackStorm helps automate common operational patterns. Some examples are:
Facilitated Troubleshooting - triggering on system 
failures captured by Nagios, Sensu, New Relic and other monitoring, 
running a series of diagnostic checks on physical nodes, OpenStack or 
Amazon instances, and application components, and posting results to a 
shared communication context, like Slack or JIRA.Automated remediation - identifying and verifying 
hardware failure on OpenStack compute node, properly evacuating 
instances and emailing VM about potential downtime, but if anything goes
 wrong - freezing the workflow and calling PagerDuty to wake up a human.Continuous deployment - build and test with 
Jenkins, provision a new AWS cluster, turn on some traffic with the load
 balancer, and roll-forth or roll-back based on NewRelic app performance
 data.","devops, python, automation, deployment, incident response",IFTTT for Ops,yes,
Geoweaver,,"a web system to allow users to automatically record history and manage complicated scientific workflows in web browsers involving the online spatial data facilities, high-performance computation platforms, and open-source libraries.",no,Active,,Existing Workflow systems,https://esipfed.github.io/Geoweaver/,https://github.com/ESIPFed/Geoweaver,https://doi.org/10.3390/ijgi9020119,Academia,,"Geoweaver is an in-browser software allowing users to easily compose 
and execute full-stack data processing workflows via taking advantage of
 online spatial data facilities, high-performance computation platforms,
 and open-source deep learning libraries. It provides all-in-one 
capacity covering server management, code repository, workflow 
orchestration software, and history recorder.

It can be run from both local and remote (distributed) machines","wfms,pipeline, docker, kubernetes, scientific computing, hpc",geo science,yes,
Popper,,Container-native task automation engine,no,Inactive,,Existing Workflow systems,https://getpopper.io/,https://github.com/getpopper/popper,https://doi.org/10.1109/IPDPSW.2017.157,Academia,,"A container-native task automation engine that runs on distinct 
container engines, orchestration frameworks and CI services. Write 
simple YAML files, run everywhere.","YAML, task automation engine, container native, docker, reproducibility,",task automation engine,yes,
Cloud Build,,GCP CI/CD Platform,no,Active,,Existing Workflow systems,https://cloud.google.com/build,,,,,GCP,"Build, test, and deploy on our serverless CI/CD platform.",google cloud platform,no,
Task,Task/Taskfile,A task runner / simpler Make alternative written in Go,maybe,Active,,Existing Workflow systems,https://taskfile.dev/,https://github.com/go-task/task,,,,"Task is a task runner / build tool that aims to be simpler and easier to use
than, for example, GNU Make.Since it's written in Go, Task is just a single binary and has no other
dependencies, which means you don't need to mess with any complicated install
setups just to use a build tool.","go, devops, makefile, task runner, make, build tool, taskfile","build tool, task runner",yes,
pypyr,,"pypyr task-runner cli & api for automation pipelines. Automate anything by combining commands, different scripts in different languages & applications into one pipeline process.",maybe,Active,,Existing Workflow systems,https://pypyr.io/,https://github.com/pypyr/pypyr/,,,,"task runner for automation pipelinesscript sequential task workflow steps in yamlconditional execution, loops, error handling & retriesFor when your shell scripts get out of hand. Less tricky than makefile.Simple variable substitution & configuration file management.Automate anything by combining commands, different scripts in different languages & applications into one pipeline process.","python, yaml, devops, cli, pipeline, task maanger, script, devops","task runner, automation",yes,
SimTool,SimTool/Sim2Ls,Functions for creating and running Simulation Tools on the HUBzero platform lead by nanoHUB,yes,Active,,Existing Workflow systems,https://simtool.readthedocs.io/en/stable/,https://github.com/hubzero/simtool,https://doi.org/10.1371/journal.pone.0264492,Academia,,"Functions for creating and running Simulation Tools on the HUBzero platform lead by nanoHUB
Features
Easily declare and validate inputs and outputs of a simulation using
 Python and Jupyter notebooks. The entire simulation code can run inside
 a notebook or the notebook can be a wrapper that invokes complex 
external codes.Uses papermill to run parameterized notebooks, saving a new copy for each run.Results saved in a datastore (filesystem or web service based).  The
 datastore can be used for machine learning and statistical analysis.  
Additionally, it functions as a cache.Can be containerized for remote execution.","HUBzero platform, input validation, python, Jupyter Notebooks, papermill, container, FAIR",SimulationTool on HUBZero,yes,
SideIO,,A Side I/O system framework for hybrid scientific workflow,,Inactive,no project/source code available,Existing Workflow systems,,,https://doi.org/10.1016/j.jpdc.2016.07.001,Academia,,,,,,
Flyte,,"Flyte is an open-source workflow orchestration platform for building data, ML and analytics workflows with ease.",no,Active,,Existing Workflow systems,https://flyte.org/,https://github.com/flyteorg/flyte,,Community,https://flyte.org/community#lf-ai-data,"Flyte is an open-source orchestrator that facilitates 
building production-grade data and ML pipelines. It is built for 
scalability and reproducibility, leveraging Kubernetes as its underlying
 platform. With Flyte, user teams can construct pipelines using the 
Python SDK, and seamlessly deploy them on both cloud and on-premises 
environments, enabling distributed processing and efficient resource 
utilization.","reproducibility, python, local, cloud, data pipelines, ML pipelines, scalebility,data preprocessing, data analysis, ML, orchestration, data science","data and ML pipelines, orchestration",yes,
StreamFlow,,StreamFlow Workflow Manager,no,Active,,Existing Workflow systems,https://streamflow.di.unito.it/,https://github.com/alpha-unito/streamflow,http://dx.doi.org/10.1109/TETC.2020.3019202,Academia,,"he StreamFlow framework is a container-native Workflow Management System written in Python 3 and based on the Common Workflow Language (CWL) standard. Developed and maintained by the Alpha research group at Università di Torino (UniTO), it has been designed around two main principles:



Allowing the execution of tasks in multi-container environments, in order to support concurrent execution of multiple communicating tasks in a multi-agent ecosystemRelaxing the requirement of a single shared data space, in order to allow for hybrid workflow executions on top of multi-cloud or hybrid cloud/HPC infrastructures.","Python CWL, multi-container, hybrid workflow, hpc",WFMS,yes,
Jupyter Workflow,,Distributed workflows design and execution with Jupyter Notebooks.,no,Active,,Existing Workflow systems,https://jupyter-workflow.di.unito.it/,https://github.com/alpha-unito/jupyter-workflow,https://doi.org/10.1016/j.future.2021.10.007,Academia,,"The Jupyter Workflow framework enables Jupyter Notebooks to describe 
complex workflows and to execute them in a distributed fashion on hybrid
 HPC-Cloud infrastructures. Jupyter Workflow relies on the StreamFlow WMS as its underlying runtime support.","distributed literate workflow, Jupyter notebooks, hybrid cloud, HPC",Jupyter notebooks,yes,
Nnodes,,"Nnodes, a workflow manager for HPC clusters",no,Active,,Existing Workflow systems,https://icui.github.io/nnodes/index.html,https://github.com/icui/nnodes,https://raw.githubusercontent.com/icui/nnodes/main/doc/source/files/poster.pdf,Academia,,Nnodes is a simple workflow manager for Python functions and command line tools. It makes your life easier when running complicated jobs either in your local computer or in a large-scale cluster.,"Python functions, workflow manager, CLI, local, cluster, MPI, process controll,",wfms,yes,
Orchest,,"Build data pipelines, the easy way",yes,Discontinued (04.2023),https://mailchi.mp/orchest/a-new-chapter-for-orchest,Existing Workflow systems,https://www.orchest.io/,https://github.com/orchest/orchest,,Enterprise,https://www.orchest.io/,"No frameworks. No YAML. Just write your data processing code directly in Python, R or Julia.","reproduciblity, orchestration, notebooks, scripts, pipelines, infrastructure, local, cloud pipelines, versioning","data pipelines, workflows",yes,
Wick,Wasmflow,"Wick is a flow-like runtime for building secure, scalable software services out of WebAssembly components.",no,Active,,Existing Workflow systems,https://candle.dev/wick.html,https://github.com/candlecorp/wick,,Enterprise,https://candle.dev/,"Wick is a low-code, flow-like runtime for stitching together WebAssembly components into full applications.","flow-based runtime, WebAssembly, ",WebAssembly,yes,
HyperShell,,"A cross-platform, high-performance computing utility for processing shell commands over a distributed, asynchronous queue.",no,Active,,Existing Workflow systems,https://hyper-shell.readthedocs.io/en/latest/,https://github.com/glentner/hyper-shell,https://doi.org/10.1145/3491418.3535138,Academia,,"HyperShell is an elegant, cross-platform, high-performance computing utility for
processing shell commands over a distributed, asynchronous queue. It is a highly
scalable workflow automation tool for many-task scenarios.
Several tools offer similar functionality but not all together in a single tool with
the user ergonomics we provide. Novel design elements include but are not limited to
(1) cross-platform, (2) client-server design, (3) staggered launch for large scales,
(4) persistent hosting of the server, and optionally (5) a database in-the-loop for
persisting task metadata and automated retries.","cross-pltaform, hpc, shell command distibution, workflow",HPC,yes,
Covalent,,Pythonic tool for running machine-learning/high performance/quantum-computing workflows in heterogenous environments.,maybe,Active,,Existing Workflow systems,https://www.covalent.xyz/,https://github.com/AgnostiqHQ/covalent,https://doi.org/10.5281/zenodo.5903364,Academia,,"Covalent is a Pythonic workflow tool for computational scientists, AI/ML software engineers, and anyone who needs to run experiments on limited or expensive computing resources including quantum computers, HPC clusters, GPU arrays, and cloud services.","workflow, data science, ETL, wfms, data pipeline, ML, orchestration",workflow orchestration,yes,
Icolos,,A workflow manager for structure based post-processing of de novo generated small molecules,no,Archived,"This repository has been archived by the owner on Mar 16, 2023. It is now read-only.",Existing Workflow systems,,https://github.com/MolecularAI/Icolos,https://doi.org/10.26434/chemrxiv-2022-vqbxg,Academia,,"The Icolos tool is a workflow manager for structure-based 
workflows in computational chemistry, that abstracts execution logic 
from implementation as much as possible. Icolos was designed to 
interface with REINVENT,
 and workflows can be called as a component of the scoring function, or 
to postprocess results with more expensive methods. Workflows are 
specified in JSON format (see folder examples). Currently wrapped are a diverse set of tools and internal steps, including docking, QM and MD capabilities.","workflow manager, computational chemistry, workflows","drug discovery, computational chemistry",yes,
dwork,,Distributed Work Scheduling System,no,Inactive,,Existing Workflow systems,,https://github.com/frobnitzem/dwork,,Personal,,Task graph scheduler with a minimalistic API.,,work scheduling,yes,
pmake,,A parallel make with rich target and rule attributes,no,Inactive,,Existing Workflow systems,https://docs.olcf.ornl.gov/software/workflows/pmake.html#workflows-pmake,https://code.ornl.gov/99R/pmake,https://docs.olcf.ornl.gov/software/workflows/pmake.html#workflows-pmake,Academia,,"pmake is a parallel
make developed for use within batch jobs.  A rules.yaml
file specifies extended make-rules with:

multiple input and multiple output filesa resource-set specificationa multi-line shell script that can use variable
substitution (e.g. {mpirun} expands to
{jsrun -g -c ...} on summit).","YAML, parallel make, batch jobs",workflow system,yes,
Texera,,Collaborative Machine-Learning-Centric Data Analytics Using Workflows,maybe,Active,,Existing Workflow systems,https://texera.github.io/,https://github.com/Texera/texera,https://arxiv.org/abs/2212.07096,Academia,,"Texera supports scalable computation and enables advanced AI/ML techniques.
  

  ""Collaboration"" is a key focus, and we enable an experience similar to Google Docs, but for data analytics.","workflow, ML, data analytics, workflow, collaboration",data analytics,yes,
Swif2,,The Scientific Workflow Indefatigable Factotum,no,Inactive (2021),,Existing Workflow systems,https://scicomp.jlab.org/docs/swif2,,,Academia,https://www.jlab.org/,,,scientific workflow,?,
Jobflow,,jobflow is a library for writing computational workflows.,no,Active,,Existing Workflow systems,https://materialsproject.github.io/jobflow/,https://github.com/materialsproject/jobflow,,Consortium,https://materialsproject.org/about,"Jobflow is a free, open-source library for writing and executing workflows. Complex
workflows can be defined using simple python functions and executed locally or on
arbitrary computing resources using the FireWorks workflow manager.","python, workflow, flow, FireWorks",computational workflow,yes,
Balsam,,High throughput workflows and automation for HPC,,Active,,Existing Workflow systems,https://balsam.readthedocs.io/en/latest/,https://github.com/argonne-lcf/balsam,https://doi.org/10.1109/XLOOP49562.2019.00010,Academia,,A unified platform to manage high-throughput workflows across the HPC landscape.,"HPC, workflow",HPC,,
Task Vine,,"Dynamic workflows that run on HPC clusters, GPU clusters, and commercial clouds",no,Active,,Existing Workflow systems,https://ccl.cse.nd.edu/software/taskvine/,https://github.com/cooperative-computing-lab/cctools,,,,"TaskVine is a task scheduler for building large scale data intensive 
dynamic workflows that run on HPC clusters, GPU clusters, and commercial
 clouds.
As tasks access external data sources and produce their own outputs, 
more and more data is pulled into local storage on workers. This data is
 used to accelerate future tasks and avoid re-computing exisiting 
results.  Data gradually grows ""like a vine"" through the cluster.
TaskVine is our third-generation workflow system, built on our twenty 
years of experience creating scalable applications in fields such as 
high energy physics, bioinformatics, molecular dynamics, and machine 
learning.","HPC, workflow","HPC, workflow system",yes,
Globus Compute,,Federated function as a service,no,Active,formerly funcxhttps://funcx.org/globus-compute.html,Existing Workflow systems,https://www.globus.org/,https://github.com/funcx-faas/funcx,https://doi.org/10.1145/3369583.3392683,,,"Globus Compute (formerly funcX) is a high-performance function-as-a-service (FaaS) platform that enables intuitive, flexible, efficient, scalable, and performant remote function execution on existing infrastructure including clouds, clusters, and supercomputers.","FAAS, scalable,  high-performance Faas, container",high performance FAAS,yes,
mkite,,distributed computing platform for high-throughput materials simulations,no,Active,,Existing Workflow systems,https://mkite.org/,https://github.com/mkite-group/mkite_core,https://doi.org/10.48550/arXiv.2301.08841,,,"mkite is a distributed computing platform for high-throughput materials simulations. The main idea of mkite is to decouple job management,and submission, and execution, enabling scaling jobs towards heterogenous computing environments. This infrastructure was designed particularly for the simulation of combinatorial materials spaces, including systems with coupling between different simulation workflows (say, crystal-molecule interfaces).","hpc, job managment, data storage",materials science,yes,
hpcflow,,"Automated simulate, process, archive workflows on HPC systems",no,Inactive,,Existing Workflow systems,https://lightform.org.uk/,https://github.com/LightForm-group/hpcflow,,,,"An automated simulate, process, archive workflow on high performance computing (HPC) systems.","workflow, hpc",scientific workflow,yes,
DVC,Data Version Control,Data Version Control,yes,Active,,Existing Workflow systems,https://dvc.org/doc/start/data-management/data-pipelines,https://github.com/iterative/dvc,,,,"Use pipelines to describe how models and other data artifacts are built, and provide an efficient way to reproduce them. Think ""Makefiles for data and ML projects"" done right.","reproducibility, data, version control",data version control,yes,
ZnFlow,,General purpose framework for computational graphs,no,,,Existing Workflow systems,,https://github.com/zincware/ZnFlow,,,https://zincwarecode.com/,"The ZnFlow package provides a basic structure
 for building computational graphs based on functions or classes. It is 
designed as a lightweight abstraction layer to
learn graph computing.build your own packages on top of it.",,computational graphs,yes,
Globus Flows,,,,Active,,Existing Workflow systems,https://docs.globus.org/api/flows/,,https://doi.org/10.1109/MIC.2011.64,"Academia, Non-Profit",,,,,,