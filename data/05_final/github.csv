id,name,full_name,description,homepage,archived,stargazers_count,updated_at,created_at,forks_count,subscribers_count,contributors_count,release_count,latest_release_name,latest_release_date,license_name,commits_count,latest_tag_name,tags_count,latest_tag_date,latest_event_date,last_commit_date_main,readme,request_date
actionchain,st2,StackStorm/st2,"StackStorm (aka ""IFTTT for Ops"") is event-driven automation for auto-remediation, incident responses, troubleshooting, deployments, and more for DevOps and SREs. Includes rules engine, workflow, 160 integration packs with 6000+ actions (see https://exchange.stackstorm.org) and ChatOps. Installer at https://docs.stackstorm.com/install/index.html",https://stackstorm.com/,False,5576,2023-07-07 13:04:06+00:00,2014-04-23 00:51:34+00:00,715,169,145,69,v3.8.0,2022-11-25 17:12:01+00:00,Apache License 2.0,21537,v3.8.0,70,2022-11-18 06:57:53+00:00,2023-07-07 13:04:06+00:00,2023-04-13 19:13:23+00:00,"[![StackStorm](https://github.com/stackstorm/st2/raw/master/stackstorm_logo.png)](https://www.stackstorm.com)

**StackStorm** is a platform for integration and automation across services and tools, taking actions in response to events. Learn more at [www.stackstorm.com](http://www.stackstorm.com/product).

[![Build Status](https://github.com/StackStorm/st2/actions/workflows/ci.yaml/badge.svg)](https://github.com/StackStorm/st2/actions/workflows/ci.yaml)
[![Packages Build Status](https://circleci.com/gh/StackStorm/st2/tree/master.svg?style=shield)](https://circleci.com/gh/StackStorm/st2)
[![Codecov](https://codecov.io/github/StackStorm/st2/badge.svg?branch=master&service=github)](https://codecov.io/github/StackStorm/st2?branch=master)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1833/badge)](https://bestpractices.coreinfrastructure.org/projects/1833)
![Python 3.6,3.8](https://img.shields.io/badge/python-3.6,%203.8-blue)
[![Apache Licensed](https://img.shields.io/github/license/StackStorm/st2)](LICENSE)
[![Join our community Slack](https://img.shields.io/badge/slack-stackstorm-success.svg?logo=slack)](https://stackstorm.com/community-signup)
[![deb/rpm packages](https://img.shields.io/badge/deb/rpm-Packagecloud-%236366f1)](https://packagecloud.io/StackStorm/)
[![Code Search](https://img.shields.io/badge/code%20search-Sourcegraph-%2300B4F2?logo=sourcegraph)](https://sourcegraph.com/stackstorm)
[![GitHub Discussions](https://img.shields.io/github/discussions/stackstorm/st2)](https://github.com/StackStorm/st2/discussions)
[![Twitter Follow](https://img.shields.io/twitter/follow/StackStorm?style=social)](https://twitter.com/StackStorm/)

---

## TL;DR

* Install Get yourself a clean 64-bit Linux box that fits the [system requirements](https://docs.stackstorm.com/install/system_requirements.html). Run the installer script:

   ```bash
   curl -sSL https://stackstorm.com/packages/install.sh | bash -s -- --user=st2admin --password=Ch@ngeMe
   ```
* Read the docs: [https://docs.stackstorm.com/index.html](https://docs.stackstorm.com/install/index.html)
* Questions? Check out [forum.stackstorm.com](https://forum.stackstorm.com/)
* Or join our [Slack community](https://stackstorm.com/community-signup)

## StackStorm Overview

[![StackStorm 5 min Intro Video](https://cloud.githubusercontent.com/assets/1294734/10356016/16278d0a-6d27-11e5-987d-c8a7629a69ed.png)](https://www.youtube.com/watch?v=pzZws3ftDtA)

### About

StackStorm is a platform for integration and automation across services and tools. It ties together your existing infrastructure and application environment so you can more easily automate that environment -- with a particular focus on taking actions in response to events.

StackStorm helps automate common operational patterns. Some examples are:

* **Facilitated Troubleshooting** - triggering on system failures captured by Nagios, Sensu, New Relic and other monitoring, running a series of diagnostic checks on physical nodes, OpenStack or Amazon instances, and application components, and posting results to a shared communication context, like Slack or JIRA.
* **Automated remediation** - identifying and verifying hardware failure on OpenStack compute node, properly evacuating instances and emailing VM about potential downtime, but if anything goes wrong - freezing the workflow and calling PagerDuty to wake up a human.
* **Continuous deployment** - build and test with Jenkins, provision a new AWS cluster, turn on some traffic with the load balancer, and roll-forth or roll-back based on NewRelic app performance data.

StackStorm helps you compose these and other operational patterns as rules and workflows or actions; and these rules and workflows - the content within the StackStorm platform - are stored *as code* which means they support the same approach to collaboration that you use today for code development and can be shared with the broader open source community via [StackStorm Exchange](https://exchange.stackstorm.org).

### Who is using StackStorm?

See the list of known StackStorm [ADOPTERS.md](/ADOPTERS.md) and [Thought Leaders](https://stackstorm.com/stackstorm-thought-leaders/).

### How it works

#### StackStorm architecture

![StackStorm architecture diagram](https://user-images.githubusercontent.com/597113/92291633-6b5aae00-eece-11ea-912e-3bf977aa3cea.png)

StackStorm plugs into the environment via an extensible set of adapters: sensors and actions.

* **Sensors** are Python plugins for inbound integration that watch for events from external systems and fire a StackStorm trigger when an event happens.

* **Triggers** are StackStorm representations of external events. There are generic triggers (e.g., timers, webhooks) and integration triggers (e.g., Sensu alert, JIRA issue updated). A new trigger type can be defined by writing a sensor plugin.

* **Actions** are StackStorm outbound integrations. There are generic actions (SSH, HTTP request), integrations (OpenStack, Docker, Puppet), or custom actions. Actions are either Python plugins, or any scripts, consumed into StackStorm by adding a few lines of metadata. Actions can be invoked directly by user via CLI, API, or the web UI, or used and called as part of automations - rules and workflows.

* **Rules** map triggers to actions (or to workflows), applying matching criterias and map trigger payload data to action inputs.

* **Workflows** stitch actions together into ""uber-actions"", defining the order, transition conditions, and passing context data from one action to the next. Most automations are multi-step (eg: more than one action). Workflows, just like ""atomic"" actions, are available in the action library, and can be invoked manually or triggered by rules.

* **Packs** are the units of content deployment. They simplify the management and sharing of StackStorm pluggable content by grouping integrations (triggers and actions) and automations (rules and workflows). A growing number of packs is available on the StackStorm Exchange. Users can create their own packs,  share them on GitHub, or submit them to the StackStorm Exchange organization.

* **Audit trail** is the historical list of action executions, manual or automated, and is recorded and stored with full details of triggering context and execution results. It is also captured in audit logs for integrating with external logging and analytical tools: LogStash, Splunk, statsd, or syslog.

StackStorm is a service with modular architecture. It is comprised of loosely coupled microservice components that communicate over a message bus, and scales horizontally to deliver automation at scale. StackStorm has a full REST API, CLI client, and web UI for admins and users to operate it locally or remotely, as well as Python client bindings for developer convenience.

StackStorm is an established project and remains actively developed by a broad community.

## Documentation

Additional documentation, including installation procedures, action/rule/workflow authoring, and how to setup and use triggers/sensors can be found at [https://docs.stackstorm.com](https://docs.stackstorm.com).

## Hacking / Contributing

To set up a development environment and run StackStorm from sources, follow [these instructions](https://docs.stackstorm.com/development/sources.html).

For information on how to contribute, our style guide, coding conventions and more,
please visit the [Development section](https://docs.stackstorm.com/development/index.html)
in our documentation.

## Security

If you believe you found a security issue or a vulnerability, please send a description of it to
our private mailing list at info [at] stackstorm [dot] com.

Once you've submitted an issue, you should receive an acknowledgment from one our of team members
in 48 hours or less. If further action is necessary, you may receive additional follow-up emails.

For more information, please refer to https://docs.stackstorm.com/latest/security.html

## Copyright, License, and Contributor Agreement

Copyright 2020 The StackStorm Authors.
Copyright 2019 Extreme Networks, Inc.
Copyright 2014-2018 StackStorm, Inc.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this work except in compliance with the License. You may obtain a copy of the License in the [LICENSE](LICENSE) file, or at:

[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)

By contributing you agree that these contributions are your own (or approved by your employer) and you grant a full, complete, irrevocable copyright license to all users and developers of the project, present and future, pursuant to the license of the project.
",2023-07-07 15:48:14+00:00
activepapers,activepapers-python,activepapers/activepapers-python,Python edition of ActivePapers,http://www.activepapers.org/python-edition/,False,36,2020-07-23 13:26:32+00:00,2013-11-27 19:23:35+00:00,9,6,3,1,v0.1.3,2014-09-09 16:26:50+00:00,Other,58,v0.1.3,2,2014-09-09 16:26:50+00:00,,2018-09-19 09:57:21+00:00,"ActivePapers is a tool for working with executable papers, which
combine data, code, and documentation in single-file packages,
suitable for publication as supplementary material or on sites such as
[figshare](http://figshare.com).

The ActivePapers Python edition requires Python 2.7 or Python 3.3 to 3.5.
It also relies on the following libraries:

  - NumPy 1.6 or later (http://numpy.scipy.org/)
  - HDF5 1.8.7 or later (http://www.hdfgroup.org/HDF5/)
  - h5py 2.2 or later (http://www.h5py.org/)
  - tempdir 0.6 or later (http://pypi.python.org/pypi/tempdir/)

Installation of ActivePapers.Py:

    python setup.py install

This installs the ActivePapers Python library and the command-line
tool ""aptool"" for managing ActivePapers.

For documentation, see the
[ActivePapers Web site](http://www.activepapers.org/python-edition/).

ActivePapers development takes place
[on Github](http://github.com/activepapers/activepapers-python).

Runnning the tests also requires the [tempdir](https://pypi.python.org/pypi/tempdir/) library and either the 
[nose](http://pypi.python.org/pypi/nose/) or the [pytest](http://pytest.org) testing framework. The recommended way to run the tests is

```
cd tests
./run_all_tests.sh nosetests
```
or
```
cd tests
./run_all_tests.sh py.test
```

This launches the test runner on each test script individually. The simpler approach of simply running `nosetests` or `py.test` in directory `tests` leads to a few test failures because the testing framework's import handling conflicts with the implementation of internal modules in ActivePapers.
",2023-07-07 15:48:18+00:00
adage,adage,yadage/adage,dynamic DAG workflows,,False,50,2023-03-15 13:42:20+00:00,2015-04-22 18:37:50+00:00,5,4,2,3,v0.11.0,2023-02-08 23:09:26+00:00,MIT License,330,v0.11.0,41,2023-02-08 23:09:26+00:00,2023-04-07 07:04:11+00:00,2023-02-08 23:09:26+00:00,"# Adage - A DAG Executor

[![CI](https://github.com/yadage/adage/actions/workflows/ci.yml/badge.svg)](https://github.com/yadage/adage/actions/workflows/ci.yml?query=branch%3Amain)
[![Code Health](https://landscape.io/github/yadage/adage/main/landscape.svg?style=flat)](https://landscape.io/github/yadage/adage/main)
[![PyPI](https://img.shields.io/pypi/v/adage.svg)](https://pypi.python.org/pypi/adage)
[![Coverage Status](https://coveralls.io/repos/github/yadage/adage/badge.svg?branch=main)](https://coveralls.io/github/yadage/adage?branch=main)
[![Documentation Status](https://readthedocs.org/projects/adage/badge/?version=latest)](http://adage.readthedocs.io/en/latest/?badge=latest)

This is a small experimental package to see how one could describe workflows that are not completely known at definition time. Tasks should be runnable both in a multiprocessing pool, or using a number of celery workers or a IPython cluster.

### Example

![example image](./example_workflow.gif ""dynamically extended workflow"")



## Problem

Workflows can be comfortably represented by directed acyclic graphs (DAGs). But sometimes the precise structure of the graph is not known before the processing starts. Instead often one only has partial information of what kind of edges are possible and depending on a certain result in a node the DAG might be appended with more nodes and edges.

For example, one node (call it ""node A"") could be downloading a list of files, which can be processed in parallel. The DAG would therefore have one node for each file-processing (let's call them    ""node_file_1"" to ""node_file_n"") depending on ""node A"". Since the exact number of files is not known until run-time, we cannot map out the DAG beforehand. Also after this ""map""-step one might want to have a ""reduce""-step to merge the individual result. This can also only be scheduled after the number of ""map""-nodes is known.

Another example is that one might have a whole set of nodes that run a certain kind of task (e.g. produce a PDF file). One could imagine wanting to have a ""reduce""-type task which merges all these individual PDF files. While any given node does not know where else PDF-generating tasks are scheduled, one can wait until no edges to PDF-generating tasks are possible anymore to then append a PDF-merging node to the DAG.

## Solution

Generically, we want individual nodes to have a limited set of operations they can do on the DAG that they are part of. Specifically we can only allow queries on the structure of the DAG as well as append operations, nodes must not be able to remove nodes. The way we implement this is that we have a append-only record of scheduled rules. A rule is a pair of functions (predicate,body) that operate on the DAG. The predicate is a query function that inspects the graph to decide whether the DAG has enough information to apply the body (e.g. are edges of a certain type still possible to append or not?). If the DAG does have enough information the body which is an append-only operation on the DAG is applied, i.e. nodes are added . Periodically the list of rules is iterated to extend the DAG where possible.

### Rules for Rules

There are a couple of rule that the rules need to obey themselves in order to make 

- it is the responsibility of the predicate to signal that all necessary nodes for the body are present in the DAG. Examples are:
	- wait until    no nodes of a particular type could possibly be added to the DAG. This requires us to know what kind of edges are valid on a global level.
        - wait until a certain number of nodes are present in the DAG (say )
	- select a certain set of nodes by their unique id (useful to attach a sub-DAG to a existing node from within that node)
	
- the only valid edges that you can dynamically add are ones that point away from existing nodes to new nodes.. edges directed *towards* existing nodes would introduce new dependencies which were not present before and so that job might have already run, or be currently running

",2023-07-07 15:48:22+00:00
adaptivemd,adaptivemd,markovmodel/adaptivemd,A python framework to run adaptive Markov state model (MSM) simulation on HPC resources,,False,18,2022-11-17 05:36:19+00:00,2017-03-09 21:01:46+00:00,5,8,7,1,,,GNU Lesser General Public License v2.1,573,v0.2.2,3,2017-05-26 22:03:23+00:00,,2019-02-25 21:43:43+00:00,"[![Build Status](https://travis-ci.org/jrossyra/adaptivemd.svg?branch=rp_integration)](https://travis-ci.org/jrossyra/adaptivemd)

# AdaptiveMD

A Python framework to run adaptive MD simulations using Markov State Model (MSM)
analysis on HPC resources.

- See below for a simple installation
- Configure & run `install_admd.sh` to deploy on HPC or cluster

The generation of MSMs requires a huge amount of trajectory data to be analyzed.
In most cases this leads to an enhanced understanding of the dynamics of the
system, which can be used to make decisions about collecting more data to
achieve a desired accuracy or level of detail in the generated MSM. This
alternating process between simulation to actively generate new observations
& analysis is currently difficult and involves human decision along the path.

AdaptiveMD aims to simplify this process with the following design goals:

1. Ease of use: Simple system setup once an HPC resource has been added.
2. Flexibility: Modular setup of multiple HPCs and different simulation engines
3. Automatism: Create a user-defined adaptive strategy that is executed
4. Compatibility: Build analysis tools and export to known formats

After installation, you might want to start working with the examples in `examples/tutorials`. You will first learn the basics of how tasks are created and executed, and then more on composing workflows.


## Prerequisites

There are a few components we need to install for `AdaptiveMD` to work. If you are installing in a regular workstation environment, you can follow the instructions below or use the installer script. If you are installing in a cluster or HPC environment, we recommend you use the script `install_admd.sh`.  The instructions here in the README should give you the gist of what's going on in the installer, which sets up the same components but does more configuration of the environment used by`AdaptiveMD`.

AdaptiveMD creates task descriptions that can be executed using a native `worker` object or via the RADICAL-Pilot execution framework. """"""RP install configuration on or off""""""

### MongoDB

`AdaptiveMD` needs access to a MongoDB. If you want to store project data locally
you need to install MongoDB. Both your user machine and compute resource (where tasks are executed) must see a port used by the database.

[MongoDB Community Edition](https://www.mongodb.com/download-center#community)
will provide an installer for your OS, just download and follow the installation instructions. Depending on the compute resource network restrictions, it might be necessary to install the database in different locations for production workflows.

For linux systems:
```bash
curl -O https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-debian81-3.4.2.tgz
tar -zxvf mongodb-linux-x86_64-debian81-3.4.2.tgz
mkdir ~/mongodb
mv mongodb-linux-x86_64-debian81-3.4.2/ ~/mongodb
# add mongodb binaries to PATH in .bashrc
echo ""export PATH=~/mongodb/mongodb-linux-x86_64-debian81-3.4.2/bin/:\$PATH"" >> ~/.bashrc
# create parent directory for database
mkdir -p ~/mongodb/data/db
# run a `mongod` deamon in the background
mongod --quiet --dbpath ~/mongodb/data/db &
```

### Conda

We recommend using contained python environments to run `AdaptiveMD`. The `AdaptiveMD` application environment can be considered separate from the task-execution environment, but for simplicity in these instructions we will load the same environment in tasks that is used when running the application. This means that `AdaptiveMD`, along with the task 'kernels' `OpenMM` and `PyEMMA` are all installed in a single environment.

Conda provides a complete and self-contained python installation that prevents all sorts of problems with software installation & compatibility, so we recommend it to get started. When you move to a cluster or HPC environment, it will likely be a better choice to use `virtualenv` as your environment container. It uses an existing python installation with env-specific libraries, and is thus faster to load and more scalable, but does not resolve installation issues as thoroughly. 

If you do not yet have conda installed, do so using:

```bash
curl -O https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

Add 2 useful channels
```bash
conda config --append channels conda-forge
conda config --append channels omnia
```

`--append` will make sure that the regular conda packages are tried first, then
use `conda-forge` and `omnia` as a fallback.

Install required packages now:

```bash
# be sure the conda python version is fully updated
conda install python
# create a new conda environment for all the installations
conda create -n admdenv python=2.7
source activate admdenv
```

Now installing adaptiveMD related packages. Note you must be inside the python environment you will be working in when installing the packages (or use: `conda install -n [packages...]`):

```bash
# jupyter notebook for tutorials and project work
conda install jupyter
# to prep for adaptivemd install
conda install pyyaml
# for simulations & analysis
# since we're using same env for tasks
conda install pyemma openmm
```

### Install _AdaptiveMD_

Let's get adaptivemd from the github repo now.

```bash
# clone and install adaptivemd 
git clone https://github.com:markovmodel/adaptivemd.git

# go to adativemd and install it
cd adaptivemd/
python setup.py install
#OR
#pip install .
# see if we pass the import test
python -c ""import adaptivemd"" || echo 'FAILED'

# run a simple test
cd adaptivemd/tests/
#FIXME
python test_simple.py
```

`pyemma` and `openmm` should
be installed on the compute resource as well as the local machine. It is
possible to exclude, say, `openmm` from the local install if simulations will
only be run on the resource. 

That's it. Have fun running adaptive simulations.

#### Documentation

To compile the doc pages, clone this github repository, go into the `docs`
folder and do

```bash 
conda install sphinx sphinx_rtd_theme pandoc
make html
```

The HTML pages are in _build/html. Please note that the docs can only be
compiled if all the above mentionend AdaptiveMD dependencies are available.
If you are using conda environments, this means that your AdaptiveMD
environment should be active.
",2023-07-07 15:48:30+00:00
adios,ADIOS2,ornladios/ADIOS2,Next generation of ADIOS developed in the Exascale Computing Program,https://adios2.readthedocs.io/en/latest/index.html,False,215,2023-07-05 11:49:17+00:00,2016-12-06 16:39:55+00:00,114,30,54,19,v2.9.0,2023-03-30 20:01:59+00:00,Apache License 2.0,9303,v2.9.0,20,2023-03-30 20:01:59+00:00,2023-07-07 15:35:12+00:00,2023-07-05 21:53:58+00:00,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Documentation](https://readthedocs.org/projects/adios2/badge/?version=latest)](https://adios2.readthedocs.io/en/latest/?badge=latest)

[![GitHub (pre-)release](https://img.shields.io/github/release/ornladios/adios2/all.svg)]()
[![Spack Version](https://img.shields.io/spack/v/adios2.svg)](https://spack.readthedocs.io/en/latest/package_list.html#adios2)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/adios2)](https://anaconda.org/conda-forge/adios2)

[![Circle CI](https://circleci.com/gh/ornladios/ADIOS2.svg?style=shield)](https://circleci.com/gh/ornladios/ADIOS2)
[![Travis CI](https://api.travis-ci.com/ornladios/ADIOS2.svg)](https://travis-ci.com/ornladios/ADIOS2)
[![AppVeyor CI](https://ci.appveyor.com/api/projects/status/0s2a3qp57hgbvlhj?svg=true)](https://ci.appveyor.com/project/ornladios/adios2)

[![Coverity Scan Build Status](https://scan.coverity.com/projects/11116/badge.svg)](https://scan.coverity.com/projects/ornladios-adios2)


# ADIOS2 : The Adaptable Input Output System version 2

This is ADIOS2: The Adaptable Input/Output (I/O) System.

ADIOS2 is developed as part of the United States Department of Energy's Exascale Computing Project.
It is a framework for scientific data I/O to publish and subscribe to data when and where required.

ADIOS2 transports data as groups of self-describing variables and attributes across different media types (such as files, wide-area-networks, and remote direct memory access) using a common application programming interface for all transport modes.
ADIOS2 can be used on supercomputers, cloud systems, and personal computers.

ADIOS2 focuses on:

1. **Performance** I/O scalability in high performance computing (HPC) applications.
2. **Adaptability** unified interfaces to allow for several modes of transport (files, memory-to-memory)  
3. **Ease of Use** two-level application programming interface (APIs)
* Full APIs for HPC applications: C++11, Fortran 90, C 99, Python 2 and 3
* Simplified High-Level APIs for data analysis: Python 2 and 3, C++11, Matlab

In addition, ADIOS2 APIs are based on:

* **MPI** Although ADIOS2 is MPI-based, it can also be used in non-MPI serial code.

* **Data Groups** ADIOS2 favors a deferred/prefetch/grouped variables transport mode by default to maximize data-per-request ratios.
Sync mode, one variable at a time, is treated as the special case.

* **Data Steps** ADIOS2 follows the actual production/consumption of data using an I/O “steps” abstraction removing the need to manage extra indexing information.

* **Data Engines** ADIOS2 Engine abstraction allows for reusing the APIs for different transport modes removing the need for drastic code changes.

## Documentation

Documentation is hosted at [readthedocs](https://adios2.readthedocs.io).

## Citing

If you find ADIOS2 useful, please cite our [SoftwareX paper](https://doi.org/10.1016/j.softx.2020.100561), which also gives a high-level overview to the motivation and goals of ADIOS; complementing the documentation.

## Getting ADIOS2

* From source: [Install ADIOS2 documentation](https://adios2.readthedocs.io/en/latest/setting_up/setting_up.html#).
For a `cmake` configuration example see [scripts/runconf/runconf.sh](https://github.com/ornladios/ADIOS2/blob/master/scripts/runconf/runconf.sh)


* Conda packages:
* [https://anaconda.org/conda-forge/adios2](https://anaconda.org/conda-forge/adios2)


* Spack: [adios2 package](https://spack.readthedocs.io/en/latest/package_list.html#adios2)


* Docker images: under [scripts/docker](https://github.com/ornladios/ADIOS2/tree/master/scripts/docker)


Once ADIOS2 is installed refer to: 

* [Linking ADIOS2](https://adios2.readthedocs.io/en/latest/setting_up/setting_up.html#linking-adios-2)


## Releases

* Latest release: [v2.9.0](https://github.com/ornladios/ADIOS2/releases/tag/v2.9.0)

* Previous releases: [https://github.com/ornladios/ADIOS2/releases](https://github.com/ornladios/ADIOS2/releases)

## Reporting Bugs

If you find a bug, please open an [issue on ADIOS2 github repository](https://github.com/ornladios/ADIOS2/issues)

## Contributing

See the [Contributor's Guide to ADIOS 2](Contributing.md) for instructions on how to contribute.

## License
ADIOS2 is licensed under the Apache License v2.0.
See the accompanying [Copyright.txt](Copyright.txt) for more details.


## Directory layout
* bindings - public application programming interface, API, language bindings (C++11, C, Fortran, Python and Matlab)

* cmake - Project specific CMake modules

* examples - Simple set of examples in different languages

* scripts - Project maintenance and development scripts

* source - Internal source code for private components 
* adios2 - source directory for the ADIOS2 library to be installed under install-dir/lib/libadios2.
* utils  - source directory for the binary utilities, to be installed under install-dir/bin

* testing - Tests using [gtest](https://github.com/google/googletest)
",2023-07-07 15:48:34+00:00
aiida,aiida-core,aiidateam/aiida-core,The official repository for the AiiDA code,https://aiida-core.readthedocs.io,False,366,2023-06-25 02:50:59+00:00,2016-12-23 15:38:21+00:00,163,26,75,27,v2.4.0,2023-06-23 03:45:12+00:00,Other,11522,v2.4.0,91,2023-06-22 11:54:43+00:00,2023-07-07 15:39:58+00:00,2023-07-07 15:39:55+00:00,"# <img src=""https://raw.githubusercontent.com/aiidateam/aiida-core/main/docs/source/images/aiida-logo.svg"" alt=""AiiDA"" width=""200""/>

AiiDA (www.aiida.net) is a workflow manager for computational science with a strong focus on provenance, performance and extensibility.

|    | |
|-----|----------------------------------------------------------------------------|
|Latest release| [![PyPI version](https://badge.fury.io/py/aiida-core.svg)](https://badge.fury.io/py/aiida-core) [![conda-forge](https://img.shields.io/conda/vn/conda-forge/aiida-core.svg?style=flat)](https://anaconda.org/conda-forge/aiida-core) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/aiida-core.svg)](https://pypi.python.org/pypi/aiida-core/) |
|Getting help| [![Docs status](https://readthedocs.org/projects/aiida-core/badge)](http://aiida-core.readthedocs.io/) [![Google Group](https://img.shields.io/badge/-Google%20Group-lightgrey.svg)](https://groups.google.com/forum/#!forum/aiidausers)
|Build status| [![Build Status](https://github.com/aiidateam/aiida-core/actions/workflows/ci-code.yml/badge.svg)](https://github.com/aiidateam/aiida-core/actions) [![Coverage Status](https://codecov.io/gh/aiidateam/aiida-core/branch/main/graph/badge.svg)](https://codecov.io/gh/aiidateam/aiida-core) [Benchmarks](https://aiidateam.github.io/aiida-core/dev/bench/ubuntu-22.04/psql_dos/) |
|Activity| [![PyPI-downloads](https://img.shields.io/pypi/dm/aiida-core.svg?style=flat)](https://pypistats.org/packages/aiida-core) [![Commit Activity](https://img.shields.io/github/commit-activity/m/aiidateam/aiida-core.svg)](https://github.com/aiidateam/aiida-core/pulse)
|Community| [![Affiliated with NumFOCUS](https://img.shields.io/badge/NumFOCUS-affiliated%20project-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/sponsored-projects/affiliated-projects) [![Twitter](https://img.shields.io/twitter/follow/aiidateam.svg?style=social&label=Follow)](https://twitter.com/aiidateam)


## Features

 -   **Workflows:** Write complex, auto-documenting workflows in
     python, linked to arbitrary executables on local and remote
     computers. The event-based workflow engine supports tens of
     thousands of processes per hour with full checkpointing.
 -   **Data provenance:** Automatically track inputs, outputs & metadata
     of all calculations in a provenance graph for full
     reproducibility. Perform fast queries on graphs containing
     millions of nodes.
 -   **HPC interface:** Move your calculations to a different computer
     by changing one line of code. AiiDA is compatible with schedulers
     like [SLURM](https://slurm.schedmd.com), [PBS
     Pro](https://www.pbspro.org/),
     [torque](http://www.adaptivecomputing.com/products/torque/),
     [SGE](http://gridscheduler.sourceforge.net/) or
     [LSF](https://www.ibm.com/support/knowledgecenter/SSETD4/product_welcome_platform_lsf.html)
     out of the box.
 -   **Plugin interface:** Extend AiiDA with [plugins](https://aiidateam.github.io/aiida-registry/) for new simulation codes (input generation & parsing), data types, schedulers, transport modes and more.
 -   **Open Science:** Export subsets of your provenance graph and share them with peers or make them available online for everyone
     on the [Materials Cloud](https://www.materialscloud.org).
 -   **Open source:** AiiDA is released under the [MIT open source license](LICENSE.txt)

## Installation

Please see AiiDA's [documentation](https://aiida-core.readthedocs.io/en/latest/).

## How to contribute [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![GitHub issues by-label](https://img.shields.io/github/issues/aiidateam/aiida-core/good%20first%20issue)](https://github.com/aiidateam/aiida-core/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)

The AiiDA team appreciates help from a wide range of different backgrounds.
Small improvements of the documentation or minor bug fixes are always welcome.

Please see the [Contributor wiki](https://github.com/aiidateam/aiida-core/wiki) on how to get started.

## Frequently Asked Questions

If you are experiencing problems with your AiiDA installation, please refer to the [FAQ page of the documentation](https://aiida-core.readthedocs.io/en/latest/howto/faq.html).

## How to cite

If you use AiiDA in your research, please consider citing the following publications:

 * **AiiDA >= 1.0**: S. P. Huber *et al.*, *AiiDA 1.0, a scalable computational infrastructure for automated reproducible workflows and data provenance*, Scientific Data **7**, 300 (2020); DOI: [10.1038/s41597-020-00638-4](https://doi.org/10.1038/s41597-020-00638-4)
 * **AiiDA >= 1.0**: M. Uhrin *et al.*, *Workflows in AiiDA: Engineering a high-throughput, event-based engine for robust and modular computational workflows*, Computational Materials Science **187**, 110086 (2021); DOI: [10.1016/j.commatsci.2020.110086](https://doi.org/10.1016/j.commatsci.2020.110086)
 * **AiiDA < 1.0**: Giovanni Pizzi, Andrea Cepellotti, Riccardo Sabatini, Nicola Marzari,and Boris Kozinsky, *AiiDA: automated interactive infrastructure and database for computational science*, Computational Materials Science **111**, 218-230 (2016); DOI: [10.1016/j.commatsci.2015.09.013](https://doi.org/10.1016/j.commatsci.2015.09.013)

## License

AiiDA is distributed under the MIT open source license (see [`LICENSE.txt`](LICENSE.txt)).
For a list of other open source components included in AiiDA, see [`open_source_licenses.txt`](open_source_licenses.txt).

## Acknowledgements

AiiDA is a [NumFOCUS Affiliated Project](https://www.numfocus.org) and supported by the [MARVEL National Centre of Competence in Research](http://www.marvel-nccr.ch), the [MaX European Centre of Excellence](http://www.max-centre.eu) and by a number of other supporting projects, partners and institutions, whose complete list is available on the [AiiDA website acknowledgements page](http://www.aiida.net/acknowledgements/).
",2023-07-07 15:48:38+00:00
apacheairavata,airavata,apache/airavata,A general purpose Distributed Systems Framework,https://airavata.apache.org/,False,89,2023-06-21 10:39:20+00:00,2014-01-29 08:00:06+00:00,116,27,46,0,,,Apache License 2.0,10100,airavata-0.19,18,2020-09-05 08:58:30+00:00,2023-07-03 19:46:36+00:00,2023-06-29 17:23:20+00:00,"# Apache Airavata

[![Build Status](https://travis-ci.org/apache/airavata.svg?branch=master)](https://travis-ci.org/apache/airavata)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.airavata/airavata/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.airavata%22)

## About

Apache Airavata is a software framework for executing and managing computational
jobs on distributed computing resources including local clusters,
supercomputers, national grids, academic and commercial clouds. Airavata builds
on general concepts of service oriented computing, distributed messaging, and
workflow composition and orchestration. Airavata bundles a server package with
an API, client software development Kits and a general purpose reference UI
implementation -
[Apache Airavata Django Portal](https://github.com/apache/airavata-django-portal).

Learn more about Airavata at
[https://airavata.apache.org](https://airavata.apache.org).

## Building Apache Airavata

### Prerequisites

- Sources compilation requires Java SDK 11.
- The project is built with Apache Maven 3+.
- Set or export JAVA_HOME to point to JDK. For example in Ubuntu:
`export JAVA_HOME=/usr/lib/jvm/adoptopenjdk-11`
- Git

### Build the distribution

    git clone https://github.com/apache/airavata.git
    cd airavata
    mvn clean install

To build without running tests, use `mvn clean install -Dmaven.test.skip=true`.
The compressed binary distribution is created at
PROJECT_DIR/modules/distribution/target.

### Build and run docker distribution (Experimental and not recommended for production deployments)

* This requires docker and docker-compose installed in your system

* Build the source and docker images for each microservice
```
    git clone https://github.com/apache/airavata.git
    cd airavata
    mvn clean install 
    mvn docker:build -pl modules/distribution
```
* Start supporting services and Airavata miroservices (API Server, Helix Components and the Job Monitors)
```
     docker-compose -f modules/ide-integration/src/main/containers/docker-compose.yml -f modules/distribution/src/main/docker/docker-compose.yml up
```

* Django portal and PGA Portal can be pointed to airavata.host:8930 (API) , airavata.host:8962 (Profile Service), airavata.host:8443 (Keycloak). 
Make sure that you add a host entry that maps airavata.host -> 127.0.0.1

* To stop all the services
```
    docker-compose -f modules/ide-integration/src/main/containers/docker-compose.yml -f modules/distribution/src/main/docker/docker-compose.yml down
```

* If you do any code change and need to reflect them in the deployment, stop the docker deployment, rebuild docker images and start the docker deployment
 
 
## Getting Started

The easiest way to get started with running Airavata locally and setting up a
development environment is to follow the instructions in the
[ide-integration README](./modules/ide-integration/README.md). Those
instructions will guide you on setting up a development environment with
IntelliJ IDEA.

## Contact

For additional information about Apache Airavata, please contact the user or dev
mailing lists: https://airavata.apache.org/mailing-list.html

## Contributing

Want to help contribute to the development of Apache Airavata? Check out our
[contributing documentation](http://airavata.apache.org/get-involved.html).

## Links

- [Documentation](https://docs.airavata.org/en/master/)
- Developer [wiki](https://cwiki.apache.org/confluence/display/AIRAVATA)
- [Issue Tracker](https://issues.apache.org/jira/projects/AIRAVATA)

## License

Licensed to the Apache Software Foundation (ASF) under one or more contributor
license agreements. See the NOTICE file distributed with this work for
additional information regarding copyright ownership. The ASF licenses this file
to you under the Apache License, Version 2.0 (the ""License""); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

Please see the [LICENSE](LICENSE) file included in the root directory of the
source tree for extended license details.
",2023-07-07 15:48:42+00:00
apacheairflow,airflow,apache/airflow,"Apache Airflow - A platform to programmatically author, schedule, and monitor workflows",https://airflow.apache.org/,False,30866,2023-07-07 15:10:19+00:00,2015-04-13 18:04:58+00:00,12555,760,418,67,helm-chart/1.10.0,2023-06-27 13:45:01+00:00,Apache License 2.0,20312,v1.8.2,3375,2017-10-09 11:45:41+00:00,2023-07-07 15:10:20+00:00,2023-07-07 14:18:40+00:00,"<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 ""License""); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
-->

# Apache Airflow

[![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)
[![GitHub Build](https://github.com/apache/airflow/workflows/CI%20Build/badge.svg)](https://github.com/apache/airflow/actions)
[![Coverage Status](https://codecov.io/github/apache/airflow/coverage.svg?branch=main)](https://app.codecov.io/gh/apache/airflow/branch/main)
[![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)
[![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)
[![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Twitter Follow](https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&label=Follow)](https://twitter.com/ApacheAirflow)
[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)
[![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors)
[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/6)](https://ossrank.com/p/6)

[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.

When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.

Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of contents**

- [Project Focus](#project-focus)
- [Principles](#principles)
- [Requirements](#requirements)
- [Getting started](#getting-started)
- [Installing from PyPI](#installing-from-pypi)
- [Official source code](#official-source-code)
- [Convenience packages](#convenience-packages)
- [User Interface](#user-interface)
- [Semantic versioning](#semantic-versioning)
- [Version Life Cycle](#version-life-cycle)
- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)
- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)
- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)
- [Contributing](#contributing)
- [Who uses Apache Airflow?](#who-uses-apache-airflow)
- [Who Maintains Apache Airflow?](#who-maintains-apache-airflow)
- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)
- [Airflow merchandise](#airflow-merchandise)
- [Links](#links)
- [Sponsors](#sponsors)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## Project Focus

Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).

Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.

Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.

## Principles

- **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.
- **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.
- **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful **Jinja** templating engine.
- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.

## Requirements

Apache Airflow is tested with:

|             | Main version (dev)           | Stable version (2.6.2)    |
|-------------|------------------------------|---------------------------|
| Python      | 3.8, 3.9, 3.10, 3.11         | 3.7, 3.8, 3.9, 3.10, 3.11 |
| Platform    | AMD64/ARM64(\*)              | AMD64/ARM64(\*)           |
| Kubernetes  | 1.23, 1.24, 1.25, 1.26, 1.27 | 1.23, 1.24, 1.25, 1.26    |
| PostgreSQL  | 11, 12, 13, 14, 15           | 11, 12, 13, 14, 15        |
| MySQL       | 5.7, 8                       | 5.7, 8                    |
| SQLite      | 3.15.0+                      | 3.15.0+                   |
| MSSQL       | 2017(\*), 2019(\*)           | 2017(\*), 2019(\*)        |

\* Experimental

**Note**: MySQL 5.x versions are unable to or have limitations with
running multiple schedulers -- please see the [Scheduler docs](https://airflow.apache.org/docs/apache-airflow/stable/scheduler.html).
MariaDB is not tested/recommended.

**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend
using the latest stable version of SQLite for local development.

**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development it is regularly
tested on fairly modern Linux Distros and recent versions of MacOS.
On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.
The work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388) but
it is not a high priority. You should only use Linux-based distros as ""Production"" execution environment
as this is the only environment that is supported. The only distro that is used in our CI tests and that
is used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is
`Debian Bullseye`.

## Getting started

Visit the official Airflow website documentation (latest **stable** release) for help with
[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation.html),
[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking
through a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).

> Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).

For more information on Airflow Improvement Proposals (AIPs), visit
the [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).

Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).

## Installing from PyPI

We publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky
because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and
applications usually pin them, but we should do neither and both simultaneously. We decided to keep
our dependencies as open as possible (in `setup.py`) so users can install different versions of libraries
if needed. This means that `pip install apache-airflow` will not work from time to time or will
produce unusable Airflow installation.

To have repeatable installation, however, we keep a set of ""known-to-be-working"" constraint
files in the orphan `constraints-main` and `constraints-2-0` branches. We keep those ""known-to-be-working""
constraints files separately per major/minor Python version.
You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify
correct Airflow tag/version/branch and Python versions in the URL.


1. Installing just Airflow:

> Note: Only `pip` installation is currently officially supported.

While it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or
[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as
`pip` - especially when it comes to constraint vs. requirements management.
Installing via `Poetry` or `pip-tools` is not currently supported.

There are known issues with ``bazel`` that might lead to circular dependencies when using it to install
Airflow. Please switch to ``pip`` if you encounter such problems. ``Bazel`` community works on fixing
the problem in `this PR <https://github.com/bazelbuild/rules_python/pull/1166>`_ so it might be that
newer versions of ``bazel`` will handle it.

If you wish to install Airflow using those tools, you should use the constraint files and convert
them to the appropriate format and workflow that your tool requires.


```bash
pip install 'apache-airflow==2.6.2' \
 --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.6.2/constraints-3.8.txt""
```

2. Installing with extras (i.e., postgres, google)

```bash
pip install 'apache-airflow[postgres,google]==2.6.2' \
 --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.6.2/constraints-3.8.txt""
```

For information on installing provider packages, check
[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).

## Official source code

Apache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,
and our official source code releases:

- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)
- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)
- Are cryptographically signed by the release manager
- Are officially voted on by the PMC members during the
  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)

Following the ASF rules, the source packages released must be sufficient for a user to build and test the
release provided they have access to the appropriate platform and tools.

## Convenience packages

There are other ways of installing and using Airflow. Those are ""convenience"" methods - they are
not ""official releases"" as stated by the `ASF Release Policy`, but they can be used by the users
who do not want to build the software themselves.

Those are - in the order of most common ways people install Airflow:

- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool
- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via
  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can
  read more about using, customising, and extending the images in the
  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and
  learn details on the internals in the [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst) document.
- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that
  were used to generate official source packages via git

All those artifacts are not official releases, but they are prepared using officially released sources.
Some of those artifacts are ""development"" or ""pre-release"" ones, and they are clearly marked as such
following the ASF Policy.

## User Interface

- **DAGs**: Overview of all DAGs in your environment.

  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)

- **Grid**: Grid representation of a DAG that spans across time.

  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/grid.png)

- **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.

  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)

- **Task Duration**: Total time spent on different tasks over time.

  ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)

- **Gantt**: Duration and overlap of a DAG.

  ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)

- **Code**: Quick way to view source code of a DAG.

  ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)

## Semantic versioning

As of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.

There are few specific rules that we agreed to that define details of versioning of the different
packages:

* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).
  Changing limits for versions of Airflow dependencies is not a breaking change on its own.
* **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.
  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.
  For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed
  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,
  they are present in providers as `install_requires` limitations. We aim to keep backwards
  compatibility of providers with all previously released Airflow 2 versions but
  there will sometimes be breaking changes that might make some, or all
  providers, have minimum Airflow version specified. Change of that minimum supported Airflow version
  is a breaking change for provider because installing the new provider might automatically
  upgrade Airflow (which might be an undesired side effect of upgrading provider).
* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR
  versions for the chart are independent from the Airflow version. We aim to keep backwards
  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might
  only work starting from specific Airflow releases. We might however limit the Helm
  Chart to depend on minimal Airflow version.
* **Airflow API clients**: SemVer MAJOR and MINOR versions follow MAJOR and MINOR versions of Airflow.
  The first MAJOR or MINOR X.Y.0 release of Airflow should always be followed by X.Y.0 release of
  all clients. An airflow PATCH X.Y.Z release can be followed by a PATCH release of API clients, only
  if this PATCH is relevant to the clients.
  The clients then can release their own PATCH releases with bugfixes, independently of Airflow PATCH releases.
  As a consequence, each API client will have its own PATCH version that may or may not be in sync with the Airflow
  PATCH version. For a specific MAJOR/MINOR Airflow version, users should favor the latest PATCH version of clients
  independently of their Airflow PATCH version.

## Version Life Cycle

Apache Airflow version life cycle:

<!-- This table is automatically updated by pre-commit scripts/ci/pre_commit/pre_commit_supported_versions.py -->
<!-- Beginning of auto-generated table -->

| Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   |
|-----------|-----------------------|-----------|-----------------|-------------------|------------------|
| 2         | 2.6.2                 | Supported | Dec 17, 2020    | TBD               | TBD              |
| 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    |
| 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     |
| 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     |
| 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |

<!-- End of auto-generated table -->

Limited support versions will be supported with security and critical bug fix only.
EOL versions will not get any fixes nor support.
We always recommend that all users run the latest available minor release for whatever major version is in use.
We **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.

## Support for Python and Kubernetes versions

As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.
They are based on the official release schedule of Python and Kubernetes, nicely summarized in the
[Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and
[Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).

1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a
   version stays supported by Airflow if two major cloud providers still provide support for it. We drop
   support for those EOL versions in main right after EOL date, and it is effectively removed when we release
   the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.8 it
   means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of
   Airflow released after will not have it.

2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we
   make them work in our CI pipeline (which might not be immediate due to dependencies catching up with
   new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.

3. This policy is best-effort which means there may be situations where we might terminate support earlier
   if circumstances require it.

## Base OS support for reference Airflow images

The Airflow Community provides conveniently packaged container images that are published whenever
we publish an Apache Airflow release. Those images contain:

* Base OS with necessary packages to install Airflow (stable Debian OS)
* Base Python installation in versions supported at the time of release for the MINOR version of
  Airflow released (so there could be different versions for 2.3 and 2.2 line for example)
* Libraries required to connect to supported Databases (again the set of databases supported depends
  on the MINOR version of Airflow.
* Predefined set of popular providers (for details see the [Dockerfile](https://raw.githubusercontent.com/apache/airflow/main/Dockerfile)).
* Possibility of building your own, custom image where the user can choose their own set of providers
  and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))
* In the future Airflow might also support a ""slim"" version without providers nor database clients installed

The version of the base OS image is the stable version of Debian. Airflow supports using all currently active
stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for
building and testing the OS version. Approximately 6 months before the end-of-life of a previous stable
version of the OS, Airflow switches the images released to use the latest supported version of the OS.
For example since ``Debian Buster`` end-of-life was August 2022, Airflow switched the images in `main` branch
to use ``Debian Bullseye`` in February/March 2022. The version was used in the next MINOR release after
the switch happened. In case of the Bullseye switch - 2.3.0 version used ``Debian Bullseye``.
The images released  in the previous MINOR version continue to use the version that all other releases
for the MINOR version used.

Support for ``Debian Buster`` image was dropped in August 2022 completely and everyone is expected to
stop building their images using ``Debian Buster``.

Users will continue to be able to build their images using stable Debian releases until the end of life and
building and verifying of the images happens in our CI but no unit tests were executed using this image in
the `main` branch.

## Approach to dependencies of Airflow

Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,
therefore our policies to dependencies has to include both - stability of installation of application,
but also ability to install newer version of dependencies for those users who develop DAGs. We developed
the approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while
we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound
version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is
needed because of importance of the dependency as well as risk it involves to upgrade specific dependency.
We also upper-bound the dependencies that we know cause problems.

The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies
automatically (providing that all the tests pass). Our `main` build failures will indicate in case there
are versions of dependencies that break our tests - indicating that we should either upper-bind them or
that we should fix our code/tests to account for the upstream changes from those dependencies.

Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have
a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the
binding.

### Approach for dependencies for Airflow Core

Those `extras` and `providers` dependencies are maintained in `setup.cfg`.

There are few dependencies that we decided are important enough to upper-bound them by default, as they are
known to follow predictable versioning scheme, and we know that new versions of those are very likely to
bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of
the dependencies as they are released, but this is manual process.

The important dependencies are:

* `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and
   introduce breaking changes especially that support for different Databases varies and changes at
   various speed (example: SQLAlchemy 1.4 broke MSSQL integration for Airflow)
* `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed
   together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version
* `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask
   are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense
* `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask
   libraries, and we should update them together
* `celery`: Celery is crucial component of Airflow as it used for CeleryExecutor (and similar). Celery
   [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so
   we should upper-bound it to the next MAJOR version. Also when we bump the upper version of the library,
   we should make sure Celery Provider minimum Airflow version is updated).
* `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor
   (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),
   so we should upper-bound it to the next MAJOR version. Also when we bump the upper version of the library,
   we should make sure Kubernetes Provider minimum Airflow version is updated.

### Approach for dependencies in Airflow Providers and extras

The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of
providers that extend the core functionality and are released separately, even if we keep them (for now)
in the same monorepo for convenience. You can read more about the providers in the
[Providers documentation](https://airflow.apache.org/docs/apache-airflow-providers/index.html). We also
have set of policies implemented for maintaining and releasing community-managed providers as well
as the approach for community vs. 3rd party providers in the [providers](PROVIDERS.rst) document.

Those `extras` and `providers` dependencies are maintained in `provider.yaml` of each provider.

By default, we should not upper-bound dependencies for providers, however each provider's maintainer
might decide to add additional limits (and justify them with comment).

## Contributing

Want to help build Apache Airflow? Check out our [contributing documentation](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst).

Official Docker (container) images for Apache Airflow are described in [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst).

## Who uses Apache Airflow?

More than 400 organizations are using Apache Airflow
[in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).

## Who Maintains Apache Airflow?

Airflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),
but the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)
are responsible for reviewing and merging PRs as well as steering conversations around new feature requests.
If you would like to become a maintainer, please review the Apache Airflow
[committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).

## Can I use the Apache Airflow logo in my presentation?

Yes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up to date logos are found in [this repo](/docs/apache-airflow/img/logos) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).

## Airflow merchandise

If you would love to have Apache Airflow stickers, t-shirt, etc. then check out
[Redbubble Shop](https://www.redbubble.com/i/sticker/Apache-Airflow-by-comdev/40497530.EJUG5).

## Links

- [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)
- [Chat](https://s.apache.org/airflow-slack)

## Sponsors

The CI infrastructure for Apache Airflow has been sponsored by:

<!-- Ordered by most recently ""funded"" -->

<a href=""https://astronomer.io""><img src=""https://assets2.astronomer.io/logos/logoForLIGHTbackground.png"" alt=""astronomer.io"" width=""250px""></a>
<a href=""https://aws.amazon.com/opensource/""><img src=""docs/integration-logos/aws/AWS-Cloud-alt_light-bg@4x.png"" alt=""AWS OpenSource"" width=""130px""></a>
",2023-07-07 15:48:47+00:00
alink,Alink,alibaba/Alink,"Alink is the Machine Learning algorithm platform based on Flink, developed by the PAI team of Alibaba computing platform. ",,False,3370,2023-07-05 07:16:41+00:00,2018-09-30 06:36:11+00:00,787,141,16,21,v1.6.1,2023-03-15 03:17:04+00:00,Apache License 2.0,269,v1.6.1,105,2023-03-15 03:17:04+00:00,2023-07-05 07:16:41+00:00,2023-04-12 07:22:31+00:00,"<font size=7>[English](README.en-US.md)| 简体中文</font>

# Alink

 Alink是基于Flink的通用算法平台,由阿里巴巴计算平台PAI团队研发,欢迎大家加入Alink开源用户钉钉群进行交流。
 
 
<div align=center>
<img src=""https://img.alicdn.com/tfs/TB1kQU0sQY2gK0jSZFgXXc5OFXa-614-554.png"" height=""25%"" width=""25%"">
</div>

- Alink组件列表：http://alinklab.cn/manual/index.html
- Alink教程：http://alinklab.cn/tutorial/index.html
- Alink插件下载器：https://www.yuque.com/pinshu/alink_guide/plugin_downloader

#### Alink教程
<div align=center>
<img src=""https://img.alicdn.com/imgextra/i2/O1CN01Z7sbCr1Hg22gLIsdk_!!6000000000786-0-tps-1280-781.jpg"" height=""50%"" width=""50%"">
</div>

- Alink教程（Java版）：http://alinklab.cn/tutorial/book_java.html
- Alink教程（Python版）：http://alinklab.cn/tutorial/book_python.html
- 源代码地址：https://github.com/alibaba/Alink/tree/master/tutorial
- Java版的数据和资料链接：http://alinklab.cn/tutorial/book_java_00_reference.html
- Python版的数据和资料链接：http://alinklab.cn/tutorial/book_python_00_reference.html
- Alink教程(Java版)代码的运行攻略  http://alinklab.cn/tutorial/book_java_00_code_help.html
- Alink教程(Python版)代码的运行攻略  http://alinklab.cn/tutorial/book_python_00_code_help.html

#### 开源算法列表

<div align=center>
<img src=""https://img.alicdn.com/imgextra/i2/O1CN01RKHbLE202moQzvYjW_!!6000000006792-2-tps-1876-955.png"" height=""100%"" width=""100%"">
</div>

#### PyAlink 使用截图

<div align=center>
<img src=""https://img.alicdn.com/tfs/TB1TmKloAL0gK0jSZFxXXXWHVXa-2070-1380.png"" height=""60%"" width=""60%"">
</div>

# 快速开始

## PyAlink 使用介绍

### 使用前准备：
---------

#### 包名和版本说明：

  - PyAlink 根据 Alink 所支持的 Flink 版本提供不同的 Python 包：
其中，`pyalink` 包对应为 Alink 所支持的最新 Flink 版本，当前为 1.13，而 `pyalink-flink-***` 为旧版本的 Flink 版本，当前提供 `pyalink-flink-1.12`, `pyalink-flink-1.11`, `pyalink-flink-1.10` 和 `pyalink-flink-1.9`。
  - Python 包的版本号与 Alink 的版本号一致，例如`1.6.1`。

####安装步骤：
1. 确保使用环境中有Python3，版本限于 3.6，3.7 和 3.8。
2. 确保使用环境中安装有 Java 8。
3. 使用 pip 命令进行安装：
  `pip install pyalink`、`pip install pyalink-flink-1.12`、`pip install pyalink-flink-1.11`、`pip install pyalink-flink-1.10` 或者 `pip install pyalink-flink-1.9`。
  
#### 安装注意事项：

1. `pyalink` 和 `pyalink-flink-***` 不能同时安装，也不能与旧版本同时安装。
如果之前安装过 `pyalink` 或者 `pyalink-flink-***`，请使用`pip uninstall pyalink` 或者 `pip uninstall pyalink-flink-***` 卸载之前的版本。
2. 出现`pip`安装缓慢或不成功的情况，可以参考[这篇文章](https://segmentfault.com/a/1190000006111096)修改pip源，或者直接使用下面的链接下载 whl 包，然后使用 `pip` 安装：
   - Flink 1.13：[链接](https://alink-release.oss-cn-beijing.aliyuncs.com/v1.6.1/pyalink-1.6.1-py3-none-any.whl) (MD5: a10d57a19c53d206d324273f377a1b13)
   - Flink 1.12：[链接](https://alink-release.oss-cn-beijing.aliyuncs.com/v1.6.1/pyalink_flink_1.12-1.6.1-py3-none-any.whl) (MD5: 82b2395740fbd960895d16350266ab4d)
   - Flink 1.11：[链接](https://alink-release.oss-cn-beijing.aliyuncs.com/v1.6.1/pyalink_flink_1.11-1.6.1-py3-none-any.whl) (MD5: 5bf901c084b51ebfa13a62489fafc2f2)
   - Flink 1.10：[链接](https://alink-release.oss-cn-beijing.aliyuncs.com/v1.6.1/pyalink_flink_1.10-1.6.1-py3-none-any.whl) (MD5: e18c620a3a3423407973b8c3d23a02e0)
   - Flink 1.9: [链接](https://alink-release.oss-cn-beijing.aliyuncs.com/v1.6.1/pyalink_flink_1.9-1.6.1-py3-none-any.whl) (MD5: 2feaed5f159bb8970400eb3f6eafc7e5)
3. 如果有多个版本的 Python，可能需要使用特定版本的 `pip`，比如 `pip3`；如果使用 Anaconda，则需要在 Anaconda 命令行中进行安装。

### 开始使用：
-------
可以通过 Jupyter Notebook 来开始使用 PyAlink，能获得更好的使用体验。

使用步骤：
1. 在命令行中启动Jupyter：`jupyter notebook`，并新建 Python 3 的 Notebook 。
2. 导入 pyalink 包：`from pyalink.alink import *`。
3. 使用方法创建本地运行环境：
`useLocalEnv(parallism, flinkHome=None, config=None)`。
其中，参数 `parallism` 表示执行所使用的并行度；`flinkHome` 为 flink 的完整路径，一般情况不需要设置；`config`为Flink所接受的配置参数。运行后出现如下所示的输出，表示初始化运行环境成功：
```
JVM listening on ***
```
4. 开始编写 PyAlink 代码，例如：
```python
source = CsvSourceBatchOp()\
    .setSchemaStr(""sepal_length double, sepal_width double, petal_length double, petal_width double, category string"")\
    .setFilePath(""https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv"")
res = source.select([""sepal_length"", ""sepal_width""])
df = res.collectToDataframe()
print(df)
```

### 编写代码：
------
在 PyAlink 中，算法组件提供的接口基本与 Java API 一致，即通过默认构造方法创建一个算法组件，然后通过 `setXXX` 设置参数，通过 `link/linkTo/linkFrom` 与其他组件相连。
这里利用 Jupyter Notebook 的自动补全机制可以提供书写便利。

对于批式作业，可以通过批式组件的 `print/collectToDataframe/collectToDataframes` 等方法或者 `BatchOperator.execute()` 来触发执行；对于流式作业，则通过 `StreamOperator.execute()` 来启动作业。


### 更多用法：
------
  - [DataFrame 与 Operator 互转](docs/pyalink/pyalink-dataframe.md)
  - [StreamOperator 数据预览](docs/pyalink/pyalink-stream-operator-preview.md)
  - [UDF/UDTF/SQL 使用](docs/pyalink/pyalink-udf.md)
  - [与 PyFlink 一同使用](docs/pyalink/pyalink-pyflink.md)
  - [PyAlink 常见问题](docs/pyalink/pyalink-qa.md)

## Java 接口使用介绍
----------

### 示例代码

```java
String URL = ""https://alink-release.oss-cn-beijing.aliyuncs.com/data-files/iris.csv"";
String SCHEMA_STR = ""sepal_length double, sepal_width double, petal_length double, petal_width double, category string"";

BatchOperator data = new CsvSourceBatchOp()
        .setFilePath(URL)
        .setSchemaStr(SCHEMA_STR);

VectorAssembler va = new VectorAssembler()
        .setSelectedCols(new String[]{""sepal_length"", ""sepal_width"", ""petal_length"", ""petal_width""})
        .setOutputCol(""features"");

KMeans kMeans = new KMeans().setVectorCol(""features"").setK(3)
        .setPredictionCol(""prediction_result"")
        .setPredictionDetailCol(""prediction_detail"")
        .setReservedCols(""category"")
        .setMaxIter(100);

Pipeline pipeline = new Pipeline().add(va).add(kMeans);
pipeline.fit(data).transform(data).print();
```

### Flink-1.13 的 Maven 依赖
```xml
<dependency>
    <groupId>com.alibaba.alink</groupId>
    <artifactId>alink_core_flink-1.13_2.11</artifactId>
    <version>1.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.13.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner_2.11</artifactId>
    <version>1.13.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-clients_2.11</artifactId>
    <version>1.13.0</version>
</dependency>
```

### Flink-1.12 的 Maven 依赖
```xml
<dependency>
    <groupId>com.alibaba.alink</groupId>
    <artifactId>alink_core_flink-1.12_2.11</artifactId>
    <version>1.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.12.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner_2.11</artifactId>
    <version>1.12.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-clients_2.11</artifactId>
    <version>1.12.1</version>
</dependency>
```

### Flink-1.11 的 Maven 依赖
```xml
<dependency>
    <groupId>com.alibaba.alink</groupId>
    <artifactId>alink_core_flink-1.11_2.11</artifactId>
    <version>1.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.11.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner_2.11</artifactId>
    <version>1.11.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-clients_2.11</artifactId>
    <version>1.11.0</version>
</dependency>
```

### Flink-1.10 的 Maven 依赖
```xml
<dependency>
    <groupId>com.alibaba.alink</groupId>
    <artifactId>alink_core_flink-1.10_2.11</artifactId>
    <version>1.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.10.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner_2.11</artifactId>
    <version>1.10.0</version>
</dependency>
```

### Flink-1.9 的 Maven 依赖

```xml
<dependency>
    <groupId>com.alibaba.alink</groupId>
    <artifactId>alink_core_flink-1.9_2.11</artifactId>
    <version>1.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.9.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner_2.11</artifactId>
    <version>1.9.0</version>
</dependency>
```



## 快速开始在集群上运行Alink算法
--------

1. 准备Flink集群
```shell
  wget https://archive.apache.org/dist/flink/flink-1.13.0/flink-1.13.0-bin-scala_2.11.tgz
  tar -xf flink-1.13.0-bin-scala_2.11.tgz && cd flink-1.13.0
  ./bin/start-cluster.sh
```

2. 准备Alink算法包
```shell
  git clone https://github.com/alibaba/Alink.git
  # add <scope>provided</scope> in pom.xml of alink_examples.
  cd Alink && mvn -Dmaven.test.skip=true clean package shade:shade
```

3. 运行Java示例
```shell
  ./bin/flink run -p 1 -c com.alibaba.alink.ALSExample [path_to_Alink]/examples/target/alink_examples-1.5-SNAPSHOT.jar
  # ./bin/flink run -p 1 -c com.alibaba.alink.GBDTExample [path_to_Alink]/examples/target/alink_examples-1.5-SNAPSHOT.jar
  # ./bin/flink run -p 1 -c com.alibaba.alink.KMeansExample [path_to_Alink]/examples/target/alink_examples-1.5-SNAPSHOT.jar
```

## 部署
----------

[集群部署](docs/deploy/cluster-deploy.md)
",2023-07-07 15:48:52+00:00
alphasql,alphasql,Matts966/alphasql,AlphaSQL provides Integrated Type and Schema Check and Parallelization for SQL file set mainly for BigQuery,,False,49,2023-05-17 06:18:55+00:00,2020-05-15 08:17:56+00:00,4,2,1,61,v1.4.1,2022-04-13 16:20:31+00:00,Apache License 2.0,499,v1.4.1,66,2022-04-13 16:20:31+00:00,2023-05-17 06:18:54+00:00,2022-04-13 16:20:31+00:00,"# AlphaSQL

[![release](https://github.com/Matts966/alphasql/workflows/release/badge.svg)](https://github.com/Matts966/alphasql/actions?query=workflow%3Arelease)
[![test](https://github.com/Matts966/alphasql/workflows/test/badge.svg?branch=master)](https://github.com/Matts966/alphasql/actions?query=branch%3Amaster+workflow%3Atest+)

AlphaSQL provides **Automatic Parallelization** for sets of SQL files and integrated **Type/Scheme Checker** to eliminate syntax, type and schema errors from your datawarehouse.

## Features

You can quickly introduce AlphaSQL by [CI Example](#ci-example).

- [Docker Image](#docker-image)
    - Use our AlphaSQL on Docker ；）
- [Fast Binaries](#fast-binaries)
    - For local use, binary installation is fast!
- [Dependency Analysis](#extract-dag-from-sql-set)
    - Extract DAG from your SQL file set.
    - [Sample DAG output](#sample-dag-output)
    - [Side effect first](#side-effect-first)
    - [With tables and functions](#with-tables-and-functions)
- [Parallel Execution](#parallel-execution)
    - Automatically parallelize your SQL file set.
    - Integrate with Airflow.
- [Schema Checker](#pipeline-level-type-check-for-sql-set)
    - Eliminate syntax, type and schema errors from your datawarehouse.
    - [Schema specification by JSON](#schema-specification-by-json)
        - Input your lake schema in JSON.
- [CI Example](#ci-example)
    - Use our AlphaSQL to continuously check your datawarehouse on BigQuery using CloudBuild.
    - Supports `_TABLE_SUFFIX`.

## Docker Image

You can run commands below with docker

```bash
docker run --rm -v `pwd`:/home matts966/alphasql:latest [command]
```

like

```bash
docker run --rm -v `pwd`:/home matts966/alphasql:latest alphacheck ./samples/sample/dag.dot
```

Commands are installed in the PATH of the image.

## Fast Binaries

```bash
# To install for MacOSX
temp=$(mktemp -d)
wget -P $temp https://github.com/Matts966/alphasql/releases/latest/download/alphasql_darwin_x86_64.tar.gz \
    && sudo tar -zxvf $temp/alphasql_darwin_x86_64.tar.gz -C /usr/local/bin
```

```bash
# To install for Linux
temp=$(mktemp -d)
wget -P $temp https://github.com/Matts966/alphasql/releases/latest/download/alphasql_linux_x86_64.tar.gz \
    && sudo tar -zxvf $temp/alphasql_linux_x86_64.tar.gz -C /usr/local/bin --strip=1
```

## Extract DAG from SQL set

`alphadag` finds dependencies between table references and create table statements, function calls and create function statements.

```bash
# To extract DAG from your SQL set
$ alphadag --output_path ./samples/sample/dag.dot ./samples/sample/

# Or you can check the output in stdout by
$ alphadag [paths]

# with graphviz
$ dot -Tpng samples/sample/dag.dot -o samples/sample/dag.png
```

Note that sometimes the output has cycle, and refactoring SQL files or manual editing of the dot file is needed (see [this issue](https://github.com/Matts966/alphasql/issues/2)).

If there are cycles, warning is emitted, type checker reports error, and bq_jobrunner raise error before execution. You can see the example in [./samples/sample-cycle](./samples/sample-cycle) .

If you want to serially execute some statements, you can write SQL script that contains multiple statements. See [samples/sample/create_interim1.sql](samples/sample/create_interim1.sql) as an example.

### Sample DAG output

The image below is extracted from SQL set in [./samples/sample](./samples/sample) . You can write tests for created tables and run them parallely only by separating SQL file.

![dag.dot](samples/sample/dag.png)

### Side effect first

You can resolve side effects such as `INSERT` and `UPDATE` statements before simple references by the `--side_effect_first` option.

![dag.dot](./samples/sample-undefined/side_effect_first/dag.png)

### With tables and functions

You can extract dependencies containing tables and functions by `--with_tables` and `--with_functions` options.

#### With tables

![dag.dot](./samples/sample/with_all/dag.png)

### With functions

![dag.dot](./samples/sample-function-dependency/with_all/dag.png)

## Parallel Execution

For BigQuery, the output DAG can be run parallely using

- [bq-airflow-dag-generator](https://pypi.org/project/bq-airflow-dag-generator)
- [bq-jobrunner](https://github.com/tsintermax/bq_jobrunner)

### bq-airflow-dag-generator

![airflow DAG](./samples/airflow.png)

`bq-airflow-dag-generator` as Python package can generate Airflow DAG by simple Python script.

```Python
dagpath = ""/path/to/dag.dot""
dag = generate_airflow_dag_by_dot_path(dagpath)
```

See [usage on README](https://github.com/Matts966/bq-airflow-dag-generator#usage) for more details.

### bq-jobrunner

```Python
from bq_jobrunner.bq_jobrunner import BQJobrunner

FILE_PATH = ""./path/to/dag.dot""
PROJECT_ID = ""your-project-id""
REGION = ""asia-northeast1"" # your region

runner = BQJobrunner(
    PROJECT_ID,
    location=REGION,
)
runner.compose_query_by_dot_path(FILE_PATH)
runner.execute()
```

Note that you should run job_runner in the same path as in extracting DAG.

## Pipeline level Type Check for SQL set

Note that you should run type_checker in the same path as in extracting DAG.

```bash
# to check type and schema of SQL set
$ alphacheck ./samples/sample.dot
Analyzing ""./samples/sample/create_datawarehouse3.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_datawarehouse2.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_interim2.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/update_interim2.sql""
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_datawarehouse1.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_interim3.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_interim1.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/update_interium1.sql""
SUCCESS: analysis finished!
Analyzing ""./samples/sample/insert_into_interim1.sql""
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_mart.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/test_mart1.sql""
SUCCESS: analysis finished!
Analyzing ""./samples/sample/test_mart2.sql""
SUCCESS: analysis finished!
Analyzing ""./samples/sample/test_mart3.sql""
SUCCESS: analysis finished!
Successfully finished type check!
```

If you change column `x`'s type in `./samples/sample/create_datawarehouse3.sql` to `STRING`, type checker reports error.

```bash
$ alphacheck ./samples/sample/dag.dot
Analyzing ""./samples/sample/create_datawarehouse3.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_datawarehouse2.sql""
DDL analyzed, adding table to catalog...
SUCCESS: analysis finished!
Analyzing ""./samples/sample/create_interim2.sql""
ERROR: INVALID_ARGUMENT: Column 1 in UNION ALL has incompatible types: INT64, STRING [at ./samples/sample/create_interim2.sql:7:1]
catalog:
        datawarehouse3
        datawarehouse2
```

### Schema specification by JSON

You can specify external schemata (not created by queries in SQL set) by passing JSON schema path.

```bash
# with external schema
$ alphacheck --json_schema_path ./samples/sample-schema.json ./samples/sample/dag.dot
```

You can extract required external tables by

```bash
$ alphadag --external_required_tables_output_path ./required_tables.txt {./path/to/sqls}
# and get schemata using bq command
$ cat ./required_tables.txt | while read line
do
    bq show \
        --schema \
        --format=prettyjson \
        $line
done
```

JSON schema file should have only a top level map element keyed by string element, which specifies table name, and each value specifies schema for its key. `name` and `type` elements in the schema elements are recognized like the official API.

```json
{
        ""tablename1"": [
                {""mode"": ""NULLABLE"", ""name"": ""column1"", ""type"": ""STRING"", ""description"": null}
        ],
        ""tablename2"": [
                {""mode"": ""NULLABLE"", ""name"": ""column1"", ""type"": ""STRING"", ""description"": null},
                {""mode"": ""NULLABLE"", ""name"": ""column2"", ""type"": ""INT64"", ""description"": null}
        ]
}
```

## CI Example

The pipeline level type check above is also useful in CI context. The sample in [./samples/sample-ci](./samples/sample-ci) contains an example for extracting DAG, retrieving schema and checking schema and type of SQL set quering bigquery public dataset. You can introduce the CI to your environment only by copying `cloudbuild_ci_sample.yaml` and `python_entrypoint.py` to your project.

You can try the example CI with `gcloud` command by

```
(cd ./samples/sample-ci && gcloud builds submit --config=cloudbuild_ci_sample.yaml .)
```

This example

- Supports `_TABLE_SUFFIX` feature!
- Does not execute actual BigQuery and very cheap!

## License

[Apache License 2.0](LICENSE)

## Sponsors

The development of this project is sponsored by [Japan Data Science Consortium](https://jdsc.ai/) and [Cybozu Lab](https://labs.cybozu.co.jp/).
",2023-07-07 15:48:56+00:00
amundsen,amundsen,amundsen-io/amundsen,"Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data.",https://www.amundsen.io/amundsen/,False,3973,2023-07-07 14:02:52+00:00,2019-05-14 15:12:40+00:00,936,240,209,120,databuilder-7.4.4,2023-06-14 20:58:25+00:00,Apache License 2.0,2645,search-4.1.1,125,2023-03-27 22:31:43+00:00,2023-07-07 02:45:11+00:00,2023-06-14 20:58:25+00:00,"<p align=""center"">
  <img
    src=""https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/logos/amundsen_logo_on_light.svg?sanitize=true""
    alt=""Amundsen""
    width=""1000""
  />
</p>

<p align=""center"">
  <a href=""https://github.com/amundsen-io/amundsen"">
    <img src=""https://img.shields.io/github/stars/amundsen-io/amundsen.svg?style=social"" />
  </a>
  <a href=""https://github.com/amundsen-io/amundsen/blob/master/LICENSE"">
    <img src=""https://img.shields.io/:license-Apache%202-blue.svg"" />
  </a>
  <a href=""https://github.com/amundsen-io/amundsen/blob/master/CONTRIBUTING.md"">
    <img src=""https://img.shields.io/badge/PRs-welcome-brightgreen.svg"" />
  </a>
  <a href=""https://github.com/amundsen-io/amundsen/pulse"">
    <img src=""https://img.shields.io/github/commit-activity/w/amundsen-io/amundsen.svg"" />
  </a>
  <a href=""https://img.shields.io/github/contributors/amundsen-io/amundsen.svg"">
    <img src=""https://img.shields.io/github/contributors/amundsen-io/amundsen.svg"" />
  </a>
  <a href=""https://twitter.com/amundsenio"">
    <img src=""https://img.shields.io/twitter/follow/amundsenio?label=Follow&style=social"" />
  </a>
  <a href=""https://join.slack.com/t/amundsenworkspace/shared_invite/zt-s8f3srsx-_0b6_WA5~eYGrv_g63L2ng"">
    <img src=""https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social"" alt=""Slack"" />
  </a>
</p>

Amundsen is a *data discovery and metadata engine* for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as **Google search for data**. The project is named after Norwegian explorer [Roald Amundsen](https://en.wikipedia.org/wiki/Roald_Amundsen), the first person to discover the South Pole.

<img
  src=""https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata/stacked/color/lfaidata-stacked-color.png""
  alt=""LF AI & Data""
  width=""200""
/>

Amundsen is hosted by the [LF AI & Data Foundation](https://lfaidata.foundation/). It includes three microservices, one data ingestion library and one common library.

- [amundsenfrontendlibrary](frontend): Frontend service which is a Flask application with a React frontend. <img src=""https://badge.fury.io/py/amundsen-frontend.svg"" />
- [amundsensearchlibrary](search): Search service, which leverages Elasticsearch for search capabilities, is used to power frontend metadata searching. <img src=""https://badge.fury.io/py/amundsen-search.svg"" />
- [amundsenmetadatalibrary](metadata): Metadata service, which leverages Neo4j or Apache Atlas as the persistent layer, to provide various metadata. <img src=""https://badge.fury.io/py/amundsen-metadata.svg"" />
- [amundsendatabuilder](databuilder): Data ingestion library for building metadata graph and search index.
  Users could either load the data with [a python script](https://github.com/amundsen-io/amundsen/blob/main/databuilder/example/scripts/sample_data_loader.py) with the library
  or with an [Airflow DAG](https://github.com/amundsen-io/amundsen/tree/main/databuilder/example/dags) importing the library. <img src=""https://badge.fury.io/py/amundsen-databuilder.svg"" />
- [amundsencommon](common): Amundsen Common library holds common codes among microservices in Amundsen. <img src=""https://badge.fury.io/py/amundsen-common.svg"" />
- [amundsengremlin](https://github.com/amundsen-io/amundsengremlin): Amundsen Gremlin library holds code used for converting model objects into vertices and edges in gremlin. It's used for loading data into an AWS Neptune backend. <img src=""https://badge.fury.io/py/amundsen-gremlin.svg"" />
- [amundsenrds](https://github.com/amundsen-io/amundsenrds): Amundsenrds contains ORM models to support relational database as metadata backend store in Amundsen. The schema in ORM models follows the logic of databuilder models. Amundsenrds will be used in databuilder and metadatalibrary for metadata storage and retrieval with relational databases. <img src=""https://badge.fury.io/py/amundsen-rds.svg"" />

## Documentation
* [Homepage](https://www.amundsen.io/)
* [Documentation](https://www.amundsen.io/amundsen/)

## Community Roadmap
We want your input about what is important, for that, add your votes using the 👍 reaction:
* [Top Feature Requests](https://github.com/amundsen-io/amundsen/issues?q=is%3Aissue+is%3Aclosed+sort%3Areactions-%2B1-desc+label%3Atype%3Afeature+label%3Astatus%3Aneeds_votes+)
* [Documentation Requests](https://github.com/amundsen-io/amundsen/issues?q=is%3Aissue+is%3Aclosed+sort%3Areactions-%2B1-desc+label%3Atype%3Adocumentation+label%3Astatus%3Aneeds_votes+)
* [Top Bugs](https://github.com/amundsen-io/amundsen/issues?q=is%3Aissue+is%3Aclosed+sort%3Areactions-%2B1-desc+label%3Atype%3Abug+label%3Astatus%3Aneeds_votes+)
* [Top Questions](https://github.com/amundsen-io/amundsen/issues?q=is%3Aissue+is%3Aclosed+sort%3Areactions-%2B1-desc+label%3Atype%3Aquestion+label%3Astatus%3Aneeds_votes)

## Requirements
- Python 3.7
- Node v12

## User Interface

Please note that the mock images only served as demonstration purpose.

- **Landing Page**: The landing page for Amundsen including 1. search bars; 2. popular used tables;

  ![](https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/landing_page.png)

- **Search Preview**: See inline search results as you type

  ![](https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/search_preview.png)

- **Table Detail Page**: Visualization of a Hive / Redshift table

  ![](https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/table_detail_page_with_badges.png)

- **Column detail**: Visualization of columns of a Hive / Redshift table which includes an optional stats display

  ![](https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/column_details.png)

- **Data Preview Page**: Visualization of table data preview which could integrate with [Apache Superset](https://github.com/apache/incubator-superset) or other Data Visualization Tools.

  ![](https://raw.githubusercontent.com/amundsen-io/amundsen/master/docs/img/data_preview.png)


## Getting Started and Installation

Please visit the Amundsen installation documentation for a [quick start](https://www.amundsen.io/amundsen/installation/) to bootstrap a default version of Amundsen with dummy data.

## Supported Entities

- Tables (from Databases)
- Dashboards
- ML Features
- People (from HR systems)

## Supported Integrations

### Table Connectors

- [Amazon Athena](https://aws.amazon.com/athena/)
- [Amazon EventBridge](https://aws.amazon.com/eventbridge/)
- [Amazon Glue](https://aws.amazon.com/glue/) and anything built over it
- [Amazon Redshift](https://aws.amazon.com/redshift/)
- [Apache Cassandra](https://cassandra.apache.org/)
- [Apache Druid](https://druid.apache.org/)
- [Apache Hive](https://hive.apache.org/)
- CSV
- [dbt](https://www.getdbt.com/)
- [Delta Lake](https://delta.io/)
- [Elasticsearch](https://www.elastic.co/)
- [Google BigQuery](https://cloud.google.com/bigquery)
- [IBM DB2](https://www.ibm.com/analytics/db2)
- [Kafka Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
- [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/default.aspx)
- [MySQL](https://www.mysql.com/)
- [Oracle](https://www.oracle.com/index.html) (through dbapi or sql_alchemy)
- [PostgreSQL](https://www.postgresql.org/)
- [PrestoDB](http://prestodb.io/)
- [Trino (formerly Presto SQL)](https://trino.io/)
- [Vertica](https://www.vertica.com/)
- [Snowflake](https://www.snowflake.com/)

Amundsen can also connect to any database that provides `dbapi` or `sql_alchemy` interface (which most DBs provide).

### Table Column Statistics

- [Pandas Profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/)

### Dashboard Connectors

- [Apache Superset](https://superset.apache.org/)
- [Mode Analytics](https://mode.com/)
- [Redash](https://redash.io/)
- [Tableau](https://tableau.com/)
- [Databricks SQL](https://databricks.com/product/databricks-sql)

### ETL Orchestration

- [Apache Airflow](https://airflow.apache.org/)

## Get Involved in the Community

Want help or want to help? Use the button in our [header](https://github.com/amundsen-io/amundsen#readme) to join our slack channel.

Contributions are also more than welcome! As explained in [CONTRIBUTING.md](https://github.com/amundsen-io/amundsen/blob/main/CONTRIBUTING.md) there are many ways to contribute, it does not all have to be code with new features and bug fixes, also documentation, like FAQ entries, bug reports, blog posts sharing experiences etc. all help move Amundsen forward. If you find a security vulnerability, [please follow this guide](https://github.com/amundsen-io/amundsen/blob/main/SECURITY.md).


## Architecture Overview

Please visit [Architecture](https://www.amundsen.io/amundsen/architecture/) for Amundsen architecture overview.

## Resources

### Blog Posts and Interviews

- [Amundsen - Lyft's data discovery & metadata engine](https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9) (April 2019)
- [Software Engineering Daily podcast on Amundsen](https://softwareengineeringdaily.com/2019/04/16/lyft-data-discovery-with-tao-feng-and-mark-grover/) (April 2019)
- [How Lyft Drives Data Discovery](https://youtu.be/WVjss62XIG0) (July 2019)
- [Data Engineering podcast on Solving Data Discovery At Lyft](https://www.dataengineeringpodcast.com/amundsen-data-discovery-episode-92/) (Aug 2019)
- [Open Sourcing Amundsen: A Data Discovery And Metadata Platform](https://eng.lyft.com/open-sourcing-amundsen-a-data-discovery-and-metadata-platform-2282bb436234) (Oct 2019)
- [Adding Data Quality into Amundsen with Programmatic Descriptions](https://technology.edmunds.com/2020/05/27/Adding-Data-Quality-into-Amundsen-with-Programmatic-Descriptions/) by [Sam Shuster](https://github.com/samshuster) from [Edmunds.com](https://www.edmunds.com/) (May 2020)
- [Facilitating Data discovery with Apache Atlas and Amundsen](https://medium.com/wbaa/facilitating-data-discovery-with-apache-atlas-and-amundsen-631baa287c8b) by [Mariusz Górski](https://github.com/mgorsk1) from [ING](https://www.ing.com/Home.htm) (June 2020)
- [Using Amundsen to Support User Privacy via Metadata Collection at Square](https://developer.squareup.com/blog/using-amundsen-to-support-user-privacy-via-metadata-collection-at-square/) by [Alyssa Ransbury](https://github.com/alran) from [Square](https://squareup.com/) (July 14, 2020)
- [Amundsen Joins LF AI as New Incubation Project](https://lfai.foundation/blog/2020/08/11/amundsen-joins-lf-ai-as-new-incubation-project/) (Aug 11, 2020)
- [Amundsen: one year later](https://eng.lyft.com/amundsen-1-year-later-7b60bf28602) (Oct 6, 2020)

### Talks

- Disrupting Data Discovery {[slides](https://www.slideshare.net/taofung/strata-sf-amundsen-presentation), [recording](https://www.youtube.com/watch?v=m1B-ptm0Rrw)} (Strata SF, March 2019)
- Amundsen: A Data Discovery Platform from Lyft {[slides](https://www.slideshare.net/taofung/data-council-sf-amundsen-presentation)} (Data Council SF, April 2019)
- Disrupting Data Discovery {[slides](https://www.slideshare.net/markgrover/disrupting-data-discovery)} (Strata London, May 2019)
- ING Data Analytics Platform (Amundsen is mentioned) {[slides](https://static.sched.com/hosted_files/kccnceu19/65/ING%20Data%20Analytics%20Platform.pdf), [recording](https://www.youtube.com/watch?v=8cE9ppbnDPs&t=465) } (Kubecon Barcelona, May 2019)
- Disrupting Data Discovery {[slides](https://www.slideshare.net/PhilippeMizrahi/meetup-sf-amundsen), [recording](https://www.youtube.com/watch?v=NgeCOVjSJ7A)} (Making Big Data Easy SF, May 2019)
- Disrupting Data Discovery {[slides](https://www.slideshare.net/TamikaTannis/neo4j-graphtour-santa-monica-2019-amundsen-presentation-173073727), [recording](https://www.youtube.com/watch?v=Gr3-RfWn49A)} (Neo4j Graph Tour Santa Monica, September 2019)
- Disrupting Data Discovery {[slides](https://www.slideshare.net/secret/56EPbcvswqyH90)} (IDEAS SoCal AI & Data Science Conference, Oct 2019)
- Data Discovery with Amundsen by [Gerard Toonstra](https://twitter.com/radialmind) from Coolblue {[slides](https://docs.google.com/presentation/d/1rkrP8ZobkLPZbwisrLWTdPN5I52SgVGM1eqAFDJXj2A/edit?usp=sharing)} and {[talk](https://www.youtube.com/watch?v=T54EO1MuE7I&list=PLqYhGsQ9iSEq7fDvXcd67iVzx5nsf9xnK&index=17)} (BigData Vilnius 2019)
- Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by [Verdan Mahmood](https://github.com/verdan) and Marek Wiewiorka from ING {[slides](https://docs.google.com/presentation/d/1FixTTNd1dt_f3PAKhL1KLOeOLsIQq0iFvQA6qlpjIg0/edit#slide=id.p1), [talk](https://bigdatatechwarsaw.eu/agenda/)} (Big Data Technology Warsaw Summit 2020)
- Airflow @ Lyft (which covers how we integrate Airflow and Amundsen) by [Tao Feng](https://github.com/feng-tao) {[slides](https://www.slideshare.net/taofung/airflow-at-lyft-airflow-summit2020) and [website](https://airflowsummit.org/sessions/how-airbnb-twitter-lyft-use-airflow/)} (Airflow Summit 2020)
- Data DAGs with lineage for fun and for profit by [Bolke de Bruin](https://github.com/bolkedebruin) {[website](https://airflowsummit.org/sessions/data-dags-with-lineage/)} (Airflow Summit 2020)
- Solving Data Discovery Challenges at Lyft with Amundsen, an Open-source Metadata Platform by [Tao Feng](https://github.com/feng-tao) ([Data+AI summit Europe 2020](https://databricks.com/session_eu20/solving-data-discovery-challenges-at-lyft-with-amundsen-an-open-source-metadata-platform))
- Data Discovery at Databricks with Amundsen by [Tao Feng](https://github.com/feng-tao) and [Tianru Zhou](https://www.linkedin.com/in/tianru-zhou-134868132/) ([Data+AI summit NA 2021](https://databricks.com/session_na21/data-discovery-at-databricks-with-amundsen))

### Related Articles

- [How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions](https://towardsdatascience.com/how-linkedin-uber-lyft-airbnb-and-netflix-are-solving-data-management-and-discovery-for-machine-9b79ee9184bb)
- [Data Discovery in 2020](https://medium.com/@torokyle/data-discovery-in-2020-3c907383caa0)
- [4 Data Trends to Watch in 2020](https://medium.com/memory-leak/4-data-trends-to-watch-in-2020-491707902c09)
- [Work-Bench Snapshot: The Evolution of Data Discovery & Catalog](https://medium.com/work-bench/work-bench-snapshot-the-evolution-of-data-discovery-catalog-2f6c0425616b)
- [Future of Data Engineering](https://www.infoq.com/presentations/data-engineering-pipelines-warehouses/)
- [Governance and Discovery](https://www.oreilly.com/radar/governance-and-discovery/)
- [A Data Engineer’s Perspective On Data Democratization](https://towardsdatascience.com/a-data-engineers-perspective-on-data-democratization-a8aed10f4253?source=friends_link&sk=63638570d03e4145265932c12af33f9d)
- [Graph Technology Landscape 2020](https://graphaware.com/graphaware/2020/02/17/graph-technology-landscape-2020.html)
- [In-house Data Discovery platforms](https://datastrategy.substack.com/p/in-house-data-discovery-platforms)
- [Linux Foundation AI Foundation Landscape](https://landscape.lfai.foundation/)
- [Lyft’s Amundsen: Data-Discovery with Built-In Trust](https://thenewstack.io/lyfts-amundsen-data-discovery-with-built-in-trust/)
- [How to find and organize your data from the command-line](https://towardsdatascience.com/how-to-find-and-organize-your-data-from-the-command-line-852a4042b2be)
- [Data Discovery Platform at Bagelcode](https://medium.com/bagelcode/data-discovery-platform-at-bagelcode-b58a622d17fd)
- [Cataloging Tools for Data Teams](https://towardsdatascience.com/cataloging-tools-for-data-teams-8d62d7a4cd95)
- [An Overview of Data Discovery Platforms and Open Source Solutions](https://eugeneyan.com/writing/data-discovery-platforms/)
- [Hacking Data Discovery in AWS with Amundsen at SEEK](https://medium.com/seek-blog/hacking-data-discovery-with-amundsen-c55d262014f4)
- [A step-by-step guide deploying Amundsen on Google Cloud Platform](https://medium.com/talabat-tech/a-step-by-step-guide-deploying-amundsen-on-google-cloud-platform-ee4ef20d81f5)
- [Machine Learning Features discovery with Feast and Amundsen](https://getindata.com/blog/machine-learning-features-discovery-feast-amundsen)
- [Data discovery at REA group](https://www.rea-group.com/blog/data-discovery/)
- [Integrating Slack with Amundsen for Ease of Data Discovery](https://medium.com/convoy-tech/integrating-slack-with-amundsen-for-ease-of-data-discovery-ef3b54834da5)
- [Building a data discovery solution with Amundsen and Amazon Neptune](https://aws.amazon.com/blogs/database/building-a-data-discovery-solution-with-amundsen-and-amazon-neptune/)
- [Amundsen — Installing in an Istio-enabled environment](https://medium.com/@owenleung_89035/amundsen-installing-in-an-istio-enabled-environment-d0b9bdaeac49)
- [Amundsen — Integrate with Okta SingleSignOn](https://medium.com/@owenleung_89035/amundsen-integrate-with-okta-singlesignon-20ad0a22d3d9)

### Community meetings

Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time. [Link to join](https://zoom.us/j/92594007352?pwd=Rmo4SGp4WnY3eGlzclgrWlp6bTNTQT09)

### Upcoming meetings & notes

You can the exact date for the next meeting and the agenda a few weeks before the meeting in [this doc](https://docs.google.com/document/d/1bsJWNt1GBFmV-aRbHFuYgMFnMgIIvAmbhsvDatb0Vis).

Notes from all past meetings are available [here](https://docs.google.com/document/d/1bsJWNt1GBFmV-aRbHFuYgMFnMgIIvAmbhsvDatb0Vis).

## Who uses Amundsen?

Here is the list of organizations that are **officially** using Amundsen today. If your organization uses Amundsen, please file a PR and update this list.

<table>
  <tbody>
    <tr>
      <td>
        <ul>
          <li><a href=""https://asana.com/"">Asana</a></li>
          <li><a href=""https://site.bagelcode.com/"">Bagelcode</a></li>
          <li><a href=""https://www.bang-olufsen.com/en"">Bang & Olufsen</a></li>
          <li><a href=""https://www.brex.com/"">Brex</a></li>
          <li><a href=""https://www.cameo.com"">Cameo</a></li>
          <li><a href=""https://chanzuckerberg.com/"">Chan Zuckerberg Initiative</a></li>
          <li><a href=""https://cimpress.com"">Cimpress Technology</a></li>
          <li><a href=""https://www.colesgroup.com.au/home/"">Coles Group</a></li>
          <li><a href=""https://www.convoy.com"">Convoy</a></li>
          <li><a href=""https://datasprints.com/"">Data Sprints</a></li>
          <li><a href=""https://www.dcard.tw/"">Dcard</a></li>
          <li><a href=""https://www.deliveryhero.com/"">Delivery Hero</a></li>
        </ul>
      </td>
      <td>
        <ul>
          <li><a href=""https://www.devoted.com/"">Devoted Health</a></li>
          <li><a href=""https://dhigroupinc.com/"">DHI Group</a></li>
          <li><a href=""https://www.edmunds.com/"">Edmunds</a></li>
          <li><a href=""https://everfi.com/"">Everfi</a></li>
          <li><a href=""https://gusto.com/"">Gusto</a></li>
          <li><a href=""https://hurb.com"">Hurb</a></li>
          <li><a href=""https://www.ing.com/Home.htm"">ING</a></li>
          <li><a href=""https://www.instacart.com/"">Instacart</a></li>
          <li><a href=""https://www.irobot.com"">iRobot</a></li>
          <li><a href=""https://lett.digital/"">Lett</a></li>
          <li><a href=""https://www.lmc.eu/cs/"">LMC</a></li>
          <li><a href=""https://loft.com.br"">Loft</a></li>
          <li><a href=""https://www.lyft.com/"">Lyft</a></li>
        </ul>
      </td>
      <td>
        <ul>
          <li><a href=""https://merlinjobs.com"">Merlin</a></li>
          <li><a href=""https://picpay.com.br"">PicPay</a></li>
          <li><a href=""https://company.plarium.com/en/studio/russia-krasnodar/"">Plarium Krasnodar</a></li>
          <li><a href=""https://careers.pubg.com/"">PUBG</a></li>
          <li><a href=""https://rapido.bike/Careers?lan=en ""India's largest bike taxi service"""">Rapido</a></li>
          <li><a href=""https://www.rea-group.com/"">REA Group</a></li>
          <li><a href=""https://www.remitly.com/"">Remitly</a></li>
          <li><a href=""https://www.snap.com/en-US"">Snap</a></li>
          <li><a href=""https://squareup.com/us/en"">Square</a></li>
          <li><a href=""https://www.thetileapp.com"">Tile</a></li>
          <li><a href=""https://go.wepay.com/"">WePay</a></li>
          <li><a href=""https://wetransfer.com"">WeTransfer</a></li>
          <li><a href=""https://www.workday.com/en-us/homepage.html"">Workday</a></li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


## Contributors ✨

Thanks goes to these incredible people:

<a href=""https://github.com/amundsen-io/amundsen/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=amundsen-io/amundsen"" />
</a>
",2023-07-07 15:49:00+00:00
anadama2,anadama2,biobakery/anadama2,AnADAMA2 is the next generation of AnADAMA (Another Automated Data Analysis Management Application). AnADAMA is a tool to capture your workflow and execute it efficiently on your local machine or in a grid compute environment (ie. sun grid engine or slurm).,http://huttenhower.sph.harvard.edu/anadama2,False,6,2023-04-28 18:47:36+00:00,2019-11-05 20:39:51+00:00,3,5,6,0,,,Other,889,v0.10.0,28,2022-09-08 20:36:12+00:00,2023-06-27 20:47:28+00:00,2023-06-27 20:47:04+00:00,"**AnADAMA2 User Manual**
========================

AnADAMA2 is the next generation of AnADAMA (Another Automated Data Analysis Management Application). AnADAMA is a tool to create reproducible workflows and execute them efficiently. AnADAMA operates in a make-like manner using targets and dependencies of each task to allow for parallelization. In cases where a workflow is modified or input files change, only those tasks impacted by the changes will be rerun. 
 
Tasks can be run locally or in a grid computing environment to increase efficiency. AnADAMA2 includes meta-schedulers for SLURM and SGE grids. Tasks that are specified to run on the grid will be submitted and monitored. If a task exceeds its time or memory allotment it will be resubmitted with double the time or memory (based on which resource needs to be increased) at most three times. Benchmarking information of time, memory, and cores will be recorded for each task run on the grid.
 
Essential information from all tasks is recorded, using the default logger and command line reporters, to ensure reproducibility. The information logged includes the command line options provided to the workflow, the function or command executed for each task, versions of tracked executables, and any output (stdout or stderr) from a task. Information reported on the command line includes status for each task (ie which task is ready, started, or running) along with an overall status of percent and total tasks complete for the workflow. A auto-doc feature allows for workflows to generate documentation automatically to further ensure reproducibility by capturing the latest essential workflow information.
 
AnADAMA2 was architected to be modular allowing users to customize the application by subclassing the base grid meta-schedulers, reporters, and tracked objects (ie files, executables, etc). 


-------

.. contents:: **Table of Contents**

-------


**Features**
............

* Captures your workflow steps along with the specific inputs, outputs, and environment used for each of your workflow runs
* Parallel workflow execution on your local machine or in a grid compute environment
* Ability to rerun a workflow, executing only sub-steps, based on changes in dependencies

-------

**Installation**
................

AnADAMA2 is easy to install.

**Requirements**
----------------

Python 2.7+ is required. All other basic dependencies will be installed when installing AnADAMA2.

The workflow documentation feature uses `Pweave <http://mpastell.com/pweave>`_ which will automatically be installed for you when installing AnADAMA2 and `Pandoc <http://pandoc.org/installing.html>`_. For workflows that use the documentation feature, `matplotlib <http://matplotlib.org/users/installing.html>`_ (version2+ required), `Pandoc <http://pandoc.org/installing.html>`_ (<version2 required), and `LaTeX <https://www.latex-project.org/get/>`_ will need to be installed manually. If your document includes hclust2 heatmaps, `hclust2 <https://bitbucket.org/nsegata/hclust2/overview>`_ will also need to be installed.

**Install**
------------------------

Run the following command to install AnADAMA2 and dependencies:
::

    $ pip install anadama2

Add the option ``--user`` to the install command if you do not have root permissions.

**Test**
--------

Once you have AnADAMA2 installed, you can optionally run the unit tests. To run the unit tests, change directories into the AnADAMA2 install folder and run the following command: 

:: 

    $ python setup.py test

-------

**Basic Usage**
...............

AnADAMA2 does not install an executable that you would run from the command line. Instead, you define your workflows as a Python script; ``anadama2`` is a module you import and use to describe your workflow.

**Definitions**
---------------

Before we get started with a basic workflow, there are a couple important definitions to review.

* Workflow

  - A collection of tasks.

* Task
  
  - A unit of work in the workflow.
  - A task has at least one action, zero or more targets, and zero or more dependencies.

* Target

  - An item that is created or modified by the task (ie like writing to a file).
  - All targets must exist after a task is run (they might not exist before the task is run).

* Dependency

  - An item that is required to run the task (ie input file or variable string).
  - All dependencies of a task must exist before the task can be run.

Targets and dependencies can be of different formats. See the section on ""Types of Tracked Items"" for all of the different types.

Tasks are run by executing all of its actions after all of its dependencies exist. After a task is run, it's marked as successful if no Python exceptions were raised, no shell commands had a non-zero exit status, and all of the targets were created.

**Run a Basic Workflow**
------------------------

A basic workflow script can be found in the examples folder in the source repository named ``exe_check.py``. The script ``exe_check.py`` gets a list of the global executables and also the local executables (those for the user running the script). It then checks to see if there are any global executables that are also installed locally. This script shows how to specify dependencies and targets in the commands directly. Lines 4-6 of the example script show targets with the format ``[t:file]`` and dependencies with the format ``[d:file]``. 

To run this example simply execute the script directly:

::

    $ python exe_check.py

The contents of this script are as follows (line numbers are shown for clarity):

::

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.do(""ls /usr/bin/ | sort > [t:global_exe.txt]"")
    5 workflow.do(""ls $HOME/.local/bin/ | sort > [t:local_exe.txt]"")
    6 workflow.do(""join [d:global_exe.txt] [d:local_exe.txt] > [t:match_exe.txt]"")
    7 workflow.go()

The first line imports AnADAMA2 and the third line creates an instance of the Workflow class removing the command line options input and output as they are not used for this workflow. These two lines are required for every AnADAMA2 workflow. Lines 4-6 add tasks to the workflow and line 7 tells AnADAMA2 to execute the tasks.

**Command Line Interface**
--------------------------

All AnADAMA2 workflows have a command line interface that includes a few default arguments. The default arguments include an input folder, an output folder, and the number of tasks to run in parallel. See the section ""Run an Intermediate Workflow"" for information on how to add custom options.

For a full list of options, run your workflow script with the ""--help"" option.

::

    $ python exe_check.py --help
    usage: exe_check.py [options]

    AnADAMA2 Workflow
    Options:
      --version             show program's version number and exit
      -h, --help            show this help message and exit
      -j JOBS, --local-jobs=JOBS
                            The number of tasks to execute in parallel locally.
      -t TARGET, --target=TARGET
                            Only execute tasks that make these targets. Use this
                            flag multiple times to build many targets. If the
                            provided value includes ? or * or [, treat it as a
                            pattern and build all targets that match.
      -d, --dry-run         Print tasks to be run but don't execute their actions.
      -l, --deploy          Create directories used by other options
      -T EXCLUDE_TARGET, --exclude-target=EXCLUDE_TARGET
                            Don't execute tasks that make these targets. Use this
                            flag multiple times to exclude many targets. If the
                            provided value includes ? or * or [, treat it as a
                            pattern and exclude all targets that match.
      -u UNTIL_TASK, --until-task=UNTIL_TASK
                            Stop after running the named task. Can refer to the
                            end task by task number or task name.
      -e, --quit-early      If any tasks fail, stop all execution immediately. If
                            not set, children of failed tasks are not executed but
                            children of successful or skipped tasks are executed.
                            The default is to keep running until all tasks that
                            are available to execute have completed or failed.
      -g GRID, --grid=GRID  Run gridable tasks on this grid type.
      -U EXCLUDE_TASK, --exclude-task=EXCLUDE_TASK
                            Don't execute these tasks. Use this flag multiple
                            times to not execute multiple tasks.
      -i INPUT, --input=INPUT
                            Collect inputs from this directory.
      -o OUTPUT, --output=OUTPUT
                            Write output to this directory. By default the
                            dependency database and log are written to this
                            directory
      -n, --skip-nothing    Skip no tasks, even if you could; run it all.
      -J GRID_JOBS, --grid-jobs=GRID_JOBS
                            The number of tasks to submit to the grid in parallel.
                            The default setting is zero jobs will be run on the
                            grid. By default, all jobs, including gridable jobs,
                            will run locally.
      -p GRID_PARTITION, --grid-partition=GRID_PARTITION
                            Run gridable tasks on this partition.
      --config              Find workflow configuration in this folder
                            [default: only use command line options] 

Options can be provided on the command line or included in a config file with the option ""--config=FILE"". The config file should be of the format
similar to Microsoft Windows INI files with a section plus key value pairs. No specific section name is required but ""default"" is recommended.
An example config file is included below setting two custom options. Please note the settings in the config file override the default
option settings while command line options override the config file settings. More specifically if a user were to set an option in the 
config file and also on the command line the value on the command line is the one that will be used for the workflow. All options set,
those that are defaults, included on the command line or in the config file, are written to the log each time the workflow is run to
capture the exact run-time settings.

::

    [default]
    input_extension = fastq.gz
    output_extension = bam


**Console Output**
------------------

As tasks are run progress information is printed to the console. The default reporter prints at least five types of information to standard output each time a status message for a task is recorded:

1. The local date/time for the message is printed first.
2. The progress of the workflow is represented as three numbers. The first number is the number of tasks that finished running. The second number is the total number of tasks in the workflow. The last number is the percent completion for the workflow. 
3. The status of the task is printed. Tasks can be skipped or started and they could complete or fail. There are a total of six different status messages.

   a. ``Ready``: All dependencies for the task are available. The task is in the queue waiting for computational resources. For example, if there are 10 tasks that are ready and 10 jobs were specified to run at one time, all ready tasks will immediately start running. If there are more ready tasks then jobs specified, these tasks will wait until other jobs have finished running before starting.
   b. ``Started``: The task started running locally or is about to submit a job to the grid depending on if a task is gridable and if the command line options specified a grid to be used.
   c. ``Completed``: The task finished running without error.
   d. ``Failed``: The task stopped running and an error was reported.
   e. ``Skipped``: The task has been skipped. It does not need to be run because the targets of the task exist and have newer timestamps than the dependencies.
   f. ``GridJob``: The task has been submitted to the grid. This status indicates incremental status messages are included at the end of the message about the status of the grid job.

4. The task number is included to identify the task associated with the status message.
5. A description for the task is included which by default is the task name. If a task does not have a name the description is the first task action. If this action is a command the description is the executable name. If this action is a function the description is the function name. The description is limited to keep the status line to at most 79 characters. The total number of characters required are based on the total number of tasks, since more tasks require more padding for the formatting of the progress section and task number parts of this message to keep all sections in their respective columns. If a description is truncated, it will be followed with an ellipsis (ie ""Very Long Task Description is Reduced ..."").
6. If a grid is selected, additional status information is printed including job submission and states. This additional column in some cases can increase the total line length to more than 79 characters.

Here is an example of the output from running a workflow of kneaddata and humann2 tasks on two input files with two local jobs running in parallel::

  (Dec 08 11:50:43) [0/4 -   0.00%] **Ready    ** Task 2: kneaddata
  (Dec 08 11:50:43) [0/4 -   0.00%] **Started  ** Task 2: kneaddata
  (Dec 08 11:50:43) [0/4 -   0.00%] **Ready    ** Task 0: kneaddata
  (Dec 08 11:50:43) [0/4 -   0.00%] **Started  ** Task 0: kneaddata
  (Dec 08 11:50:44) [1/4 -  25.00%] **Completed** Task 2: kneaddata
  (Dec 08 11:50:44) [1/4 -  25.00%] **Ready    ** Task 5: humann2
  (Dec 08 11:50:44) [1/4 -  25.00%] **Started  ** Task 5: humann2
  (Dec 08 11:50:44) [2/4 -  50.00%] **Completed** Task 0: kneaddata
  (Dec 08 11:50:44) [2/4 -  50.00%] **Ready    ** Task 4: humann2
  (Dec 08 11:50:44) [2/4 -  50.00%] **Started  ** Task 4: humann2
  (Dec 08 11:52:48) [3/4 -  75.00%] **Completed** Task 5: humann2
  (Dec 08 11:52:49) [4/4 - 100.00%] **Completed** Task 4: humann2
  Run Finished


Here is an example of the output from running a kneaddata workflow on three input files using a grid and allowing two grid jobs at a time::

  (Dec 08 14:33:07) [0/3 -   0.00%] **Ready    ** Task 4: kneaddata
  (Dec 08 14:33:07) [0/3 -   0.00%] **Started  ** Task 4: kneaddata
  (Dec 08 14:33:07) [0/3 -   0.00%] **Ready    ** Task 2: kneaddata
  (Dec 08 14:33:07) [0/3 -   0.00%] **Started  ** Task 2: kneaddata
  (Dec 08 14:33:07) [0/3 -   0.00%] **Ready    ** Task 0: kneaddata
  (Dec 08 14:33:25) [0/3 -   0.00%] **GridJob  ** Task 4: kneaddata <Grid JobId 76918649 : Submitted>
  (Dec 08 14:33:34) [0/3 -   0.00%] **GridJob  ** Task 2: kneaddata <Grid JobId 76918676 : Submitted>
  (Dec 08 14:34:25) [0/3 -   0.00%] **GridJob  ** Task 4: kneaddata <Grid JobId 76918649 : PENDING>
  (Dec 08 14:47:26) [0/3 -   0.00%] **GridJob  ** Task 4: kneaddata <Grid JobId 76918649 : PENDING>
  (Dec 08 14:47:26) [0/3 -   0.00%] **GridJob  ** Task 4: kneaddata <Grid JobId 76918649 : Getting benchmarking data>
  (Dec 08 14:47:35) [0/3 -   0.00%] **GridJob  ** Task 2: kneaddata <Grid JobId 76918676 : PENDING>
  (Dec 08 14:47:35) [0/3 -   0.00%] **GridJob  ** Task 2: kneaddata <Grid JobId 76918676 : Getting benchmarking data>
  (Dec 08 14:49:35) [0/3 -   0.00%] **GridJob  ** Task 4: kneaddata <Grid JobId 76918649 : Final status of COMPLETED>
  (Dec 08 14:49:35) [0/3 -   0.00%] **GridJob  ** Task 2: kneaddata <Grid JobId 76918676 : Final status of COMPLETED>
  (Dec 08 14:49:35) [0/3 -   0.00%] **Started  ** Task 0: kneaddata
  (Dec 08 14:49:35) [1/3 -  33.33%] **Completed** Task 4: kneaddata
  (Dec 08 14:49:35) [2/3 -  66.67%] **Completed** Task 2: kneaddata
  (Dec 08 14:49:44) [2/3 -  66.67%] **GridJob  ** Task 0: kneaddata <Grid JobId 76922265 : Submitted>
  (Dec 08 14:50:44) [2/3 -  66.67%] **GridJob  ** Task 0: kneaddata <Grid JobId 76922265 : Waiting>
  (Dec 08 14:50:44) [2/3 -  66.67%] **GridJob  ** Task 0: kneaddata <Grid JobId 76922265 : Getting benchmarking data>
  (Dec 08 14:54:46) [2/3 -  66.67%] **GridJob  ** Task 0: kneaddata <Grid JobId 76922265 : Final status of COMPLETED>
  (Dec 08 14:54:46) [3/3 - 100.00%] **Completed** Task 0: kneaddata
  Run Finished


**Logging Output**
------------------

A default setting for workflows is to thoroughly log execution information to a text file named ``anadama.log`` in the current directory.  This log contains information how tasks relate to one another, why tasks were skipped or not skipped, action information like the exact shell command used (if a task dispatched a command to the shell), and full traceback information for all exceptions. With each workflow executed the log appends. It will include information on all runs for a workflow. Below is an example of what to expect from the ``anadama.log``::

  2016-10-28 09:07:38,613	anadama2.runners	_run_task_locally	DEBUG: Completed executing task 3 action 0
  2016-10-28 09:07:38,613	LoggerReporter	task_completed	INFO: task 3, `Task(name=""sed 's|.*Address: \\(.*[0-9]\\)<.*|\\1|' my_ip.txt > ip.txt"", actions=[<function actually_sh at 0x7f32ff082410>], depends=[<anadama2.tracked.TrackedFile object at 0x7f32ff07dfd0>, <anadama2.tracked.TrackedVariable object at 0x7f32ff07df90>, <anadama2.tracked.TrackedExecutable object at 0x7f32ff092050>], targets=[<anadama2.tracked.TrackedFile object at 0x7f32ff092190>], task_no=3)' completed successfully.
  2016-10-28 09:07:38,613	LoggerReporter	task_started	INFO: task 5, `whois $(cat ip.txt) > whois.txt' started. 2 parents: [3, 4].  0 children: [].
  2016-10-28 09:07:38,613	anadama2.runners	_run_task_locally	DEBUG: Executing task 5 action 0
  2016-10-28 09:07:38,613	anadama2.helpers	actually_sh	INFO: Executing with shell: whois $(cat ip.txt) > whois.txt
  2016-10-28 09:07:38,875	anadama2.helpers	actually_sh	INFO: Execution complete. Stdout: 
  Stderr: 
  2016-10-28 09:07:38,875	anadama2.runners	_run_task_locally	DEBUG: Completed executing task 5 action 0
  2016-10-28 09:07:38,875	LoggerReporter	task_completed	INFO: task 5, `Task(name='whois $(cat ip.txt) > whois.txt', actions=[<function actually_sh at 0x7f32ff082488>], depends=[<anadama2.tracked.TrackedFile object at 0x7f32ff092190>, <anadama2.tracked.TrackedVariable object at 0x7f32ff07df10>, <anadama2.tracked.TrackedExecutable object at 0x7f32ff0921d0>], targets=[<anadama2.tracked.TrackedFile object at 0x7f32ff092390>], task_no=5)' completed successfully.
  2016-10-28 09:07:38,875	LoggerReporter	finished	INFO: AnADAMA run finished.



**Run with Specific Targets**
-----------------------------

Tasks in each workflow can have one or more targets. A target is an item, usually a file, that is created or modified by a task. With AnADAMA2, you can select specific targets for each of your runs. This is useful in that it allows you to only run a portion of your workflow.

Targets for each run are selected through the command line interface. The options that allow you to specify a target or set of targets can be shown by running your workflow with the help option. See the prior section for the full list.

Running the example workflow with the target option would only create the ""global_exe.txt"" file. Targets that do not include a full path are expected to be located relative to your current working directory. If a target is in a different folder, provide the full path to the target.

::

    $ python check_exe.py --target global_exe.txt

Running the example workflow with the expanded target option would only create the ""global_exe.txt"" and the ""local_exe.txt"" files. For targets with patterns, place the target in quotes so the pattern is not evaluated by the shell.

::

    $ python check_exe.py --target ""*l_exe.txt""


**Output Files**
----------------

AnADAMA2 will always place at least two items in the output folder. The output folder by default is the directory of your workflow script. If in your workflow, you remove the output folder the database will be written to your home directory and the log will be written to your current working directory. All workflows without output folders will share the same database. Currently it is not possible to run two workflows at once that share the same database.

**Database**
~~~~~~~~~~~~

AnADAMA2 stores a target and dependencies tracking database in the output folder for each run. This includes information on all of the items AnADAMA2 tracks from the workflow. You shouldn't need to look at this file, but you can remove it to erase all history of past runs.

**Log**
~~~~~~~

The log file will contain information on all of the tasks run including standard out and standard error from every command line task. It will also include the values of all workflow arguments.


**Other Basic Workflow Examples**
---------------------------------

Example AnADAMA2 workflow scripts can be found in the examples folder of the source download. Three of these example scripts are described below.

The script ``simple.py`` downloads a file and then runs two linux commands sed and whois. This script shows how to specify dependencies and targets in the commands directly. 

To run this example simply execute the script directly: 

::

    $ python simple.py

The script ``has_a_function.py`` shows how to add a task which is a function to a workflow. This script will download a file, decompress the file, and then remove the trailing tabs in the file. 

To run this example simply execute the script directly:

::

    $ python has_a_function.py

The script ``kvcontainer.py`` shows how to use a container as a dependency. This container allows you to track a set of strings without having them overlap with other strings of the same name in another workflow. 

To run this example simply execute the script directly:

::

    $ python kvcontainer.py


**Jupyter Notebook Examples**
---------------------------------

Examples of running AnADAMA2 in Jupyter Notebooks are included in the examples folder. `Jupyter nbviewer <http://nbviewer.jupyter.org/>`_ can be used to render the notebooks online.

The first notebook shows a simple example on how to download files. To see the notebook rendered with nbviewer visit `AnADAMA2_download_files_example.ipynb <http://nbviewer.jupyter.org/urls/bitbucket.org/biobakery/anadama2/raw/tip/examples/jupyter_notebooks/AnADAMA2_download_files_example.ipynb>`_.

The second notebook illustrates a more complex example of processing shotgun sequencing data. This example requires additional dependencies be installed prior to running. It also requires input files and database files to be downloaded. To see the notebook rendered with nbviewer visit `AnADAMA2_shotgun_workflow_example.ipynb <http://nbviewer.jupyter.org/urls/bitbucket.org/biobakery/anadama2/raw/tip/examples/jupyter_notebooks/AnADAMA2_shotgun_workflow_example.ipynb>`_. 

-------

**Intermediate Usage**
......................


**Add a Task**
---------------------

The do function allows you to specify targets and dependencies in the task command with the special formatting surrounding the full paths to specific files. These tasks have binaries and also the full commands tracked. The do function can be replaced with the add_task function to provide you with additional types of targets and dependencies. Taking the example workflow and replacing lines 4-6 with add_task functions instead of do functions you can see how the syntax changes.

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task(""ls /usr/bin/ | sort > [targets[0]]"", targets=""global_exe.txt"")
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    7 workflow.go()

**Sections of a Workflow**
----------------------------------

An intermediate workflow can contain five sections. Some of these sections are optional to include based on your workflow. The example workflow below has one task which is to run MetaPhlAn2 on a set of fastq input files.  

:: 

    1 ### Section #1: Import anadama2 and create a workflow instance (Required)
    2 from anadama2 import Workflow
    3 workflow = Workflow(version=""0.0.1"", description=""A workflow to run MetaPhlAn2"" )
    4
    5 ### Section #2: Add custom arguments and parse arguments (Optional)
    6 workflow.add_argument(""input-extension"", desc=""the extensions of the input files"", default=""fastq"")
    7 args = workflow.parse_args()
    8
    9 ### Section #3: Get input/output file names (Optional)
    10 in_files = workflow.get_input_files(extension=args.input_extension)
    11 out_files = workflow.name_output_files(name=in_files, tag=""metaphlan2_taxonomy"")
    12
    13 ### Section #4: Add tasks (Required)
    14 workflow.add_task_group(""metaphlan2.py [depends[0]] --input_type [extension] > [targets[0]]"", depends=in_files, targets=out_files, extension=args.input_extension)
    15
    16 ### Section #5: Run tasks (Required)   
    17 workflow.go()


**Section 1** is required. In this section the anadama2 workflow is imported and a workflow instance is created. This section needs to be included in all AnADAMA2 workflows basic, intermediate, or advanced. Line 3 adds the version and description information to the workflow. This is optional. These values will be printed when using the command line ``--version`` and ``--help`` options. 

**Section 2** is optional. In this section custom arguments are added to the command line options. Next the workflow parses the options. This command is not required as by default the workflow will parse the arguments from the command line when they are needed by the workflow. Because the options are parsed on demand, it is not possible to add custom arguments later on in the workflow, like after running tasks, because the command line arguments are needed before tasks can be created and run.

**Section 3** is optional. These two helper functions allow you to easily get input and output files. You do not need to include section 2 in you workflow to use these functions. The first function will return all of the input files with the extension provided in the input folder. The input folder can be provided by the user on the command line. The default location of the input folder is the current working directory. The output files function will only return the name of the output files providing the basenames, extensions, and tags. These two functions differ in that the input files function will search the input folder for files with the selected properties (ie. extension) while the output files function will only return full path names. The output files function will not search the output folder as the files to be named likely do not exist yet and will be created by the tasks. The output folder is a required option which can be set in the workflow or can be provided on the command line by the user.

**Section 4** is required. In all workflows, tasks will be added. These tasks can be added with ``do``, ``add_task``, ``add_task_group``, ``do_gridable``, ``add_task_gridable``, and ``add_task_group_gridable`` functions. The gridable functions allow for tasks to be run on a grid. See the grid computing section for more information. Tasks can have targets and dependencies. Targets are usually files that are created by the task and dependencies are usually files that are required as input for the task. Optionally parameters can be provided to the ``add_task`` function with their names substituted into the command if written in the square bracket format. For example, the task in the example workflow replaces ``[extension]`` in the command with the variable provided so it evaluated as ``args.input_extension`` which is the value of the input file extension, either the default or the value provided by the user on the command line.

**Section 5** is required. In all workflows, there must be at least one call to the ``go`` function. This is the function that will start the tasks running. If this function is not included, none of the tasks will be run.


**Run an Intermediate Workflow**
---------------------------------

The intermediate workflow shown below will run a set of fastq files with KneadData. This script is named ``kneaddata_workflow.py`` and can be found in the examples folder. Line 3 adds version information and a description to the workflow. Line 5 adds a custom option so that the user can provide the location of the kneaddata database on the command line with ``--kneaddata_db <folder>``. Line 6 adds another custom option allowing the user to set the input file extensions. Line 8 parses the arguments from the command line and returns their values. Line 10 returns a list of all of the files in the input folder with the extension provided. Line 11 returns the matched output files replacing the input folder location with the output folder and the input extension with that expected from kneaddata. Line 13 adds a group of tasks. One task is added for each of the input and matched output files. Line 15 executes the tasks.

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(version=""0.0.1"", description=""A workflow to run KneadData"" )
    4
    5 workflow.add_argument(name=""kneaddata_db"", description=""the kneaddata database"", default=""/work/code/kneaddata/db/"")
    6 workflow.add_argument(name=""input_extension"", description=""the input file extension"", default=""fastq"")
    7 workflow.add_argument(""threads"", desc=""number of threads for kneaddata to use"", default=1)
    8 args = workflow.parse_args()
    9
    10 in_files = workflow.get_input_files(extension=args.input_extension)
    11 out_files = workflow.name_output_files(name=in_files, tag=""kneaddata"")
    12
    13 workflow.add_task_group(""kneaddata -i [depends[0]] -o [output_folder] -db [kneaddata_db] -t [threads]"", depends=in_files, targets=out_files, output_folder=args.output, kneaddata_db=args.kneaddata_db, threads=args.threads)
    14   
    15 workflow.go()

To see the version number for this workflow run:

:: 

    $ python kneaddata_workflow.py --version

To print the options (default and custom) along with the custom description for this workflow run:

:: 

    $ python kneaddata_workflow.py --help

To run the workflow (providing the input folder, output folder, extension, and threads):

:: 

    $ python kneaddata_workflow.py --input input_dir --output output_dir --input_extension .fastq.gz --threads 4

**Add a Document**
---------------------

A document can be added to the workflow to allow for automatic generation of reports including text, tables, plots, and heatmaps based on data generated from the workflow tasks. The reports can be formatted as any `Pandoc <http://pandoc.org/installing.html>`_ output type (ie pdf, html, html5, json, docx, etc). The report format requested is indicated by the extension of the report file name.

The default document feature uses `Pweave <http://mpastell.com/pweave>`_ and `Pandoc <http://pandoc.org/installing.html>`_. Alternatively, a custom documentation class can be provided to your workflow instance if you would like to customize the documentation feature.

**Document Template**

To add a document to your workflow, first create a Pweave template. It can be formatted as any Pweave input type (ie pandoc markdown, script, tex, or rst). For more information on these formats, see the `Pweave documentation <http://mpastell.com/pweave>`_. The template file extension indicates the file format with ``.py`` for python and ``.mdw`` for Pandoc markdown. 

The templates can use the AnADAMA2 document to add features like tables, plots, and heatmaps. For more examples\, see the `bioBakery Workflows <https://bitbucket.org/biobakery/biobakery_workflows/wiki/Home>`_ document templates.

*Example Template*

:: 

    1 #+ echo=False
    2 import time
    3 from anadama2 import PweaveDocument
    4 document = PweaveDocument()
    5 
    6 vars = document.get_vars()
    7 
    8 #' % <% print(vars[""title""]) %>
    9 #' % Project: <% print(vars[""project""]) %>
    10 #' % Date: <%= time.strftime(""%m/%d/%Y"") %>
    11
    12 #' # Introduction
    13 #' <% print(vars[""introduction_text""]) %>
    14
    15 #' # Functional profiling
    16
    17 #' This report section contains information about the functional profiling run
    18 #' on all samples. These samples were
    19 #' run through [HUMAnN2](http://huttenhower.sph.harvard.edu/humann2).
    20 
    21 #+ echo=False
    22 samples, pathways, data = document.read_table(vars[""file_pathabundance_unstratified""])
    23 
    24 #' There were a total of <% print(len(samples)) %> samples run for this data set. Overall
    25 #' there were <% print(len(pathways)) %> pathways identified.


* Line 1: Start a python section indicating that the code should not be written to the document.

  - If echo is set to true, the text will appear in the document. 

* Lines 3 and 4: Import the default AnADAMA2 document and create an instance. 
* Line 6: Get the variables for the document.

  - The variables are the ""vars"" argument provided to the ""add_document"" function in the workflow.
  - The variables for this template are ""title"", ""project"", ""introduction_text"", and ""file_pathabundance_unstratified"". 

* Line 8: Add a title using the document variable ""title"".
* Line 9: Add the project name to the report.
* Line 10 Add the current date. 
* Line 12: Add the introduction section header.
* Line 13: Add the introduction text.
* Line 19: Add text including a link to the HUMAnN2 landing page.
* Line 21: Start a section of python which will not be echoed to the document.
* Line 22: Read the pathway abundance file returning sample names, pathway names, and the data matrix.
* Lines 24-25: Compute the total number of samples and pathways and add them to the generated text.

**Workflow to Create Document**

Next create a workflow that includes a document. A workflow that creates a document can be short with just code to create a workflow instance, add a document, and then run go to start the workflow. This example workflow will run all of the fastq.gz files in the input folder through humann2, then merge the pathway abundance output files, and split the file to create a stratified and unstratified pathway abundance file with data for all samples.

*Example Workflow*

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(version=""0.1"", description=""A workflow to run hummann2 and create a document"")
    4 
    5 args = workflow.parse_args()
    6
    7 input_files = workflow.get_input_files(extension="".fastq.gz"")
    8 humann2_output_files = workflow.name_output_files(name=input_files, tag=""pathabundance"", extension=""tsv"")
    9 humann2_merged_pathabundance = workflow.name_output_files(""merged_pathabundance.tsv"")
    10 humann2_merged_pathab_unstrat = workflow.name_output_files(""merged_pathabundance_unstratified.tsv"")
    11 document_file = workflow.name_output_files(""functional_profiling_document.pdf"")
    12
    13 workflow.add_task_group(
    14    ""humann2 --input [depends[0]] --output [vars[0]]"",
    15    depends=input_files,
    16    targets=humann2_output_files,
    17    vars=args.output)
    18
    19 workflow.add_task(
    20    ""humann2_join_tables --input [vars[0]] --output [targets[0]] --file_name pathab"",
    21     depends=humann2_output_files,
    22     targets=humann2_merged_pathabundance,
    23     vars=args.output)
    24
    25 workflow.add_task(
    26    ""humann2_split_stratified_table --input [depends[0]] --output [vars[0]]"",
    27     depends=humann2_merged_pathabundance,
    28     targets=humann2_merged_pathab_unstrat,
    29     vars=args.output)
    30
    31 workflow.add_document(
    32    templates=""template.py"",
    33    depends=humann2_merged_pathab_unstrat,
    34    targets=document_file,
    35    vars={""title"":""Demo Title"",
    36        ""project"":""Demo Project"",
    37        ""introduction_text"":""This is a demo document."",
    38        ""file_pathabundance_unstratified"":humann2_merged_pathab_unstrat})
    39
    40 workflow.go()

* Lines 13-17: Add a group of tasks to run HUMAnN2 on each fastq.gz input file.
* Lines 19-23: Add a task to join the pathway abundance tables for each sample.
* Lines 25-26: Add a task to split the pathway abundance table into a stratified and unstratified table. 
* Lines 31-38: Add a document to the workflow.

  - In this example the template file is named ""template.py"". It is a python script with pandoc markdown.
  - Depends and targets are set for documents just like for tasks since generating a document is a task. 
  - The variables provided to the document task in lines 35-38 are passed to the document template. 
  - Document variables can be any python object that is pickleable. 
  - The document file generated is named ""functional_profiling_document.pdf"" and is located in the output folder. Changing the document file name to ""functional_profiling_document.html"" would generate a html report.
  - To add a table of contents to your report, add ``table_of_contents=True`` to the add_document function.

**Run the Workflow**

Finally run the workflow with python. For this example, if the workflow is saved as a file named ""example_doc_workflow.py"", it would be run as follows.

:: 

    $ python example_doc_workflow.py --input input_folder --output output_folder

The document created will look like the following image (with the date replaced with the date the workflow was run).

.. image:: https://bitbucket.org/repo/gaqz78/images/3478728655-Screenshot%20from%202017-01-18%2017-04-22.png


**Add an Archive**
-------------------

An archive can be added to a workflow to allow for automated generation of a packaged file containing products from various steps in the workflow. The user specifies which items are included in the archive and the type of archive. Archives are useful for packaging documents plus their corresponding figures and data files.

To add an archive to your workflow, call the ``add_archive`` workflow function with the path to the archive you would like to create along with the folders/files you would like to archive. Here is the document workflow example from the prior section with an archive added.

*Example Workflow*

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(version=""0.1"", description=""A workflow to run hummann2 and create a document"")
    4 
    5 args = workflow.parse_args()
    6
    7 input_files = workflow.get_input_files(extension="".fastq.gz"")
    8 humann2_output_files = workflow.name_output_files(name=input_files, tag=""pathabundance"", extension=""tsv"")
    9 humann2_merged_pathabundance = workflow.name_output_files(""merged_pathabundance.tsv"")
    10 humann2_merged_pathab_unstrat = workflow.name_output_files(""merged_pathabundance_unstratified.tsv"")
    11 document_file = workflow.name_output_files(""functional_profiling_document.pdf"")
    12
    13 workflow.add_task_group(
    14    ""humann2 --input [depends[0]] --output [vars[0]]"",
    15    depends=input_files,
    16    targets=humann2_output_files,
    17    vars=args.output)
    18
    19 workflow.add_task(
    20    ""humann2_join_tables --input [vars[0]] --output [targets[0]] --file_name pathab"",
    21     depends=humann2_output_files,
    22     targets=humann2_merged_pathabundance,
    23     vars=args.output)
    24
    25 workflow.add_task(
    26    ""humann2_split_stratified_table --input [depends[0]] --output [vars[0]]"",
    27     depends=humann2_merged_pathabundance,
    28     targets=humann2_merged_pathab_unstrat,
    29     vars=args.output)
    30
    31 doc_task=workflow.add_document(
    32    templates=""template.py"",
    33    depends=humann2_merged_pathab_unstrat,
    34    targets=document_file,
    35    vars={""title"":""Demo Title"",
    36        ""project"":""Demo Project"",
    37        ""introduction_text"":""This is a demo document."",
    38        ""file_pathabundance_unstratified"":humann2_merged_pathab_unstrat})
    39
    40 workflow.add_archive(
    41     targets=args.output+"".zip"",
    42     depends=[args.output,doc_task],
    43     remove_log=True)
    44
    45 workflow.go()


* Line 31: This line is modified to capture the add document task in a variable.
* Lines 40-43: These are the new lines added to add the archive. The target is the path to the archive that will be created. It is named the same as the output folder provided by the user on the command line. The list of dependencies are those items to include in the archive. This example includes the full output folder in the archive. Dependencies can also be tasks which must run before the archive is to be created. These task dependencies will determine when the archive will be rerun. For example whenever a new document is created the archive task will be rerun. The ""zip"" target extension sets the archive type. An optional parameter is set to indicate the workflow log should not be included in the archive.


**Run in a Grid Computing Environment**
---------------------------------------

Writing a workflow to run in a grid computing environment is very
similar to writing a workflow to run on your own local machine. We can
make just a few changes to modify the example ""exe_check.py"" workflow
to run all tasks on the grid. We just need to specify which tasks
we would like to run on the grid replacing ``do`` with ``do_gridable``.
These tasks by default will run locally. If the command line option ``--grid-run``
is provided the ``gridable`` tasks will be run on the grid partition provided.
In lines 4-6 we also add three arguments to specify
memory (in MB), the number of cores, and the time in minutes that each
task should be allowed in the queue.

:: 

    1 from anadama2 import Workflow
    2 
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.do_gridable(""ls /usr/bin/ | sort > [t:global_exe.txt]"", mem=20, cores=1, time=2)
    5 workflow.do_gridable(""ls $HOME/.local/bin/ | sort > [t:local_exe.txt]"", mem=20, cores=1, time=2)
    6 workflow.do_gridable(""join [d:global_exe.txt] [d:local_exe.txt] > [t:match_exe.txt]"", mem=20, cores=1, time=2)
    7 workflow.go()

To run this workflow with 2 tasks running in parallel on a slurm grid on partition general, run:

:: 

    $ python exe_check.py --grid-jobs 2

Another option is to submit only some of the tasks to the grid computing environment. This can be done by changing the ``do_gridable`` to ``do``. We can modify our example to only run the global executable check on the grid. 

:: 

    1 from anadama2 import Workflow
    2 
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.do_gridable(""ls /usr/bin/ | sort > [t:global_exe.txt]"", mem=20, cores=1, time=2)
    5 workflow.do(""ls $HOME/.local/bin/ | sort > [t:local_exe.txt]"")
    6 workflow.do(""join [d:global_exe.txt] [d:local_exe.txt] > [t:match_exe.txt]"")
    7 workflow.go()

Similarly if we were using ``add_task`` instead of ``do``, ``add_task_gridable`` will run the task on the grid while ``add_task`` will always run the task locally.

:: 

    1 from anadama2 import Workflow
    2 
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task_gridable(""ls /usr/bin/ | sort > [targets[0]]"", targets=""global_exe.txt"", mem=20, cores=1, time=2)
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[0]] > [targets[0]]"", targets=""match_exe.txt"", depends=[""global_exe.txt"",""local_exe.txt""])
    7 workflow.go()

If you are running with sun grid engine instead of slurm, you do not need to modify your workflow. The software will identify the grid engine installed and run on the grid available. Alternatively, when running your workflow provide the options ``--grid <slurm/sge>`` and ``--grid-partition <general>`` on the command line.

The time and memory requests for each task can be an integer or an equation. Time is always specified in minutes and memory is specified in MB. Equations can include the same formatting as task commands with replacement for dependency and core variables. For example ``time=""10 / [cores]""`` would request times based on the number of cores for that specific task (ie. 10 minutes for 1 core and 5 minutes for 2 cores). Equations can also use the size of the files that the task depends on. For example, ``time=""10 * [cores] * file_size('[depends[0]]')""`` could be used to set the time for a task. The ``file_size`` function is an AnADAMA2 helper function that returns the size of a file in GB. This equation will be evaluated for the time right before the task is to be started. This allows for equations to use the size of files that might not exist when the workflow starts but will exist before a task is set to start running.

-----------

**Advanced Usage**
..................

**Functions**
-------------

All of the tasks in the example are currently commands. They are strings that would be run on the command line. Instead of passing commands as a task you can provide functions. Here is the example after replacing line 5 with a function that writes a file with the list of the global executables.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.util import get_name
    3
    4 workflow = Workflow(remove_options=[""input"",""output""])
    5 def get_global_exe(task):
    6    import subprocess
    7    outfile=open(get_name(task.targets[0]), ""w"")
    8    process=subprocess.check_call(""ls /usr/bin/ | sort"", shell=True, stdout=outfile)
    9    outfile.close()
    10
    11 workflow.add_task(get_global_exe, targets=""global_exe.txt"")
    12 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    13 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    14 workflow.go()

**Decorators**
--------------

A decorator is a function that takes another function as input and extends the behavior of the input function. Instead of providing a function as input to a task you can use the add_task to decorate your input function. Taking the example workflow which already included a function in the prior section and using the decorator method moves line 10 to line 4, right before the function get_global_exe.

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 @workflow.add_task(targets=""global_exe.txt"")
    5 def get_global_exe(task):
    6    import subprocess
    7    outfile=open(task.targets[0].name, ""w"")
    8    process=subprocess.check_call(""ls /usr/bin/ | sort"", shell=True, stdout=outfile)
    9    outfile.close()
    10
    11 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    12 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    13 workflow.go()

**Types of Tracked Items**
--------------------------

Targets and dependencies, both of which are tracked items, can be one of six types. They are provided to the function add_task.

**Files**
~~~~~~~~~

By default all targets and dependencies provided as strings to add_task are file dependencies. Depending on the size of the file these could be large file dependencies for which the checksums are not tracked to save time. We can specify the types of dependences as file and huge files in the add task function. In line 4 we specify a file dependency and in line 6 we specify a huge file dependency. In line 2 we import the AnADAMA2 dependency functions.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import TrackedFile, HugeTrackedFile
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task(""ls /usr/bin/ | sort > [targets[0]]"", targets=TrackedFile(""global_exe.txt""))
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=HugeTrackedFile(""match_exe.txt""))
    7 workflow.go()

**Directories**
~~~~~~~~~~~~~~~

Directories can be specified in the same way as file dependencies. In line 4 of the example, a directory dependency is added. In line 2 we import the AnADAMA2 dependency function.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import TrackedDirectory
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task(""ls /usr/bin/ | sort > [targets[0]]"", targets=""global_exe.txt"", depends=TrackedDirectory(""/usr/bin/""))
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    7 workflow.go()

**Executables**
~~~~~~~~~~~~~~~

Executables can be specified in the same way as file and directory dependencies. Adding line 2 and then including an executable dependency in line 4 of the example adds the ""ls"" dependency.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import TrackedExecutable
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task(""ls /usr/bin/ | sort > [targets[0]]"", targets=""global_exe.txt"", depends=TrackedExecutable(""ls""))
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    7 workflow.go()

**Functions**
~~~~~~~~~~~~~

Function dependencies are specified similar to the other dependencies. In the example workflow, modified to include the function, we add the function itself as a dependency.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import TrackedFunction
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 def get_global_exe(task):
    5    import subprocess
    6    outfile=open(task.targets[0].name, ""w"")
    7    process=subprocess.check_call(""ls /usr/bin/ | sort"", shell=True, stdout=outfile)
    8    outfile.close()
    9
    10 workflow.add_task(get_global_exe, targets=""global_exe.txt"", TrackedFunction(get_global_exe))
    11 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    12 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    13 workflow.go()

**Containers**
~~~~~~~~~~~~~~

You can also have tasks that depend on variables or arguments to your task actions. In our example, say we only want the first N global executables. We would create a container with our variable name and then provide it as one of the task dependencies.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import Container
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 variables=Container(lines=2)
    5 workflow.add_task(""ls /usr/bin/ | sort | head -n [depends[0]] > [targets[0]]"", targets=""global_exe.txt"", depends=variables.lines)
    6 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    7 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt)
    8 workflow.go()

**File Patterns**
~~~~~~~~~~~~~~~~~

A set of files can also be a dependency. For example, if we only wanted to list all global executables that started with the letter ""a"", we could add them all as a dependency that is a set of files.

:: 

    1 from anadama2 import Workflow
    2 from anadama2.tracked import TrackedFilePattern
    3 workflow = Workflow(remove_options=[""input"",""output""])
    4 workflow.add_task(""ls /usr/bin/a* | sort > [targets[0]]"", targets=""global_exe.txt"", depends=TrackedFilePattern(""/usr/bin/a*""))
    5 workflow.add_task(""ls $HOME/.local/bin/ | sort > [targets[0]]"", targets=""local_exe.txt"")
    6 workflow.add_task(""join [depends[0]] [depends[1]] > [targets[0]]"", depends=[""global_exe.txt"",""local_exe.txt""], targets=""match_exe.txt"")
    7 workflow.go()

**Pre-Existing Dependencies**
-----------------------------

Tasks that depend on items that are not created by other tasks have pre-existing dependencies. By default AnADAMA2 will automatically try to register the pre-existing dependencies. If the dependency does not exist at runtime, an error will be issued. For example, if the file ""global_exe.txt"" did not exist and the following workflow was run an error will be issued.

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(remove_options=[""input"",""output""],strict=True)
    4 workflow.do(""ls $HOME/.local/bin/ | sort > [t:local_exe.txt]"")
    5 workflow.do(""join [d:global_exe.txt] [d:local_exe.txt] > [t:match_exe.txt]"")
    6 workflow.go()

There is a function that can register these dependencies. To use this function we would add line four to the prior workflow example.

:: 

    1 from anadama2 import Workflow
    2
    3 workflow = Workflow(remove_options=[""input"",""output""],strict=True)
    4 workflow.already_exists(""global_exe.txt"")
    5 workflow.do(""ls $HOME/.local/bin/ | sort > [t:local_exe.txt]"")
    6 workflow.do(""join [d:global_exe.txt] [d:local_exe.txt] > [t:match_exe.txt]"")
    7 workflow.go()


**API Documentation**
---------------------

The API documentation is hosted by Read the Docs at: `http://anadama2.readthedocs.io/ <http://anadama2.readthedocs.io/>`_ 

The API is automatically generated and will reflect the latest codebase.

If you would like to generate your own copy of the API, run the following command from the AnADAMA2 folder:

``$ python setup.py sphinx_build``
",2023-07-07 15:49:04+00:00
analyticszoo,analytics-zoo,intel-analytics/analytics-zoo,"Distributed Tensorflow, Keras and PyTorch on Apache Spark/Flink & Ray",https://analytics-zoo.readthedocs.io/,False,2574,2023-07-06 12:45:51+00:00,2017-05-05 02:27:30+00:00,728,107,98,15,v0.11.2,2022-01-24 01:40:34+00:00,Apache License 2.0,3451,v0.11.2,15,2022-01-24 01:40:34+00:00,2023-07-06 12:45:53+00:00,2023-07-06 00:49:11+00:00,"***Note: We have merged Analytics Zoo into [BigDL 2.0](https://github.com/intel-analytics/BigDL/issues/4085), and our future development will move to the [BigDL](https://github.com/intel-analytics/BigDL) project***


---

<div align=""center"">

<p align=""center""> <img src=""docs/docs/Image/logo.jpg"" height=""140px""><br></p>

**Distributed TensorFlow, PyTorch, Keras and BigDL on Apache Spark & Ray**

</div>

---

Analytics Zoo is an open source _**Big Data AI**_ platform, and includes the following features for scaling end-to-end AI to distributed Big Data: 

 * [Orca](#getting-started-with-orca): seamlessly scale out TensorFlow and PyTorch for Big Data (using Spark & Ray)
 
 * [RayOnSpark](#getting-started-with-rayonspark): run Ray programs directly on Big Data clusters
 
 * [BigDL Extensions](#getting-started-with-bigdl-extensions): high-level Spark ML pipeline and Keras-like APIs for BigDL
 
 * [Chronos](#getting-started-with-chronos): scalable time series analysis using AutoML
 
 * [PPML](#ppml-privacy-preserving-machine-learning): privacy preserving big data analysis and machine learning (*experimental*)

For more information, you may [read the docs](https://analytics-zoo.readthedocs.io/).

---

## Installing
You can use Analytics Zoo on [Google Colab](https://analytics-zoo.readthedocs.io/en/latest/doc/UserGuide/colab.html) without any installation. Analytics Zoo also includes a set of [notebooks](https://analytics-zoo.readthedocs.io/en/latest/doc/UserGuide/notebooks.html) that you can directly open and run in Colab.

To install Analytics Zoo, we recommend using [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/)  environments.

```bash
conda create -n my_env 
conda activate my_env
pip install analytics-zoo 
```

To install latest nightly build, use ```pip install --pre --upgrade analytics-zoo```; see [Python](https://analytics-zoo.readthedocs.io/en/latest/doc/UserGuide/python.html)  and [Scala](https://analytics-zoo.readthedocs.io/en/latest/doc/UserGuide/scala.html) user guide for more details.

## Getting Started with Orca

Most AI projects start with a Python notebook running on a single laptop; however, one usually needs to go through a mountain of pains to scale it to handle larger data set in a distributed fashion. The  _**Orca**_ library seamlessly scales out your single node TensorFlow or PyTorch notebook across large clusters (so as to process distributed Big Data).

First, initialize [Orca Context](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca-context.html):

```python
from zoo.orca import init_orca_context

# cluster_mode can be ""local"", ""k8s"" or ""yarn""
sc = init_orca_context(cluster_mode=""yarn"", cores=4, memory=""10g"", num_nodes=2) 
```

Next, perform [data-parallel processing in Orca](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) (supporting standard Spark Dataframes, TensorFlow Dataset, PyTorch DataLoader, Pandas, etc.):

```python
from pyspark.sql.functions import array

df = spark.read.parquet(file_path)
df = df.withColumn('user', array('user')) \  
       .withColumn('item', array('item'))
```

Finally, use [sklearn-style Estimator APIs in Orca](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/distributed-training-inference.html) to perform distributed _TensorFlow_, _PyTorch_ or _Keras_ training and inference:

```python
from tensorflow import keras
from zoo.orca.learn.tf.estimator import Estimator

user = keras.layers.Input(shape=[1])  
item = keras.layers.Input(shape=[1])  
feat = keras.layers.concatenate([user, item], axis=1)  
predictions = keras.layers.Dense(2, activation='softmax')(feat)  
model = keras.models.Model(inputs=[user, item], outputs=predictions)  
model.compile(optimizer='rmsprop',  
              loss='sparse_categorical_crossentropy',  
              metrics=['accuracy'])

est = Estimator.from_keras(keras_model=model)  
est.fit(data=df,  
        batch_size=64,  
        epochs=4,  
        feature_cols=['user', 'item'],  
        label_cols=['label'])
```

See [TensorFlow](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/QuickStart/orca-tf-quickstart.html) and [PyTorch](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/QuickStart/orca-pytorch-quickstart.html) quickstart, as well as the [document website](https://analytics-zoo.readthedocs.io/), for more details.

## Getting Started with RayOnSpark

Ray is an open source distributed framework for emerging AI applications. _**RayOnSpark**_ allows users to directly run Ray programs on existing Big Data clusters, and directly write Ray code inline with their Spark code (so as to process the in-memory Spark RDDs or DataFrames).

```python
from zoo.orca import init_orca_context

# cluster_mode can be ""local"", ""k8s"" or ""yarn""
sc = init_orca_context(cluster_mode=""yarn"", cores=4, memory=""10g"", num_nodes=2, init_ray_on_spark=True) 

import ray

@ray.remote
class Counter(object):
      def __init__(self):
          self.n = 0

      def increment(self):
          self.n += 1
          return self.n

counters = [Counter.remote() for i in range(5)]
print(ray.get([c.increment.remote() for c in counters]))
```

See the RayOnSpark [user guide](https://analytics-zoo.readthedocs.io/en/latest/doc/Ray/Overview/ray.html) and [quickstart](https://analytics-zoo.readthedocs.io/en/latest/doc/Ray/QuickStart/ray-quickstart.html) for more details.

## Getting Started with BigDL Extensions

Analytics Zoo makes it easier to develop large-scale deep learning applications on Apache Spark, by providing high-level ***Spark ML pipeline*** and ***Keras***-like APIs on top of [BigDL](https://github.com/intel-analytics/BigDL) (a distributed deep learning framework for Spark).

First, call `initNNContext` at the beginning of the code: 

```scala
import com.intel.analytics.zoo.common.NNContext
val sc = NNContext.initNNContext()
```

Then, define the BigDL model using Keras-style API:

```scala
val input = Input[Float](inputShape = Shape(10))  
val dense = Dense[Float](12).inputs(input)  
val output = Activation[Float](""softmax"").inputs(dense)  
val model = Model(input, output)
```

After that, use `NNEstimator` to train/predict/evaluate the model using Spark Dataframes and ML pipelines:

```scala
val trainingDF = spark.read.parquet(""train_data"")
val validationDF = spark.read.parquet(""val_data"")
val scaler = new MinMaxScaler().setInputCol(""in"").setOutputCol(""value"")
val estimator = NNEstimator(model, CrossEntropyCriterion())  
        .setBatchSize(size).setOptimMethod(new Adam()).setMaxEpoch(epoch)
val pipeline = new Pipeline().setStages(Array(scaler, estimator))

val pipelineModel = pipeline.fit(trainingDF)  
val predictions = pipelineModel.transform(validationDF)
```
See the [Scala](https://analytics-zoo.readthedocs.io/en/latest/doc/UserGuide/scala.html), [NNframes](https://analytics-zoo.readthedocs.io/en/latest/doc/UseCase/nnframes.html) and [Keras API](https://analytics-zoo.readthedocs.io/en/latest/doc/UseCase/keras-api.html) user guides for more details.

## Getting Started with Chronos

Time series prediction takes observations from previous time steps as input and predicts the values at future time steps. The _**Chronos**_ library makes it easy to build end-to-end time series analysis by applying AutoML to extremely large-scale time series prediction.

To train a time series model with AutoML, first initialize [Orca Context](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca-context.html):

```python
from zoo.orca import init_orca_context

#cluster_mode can be ""local"", ""k8s"" or ""yarn""
sc = init_orca_context(cluster_mode=""yarn"", cores=4, memory=""10g"", num_nodes=2, init_ray_on_spark=True)
```

Next, create an _AutoTSTrainer_.

```python
from zoo.chronos.autots.deprecated.forecast import AutoTSTrainer

trainer = AutoTSTrainer(dt_col=""datetime"", target_col=""value"")
```

Finally, call ```fit``` on _AutoTSTrainer_, which applies AutoML to find the best model and hyper-parameters; it returns a _TSPipeline_ which can be used for prediction or evaluation.

```python
#train a pipeline with AutoML support
ts_pipeline = trainer.fit(train_df, validation_df)

#predict
ts_pipeline.predict(test_df)
```

See the Chronos [user guide](https://analytics-zoo.readthedocs.io/en/latest/doc/Chronos/Overview/chronos.html) and [example](https://analytics-zoo.readthedocs.io/en/latest/doc/Chronos/QuickStart/chronos-autots-quickstart.html) for more details.

## PPML (Privacy Preserving Machine Learning)

***Analytics Zoo PPML*** provides a *Trusted Cluster Environment* for protecting the end-to-end Big Data AI pipeline. It combines various low level hardware and software security technologies (e.g., Intel SGX, LibOS such as Graphene and Occlum, Federated Learning, etc.), and allows users to run unmodified Big Data analysis and ML/DL programs (such as Apache Spark, Apache Flink, Tensorflow, PyTorch, etc.) in a secure fashion on (private or public) cloud.

See the [PPML user guide](https://analytics-zoo.readthedocs.io/en/latest/doc/PPML/Overview/ppml.html) for more details. 

## More information

- [Document Website](https://analytics-zoo.readthedocs.io/)
- [Mail List](mailto:bigdl-user-group+subscribe@googlegroups.com)
- [User Group](https://groups.google.com/forum/#!forum/bigdl-user-group)
- [Powered-By](https://analytics-zoo.readthedocs.io/en/latest/doc/Application/powered-by.html)
- [Presentations](https://analytics-zoo.readthedocs.io/en/latest/doc/Application/presentations.html)

_Older Documents_
- [BigDL Paper](https://arxiv.org/abs/1804.05839)
- [Documentation (old)](https://analytics-zoo.github.io/)
",2023-07-07 15:49:08+00:00
angel-ml,angel,Angel-ML/angel,A Flexible and Powerful Parameter Server for large-scale machine learning,,False,6671,2023-07-07 08:05:13+00:00,2017-04-25 05:57:43+00:00,1632,451,50,19,Release-3.2.0,2021-08-06 08:26:25+00:00,Other,2963,v3.1.0,19,2020-05-06 09:25:16+00:00,2023-07-03 03:14:22+00:00,2022-11-24 08:07:54+00:00,"![](assets/angel_logo.png)

[![license](http://img.shields.io/badge/license-Apache2.0-brightgreen.svg?style=flat)](https://github.com/Angel-ML/angel/blob/branch-3.2.0/LICENSE.TXT)
[![Release Version](https://img.shields.io/badge/release-3.2.0-red.svg)](https://github.com/tencent/angel/releases)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/tencent/angel/pulls)
[![Download Code](https://img.shields.io/badge/download-zip-green.svg)](https://github.com/Angel-ML/angel/archive/refs/heads/branch-3.2.0.zip)

[(ZH-CN Version)](./README_CN.md)

**Angel** is a high-performance distributed machine learning and graph computing platform based on the philosophy of Parameter Server. It is tuned for performance with big data from Tencent and has a wide range of applicability and stability, demonstrating increasing advantage in handling higher dimension model. Angel is jointly developed by Tencent and Peking University, taking account of both high availability  in industry and innovation in academia.

With model-centered core design concept, **Angel** partitions parameters of complex models into multiple parameter-server nodes, and implements a variety of machine learning algorithms and graph algorithms using efficient model-updating interfaces and functions, as well as flexible consistency model for synchronization.

**Angel** is developed with **Java** and **Scala**.  It supports running on **Yarn**. With **PS Service** abstraction, it supports **Spark on Angel**.  Graph computing and deep learning frameworks support is under development and will be released in the future.

We welcome everyone interested in machine learning or graph computing to contribute code, create issues or pull requests. Please refer to  [Angel Contribution Guide](https://github.com/Tencent/angel/blob/master/CONTRIBUTING.md) for more detail.

## Introduction to Angel

* [Architecture](./docs/overview/architecture_en.md)
* [Code Framework](./docs/overview/code_framework_en.md)
* [Design](./docs/overview/design_philosophy_en.md)
* [Spark on Angel](./docs/overview/spark_on_angel_en.md)
  * [Machine Learning](./docs/overview/spark_on_angel_en.md)
  * [Graph Computing](./docs/overview/angel_graph_sona_en.md)

## Design

- [Model Partitioner](./docs/design/model_partitioner_en.md)
- [SyncController](./docs/design/sync_controller_en.md)
- [psFunc](./docs/design/psfFunc_en.md)
- [Core API](./docs/apis/core_api_en.md)


## Quick Start

* [Quick Start Example](./docs/tutorials/spark_on_angel_quick_start_en.md)

## Deployment

* [Compilation Guide](./docs/deploy/source_compile_en.md)
* [Running on Local](./docs/deploy/local_run_en.md)
* [Running on Yarn](./docs/deploy/run_on_yarn_en.md)
* [Configuration Details](./docs/deploy/config_details_en.md)
* [Resource Configuration Guide](./docs/deploy/resource_config_guide_en.md)

## Programming Guide

* [Spark on Angel Programming Guide](./docs/programmers_guide/spark_on_angel_programing_guide_en.md)

## Algorithm

- [**Angel or Spark On Angel？**](./docs/algo/angel_or_spark_on_angel.md)
- [**Algorithm Parameter Description**](./docs/algo/model_config_details.md)
- **Angel**
  - **Traditional Machine Learning Methods**
    - [Logistic Regression(LR)](./docs/algo/lr_on_angel_en.md)
    - [Support Vector Machine(SVM)](./docs/algo/svm_on_angel_en.md)
    - [Factorization Machine(FM)](./docs/algo/fm_on_angel.md)
    - [Linear Regression](./docs/algo/linear_on_angel_en.md)
    - [Robust Regression](./docs/algo/robust_on_angel_en.md)
    - [Softmax Regression](./docs/algo/softmax_on_angel_en.md)
    - [KMeans](./docs/algo/kmeans_on_angel_en.md)
    - [GBDT](./docs/algo/gbdt_on_angel_en.md)
    - [LDA\*](./docs/algo/lda_on_angel_en.md) ([WarpLDA](./docs/algo/warp_lda_on_angel.md))
- **Spark on Angel**
  - **Angel Mllib**
    - [FM](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [DeepFM](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [DeepAndWide](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [DCN](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [XDeepFM](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [AttentionFM](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [PNN](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/recommendation.md)
    - [FTRL](./docs/algo/ftrl_lr_spark.md)
    - [Logistic Regression(LR)](./docs/algo/sona/lr_sona.md)
    - [FTRLFM](./docs/algo/ftrl_fm_spark_en.md)
    - [GBDT](./docs/algo/sona/feature_gbdt_sona.md)
  - **Angel Graph**
    - [PageRank](./docs/algo/sona/pagerank_on_sona_en.md)
    - [KCORE](./docs/algo/sona/kcore_sona_en.md)
    - [HIndex](./docs/algo/sona/hindex_sona_en.md)
    - [Closeness](./docs/algo/sona/closeness_sona_en.md)
    - [CommonFriends](./docs/algo/sona/commonfriends_sona_en.md)
    - [ConnectedComponents](./docs/algo/sona/CC_sona_en.md)
    - [TriangleCountingUndirected](./docs/algo/sona/triangle_count_undirected_en.md)
    - [Louvain](./docs/algo/sona/louvain_sona_en.md)
    - [LPA](./docs/algo/sona/LPA_sona_en.md)
    - [LINE](./docs/algo/sona/line_sona_en.md)
    - [Word2Vec](./docs/algo/sona/word2vec_sona_en.md)
    - [GraphSage](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/graph.md)
    - [GCN](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/graph.md)
    - [DGI](https://github.com/Angel-ML/PyTorch-On-Angel/blob/branch-0.2.0/docs/graph.md)

## Community
* Mailing list: angel-tsc@lists.deeplearningfoundation.org
* Angel homepage in Linux FD: https://angelml.ai/
* [Committers & Contributors](./COMMITTERS.md)
* [Contributing to Angel](./CONTRIBUTING.md)
* [Roadmap](https://github.com/Angel-ML/angel/wiki/Roadmap)

## FAQ
* [Angel FAQ](https://github.com/Tencent/angel/wiki/Angel%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)

## Papers
  1. [PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm](https://dl.acm.org/doi/pdf/10.1145/3485447.3511986). WWW, 2022
  2. [Graph Attention Multi-Layer Perceptron](https://dl.acm.org/doi/pdf/10.1145/3534678.3539121). KDD, 2022
  3. [Node Dependent Local Smoothing for Scalable Graph Learning](https://proceedings.neurips.cc/paper/2021/file/a9eb812238f753132652ae09963a05e9-Paper.pdf). NeurlPS, 2021
  4. [PSGraph: How Tencent trains extremely large-scale graphs with Spark?](https://conferences.computer.org/icde/2020/pdfs/ICDE2020-5acyuqhpJ6L9P042wmjY1p/290300b549/290300b549.pdf).ICDE, 2020.
  5. [DimBoost: Boosting Gradient Boosting Decision Tree to Higher Dimensions](https://dl.acm.org/citation.cfm?id=3196892). SIGMOD, 2018.
  6. [LDA*: A Robust and Large-scale Topic Modeling System](http://www.vldb.org/pvldb/vol10/p1406-yu.pdf). VLDB, 2017
  7. [Heterogeneity-aware Distributed Parameter Servers](http://net.pku.edu.cn/~cuibin/Papers/2017%20sigmod.pdf). SIGMOD, 2017
  8. [Angel: a new large-scale machine learning system](http://net.pku.edu.cn/~cuibin/Papers/2017NSRangel.pdf). National Science Review (NSR), 2017
  9. [TencentBoost: A Gradient Boosting Tree System with Parameter Server](http://net.pku.edu.cn/~cuibin/Papers/2017%20ICDE%20boost.pdf).	ICDE, 2017
",2023-07-07 15:49:12+00:00
archivematica,archivematica,artefactual/archivematica,"Free and open-source digital preservation system designed to maintain standards-based, long-term access to collections of digital objects.",http://www.archivematica.org,False,369,2023-06-24 09:18:48+00:00,2012-09-14 16:43:38+00:00,101,45,53,42,v1.14.0,2023-06-14 03:32:10+00:00,GNU Affero General Public License v3.0,4291,v1.14.0,104,2023-06-14 03:17:02+00:00,2023-06-30 22:04:33+00:00,2023-06-14 03:17:02+00:00,"[![GitHub CI](https://github.com/artefactual/archivematica/actions/workflows/test.yml/badge.svg)](https://github.com/artefactual/archivematica/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/artefactual/archivematica/branch/qa/1.x/graph/badge.svg?token=tKlfjhmrlC)](https://codecov.io/gh/artefactual/archivematica)

# [Archivematica](https://www.archivematica.org/)

By [Artefactual](https://www.artefactual.com/)

Archivematica is a web- and standards-based, open-source application which allows your institution to preserve long-term access to trustworthy, authentic and reliable digital content.
Our target users are archivists, librarians, and anyone working to preserve digital objects.

You are free to copy, modify, and distribute Archivematica with attribution under the terms of the AGPLv3 license.
See the [LICENSE](LICENSE) file for details.


## Installation

* [Production installation](https://www.archivematica.org/docs/latest/admin-manual/installation-setup/installation/installation/)
* [Development installation](https://github.com/artefactual/archivematica/tree/qa/1.x/hack)


## Other resources

* [Website](https://www.archivematica.org/): User and administrator documentation
* [Wiki](https://www.archivematica.org/wiki/Development): Developer facing documentation, requirements analysis and community resources
* [Issues](https://github.com/archivematica/Issues): Git repository used for tracking Archivematica issues and feature/enhancement ideas
* [User Google Group](https://groups.google.com/forum/#!forum/archivematica): Forum/mailing list for user questions (both technical and end-user)
* [Paid support](https://www.artefactual.com/services/): Paid support, hosting, training, consulting and software development contracts from Artefactual


## Contributing

Thank you for your interest in Archivematica!
For more details, see the [contributing guidelines](CONTRIBUTING.md)


## Reporting an issue

Issues related to Archivematica, the Storage Service, or any related repository can be filed in the [Archivematica Issues repository](https://github.com/archivematica/Issues/issues).


### Security

If you have a security concern about Archivematica or any related repository, please see the [SECURITY file](SECURITY.md) for information about how to safely report vulnerabilities.


## Related projects

Archivematica consists of several projects working together, including:

* [Archivematica](https://github.com/artefactual/archivematica): This repository! Main repository containing the user-facing dashboard, task manager MCPServer and clients scripts for the MCPClient
* [Storage Service](https://github.com/artefactual/archivematica-storage-service): Responsible for moving files to Archivematica for processing, and from Archivematica into long-term storage
* [Format Policy Registry](https://github.com/artefactual/archivematica/tree/qa/1.x/src/dashboard/src/fpr): Submodule shared between Archivematica and the Format Policy Registry (FPR) server that displays and updates FPR rules and commands

For more projects in the Archivematica ecosystem, see the [getting started](https://wiki.archivematica.org/Getting_started#Projects) page.
",2023-07-07 15:49:16+00:00
argo,argo-workflows,argoproj/argo-workflows,Workflow engine for Kubernetes,https://argoproj.github.io/argo-workflows/,False,13115,2023-07-07 15:10:53+00:00,2017-08-21 18:50:44+00:00,2847,200,429,270,v3.4.8,2023-05-25 19:09:06+00:00,Apache License 2.0,4432,v3.4.8,311,2023-05-25 19:09:06+00:00,2023-07-07 15:13:30+00:00,2023-07-07 14:40:52+00:00,"[![slack](https://img.shields.io/badge/slack-argoproj-brightgreen.svg?logo=slack)](https://argoproj.github.io/community/join-slack)
[![CI](https://github.com/argoproj/argo-workflows/workflows/CI/badge.svg)](https://github.com/argoproj/argo-workflows/actions?query=event%3Apush+branch%3Amaster)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3830/badge)](https://bestpractices.coreinfrastructure.org/projects/3830)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/argo-workflows)](https://artifacthub.io/packages/helm/argo/argo-workflows)
[![Twitter Follow](https://img.shields.io/twitter/follow/argoproj?style=social)](https://twitter.com/argoproj)

## What is Argo Workflows?

Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo
Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).

* Define workflows where each step in the workflow is a container.
* Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a directed acyclic
  graph (DAG).
* Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo
  Workflows on Kubernetes.

Argo is a [Cloud Native Computing Foundation (CNCF)](https://cncf.io/) hosted project.

[![Argo Workflows in 5 minutes](https://img.youtube.com/vi/TZgLkCFQ2tk/0.jpg)](https://www.youtube.com/watch?v=TZgLkCFQ2tk)

## Use Cases

* [Machine Learning pipelines](https://argoproj.github.io/argo-workflows/use-cases/machine-learning/)
* [Data and batch processing](https://argoproj.github.io/argo-workflows/use-cases/data-processing/)
* [Infrastructure automation](https://argoproj.github.io/argo-workflows/use-cases/infrastructure-automation/)
* [CI/CD](https://argoproj.github.io/argo-workflows/use-cases/ci-cd/)
* [Other use cases](https://argoproj.github.io/argo-workflows/use-cases/other/)

## Why Argo Workflows?

* Argo Workflows is the most popular workflow execution engine for Kubernetes.
* Light-weight, scalable, and easier to use.
* Designed from the ground up for containers without the overhead and limitations of legacy VM and server-based
  environments.
* Cloud agnostic and can run on any Kubernetes cluster.

[Read what people said in our latest survey](https://blog.argoproj.io/argo-workflows-2021-survey-results-d6fa890030ee)

## Try Argo Workflows

[Access the demo environment](https://workflows.apps.argoproj.io/workflows/argo) (login using Github)

![Screenshot](docs/assets/screenshot.png)

## Who uses Argo Workflows?

[About 200+ organizations are officially using Argo Workflows](USERS.md)

## Ecosystem

Just some of the projects that use or rely on Argo Workflows (complete list [here](https://github.com/akuity/awesome-argo#ecosystem-projects)):

* [Argo Events](https://github.com/argoproj/argo-events)
* [Couler](https://github.com/couler-proj/couler)
* [Hera](https://github.com/argoproj-labs/hera-workflows)
* [Katib](https://github.com/kubeflow/katib)
* [Kedro](https://kedro.readthedocs.io/en/stable/)
* [Kubeflow Pipelines](https://github.com/kubeflow/pipelines)
* [Netflix Metaflow](https://metaflow.org)
* [Onepanel](https://www.onepanel.ai/)
* [Orchest](https://github.com/orchest/orchest/)
* [Ploomber](https://github.com/ploomber/ploomber)
* [Seldon](https://github.com/SeldonIO/seldon-core)
* [SQLFlow](https://github.com/sql-machine-learning/sqlflow)

## Client Libraries

Check out our [Java, Golang and Python clients](docs/client-libraries.md).

## Quickstart 

* [Get started here](docs/quick-start.md)
* [How to write Argo Workflow specs](https://github.com/argoproj/argo-workflows/blob/master/examples/README.md)
* [How to configure your artifact repository](docs/configure-artifact-repository.md)

## Documentation

[View the docs](https://argoproj.github.io/argo-workflows/)

## Features

Incomplete list of features Argo Workflows provide:

* UI to visualize and manage Workflows
* Artifact support (S3, Artifactory, Alibaba Cloud OSS, Azure Blob Storage, HTTP, Git, GCS, raw)
* Workflow templating to store commonly used Workflows in the cluster
* Archiving Workflows after executing for later access
* Scheduled workflows using cron
* Server interface with REST API (HTTP and GRPC)
* DAG or Steps based declaration of workflows
* Step level input & outputs (artifacts/parameters)
* Loops
* Parameterization
* Conditionals
* Timeouts (step & workflow level)
* Retry (step & workflow level)
* Resubmit (memoized)
* Suspend & Resume
* Cancellation
* K8s resource orchestration
* Exit Hooks (notifications, cleanup)
* Garbage collection of completed workflow
* Scheduling (affinity/tolerations/node selectors)
* Volumes (ephemeral/existing)
* Parallelism limits
* Daemoned steps
* DinD (docker-in-docker)
* Script steps
* Event emission
* Prometheus metrics
* Multiple executors
* Multiple pod and workflow garbage collection strategies
* Automatically calculated resource usage per step
* Java/Golang/Python SDKs
* Pod Disruption Budget support
* Single-sign on (OAuth2/OIDC)
* Webhook triggering
* CLI
* Out-of-the box and custom Prometheus metrics
* Windows container support
* Embedded widgets
* Multiplex log viewer

## Community Meetings

We host monthly community meetings where we and the community showcase demos and discuss the current and future state of
the project. Feel free to join us! For Community Meeting information, minutes and recordings
please [see here](https://bit.ly/argo-wf-cmty-mtng).

Participation in the Argo Workflows project is governed by
the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md)

## Community Blogs and Presentations

* [Awesome-Argo: A Curated List of Awesome Projects and Resources Related to Argo](https://github.com/terrytangyuan/awesome-argo)
* [Automation of Everything - How To Combine Argo Events, Workflows & Pipelines, CD, and Rollouts](https://youtu.be/XNXJtxkUKeY)
* [Argo Workflows and Pipelines - CI/CD, Machine Learning, and Other Kubernetes Workflows](https://youtu.be/UMaivwrAyTA)
* [Argo Ansible role: Provisioning Argo Workflows on OpenShift](https://medium.com/@marekermk/provisioning-argo-on-openshift-with-ansible-and-kustomize-340a1fda8b50)
* [Argo Workflows vs Apache Airflow](http://bit.ly/30YNIvT)
* [CI/CD with Argo on Kubernetes](https://medium.com/@bouwe.ceunen/ci-cd-with-argo-on-kubernetes-28c1a99616a9)
* [Running Argo Workflows Across Multiple Kubernetes Clusters](https://admiralty.io/blog/running-argo-workflows-across-multiple-kubernetes-clusters/)
* [Open Source Model Management Roundup: Polyaxon, Argo, and Seldon](https://www.anaconda.com/blog/developer-blog/open-source-model-management-roundup-polyaxon-argo-and-seldon/)
* [Producing 200 OpenStreetMap extracts in 35 minutes using a scalable data workflow](https://www.interline.io/blog/scaling-openstreetmap-data-workflows/)
* [Argo integration review](http://dev.matt.hillsdon.net/2018/03/24/argo-integration-review.html)
* TGI Kubernetes with Joe Beda: [Argo workflow system](https://www.youtube.com/watch?v=M_rxPPLG8pU&start=859)

## Project Resources

* [Argo Project GitHub organization](https://github.com/argoproj)
* [Argo Website](https://argoproj.github.io/)
* [Argo Slack](https://argoproj.github.io/community/join-slack)

## Security

See [SECURITY.md](SECURITY.md).
",2023-07-07 15:49:20+00:00
arteria,arteria-packs,arteria-project/arteria-packs,StackStorm packs to automate sequencing center operations,,False,9,2022-11-23 14:57:13+00:00,2015-06-09 12:36:27+00:00,19,3,10,34,v2.9.2,2017-10-09 07:22:42+00:00,MIT License,363,v2.9.2,34,2017-10-09 07:22:42+00:00,,2022-12-06 11:46:33+00:00,"Arteria Stackstorm Pack
=======================

[![Build Status](https://travis-ci.org/arteria-project/arteria-packs.svg?branch=master)](https://travis-ci.org/arteria-project/arteria-packs)
[![Join the chat at https://gitter.im/arteria-project/arteria-project](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/arteria-project/arteria-project)

This pack provides re-usable units for automating tasks at a
sequencing core facility using the [StackStorm](http://stackstorm.com/)
event-driven automation platform.

It forms the core of the Arteria automation system,
which you can read about on our [website](https://arteria-project.github.io/)
or [preprint](https://www.biorxiv.org/content/early/2017/11/06/214858).
This pack integrates with a series of bioinformatic micro-services,
which can be found at https://github.com/arteria-project.

This repository includes a Docker environment allowing you to install
Arteria and its dependencies within a containerized environment.

This pack is intended as a starting point, not a turn-key solution. Most sequencing cores
will have a sufficiently unique environment that a specialized solution must be developed,
but our goal is to provide components to facilitate this development.

Mission
=======
The components provided by Arteria pack have a two-fold purpose:

- To be a point of collaboration for the Arteria community where potentially reusable StackStorm components can be deposited
- To provide a quick-start launchpad for organizations interested in implementing an Arteria system

Demo
=====

Here we demonstrate using Docker to bootstrap an Arteria system
comprised of arteria-packs and several Arteria microservices.
We then use the system to run a simple workflow on a runfolder.

[![asciicast](https://asciinema.org/a/YSz20Jfo7U1hCYzWP5K05mT1S.png)](https://asciinema.org/a/YSz20Jfo7U1hCYzWP5K05mT1S)


Getting Started
===============

System requirements
-------------------
You will need to have the following installed:
- [docker](https://docs.docker.com/)
- [docker-compose](https://docs.docker.com/compose/)
- make

bcl2fastq
---------
You also need to download Illumina software bcl2fastq manually since Illumina requires that you register before donwloading.
Download the [zip file](https://support.illumina.com/softwaredownload.html?assetId=e8ed3335-5201-48ff-a2bc-db4bfb792c85&assetDetails=bcl2fastq2-v2-20-0-linux-x86-64.zip) containing the rpm. Save in at docker-images/bcl2fastq-service/local_files

Installation
------------
```
git clone https://github.com/arteria-project/arteria-packs
cd arteria-packs
make up
```

To register the Arteria pack with Stackstorm, run:
```
docker exec arteria-packs_st2client_1 st2ctl reload --register-all
docker exec arteria-packs_st2client_1 st2 run packs.setup_virtualenv packs=arteria
```

Congratulations, you're now ready to run workflows.


Running the sample workflow
---------------------------

Put a runfolder in the `docker-mountpoints/monitored-folder` directory.

You can find a suitably small test data set here: https://doi.org/10.5281/zenodo.1204292

Then run:

```
docker exec arteria-packs_st2client_1 st2 run arteria.workflow_bcl2fastq_and_checkqc \
  runfolder_path='/opt/monitored-folder/<name of the runfolder>' \
  bcl2fastq_body='{""additional_args"": ""--ignore-missing-bcls --ignore-missing-filter --ignore-missing-positions --tiles s_1"", ""use_base_mask"": ""--use-bases-mask y1n*,n*""}'
```


Eventually you should see something like this:

```
id: 5a2516ea10895200eb467b63
action.ref: arteria.workflow_bcl2fastq_and_checkqc
parameters:
  runfolder_path: /opt/monitored-folder/my_runfolder
status: succeeded (286s elapsed)
result_task: mark_as_done
result:
  exit_code: 0
  result: true
  stderr: ''
  stdout: ''
start_timestamp: 2017-12-04T09:35:38.361039Z
end_timestamp: 2017-12-04T09:40:24.743737Z
+--------------------------+--------------------------+--------------------+---------------------------+-------------------------------+
| id                       | status                   | task               | action                    | start_timestamp               |
+--------------------------+--------------------------+--------------------+---------------------------+-------------------------------+
| 5a2516eb10895200eb467b66 | succeeded (1s elapsed)   | get_runfolder_name | core.local                | Mon, 04 Dec 2017 09:35:38 UTC |
| 5a2516eb10895200eb467b68 | succeeded (1s elapsed)   | mark_as_started    | arteria.runfolder-service | Mon, 04 Dec 2017 09:35:39 UTC |
| 5a2516ed10895200eb467b6a | succeeded (1s elapsed)   | start_bcl2fastq    | arteria.bcl2fastq-service | Mon, 04 Dec 2017 09:35:41 UTC |
| 5a2516ef10895200eb467b6c | succeeded (267s elapsed) | poll_bcl2fastq     | arteria.bcl2fastq-service | Mon, 04 Dec 2017 09:35:42 UTC |
| 5a2517fd10895200eb467b6e | succeeded (1s elapsed)   | checkqc            | core.http                 | Mon, 04 Dec 2017 09:40:13 UTC |
| 5a2517fe10895200eb467b70 | succeeded (1s elapsed)   | mark_as_done       | arteria.runfolder-service | Mon, 04 Dec 2017 09:40:14 UTC |
+--------------------------+--------------------------+--------------------+---------------------------+-------------------------------+
```

Indicating that you have successfully executed a workflow which has demultiplexed the runfolder
using bcl2fastq and and checked its quality control statistics using [CheckQC](https://github.com/Molmed/checkQC).

You can find bcl2fastq output in `docker-mountpoints/bcl2fastq-output`.

Architecture
============

This project provides re-usable components for StackStorm in the
form of actions, workflows, sensors, and rules.

The [StackStorm docs](https://docs.stackstorm.com) are a
comprehensive guide to these concept, but here we provide a summary:

- **Actions** encapsulate system tasks such as calling a web service or running a shell script
- **Workflows** tie actions together
- **Sensors** pick up events from the environment, e.g. listening for new files to appear in a directory, or polling a web service for new events
- **Rules** parse events from sensors and determine if an action or a workflow should be initiated

In order to facilitate quick setup, this repo also provides a Docker environment.
In addition to running a StackStorm instance, it also runs a set of Arteria micro-services,
which make it possible to run bcl2fastq on an Illumina runfolder,
and then check that is passes a set of quality criteria using [checkQC](https://github.com/Molmed/checkQC)

The code is structured as follows:

```
.
├── actions = StackStorm actions
│   └── workflows = StackStorm workflows
├── docker-conf = config files for the docker images
├── docker-images = Dockerfiles for Arteria containers
│   ├── bcl2fastq-service
│   ├── checkqc-service
│   └── runfolder-service
├── docker-mountpoints = directories mounted to Docker containers
│   ├── bcl2fastq-output = will contain bcl2fastq output from the sample workflow
│   └── monitored-folder = deposit your runfolders here for processing
├── docker-runtime = startup container scripts, see: https://github.com/StackStorm/st2-docker#running-custom-shell-scripts-on-boot
├── rules = StackStorm rules
├── sensors = StackStorm sensors
└── tests = unit and integration tests
```

Advanced Usage
==============

Container access
----------------

To get into the StackStorm master node, run:

```
make interact
```

From there you can issue st2 commands directly, without the ```docker exec stackstorm``` prefix.

Running as sudo
---------------
If you are running make and docker with `sudo` you need to do so with the `-E` flag to
ensure that the environment variables get passed correctly. For example:

```
sudo -E make up
```

Troubleshooting
---------------

You may encounter failures during one or more steps in the workflow:

```
+--------------------------+------------------------+--------------------+--------------------------+--------------------------+
| id                       | status                 | task               | action                   | start_timestamp          |
+--------------------------+------------------------+--------------------+--------------------------+--------------------------+
| 5c78e3ba8123e6012739119c | succeeded (0s elapsed) | get_runfolder_name | core.local               | Fri, 01 Mar 2019         |
|                          |                        |                    |                          | 07:48:10 UTC             |
| 5c78e3ba8123e6012739119e | succeeded (1s elapsed) | mark_as_started    | arteria.runfolder_servic | Fri, 01 Mar 2019         |
|                          |                        |                    | e                        | 07:48:10 UTC             |
| 5c78e3bb8123e601273911a0 | failed (0s elapsed)    | start_bcl2fastq    | arteria.bcl2fastq_servic | Fri, 01 Mar 2019         |
|                          |                        |                    | e                        | 07:48:11 UTC             |
+--------------------------+------------------------+--------------------+--------------------------+--------------------------+
```

You can troubleshoot the failed step further by getting the execution id, in this case:

```
docker exec arteria-packs_st2client_1 st2 execution get 5c78e3bb8123e601273911a0
```

Activating sensors
------------------
Stackstorm can detect changes in the surrounding environment through sensors.
This pack provides a `RunfolderSensor`, which queries the the runfolder
service for state information.

By activating this sensor, we can automatically trigger
a workflow once a runfolder is marked ""ready"" in the runfolder service.

You can confirm that the sensor is activated by running:

```
docker exec arteria-packs_st2client_1 st2 sensor list
```

To connect the sensor and workflow, activate the rule:

```
docker exec arteria-packs_st2client_1 st2 rule enable arteria.when_runfolder_is_ready_start_bcl2fastq
```

Put a runfolder in `docker-mountpoints/monitored-folder`, and
set its state to `ready` using:

```
docker exec arteria-packs_st2client_1 st2 run arteria.runfolder_service cmd=""set_state"" state=""ready"" runfolder=""/opt/monitored-folder/<name of your runfolder>"" url=""http://runfolder-service""
```

Within 15s you should if you execute `docker exec arteria-packs_st2client_1 st2 execution list` see that a workflow processing that runfolder
has started. This is the way that Arteria can be used to automatically start processes as needed.

You can see details of the sensor's inner workings with:

```
docker exec arteria-packs_st2client_1 /opt/stackstorm/st2/bin/st2sensorcontainer --config-file=/etc/st2/st2.conf --debug --sensor-ref=arteria.RunfolderSensor
```

Re-building the environment
---------------------------

You can remove the existing environment with:

```
make remove-all
```

Then, re-run the Installation instructions.

Running tests
-------------

```
docker exec arteria-packs_st2client_1 st2-run-pack-tests -c -v -p /opt/stackstorm/packs/arteria
```

Acknowledgements
================
The docker environment provided here has been heavily inspired by the ones provided by
[StackStorm](https://github.com/StackStorm/st2-docker) and [UMCCR](https://github.com/umccr/st2-arteria-docker).
",2023-07-07 15:49:25+00:00
artigraph,artigraph,artigraph/artigraph,"Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks.",,False,21,2023-04-29 14:37:38+00:00,2020-09-04 05:57:36+00:00,5,6,8,4,v0.0.4,2023-03-22 04:40:07+00:00,Apache License 2.0,662,v0.0.4,4,2023-03-22 04:40:07+00:00,2023-07-01 14:13:43+00:00,2023-07-01 14:13:41+00:00,"# artigraph

[![pypi](https://img.shields.io/pypi/v/arti.svg)](https://pypi.python.org/pypi/arti)
[![changelog](https://img.shields.io/github/v/release/artigraph/artigraph?label=changelog)](https://github.com/artigraph/artigraph/releases)
[![downloads](https://pepy.tech/badge/arti/month)](https://pepy.tech/project/arti)
[![versions](https://img.shields.io/pypi/pyversions/arti.svg)](https://github.com/artigraph/artigraph)
[![license](https://img.shields.io/github/license/artigraph/artigraph.svg)](https://github.com/artigraph/artigraph/blob/golden/LICENSE)
[![CI](https://github.com/artigraph/artigraph/actions/workflows/ci.yaml/badge.svg)](https://github.com/artigraph/artigraph/actions/workflows/ci.yaml)
[![codecov](https://codecov.io/gh/artigraph/artigraph/branch/golden/graph/badge.svg?token=6LUCpjcGdN)](https://codecov.io/gh/artigraph/artigraph)
[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/5561/badge)](https://bestpractices.coreinfrastructure.org/projects/5561)

Declarative Data Production

Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks.

Artigraph is hosted by the [LF AI and Data Foundation](https://lfaidata.foundation) as a Sandbox project.

## Installation

Artigraph can be installed from PyPI on python 3.9+ with `pip install arti`.

## Example

This sample from the [spend example](docs/examples/spend/demo.py) highlights computing the total amount spent from a series of purchase transactions:

```python
from pathlib import Path
from typing import Annotated

from arti import Annotation, Artifact, Graph, producer
from arti.formats.json import JSON
from arti.storage.local import LocalFile
from arti.types import Collection, Date, Float64, Int64, Struct
from arti.versions import SemVer

DIR = Path(__file__).parent


class Vendor(Annotation):
    name: str


class Transactions(Artifact):
    """"""Transactions partitioned by day.""""""

    type = Collection(
        element=Struct(fields={""id"": Int64(), ""date"": Date(), ""amount"": Float64()}),
        partition_by=(""date"",),
    )


class TotalSpend(Artifact):
    """"""Aggregate spend over all time.""""""

    type = Float64()
    format = JSON()
    storage = LocalFile()


@producer(version=SemVer(major=1, minor=0, patch=0))
def aggregate_transactions(
    transactions: Annotated[list[dict], Transactions]
) -> Annotated[float, TotalSpend]:
    return sum(txn[""amount""] for txn in transactions)


with Graph(name=""test-graph"") as g:
    g.artifacts.vendor.transactions = Transactions(
        annotations=[Vendor(name=""Acme"")],
        format=JSON(),
        storage=LocalFile(path=str(DIR / ""transactions"" / ""{date.iso}.json"")),
    )
    g.artifacts.spend = aggregate_transactions(
        transactions=g.artifacts.vendor.transactions
    )
```

The full example can be run easily with `docker run --rm artigraph/example-spend`:
```
INFO:root:Writing mock Transactions data:
INFO:root:      /usr/src/app/transactions/2021-10-01.json: [{'id': 1, 'amount': 9.95}, {'id': 2, 'amount': 7.5}]
INFO:root:      /usr/src/app/transactions/2021-10-02.json: [{'id': 3, 'amount': 5.0}, {'id': 4, 'amount': 12.0}, {'id': 4, 'amount': 7.55}]
INFO:root:Building aggregate_transactions(transactions=Transactions(format=JSON(), storage=LocalFile(path='/usr/src/app/transactions/{date.iso}.json'), annotations=(Vendor(name='Acme'),)))...
INFO:root:Build finished.
INFO:root:Final Spend data:
INFO:root:      /tmp/test-graph/spend/7564053533177891797/spend.json: 42.0
```

## Community

Everyone is welcome to join the community - learn more in out [support](SUPPORT.md) and [contributing](CONTRIBUTING.md) pages!

## Presentations

- 2022-01-27: Requesting Sandbox Incubation with [LF AI & Data](https://lfaidata.foundation/) ([deck](https://docs.google.com/presentation/d/1KLM9r0L5sTbpb_UPR5nx4fil-7fO-UnmhTeatSiaN3Y), [presentation](https://wiki.lfaidata.foundation/download/attachments/7733341/GMT20220127-140219_Recording_3840x2160.mp4?version=1&modificationDate=1643716019000&api=v2) @ 6m35s)
",2023-07-07 15:49:28+00:00
arvados,arvados,arvados/arvados,An open source platform for managing and analyzing biomedical big data,https://arvados.org,False,347,2023-06-29 22:22:14+00:00,2013-04-11 01:06:19+00:00,110,36,40,0,,,Other,24415,2.6.3,40,2023-06-07 20:33:44+00:00,2023-07-07 13:53:57+00:00,2023-07-07 13:49:24+00:00,"[comment]: # (Copyright © The Arvados Authors. All rights reserved.)
[comment]: # ()
[comment]: # (SPDX-License-Identifier: CC-BY-SA-3.0)

[![Join the chat at https://gitter.im/arvados/community](https://badges.gitter.im/arvados/community.svg)](https://gitter.im/arvados/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) | [Installing Arvados](https://doc.arvados.org/install/index.html) | [Installing Client SDKs](https://doc.arvados.org/sdk/index.html) | [Report a bug](https://dev.arvados.org/projects/arvados/issues/new) | [Development and Contributing](CONTRIBUTING.md)

<img align=""right"" src=""doc/images/dax.png"" height=""240px"">

[Arvados](https://arvados.org) is an open source platform for
managing, processing, and sharing genomic and other large scientific
and biomedical data.  With Arvados, bioinformaticians run and scale
compute-intensive workflows, developers create biomedical
applications, and IT administrators manage large compute and storage
resources.

The key components of Arvados are:

* *Keep*: Keep is the Arvados storage system for managing and storing large
collections of files.  Keep combines content addressing and a
distributed storage architecture resulting in both high reliability
and high throughput.  Every file stored in Keep can be accurately
verified every time it is retrieved.  Keep supports the creation of
collections as a flexible way to define data sets without having to
re-organize or needlessly copy data. Keep works on a wide range of
underlying filesystems and object stores.

* *Crunch*: Crunch is the orchestration system for running [Common Workflow Language](https://www.commonwl.org) workflows. It is
designed to maintain data provenance and workflow
reproducibility. Crunch automatically tracks data inputs and outputs
through Keep and executes workflow processes in Docker containers.  In
a cloud environment, Crunch optimizes costs by scaling compute on demand.

* *Workbench*: The Workbench web application allows users to interactively access
Arvados functionality.  It is especially helpful for querying and
browsing data, visualizing provenance, and tracking the progress of
workflows.

* *Command Line tools*: The command line interface (CLI) provides convenient access to Arvados
functionality in the Arvados platform from the command line.

* *API and SDKs*: Arvados is designed to be integrated with existing infrastructure. All
the services in Arvados are accessed through a RESTful API.  SDKs are
available for Python, Go, R, Perl, Ruby, and Java.

# Quick start

To try out Arvados on your local workstation, you can use Arvbox, which
provides Arvados components pre-installed in a Docker container (requires
Docker 1.9+).  After cloning the Arvados git repository:

```
$ cd arvados/tools/arvbox/bin
$ ./arvbox start localdemo
```

In this mode you will only be able to connect to Arvbox from the same host.  To
configure Arvbox to be accessible over a network and for other options see
http://doc.arvados.org/install/arvbox.html for details.

# Documentation

Complete documentation, including the [User Guide](https://doc.arvados.org/user/index.html), [Installation documentation](https://doc.arvados.org/install/index.html), [Administrator documentation](https://doc.arvados.org/admin/index.html) and
[API documentation](https://doc.arvados.org/api/index.html) is available at http://doc.arvados.org/

If you wish to build the Arvados documentation from a local git clone, see
[doc/README.textile](doc/README.textile) for instructions.

# Community

[![Join the chat at https://gitter.im/arvados/community](https://badges.gitter.im/arvados/community.svg)](https://gitter.im/arvados/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

The [Arvados community channel](https://gitter.im/arvados/community)
channel at [gitter.im](https://gitter.im) is available for live
discussion and support.

The [Arvados developement channel](https://gitter.im/arvados/development)
channel at [gitter.im](https://gitter.im) is used to coordinate development.

The [Arvados user mailing list](http://lists.arvados.org/mailman/listinfo/arvados)
is used to announce new versions and other news.

All participants are expected to abide by the [Arvados Code of Conduct](CODE_OF_CONDUCT.md).

# Reporting bugs

[Report a bug](https://dev.arvados.org/projects/arvados/issues/new) on [dev.arvados.org](https://dev.arvados.org).

# Development and Contributing

See [CONTRIBUTING](CONTRIBUTING.md) for information about Arvados development and how to contribute to the Arvados project.

The [development road map](https://dev.arvados.org/issues/gantt?utf8=%E2%9C%93&set_filter=1&gantt=1&f%5B%5D=project_id&op%5Bproject_id%5D=%3D&v%5Bproject_id%5D%5B%5D=49&f%5B%5D=&zoom=1) outlines some of the project priorities over the next twelve months.

# Licensing

Arvados is Free Software.  See [COPYING](COPYING) for information about the open source licenses used in Arvados.
",2023-07-07 15:49:32+00:00
awe,AWE,MG-RAST/AWE,workflow and resource management system for bioinformatics data analysis,,False,67,2022-11-11 10:53:12+00:00,2012-05-04 20:43:34+00:00,22,10,14,41,v2.0.0-beta-77-g24ffd2d6,2019-05-13 21:48:06+00:00,"BSD 2-Clause ""Simplified"" License",3065,v2.0.0-beta-77-g24ffd2d6,48,2019-05-13 21:48:06+00:00,,2019-10-03 19:23:36+00:00,"![AWE](docs/images/awe-lg.png)
=====

 A light weight workflow manager for scientific computing at scale that:

- executes [CWL](http://www.commonwl.org) workflows

- is a cloud native workflow platform designed from the ground up to be fast, scalable and fault tolerant

- supports [CWLProv](https://github.com/common-workflow-language/cwlprov)

- is RESTful. The [API](./API/) is accessible from desktops, HPC systems, exotic hardware, the cloud and your smartphone

- is designed for complex scientific computing and supports computing on multiple platforms with zero setup

- supports containerized environments with Docker and Singularity

- is part of our reproducible science platform [Skyport](https://github.com/MG-RAST/Skyport2) combined to create [Researchobjects](http://www.researchobject.org/) when combined with [CWL](http://www.commonwl.org) and [CWLProv](https://github.com/common-workflow-language/cwlprov)


<br>

Documentation can be found on the AWE github pages:

https://mg-rast.github.io/AWE/

",2023-07-07 15:49:36+00:00
ax,Ax,facebook/Ax,Adaptive Experimentation Platform,https://ax.dev,False,2095,2023-07-05 15:01:49+00:00,2019-02-09 15:23:44+00:00,264,68,87,35,0.3.3,2023-06-26 02:19:47+00:00,MIT License,2446,v0.1.20,39,2021-02-24 16:02:49+00:00,2023-07-07 15:46:43+00:00,2023-07-07 03:43:14+00:00,"<img width=""300"" src=""https://ax.dev/img/ax_logo_lockup.svg"" alt=""Ax Logo"" />

<hr/>

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)
[![Build Status](https://img.shields.io/pypi/v/ax-platform.svg)](https://pypi.org/project/ax-platform/)
[![Build Status](https://img.shields.io/pypi/pyversions/ax-platform.svg)](https://pypi.org/project/ax-platform/)
[![Build Status](https://img.shields.io/pypi/wheel/ax-platform.svg)](https://pypi.org/project/ax-platform/)
[![Build Status](https://github.com/facebook/Ax/workflows/Build%20and%20Test%20Workflow/badge.svg)](https://github.com/facebook/Ax/actions?query=workflow%3A%22Build+and+Test+Workflow%22)
[![codecov](https://codecov.io/gh/facebook/Ax/branch/main/graph/badge.svg)](https://codecov.io/gh/facebook/Ax)
[![Build Status](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

Ax is an accessible, general-purpose platform for understanding, managing,
deploying, and automating adaptive experiments.

Adaptive experimentation is the machine-learning guided process of iteratively
exploring a (possibly infinite) parameter space in order to identify optimal
configurations in a resource-efficient manner. Ax currently supports Bayesian
optimization and bandit optimization as exploration strategies. Bayesian
optimization in Ax is powered by [BoTorch](https://github.com/facebookexternal/botorch),
a modern library for Bayesian optimization research built on PyTorch.

For full documentation and tutorials, see the [Ax website](https://ax.dev)

## Why Ax?

* **Versatility**: Ax supports different kinds of experiments, from dynamic ML-assisted A/B testing, to hyperparameter optimization in machine learning.
* **Customization**: Ax makes it easy to add new modeling and decision algorithms, enabling research and development with minimal overhead.
* **Production-completeness**: Ax comes with storage integration and ability to fully save and reload experiments.
* **Support for multi-modal and constrained experimentation**: Ax allows for running and combining multiple experiments (e.g. simulation with a real-world ""online"" A/B test) and for constrained optimization (e.g. improving classification accuracy without significant increase in resource-utilization).
* **Efficiency in high-noise setting**: Ax offers state-of-the-art algorithms specifically geared to noisy experiments, such as simulations with reinforcement-learning agents.
* **Ease of use**: Ax includes 3 different APIs that strike different balances between lightweight structure and flexibility. Using the most concise Loop API, a whole optimization can be done in just one function call. The Service API integrates easily with external schedulers. The most elaborate Developer API affords full algorithm customization and experiment introspection.

## Getting Started

To run a simple optimization loop in Ax (using the
[Booth response surface](https://www.sfu.ca/~ssurjano/booth.html) as the
artificial evaluation function):

```python
>>> from ax import optimize
>>> best_parameters, best_values, experiment, model = optimize(
        parameters=[
          {
            ""name"": ""x1"",
            ""type"": ""range"",
            ""bounds"": [-10.0, 10.0],
          },
          {
            ""name"": ""x2"",
            ""type"": ""range"",
            ""bounds"": [-10.0, 10.0],
          },
        ],
        # Booth function
        evaluation_function=lambda p: (p[""x1""] + 2*p[""x2""] - 7)**2 + (2*p[""x1""] + p[""x2""] - 5)**2,
        minimize=True,
    )

# best_parameters contains {'x1': 1.02, 'x2': 2.97}; the global min is (1, 3)
```

## Installation

### Requirements
You need Python 3.8 or later to run Ax.

The required Python dependencies are:

* [botorch](https://www.botorch.org)
* jinja2
* pandas
* scipy
* sklearn
* plotly >=2.2.1

### Stable Version

#### Installing via pip
We recommend installing Ax via pip (even if using Conda environment):

```
conda install pytorch torchvision -c pytorch  # OSX only (details below)
pip install ax-platform
```

Installation will use Python wheels from PyPI, available for [OSX, Linux, and Windows](https://pypi.org/project/ax-platform/#files).

*Note*: Make sure the `pip` being used to install `ax-platform` is actually the one from the newly created Conda environment.
If you're using a Unix-based OS, you can use `which pip` to check.

*Recommendation for MacOS users*: PyTorch is a required dependency of BoTorch, and can be automatically installed via pip.
However, **we recommend you [install PyTorch manually](https://pytorch.org/get-started/locally/#anaconda-1) before installing Ax, using the Anaconda package manager**.
Installing from Anaconda will link against MKL (a library that optimizes mathematical computation for Intel processors).
This will result in up to an order-of-magnitude speed-up for Bayesian optimization, as at the moment, installing PyTorch from pip does not link against MKL.

If you need CUDA on MacOS, you will need to build PyTorch from source. Please consult the PyTorch installation instructions above.

#### Optional Dependencies

To use Ax with a notebook environment, you will need Jupyter. Install it first:
```
pip install jupyter
```

If you want to store the experiments in MySQL, you will need SQLAlchemy:
```
pip install SQLAlchemy
```

### Latest Version

#### Installing from Git

You can install the latest (bleeding edge) version from Git.

First, see recommendation for installing PyTorch for MacOS users above.

At times, the bleeding edge for Ax can depend on bleeding edge versions of BoTorch (or GPyTorch). We therefore recommend installing those from Git as well:

```
pip install git+https://github.com/cornellius-gp/linear_operator.git
pip install git+https://github.com/cornellius-gp/gpytorch.git
export ALLOW_LATEST_GPYTORCH_LINOP=true
pip install git+https://github.com/pytorch/botorch.git
export ALLOW_BOTORCH_LATEST=true
pip install git+https://github.com/facebook/Ax.git#egg=ax-platform
```

#### Optional Dependencies

If using Ax in Jupyter notebooks:

```
pip install git+https://github.com/facebook/Ax.git#egg=ax-platform[notebook]
```

To support plotly-based plotting in newer Jupyter notebook versions

```
pip install ""notebook>=5.3"" ""ipywidgets==7.5""
```

[See Plotly repo's README](https://github.com/plotly/plotly.py#jupyter-notebook-support) for details and JupyterLab instructions.

If storing Ax experiments via SQLAlchemy in MySQL or SQLite:
```
pip install git+https://github.com/facebook/Ax.git#egg=ax-platform[mysql]
```

## Join the Ax Community

### Getting help

Please open an issue on our [issues page](https://github.com/facebook/Ax/issues) with any questions, feature requests or bug reports! If posting a bug report, please include a minimal reproducible example (as a code snippet) that we can use to reproduce and debug the problem you encountered.

### Contributing

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.

When contributing to Ax, we recommend cloning the [repository](https://github.com/facebook/Ax) and installing all optional dependencies:

```
pip install git+https://github.com/cornellius-gp/linear_operator.git
pip install git+https://github.com/cornellius-gp/gpytorch.git
export ALLOW_LATEST_GPYTORCH_LINOP=true
pip install git+https://github.com/pytorch/botorch.git
export ALLOW_BOTORCH_LATEST=true
git clone https://github.com/facebook/ax.git --depth 1
cd ax
pip install -e .[tutorial]
```

See recommendation for installing PyTorch for MacOS users above.

The above example limits the cloned directory size via the
[`--depth`](https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt)
argument to `git clone`. If you require the entire commit history you may remove this
argument.

## License

Ax is licensed under the [MIT license](./LICENSE).
",2023-07-07 15:49:40+00:00
azkaban,azkaban,azkaban/azkaban,Azkaban workflow manager.,https://azkaban.github.io,False,4284,2023-07-06 23:38:42+00:00,2012-10-18 01:34:53+00:00,1575,253,111,48,4.0.0,2021-03-17 19:01:52+00:00,Apache License 2.0,2964,4.0.0,168,2021-03-17 19:01:52+00:00,2023-07-06 03:06:25+00:00,2023-06-29 01:16:50+00:00,"# Azkaban 

[![Build Status](https://travis-ci.com/azkaban/azkaban.svg?branch=master)](https://travis-ci.com/azkaban/azkaban)[![codecov.io](https://codecov.io/github/azkaban/azkaban/branch/master/graph/badge.svg)](https://codecov.io/github/azkaban/azkaban)[![Join the chat at https://gitter.im/azkaban-workflow-engine/Lobby](https://badges.gitter.im/azkaban-workflow-engine/Lobby.svg)](https://gitter.im/azkaban-workflow-engine/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)[![Documentation Status](https://readthedocs.org/projects/azkaban/badge/?version=latest)](https://azkaban.readthedocs.org/en/latest/?badge=latest)


## Build
Azkaban builds use Gradle and requires Java 8 or higher.

The following set of commands run on *nix platforms like Linux, OS X.

```
# Build Azkaban
./gradlew build

# Clean the build
./gradlew clean

# Build and install distributions
./gradlew installDist

# Run tests
./gradlew test

# Build without running tests
./gradlew build -x test
```

### Build a release

Pick a release from [the release page](https://github.com/azkaban/azkaban/releases). 
Find the tag corresponding to the release.

Check out the source code corresponding to that tag.
e.g.

`
git checkout 3.30.1
`

Build 
```
./gradlew clean build
```

## Documentation

The current documentation will be deprecated soon at [azkaban.github.io](https://azkaban.github.io). 
The [new Documentation site](https://azkaban.readthedocs.io/en/latest/) is under development.
The source code for the documentation is inside `docs` directory.

For help, please visit the [Azkaban Google Group](https://groups.google.com/forum/?fromgroups#!forum/azkaban-dev).

## Developer Guide

See [the contribution guide](https://github.com/azkaban/azkaban/blob/master/CONTRIBUTING.md).

#### Documentation development

If you want to contribute to the documentation or the release tool (inside the `tools` folder), 
please make sure python3 is installed in your environment. python virtual environment is recommended to run these scripts.

To create a venv & install the python3 dependencies inside it, run

```bash
python3 -m venv venv
source venv/bin/activate
pip3 install -r requirements.txt
```
After, enter the documentation folder `docs` and make the build by running
```bash
cd docs
make html
```

Find the built docs under `_build/html/`.

For example on a Mac, open them in browser with:

```bash
open -a ""Google Chrome"" _build/html/index.html
```

**[July, 2018]** We are actively improving our documentation. Everyone in the AZ community is 
welcome to submit a pull request to edit/fix the documentation.
",2023-07-07 15:49:45+00:00
balsam,balsam,argonne-lcf/balsam,High throughput workflows and automation for HPC,,False,67,2023-06-27 02:08:07+00:00,2018-11-06 19:32:55+00:00,22,8,10,10,,,,1308,0.7.0.a22,38,2023-04-10 20:43:50+00:00,2023-07-07 15:41:42+00:00,2023-06-30 17:34:21+00:00,"

<img src=""./img/balsam-all-green-nobg.png"" width=600/>

A unified platform to manage high-throughput workflows across the HPC landscape.

*  [Balsam Documentation (branch: main)](https://balsam.readthedocs.io/en/latest/)
*  [Legacy Balsam Documentation (branch: master)](https://balsam.readthedocs.io/en/master)

**Run Balsam on any laptop, cluster, or supercomputer.**

```console
$ pip install --pre balsam
$ balsam login
$ balsam site init my-site
```

![site-init](./img/balsam-init.gif)

**Python class-based declaration of Apps and execution lifecycles.**

```python
from balsam.api import ApplicationDefinition

class Hello(ApplicationDefinition):
    site = ""my-laptop""
    command_template = ""echo hello {{ name }}""

    def handle_timeout(self):
        self.job.state = ""RESTART_READY""
```

**Seamless remote job management.**

```python
# On any machine with internet access...
from balsam.api import Job, BatchJob

# Create Jobs:
job = Job.objects.create(
    site_name=""my-laptop"",
    app_id=""Hello"",
    workdir=""test/say-hello"",
    parameters={""name"": ""world!""},
)

# Or allocate resources:
BatchJob.objects.create(
    site_id=job.site_id,
    num_nodes=1,
    wall_time_min=10,
    job_mode=""serial"",
    project=""local"",
    queue=""local"",
)
```

**Dispatch Python Apps across heterogeneous resources from a single session.**

```python
import numpy as np

class MyApp(ApplicationDefinition):
    site = ""theta-gpu""

    def run(self, vec):
        from mpi4py import MPI
        rank = MPI.COMM_WORLD.Get_rank()
        print(""Hello from rank"", rank)
        return np.linalg.norm(vec)

jobs = [
    MyApp.submit(
        workdir=f""test/{i}"", 
        vec=np.random.rand(3), 
        ranks_per_node=4,
        gpus_per_rank=0,
    )
    for i in range(10)
]

for job in Job.objects.as_completed(jobs):
   print(job.workdir, job.result())
```



## Features

* Easy `pip` installation [runs out-of-the-box on several HPC systems](user-guide/installation.md) and is [easily adaptable to others](./development/porting.md).
* [Balsam Sites](./user-guide/site-config.md) are remotely  controlled by design: submit and monitor workflows from *anywhere*
* [Run any existing application, with flexible execution environments and job lifecycle hooks](./user-guide/appdef.md)
* [High-throughput and fault-tolerant task execution](./user-guide/batchjob.md) on diverse resources
* Define data dependencies for any task: [Balsam orchestrates the necessary data transfers](./user-guide/transfer.md)
* [Elastic queueing](./user-guide/elastic.md): auto-scale resources to the workload size
* [Monitoring APIs](./user-guide/monitoring.md): query recent task failures, node utilization, or throughput


",2023-07-07 15:49:49+00:00
bastet,BASTet,biorack/BASTet,Berkeley Analysis and Storage Toolkit (BASTet),,False,6,2022-11-23 22:27:20+00:00,2016-03-21 20:58:24+00:00,1,3,2,0,,,Other,707,,0,,2023-04-18 21:43:22+00:00,2023-04-18 21:43:16+00:00,"# BASTet
Berkeley Analysis and Storage Toolkit (BASTet)

<img src=""https://raw.githubusercontent.com/biorack/BASTet/master/doc/_static/bastet_logo_full.png"" width=""400"" />

The BASTet project page is located at [https://openmsi.nersc.gov/openmsi/client/bastet.html](https://openmsi.nersc.gov/openmsi/client/bastet.html)

## Documentation

The online documentation for BASTet is available at:

* HTML: [https://biorack.github.io/BASTet/](https://biorack.github.io/BASTet/).
* PDF: [https://biorack.github.io/BASTet/_static/bastet_doc.pdf](https://biorack.github.io/BASTet/_static/bastet_doc.pdf)

## Developer Notes

 * Developer documentation of the main `omsi` package see [here](https://biorack.github.io/BASTet/omsi.html)
 * Building the online documentation see [here](https://biorack.github.io/BASTet/developer_notes.html#building-the-online-documentation)
 * Developing a new analysis see [here](https://biorack.github.io/BASTet/custom_analysis.html)
 * Defining and executing workflows see [here](https://biorack.github.io/BASTet/basic_workflows.html)
 * High-level description of the OpenMSI data format see [here](https://biorack.github.io/BASTet/HDF5_format.html)

## Publications

The data format for storage of MSI data is described in the following publication:

*Oliver Rübel, Annette Greiner, Shreyas Cholia, Katherine Louie, E. Wes Bethel, Trent R. Northen, and Benjamin P. Bowen, ""OpenMSI: A High-Performance Web-Based Platform for Mass Spectrometry Imaging"" Analytical Chemistry 2013 85 (21), 10354-10361, DOI: 10.1021/ac402540a. [[BibTeX]](https://openmsi.nersc.gov/site_media/openmsi/images/publications/openmsi_acs_2013.bib)[[Online at ACS]](http://pubs.acs.org/doi/abs/10.1021/ac402540a)*

The broader storage, analysis, provenance, and workflow capabilities of BASTet are described in an upcoming publication:

*Oliver Rübel and Benjamin P. Bowen, ""BASTet: Shareable and reproducible analysis and visualization of mass spectrometry imaging data via OpenMSI,"" submitted March, 2016*

## Licence

See [license.txt](license.txt)

## Copyright Notice

See [copyright.txt](copyright.txt)
",2023-07-07 15:49:54+00:00
bcbio,bcbio-nextgen,bcbio/bcbio-nextgen,"Validated, scalable, community developed variant calling, RNA-seq and small RNA analysis",https://bcbio-nextgen.readthedocs.io,False,940,2023-06-19 19:57:20+00:00,2013-02-06 11:14:50+00:00,357,86,76,14,v1.2.9,2021-12-15 03:34:44+00:00,MIT License,8556,v1.2.9,65,2021-12-15 03:34:44+00:00,2023-07-04 03:42:13+00:00,2023-03-20 17:07:50+00:00,"![bcbio banner](https://raw.githubusercontent.com/bcbio/bcbio-nextgen/master/docs/contents/images/banner.png)

Validated, scalable, community developed variant calling, RNA-seq and small RNA analysis. You write a high level configuration file specifying your inputs and analysis parameters. This input drives a parallel run that handles distributed execution, idempotent processing restarts and safe transactional steps. bcbio provides a shared community resource that handles the data processing component of sequencing analysis, providing researchers with more time to focus on the downstream biology.

[![Build Status](https://travis-ci.org/bcbio/bcbio-nextgen.svg?branch=master)](https://travis-ci.org/bcbio/bcbio-nextgen)
[![Documentation status](https://readthedocs.org/projects/bcbio-nextgen/badge/?version=latest)](https://bcbio-nextgen.readthedocs.io/en/latest/?badge=latest)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3564938.svg)](https://doi.org/10.5281/zenodo.3564938)

## Features

* Community developed: We welcome contributors with the goal of overcoming the biological, algorithmic and computational challenges that face individual developers working on complex pipelines in quickly changing research areas. See our [users page](https://bcbio-nextgen.readthedocs.io/en/latest/contents/users.html) for examples of bcbio-nextgen deployments, and the [developer documentation](https://bcbio-nextgen.readthedocs.io/en/latest/contents/development.html) for tips on contributing.
* Installation: [A single installer script](https://bcbio-nextgen.readthedocs.io/en/latest/contents/installation.html#automated) prepares all third party software, data libraries and system configuration files.
* [Automated validation](https://bcb.io/2014/05/12/wgs-trio-variant-evaluation/): Compare variant calls against common reference materials or sample specific SNP arrays to ensure call correctness. Incorporation of multiple approaches for alignment, preparation and variant calling enable unbiased comparisons of algorithms.
* Distributed: Focus on [parallel analysis and scaling](https://bcb.io/2013/05/22/scaling-variant-detection-pipelines-for-whole-genome-sequencing-analysis/) to handle large population studies and whole genome analysis. Runs on single multicore computers, in compute clusters using [IPython parallel](https://ipyparallel.readthedocs.io/en/latest/), or on the Amazon cloud. See the [parallel documentation](https://bcbio-nextgen.readthedocs.org/en/latest/contents/parallel.html) for full details.
* Multiple analysis algorithms: bcbio-nextgen provides configurable [variant calling (small and copy number), RNA-seq, ATAC-seq, , BS-Seq, SC RNA-seq, and small RNA pipelines](https://bcbio-nextgen.readthedocs.io/en/latest/).

## Quick start

1. [Install](https://bcbio-nextgen.readthedocs.io/en/latest/contents/installation.html#automated) `bcbio-nextgen` with all tool dependencies and data files:
    ```shell script
    wget https://raw.githubusercontent.com/bcbio/bcbio-nextgen/master/scripts/bcbio_nextgen_install.py
    python bcbio_nextgen_install.py /usr/local/share/bcbio --tooldir=/usr/local \
          --genomes hg38 --aligners bwa --aligners bowtie2
    ```
   producing an editable [system configuration file](https://github.com/bcbio/bcbio-nextgen/blob/master/config/bcbio_system.yaml) referencing the installed software, data and system information.

1. [Automatically create a processing description](https://bcbio-nextgen.readthedocs.io/en/latest/contents/configuration.html#automated-sample-configuration) of sample FASTQ and BAM files from your project, and a CSV file of sample metadata:
    ```shell script
    bcbio_nextgen.py -w template freebayes-variant project1.csv sample1.bam sample2_1.fq sample2_2.fq
    ```
    This produces a [sample description file](https://github.com/bcbio/bcbio-nextgen/blob/master/config/bcbio_sample.yaml) containing pipeline [configuration options](https://bcbio-nextgen.readthedocs.io/en/latest/contents/configuration.html).

1.  Run analysis, distributed across 8 local cores:
    ```shell script
    cd project1/work
    bcbio_nextgen.py ../config/project1.yaml -n 8
    ```

## Documentation

See the [full documentation](https://bcbio-nextgen.readthedocs.io/en/latest/) and [longer analysis-based articles](https://bcb.io). We welcome enhancements or problem reports using [GitHub](https://github.com/bcbio/bcbio-nextgen/issues) and discussion on the [biovalidation mailing list](https://groups.google.com/d/forum/biovalidation).

## Contributors

* [Miika Ahdesmaki](https://github.com/mjafin), AstraZeneca
* [Luca Beltrame](https://github.com/lbeltrame), IRCCS ""Mario Negri"" Institute for Pharmacological Research, Milan, Italy
* [Christian Brueffer](https://github.com/cbrueffer), Lund University, Lund, Sweden
* [Alla Bushoy](https://github.com/abushoy), AstraZeneca
* [Guillermo Carrasco](https://github.com/guillermo-carrasco), Science for Life Laboratory, Stockholm
* [Nick Carriero](https://www.simonsfoundation.org/team/nick-carriero/), Simons Foundation
* [Brad Chapman](https://github.com/chapmanb), Harvard Chan Bioinformatics Core
* [Saket Choudhary](https://github.com/saketkc), University Of Southern California
* [Peter Cock](https://github.com/peterjc), The James Hutton Institute
* [Matthias De Smet](https://github.com/matthdsm), Center for Medical Genetics, Ghent University Hospital, Belgium
* [Matt Edwards](https://github.com/matted), MIT
* [Mario Giovacchini](https://github.com/mariogiov), Science for Life Laboratory, Stockholm
* [Karl Gutwin](https://twitter.com/kgutwin), Biogen
* [Jeff Hammerbacher](https://github.com/hammer), Icahn School of Medicine at Mount Sinai
* [Oliver Hofmann](https://umccr.github.io/), University of Melbourne Centre for Cancer Research
* [John Kern](https://github.com/kern3020)
* [Rory Kirchner](https://github.com/roryk), Harvard Chan Bioinformatics Core
* [Tetiana Khotiainsteva](https://github.com/tetianakh), Ardigen
* [Kerrin Mendler](https://github.com/kmendler), AstraZeneca
* [Sergey Naumenko](https://github.com/naumenko-sa), Harvard Chan Bioinformatics Core
* [Jakub Nowacki](https://github.com/jsnowacki), AstraZeneca
* [John Morrissey](https://github.com/jwm), Harvard Chan Bioinformatics Core
* [Lorena Pantano](https://github.com/lpantano), Harvard Chan Bioinformatics Core
* [Brent Pedersen](https://github.com/brentp), University of Colorado Denver
* [James Porter](https://github.com/porterjamesj), The University of Chicago
* [Vlad Saveliev](https://github.com/vladsaveliev), Center for Algorithmic Biotechnology, St. Petersburg University
* [Valentine Svensson](https://github.com/vals), Science for Life Laboratory, Stockholm
* [Paul Tang](https://github.com/tanglingfung), UCSF
* [Stephen Turner](https://github.com/stephenturner), University of Virginia
* [Roman Valls](https://github.com/brainstorm), Science for Life Laboratory, Stockholm
* [Kevin Ying](https://github.com/kevyin), Garvan Institute of Medical Research, Sydney, Australia
* [Steffen Möller](https://github.com/smoe), University of Rostock, Germany
* [WimSpee](https://github.com/wimspee)

## License

The code is freely available under the [MIT license](https://opensource.org/licenses/mit-license.html).
",2023-07-07 15:49:57+00:00
bd-processor,bd-processor,big-data-processor/bd-processor,A next-generation data science workbench,,False,1,2022-11-29 14:37:57+00:00,2021-03-14 07:09:55+00:00,0,1,1,4,,,Apache License 2.0,115,0.9.9-beta.0,4,2021-08-02 07:45:15+00:00,,2023-01-14 23:42:06+00:00,"# The BD-Processor workbench

This is the production repo of BD-Processor.

The BD-Processor (BDP) is a general-purpose web-based data workbench that focuses not only on workflow management but also interactive data analysis.

### Our goal is to provide a full-featured data science workbench.

## Game Changing Features
1. All the following operations can be done via just web browsers! The BDP workbench is designed to be both user- and developer-friendly.
2. Workflow developers can **write scripts**, **define tasks and workflows**, **freely design web interfaces**, **wrap tasks/workflows and customized interfaces into portable packages** for different data anlaysis workflows. That is these portable packages can carry its own friendly web interfaces to guide users to execute tasks or analyze results interactively.
3. End-users who have set up the BD-Processor can easily install the portable packages with zero configuration.
4. BDP also supports web proxy to serve containerized web services so that everyone can define a task to run web services, such as **R/Shiny app**, **Jupyer Notebook**, or even containerized Desktop applications as long as it can be access via web (e.g. using noVNC for VNC remote desktop services in a container).
5. BDP is the first system that allows workflow/package developers to make friendly web pages specifically for their workflows. With our Page API design, developers can freely develop web user interfaces to guide their users to upload files, specify parameters, execute and monitor tasks/workflows, and, the most important, visualize results interactively!  
6. BDP workbench off-course provides its own built-in web interfaces for users to **manage projects**, **upload files**, **specify parameters**, **execute and monitor tasks/workflows**, and **records all the provenence of each task run**. Also, the BDP workbench is designed for multiple users. Administrators can manage users via web pages.
7. BDP can support versatile computing environments by extending and implementing the base class of the Task Adapter (see below). The Task Adapter is an independent component that help BDP to deploy tasks to different computing resources. With different Adapters implemented, BDP can be made compatible with other workflow systems!!

## Demo
1. Web Proxy: With the RStudio task defined, users can run multiple RStudio instances on BDP. The following screen recording was made by Jen-Han Chen (陳正翰)
![rstudio-demo](https://user-images.githubusercontent.com/1439838/122026141-90386600-cdfc-11eb-9ceb-1e5e8bdbe9ad.gif)
I've made a bdp-development-kit package that provides several containerized tools as examples for development, such as Jupyter Notebook or Matlab.
Containerized Linux desktop applications are also included in this bdp-development-kit. You are more than welcome to define your Task/Workflow.

    **Note**: Since the BDP workbench is made for general purposes, the BDP workbench itself does not come with the RStudio. It is the bdp-development-kit package that contains this RStudio Task.

2. More demonstrations are coming.


## Documentation site

Please see the [bdp-document](https://big-data-processor.github.io/bdp-document/) page.


## Installation
To setup the BD-Processor, please follow the steps. (Or follow [the complete installation guide](https://big-data-processor.github.io/bdp-document/installation.html).)
### 1. Install NodeJS(v14+), Git, and Docker.
Please install the [NodeJS](https://nodejs.org/), [Git](https://git-scm.com/), and [Docker](https://www.docker.com/).

### 2. Use the following commands to install the server.
```
git clone https://github.com/big-data-processor/bd-processor.git
cd bd-processor
npm install
```

### 3. Configure the BD-Processor
Copy the file `./configs/server-config-template.yaml` to `./configs/server-config.yaml` and edit the `./configs/server-config.yaml` file.

It is strongly recommended to configure the file for your own preferences. Please see [here](https://big-data-processor.github.io/bdp-document/installation.html#configure-the-mongo-database-connection) for detailed information.

### 4. Start the server
Go to the folder (the bd-processor folder) which contains the `bd-processor.js` file.
```
npm start
```

### 5. Register the first account as the system root.
Use a web browser and open the link http://localhost:8080 (depends on your configurations).
On the top-right of the landing page, you can find the Sign In link.


## Components
1. bdp-server: the server-side development repo of BDP. The production server code is the bd-processor.js in this production repo.
2. bdp-client: the client-side development repo of BDP. The production client is in the public folder in this production repo. 
3. [bdp-document](https://big-data-processor.github.io/bdp-document/): the document site hosted via github pages.
4. [bdp-page-api](https://big-data-processor.github.io/bdp-page-api/): the Page API documentation site via github pages.
5. [@big-data-processor/task-reader](https://www.npmjs.com/package/@big-data-processor/task-reader): The task reader parses the workflow playbook to get task/workflow specifications.
6. [@big-data-processor/task-adapter-base](https://big-data-processor.github.io/task-adapter-base/): This is the base class of the Task Adapter to extend and implement for different runtime environments.
7. [@big-data-processor/default-filters](https://github.com/big-data-processor/default-filters): This is the default filter function set for the task-reader to parse the workflow playbook. Additional filter functions can be developed to extend the capability of the workflow playbook.

## Exemplary Task Adapters
Instead of providing our official built-in adapters for all kinds of runtime environments, we provide the extensible base class of the task adapter for developers ([@big-data-processor/task-adapter-base](https://big-data-processor.github.io/task-adapter-base/)).
The following shows our example task adapters.

1. [@big-data-processor/task-adapter-local](https://github.com/big-data-processor/task-adapter-local) and its [npm pacakge](https://www.npmjs.com/package/@big-data-processor/task-adapter-local)
2. [@big-data-processor/task-adapter-docker](https://github.com/big-data-processor/task-adapter-docker) and its [npm package](https://www.npmjs.com/package/@big-data-processor/task-adapter-docker)
3. [@big-data-processor/task-adapter-ssh-docker](https://github.com/big-data-processor/task-adapter-ssh-docker) and its [npm package](https://www.npmjs.com/package/@big-data-processor/task-adapter-ssh-docker)
4. [@big-data-processor/task-adapter-pbs-docker](https://github.com/big-data-processor/task-adapter-pbs-docker) and its [npm package](https://www.npmjs.com/package/@big-data-processor/task-adapter-pbs-docker)

5. Additional Adapters are coming soon (e.g. k8s adapter or cloud adapters like aws-eks or gke adapters). Feel free to create desired Task Adapters for specific runtime environments.

# Affiliation
chiyang[at]mail.cgu.edu.tw

Chi Yang, PhD

Assistant Research Fellow,

Artificial Intelligence Research Center,

Molecular Medicine Research Center, 

Chang Gung University, Taiwan


## Roadmaps
(comming soon)

## Licenses

### The BDP workbench
Licensed under the Apache-2.0 License (see the [license file](https://github.com/big-data-processor/bd-processor/blob/master/LICENSE)).

### The Page API
Licensed under the MIT License (please see the [license file](https://github.com/big-data-processor/bdp-page-api/blob/master/LICENSE)).

### The Base Class of the Task Adapter
Licensed under the MIT License (please see the [license file](https://github.com/big-data-processor/task-adapter-base/blob/master/LICENSE)).


## Buy me a coffee☕
If you find this workbench useful, you may support me by [Buy me a coffee](https://www.buymeacoffee.com/chiyang)☕. 
",2023-07-07 15:50:02+00:00
bds,bds,pcingola/bds,,,False,5,2023-05-09 06:23:08+00:00,2021-07-28 12:48:12+00:00,3,1,2,16,v3.6b,2023-01-10 17:59:20+00:00,Apache License 2.0,484,v3.6,26,2022-12-13 18:24:23+00:00,2023-05-09 06:23:08+00:00,2023-03-17 15:06:43+00:00,"
# bds

A simple script-language for ""Big Data"" piplines.

- [Main Web](http://pcingola.github.io/bds/): Web pages
- [Documentation](http://pcingola.github.io/bds/manual/site/index.html): Manual pages and Documentation
- [Discussions](https://github.com/pcingola/bds/discussions): Ask questions, discuss ideas, etc.
- [Issues](https://github.com/pcingola/bds/issues): Report issues, bugs or request features

# Author: [Pablo Cingolani](https://www.linkedin.com/in/pablocingolani/)
",2023-07-07 15:50:06+00:00
apachebeam,beam,apache/beam,Apache Beam is a unified programming model for Batch and Streaming data processing.,https://beam.apache.org/,False,6991,2023-07-07 10:07:04+00:00,2016-02-02 08:00:06+00:00,3989,259,303,31,v2.48.0,2023-05-31 20:10:27+00:00,Apache License 2.0,39607,website-to-hugo,275,2020-05-14 22:39:54+00:00,2023-07-07 15:17:34+00:00,2023-07-07 14:09:31+00:00,"<!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    ""License""); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.
-->

# Apache Beam

[Apache Beam](http://beam.apache.org/) is a unified model for defining both batch and streaming data-parallel processing pipelines, as well as a set of language-specific SDKs for constructing pipelines and Runners for executing them on distributed processing backends, including [Apache Flink](http://flink.apache.org/), [Apache Spark](http://spark.apache.org/), [Google Cloud Dataflow](http://cloud.google.com/dataflow/), and [Hazelcast Jet](https://jet.hazelcast.org/).

## Status

[![Maven Version](https://maven-badges.herokuapp.com/maven-central/org.apache.beam/beam-sdks-java-core/badge.svg)](http://search.maven.org/#search|gav|1|g:""org.apache.beam"")
[![PyPI version](https://badge.fury.io/py/apache-beam.svg)](https://badge.fury.io/py/apache-beam)
[![Go version](https://pkg.go.dev/badge/github.com/apache/beam/sdks/v2/go.svg)](https://pkg.go.dev/github.com/apache/beam/sdks/v2/go)
[![Python coverage](https://codecov.io/gh/apache/beam/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/beam)
[![Build python source distribution and wheels](https://github.com/apache/beam/workflows/Build%20python%20source%20distribution%20and%20wheels/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Build+python+source+distribution+and+wheels%22+branch%3Amaster+event%3Aschedule)
[![Python tests](https://github.com/apache/beam/workflows/Python%20tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Python+Tests%22+branch%3Amaster+event%3Aschedule)
[![Java tests](https://github.com/apache/beam/workflows/Java%20Tests/badge.svg?branch=master&event=schedule)](https://github.com/apache/beam/actions?query=workflow%3A%22Java+Tests%22+branch%3Amaster+event%3Aschedule)
[![Go tests (Jenkins)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon?subject=Go%20Tests%28Jenkins%29)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/)
[![Java tests (Jenkins)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon?subject=Java%20Tests%28Jenkins%29)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/)
[![Python tests (Jenkins)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon?subject=Python%20Tests%28Jenkins%29)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)

## Overview

Beam provides a general approach to expressing [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) data processing pipelines and supports three categories of users, each of which have relatively disparate backgrounds and needs.

1. _End Users_: Writing pipelines with an existing SDK, running it on an existing runner. These users want to focus on writing their application logic and have everything else just work.
2. _SDK Writers_: Developing a Beam SDK targeted at a specific user community (Java, Python, Scala, Go, R, graphical, etc). These users are language geeks and would prefer to be shielded from all the details of various runners and their implementations.
3. _Runner Writers_: Have an execution environment for distributed processing and would like to support programs written against the Beam Model. Would prefer to be shielded from details of multiple SDKs.

### The Beam Model

The model behind Beam evolved from several internal Google data processing projects, including [MapReduce](http://research.google.com/archive/mapreduce.html), [FlumeJava](http://research.google.com/pubs/pub35650.html), and [Millwheel](http://research.google.com/pubs/pub41378.html). This model was originally known as the “[Dataflow Model](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf)”.

To learn more about the Beam Model (though still under the original name of Dataflow), see the World Beyond Batch: [Streaming 101](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101) and [Streaming 102](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102) posts on O’Reilly’s Radar site, and the [VLDB 2015 paper](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf).

The key concepts in the Beam programming model are:

* `PCollection`: represents a collection of data, which could be bounded or unbounded in size.
* `PTransform`: represents a computation that transforms input PCollections into output PCollections.
* `Pipeline`: manages a directed acyclic graph of PTransforms and PCollections that is ready for execution.
* `PipelineRunner`: specifies where and how the pipeline should execute.

### SDKs

Beam supports multiple language-specific SDKs for writing pipelines against the Beam Model.

Currently, this repository contains SDKs for Java, Python and Go.

Have ideas for new SDKs or DSLs? See the [sdk-ideas label](https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Asdk-ideas).

### Runners

Beam supports executing programs on multiple distributed processing backends through PipelineRunners. Currently, the following PipelineRunners are available:

- The `DirectRunner` runs the pipeline on your local machine.
- The `DataflowRunner` submits the pipeline to the [Google Cloud Dataflow](http://cloud.google.com/dataflow/).
- The `FlinkRunner` runs the pipeline on an Apache Flink cluster. The code has been donated from [dataArtisans/flink-dataflow](https://github.com/dataArtisans/flink-dataflow) and is now part of Beam.
- The `SparkRunner` runs the pipeline on an Apache Spark cluster. The code has been donated from [cloudera/spark-dataflow](https://github.com/cloudera/spark-dataflow) and is now part of Beam.
- The `JetRunner` runs the pipeline on a Hazelcast Jet cluster. The code has been donated from [hazelcast/hazelcast-jet](https://github.com/hazelcast/hazelcast-jet) and is now part of Beam.
- The `Twister2Runner` runs the pipeline on a Twister2 cluster. The code has been donated from [DSC-SPIDAL/twister2](https://github.com/DSC-SPIDAL/twister2) and is now part of Beam.

Have ideas for new Runners? See the [runner-ideas label](https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Arunner-ideas).

## Getting Started

To learn how to write Beam pipelines, read the Quickstart for [[Java](https://beam.apache.org/get-started/quickstart-java), [Python](https://beam.apache.org/get-started/quickstart-py), or
[Go](https://beam.apache.org/get-started/quickstart-go)] available on our website.
To play with some Beam examples, try [Beam Playground](https://play.beam.apache.org/?path=SDK_JAVA_MinimalWordCount&sdk=java).
To learn basic Beam concepts, try [Tour of Beam](https://tour.beam.apache.org/).



## Contact Us

To get involved in Apache Beam:

* [Subscribe](mailto:user-subscribe@beam.apache.org) or [mail](mailto:user@beam.apache.org) the [user@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-user/) list.
* [Subscribe](mailto:dev-subscribe@beam.apache.org) or [mail](mailto:dev@beam.apache.org) the [dev@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-dev/) list.
* [Join ASF Slack](https://s.apache.org/slack-invite) on [#beam channel](https://s.apache.org/beam-slack-channel)
* [Report an issue](https://github.com/apache/beam/issues/new/choose).

Instructions for building and testing Beam itself
are in the [contribution guide](https://beam.apache.org/contribute/).

## More Information

* [Apache Beam](https://beam.apache.org)
* [Overview](https://beam.apache.org/use/beam-overview/)
* Quickstart: [Java](https://beam.apache.org/get-started/quickstart-java), [Python](https://beam.apache.org/get-started/quickstart-py), [Go](https://beam.apache.org/get-started/quickstart-go)
* [Community metrics](https://s.apache.org/beam-community-metrics)
",2023-07-07 15:50:11+00:00
bentoml,BentoML,bentoml/BentoML,Build Production-Grade AI Applications,https://bentoml.com,False,5231,2023-07-07 09:07:50+00:00,2019-04-02 01:39:27+00:00,597,66,158,85,v1.0.22,2023-06-12 17:35:57+00:00,Apache License 2.0,2530,v1.0.23,100,2023-06-28 22:37:46+00:00,2023-07-07 14:29:35+00:00,2023-07-07 04:01:38+00:00,"<div align=""center"">
  <img src=""https://github.com/bentoml/BentoML/assets/489344/398274c1-a572-477b-b115-52497a085496"" width=""180px"" alt=""bentoml"" />
  <h1 align=""center"">BentoML: The Unified AI Application Framework</h1>
  <a href=""https://pypi.org/project/bentoml""><img src=""https://img.shields.io/pypi/v/bentoml.svg"" alt=""pypi_status"" /></a>
  <a href=""https://github.com/bentoml/BentoML/actions/workflows/ci.yml?query=branch%3Amain""><img src=""https://github.com/bentoml/bentoml/workflows/CI/badge.svg?branch=main"" alt=""CI"" /></a>
  <a href=""https://twitter.com/bentomlai""><img src=""https://badgen.net/badge/icon/@bentomlai/1DA1F2?icon=twitter&label=Follow%20Us"" alt=""Twitter"" /></a>
  <a href=""https://join.slack.bentoml.org""><img src=""https://badgen.net/badge/Join/Community/cyan?icon=slack"" alt=""Community"" /></a>
  <p>BentoML is a framework for building <b>reliable, scalable, and cost-efficient AI
applications</b>. It comes with everything you need for model serving, application
packaging, and production deployment.</p>
  <i><a href=""https://l.bentoml.com/join-slack"">👉 Join our Slack community!</a></i>
</div>

# Highlights

### 🍱 Bento is the container for AI apps

- Open standard and SDK for AI apps, pack your code, inference pipelines, model
  files, dependencies, and runtime configurations in a
  [Bento](https://docs.bentoml.com/en/latest/concepts/bento.html).
- Auto-generate API servers, supporting REST API, gRPC, and long-running
  inference jobs.
- Auto-generate Docker container images.

### 🏄 Freedom to build with any AI models

- Import from any model hub or bring your own models built with frameworks like
  PyTorch, TensorFlow, Keras, Scikit-Learn, XGBoost and
  [many more](https://docs.bentoml.com/en/latest/frameworks/index.html).
- Native support for
  [LLM inference](https://github.com/bentoml/openllm/#bentoml),
  [generative AI](https://github.com/bentoml/stable-diffusion-bentoml),
  [embedding creation](https://github.com/bentoml/CLIP-API-service), and
  [multi-modal AI apps](https://github.com/bentoml/Distributed-Visual-ChatGPT).
- Run and debug your BentoML apps locally on Mac, Windows, or Linux.

### 🍭 Simplify modern AI application architecture

- Python-first! Effortlessly scale complex AI workloads.
- Enable GPU inference
  [without the headache](https://docs.bentoml.com/en/latest/guides/gpu.html).
- [Compose multiple models](https://docs.bentoml.com/en/latest/guides/graph.html)
  to run concurrently or sequentially, over
  [multiple GPUs](https://docs.bentoml.com/en/latest/guides/scheduling.html) or
  [on a Kubernetes Cluster](https://github.com/bentoml/yatai).
- Natively integrates with
  [MLFlow](https://docs.bentoml.com/en/latest/integrations/mlflow.html),
  [LangChain](https://github.com/ssheng/BentoChain),
  [Kubeflow](https://www.kubeflow.org/docs/external-add-ons/serving/bentoml/),
  [Triton](https://docs.bentoml.com/en/latest/integrations/triton.html),
  [Spark](https://docs.bentoml.com/en/latest/integrations/spark.html),
  [Ray](https://docs.bentoml.com/en/latest/integrations/ray.html), and many more
  to complete your production AI stack.

### 🚀 Deploy Anywhere

- One-click deployment to [☁️ BentoCloud](https://bentoml.com/cloud), the
  Serverless platform made for hosting and operating AI apps.
- Scalable BentoML deployment with [🦄️ Yatai](https://github.com/bentoml/yatai)
  on Kubernetes.
- Deploy auto-generated container images anywhere docker runs.

# Documentation

- Installation: `pip install bentoml`
- Full Documentation: [docs.bentoml.com](https://docs.bentoml.com/en/latest/)
- Tutorial: [Intro to BentoML](https://docs.bentoml.com/en/latest/tutorial.html)

### 🛠️ What you can build with BentoML

- [OpenLLM](https://github.com/bentoml/OpenLLM) - An open platform for operating
  large language models (LLMs) in production.
- [StableDiffusion](https://github.com/bentoml/stable-diffusion-bentoml) -
  Create your own text-to-image service with any diffusion models.
- [CLIP-API-service](https://github.com/bentoml/CLIP-API-service) - Embed images
  and sentences, object recognition, visual reasoning, image classification, and
  reverse image search.
- [Transformer NLP Service](https://github.com/bentoml/transformers-nlp-service) -
  Online inference API for Transformer NLP models.
- [Distributed TaskMatrix(Visual ChatGPT)](https://github.com/bentoml/Distributed-Visual-ChatGPT) -
  Scalable deployment for TaskMatrix from Microsoft.
- [Fraud Detection](https://github.com/bentoml/Fraud-Detection-Model-Serving) -
  Online model serving with custom XGBoost model.
- [OCR as a Service](https://github.com/bentoml/OCR-as-a-Service) - Turn any OCR
  models into online inference API endpoints.
- [Replace Anything](https://github.com/yuqwu/Replace-Anything) - Combining the
  power of Segment Anything and Stable Diffusion.
- [DeepFloyd IF Multi-GPU serving](https://github.com/bentoml/IF-multi-GPUs-demo) -
  Serve IF models easily across multiple GPUs
- Check out more examples
  [here](https://github.com/bentoml/BentoML/tree/main/examples).

# Getting Started

Save or import models in BentoML local model store:

```python
import bentoml
import transformers

pipe = transformers.pipeline(""text-classification"")

bentoml.transformers.save_model(
  ""text-classification-pipe"",
  pipe,
  signatures={
    ""__call__"": {""batchable"": True}  # Enable dynamic batching for model
  }
)
```

View all models saved locally:

```bash
$ bentoml models list

Tag                                     Module                Size        Creation Time
text-classification-pipe:kn6mr3aubcuf…  bentoml.transformers  256.35 MiB  2023-05-17 14:36:25
```

Define how your model runs in a `service.py` file:

```python
import bentoml

model_runner = bentoml.models.get(""text-classification-pipe"").to_runner()

svc = bentoml.Service(""text-classification-service"", runners=[model_runner])

@svc.api(input=bentoml.io.Text(), output=bentoml.io.JSON())
async def classify(text: str) -> str:
    results = await model_runner.async_run([text])
    return results[0]
```

Now, run the API service locally:

```bash
bentoml serve service.py:svc
```

Sent a prediction request:

```bash
$ curl -X POST -H ""Content-Type: text/plain"" --data ""BentoML is awesome"" http://localhost:3000/classify

{""label"":""POSITIVE"",""score"":0.9129443168640137}%
```

Define how a [Bento](https://docs.bentoml.com/en/latest/concepts/bento.html) can
be built for deployment, with `bentofile.yaml`:

```yaml
service: 'service.py:svc'
name: text-classification-svc
include:
  - 'service.py'
python:
  packages:
  - torch>=2.0
  - transformers
```

Build a Bento and generate a docker image:

```bash
$ bentoml build
...
Successfully built Bento(tag=""text-classification-svc:mc322vaubkuapuqj"").
```

```bash
$ bentoml containerize text-classification-svc
Building OCI-compliant image for text-classification-svc:mc322vaubkuapuqj with docker
...
Successfully built Bento container for ""text-classification-svc"" with tag(s) ""text-classification-svc:mc322vaubkuapuqj""
```

```bash
$ docker run -p 3000:3000 text-classification-svc:mc322vaubkuapuqj
```

For a more detailed user guide, check out the
[BentoML Tutorial](https://docs.bentoml.com/en/latest/tutorial.html).

---

## Community

BentoML supports billions of model runs per day and is used by thousands of
organizations around the globe.

Join our [Community Slack 💬](https://l.bentoml.com/join-slack), where thousands
of AI application developers contribute to the project and help each other.

To report a bug or suggest a feature request, use
[GitHub Issues](https://github.com/bentoml/BentoML/issues/new/choose).

## Contributing

There are many ways to contribute to the project:

- Report bugs and ""Thumbs up"" on issues that are relevant to you.
- Investigate issues and review other developers' pull requests.
- Contribute code or documentation to the project by submitting a GitHub pull
  request.
- Check out the
  [Contributing Guide](https://github.com/bentoml/BentoML/blob/main/CONTRIBUTING.md)
  and
  [Development Guide](https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md)
  to learn more
- Share your feedback and discuss roadmap plans in the `#bentoml-contributors`
  channel [here](https://l.bentoml.com/join-slack).

Thanks to all of our amazing contributors!

<a href=""https://github.com/bentoml/BentoML/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=bentoml/BentoML"" />
</a>

---

### Usage Reporting

BentoML collects usage data that helps our team to improve the product. Only
BentoML's internal API calls are being reported. We strip out as much
potentially sensitive information as possible, and we will never collect user
code, model data, model names, or stack traces. Here's the
[code](./src/bentoml/_internal/utils/analytics/usage_stats.py) for usage
tracking. You can opt-out of usage tracking by the `--do-not-track` CLI option:

```bash
bentoml [command] --do-not-track
```

Or by setting environment variable `BENTOML_DO_NOT_TRACK=True`:

```bash
export BENTOML_DO_NOT_TRACK=True
```

---

### License

[Apache License 2.0](https://github.com/bentoml/BentoML/blob/main/LICENSE)

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fbentoml%2FBentoML.svg?type=small)](https://app.fossa.com/projects/git%2Bgithub.com%2Fbentoml%2FBentoML?ref=badge_small)

### Citation

If you use BentoML in your research, please cite using the following
[citation](./CITATION.cff:

```bibtex
@software{Yang_BentoML_The_framework,
author = {Yang, Chaoyu and Sheng, Sean and Pham, Aaron and  Zhao, Shenyang and Lee, Sauyon and Jiang, Bo and Dong, Fog and Guan, Xipeng and Ming, Frost},
license = {Apache-2.0},
title = {{BentoML: The framework for building reliable, scalable and cost-efficient AI application}},
url = {https://github.com/bentoml/bentoml}
}
```
",2023-07-07 15:50:17+00:00
biocloud,biocloud-server-kai,ccwang002/biocloud-server-kai,The latest BioCloud server,,True,2,2023-01-28 16:13:33+00:00,2016-03-24 21:45:55+00:00,1,3,1,0,,,MIT License,258,,0,,,2017-06-29 04:21:37+00:00,"# BioCloud

The ultimate Django server to run NGS analysis pipeline.


## Getting Started

### Requirements

- Git
- Python 3.5+

### Set up a Python Virtual Environment

#### Built-in `venv`

Create the virtual environment at `<repo>/venv`:

    python3 -m venv venv

To activate it:

    . venv/bin/activate

Or on Windows, use

    . venv\Scripts\activate.bat

### Install Dependencies

Use pip:

    pip install -r requirements.txt


### Set up Local Environment Variables and Database

Settings are stored in environment variables via [django-environ]. The
quickiest way to start is to copy `local.sample.env` into `local.env`:

    cp src/biocloud/settings/local.sample.env src/biocloud/settings/local.env

Then edit the `SECRET_KEY` line in `local.env`, replacing `{{ secret_key }}` into any [Django Secret Key] value. An example:

    SECRET_KEY=-@5#xz3#f)4waw+p=l^c$1!6ei8&c5u_=^%*sdu(6vy@m$*-v&

After that, just run the migration.


### Go Develop

Change to the `src` directory:

    cd src

The website uses [PostgreSQL] database, make sure it has been running. On OSX, PostgreSQL may not run in the background, which can be started manually by

    fab start_db

Then run the database migration.

    python manage.py migrate

Run the development server

    python manage.py runserver

the Django-Q job cluster for executing pipeline jobs

    python manage.py qcluster

and a local SMTP server so all email sending will be captured

    fab start_smtp


[PostgreSQL]: https://www.postgresql.org/


## License

Release under MIT License. A great portion of code is adapted from [PyCon Taiwan 2016 website]'s source code under license MIT.


[Anaconda]: https://www.continuum.io/downloads
[conda]: http://conda.pydata.org/docs/intro.html
[django-environ]: http://django-environ.readthedocs.org/en/latest/
[Django Secret Key]: http://www.miniwebtool.com/django-secret-key-generator/
[PyCon Taiwan 2016 website]: https://github.com/pycontw/pycontw2016
",2023-07-07 15:50:21+00:00
biodepotworkflowbuilder,BioDepot-workflow-builder,BioDepot/BioDepot-workflow-builder,:computer:  GUI for creating bioinformatics workflows from :whale: Docker containers:,,False,58,2023-07-01 15:50:04+00:00,2016-12-16 17:46:10+00:00,19,12,8,3,v1.1.0,2023-02-10 09:32:22+00:00,Other,832,v1.1.0,3,2023-02-10 09:32:22+00:00,2023-07-06 21:16:16+00:00,2023-06-26 05:21:18+00:00,"# BioDepot-workflow-builder (Bwb)

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image19.png) ![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image23.png)


Bioinformatics Group
University of Washington Tacoma

# Table of Contents
   * [FAQ](#faq)
      * [General](#general)
         * [Why should I use Bwb?](#why-should-i-use-bwb)
      * [Usage](#usage)
         * [How do I use Bwb on my own data files?](#how-do-i-use-bwb-on-my-own-data-files)
         * [How do I connect one widget to another?](#how-do-i-connect-one-widget-to-another)
         * [How do I run Bwb on the cloud?](#how-do-i-run-bwb-on-the-cloud)
         * [What browser should I use with Bwb?](#what-browser-should-i-use-with-bwb)
         * [Where are the sample workflows and datasets?](#where-are-the-sample-workflows-and-datasets)
         * [Is it possible to use Bwb to run a batch of datasets?](#is-it-possible-to-use-bwb-to-run-a-batch-of-datasets)
         * [How do I add my own scripts to a Bwb pipeline?](#how-do-i-add-my-own-scripts-to-a-bwb-pipeline)
         * [How do I cut and paste into Bwb?](#how-do-i-cut-and-paste-into-bwb)
      * [Common problems](#common-problems)
         * [I'm having problems with windows](#im-having-problems-with-windows)
         * [My window or desktop are too small](#my-window-or-desktop-are-too-small)
         * [STAR and Kallisto won't run](#star-and-kallisto-wont-run)
         * [The Bwb container won't build on Windows when using the git repository](#the-bwb-container-wont-build-on-windows-when-using-the-git-repository)
      * [Miscellaneous](#miscellaneous)
         * [How should I reference Bwb if I use it?](#how-should-i-reference-bwb-if-i-use-it)
   * [MANUAL](#manual)
      * [GENERAL INFORMATION](#general-information)
         * [Overview: Running Bwb](#overview-running-bwb)
      * [Installing and starting Docker](#installing-and-starting-docker)
         * [Linux](#linux)
         * [Linux virtual machine on Windows or MacOS](#linux-virtual-machine-on-windows-or-macos)
         * [Docker for Mac](#docker-for-mac)
         * [Docker for Windows](#docker-for-windows)
         * [On The Cloud](#on-the-cloud)
            * [Amazon AWS](#amazon-aws)
      * [Starting Bwb](#starting-bwb)
      * [The Bwb/fluxbox work environment](#the-bwbfluxbox-work-environment)
         * [Graphics support for containerized apps](#graphics-support-for-containerized-apps)
         * [Basic window manipulations](#basic-window-manipulations)
         * [Application menu](#application-menu)
         * [Multiple Bwb instances and workspaces](#multiple-bwb-instances-and-workspaces)
         * [Interaction with host windowing system](#interaction-with-host-windowing-system)
      * [Bwb application](#bwb-application)
         * [Bwb window with kallisto-sleuth-jupyter workflow showing main features](#bwb-window-with-kallisto-sleuth-jupyter-workflow-showing-main-features)
         * [Bwb fluxbox desktop with Bwb minimized](#bwb-fluxbox-desktop-with-bwb-minimized)
         * [Overview](#overview)
         * [Tool Dock](#tool-dock)
         * [Editing the Tool dock](#editing-the-tool-dock)
         * [Interacting with widgets](#interacting-with-widgets)
            * [Widget user interaction window](#widget-user-interaction-window)
               * [Required parameters screen](#required-parameters-screen)
               * [Optional parameters screen](#optional-parameters-screen)
               * [Console screen](#console-screen)
               * [Execution bar](#execution-bar)
                  * [Start](#start)
                  * [Stop](#stop)
                  * [Export graphics](#export-graphics)
                  * [Test mode](#test-mode)
                  * [Run mode](#run-mode)
                  * [Select triggers](#select-triggers)
            * [Widget definition window](#widget-definition-window)
               * [General tab](#general-tab)
                  * [description](#description)
                  * [docker_image_name](#docker_image_name)
                  * [docker image tag](#docker-image-tag)
                  * [priority](#priority)
                  * [icon](#icon)
               * [Inputs tab](#inputs-tab)
               * [Outputs tab](#outputs-tab)
               * [Volumes tab](#volumes-tab)
               * [Ports tab](#ports-tab)
               * [Parameters tab](#parameters-tab)
               * [Command tab](#command-tab)
               * [Docker tab](#docker-tab)
               * [Widget definition save options and further customization by editing the .py file](#widget-definition-save-options-and-further-customization-by-editing-the-py-file)
                  * [Save mode: Overwrite](#save-mode-overwrite)
                  * [Save mode: Merge](#save-mode-merge)
                  * [Save mode: Data](#save-mode-data)
                  * [Load button](#load-button)
         * [Building workflows from widgets](#building-workflows-from-widgets)
            * [TLDR;](#tldr)
            * [Workflow structure](#workflow-structure)
            * [Connecting widgets](#connecting-widgets)
               * [Input from connections override user input](#input-from-connections-override-user-input)
               * [Connections to triggers are used to control the execution of widgets](#connections-to-triggers-are-used-to-control-the-execution-of-widgets)
            * [Saving workflows](#saving-workflows)
            * [Loading and executing a workflow](#loading-and-executing-a-workflow)
            * [Testing and exporting workflows as a bash script](#testing-and-exporting-workflows-as-a-bash-script)
      * [Demo workflows](#demo-workflows)
         * [DToxS demo](#dtoxs-demo)
         * [kallisto-sleuth demo](#kallisto-sleuth-demo)
         * [kallisto-sleuth with Jupyter demo](#kallisto-sleuth-with-jupyter-demo)
         * [STAR demo](#star-demo)
      * [Tutorial - Adding a Python script to a Bwb workflow](#tutorial---adding-a-python-script-to-a-bwb-workflow)
         * [Overview](#overview-1)
         * [Add Python2 widget to the kallisto-sleuth-jupyter workflow](#add-python2-widget-to-the-kallisto-sleuth-jupyter-workflow)
         * [Rename and customize the Python2 widget](#rename-and-customize-the-python2-widget)
         * [Creating the Docker image](#creating-the-docker-image)
         * [Save widget and load the workflow again](#save-widget-and-load-the-workflow-again)
         * [Creating or copying the wrapper Python script](#creating-or-copying-the-wrapper-python-script)
         * [Connecting the widget to the workflow](#connecting-the-widget-to-the-workflow)
         * [Running and testing the workflow](#running-and-testing-the-workflow)
      * [Appendices](#appendices)
         * [How Bwb executes workflows](#how-bwb-executes-workflows)
            * [TLDR;](#tldr-1)
         * [Organization of code](#organization-of-code)
            * [coreutils](#coreutils)
               * [BwBase](#bwbase)
               * [DockerClient](#dockerclient)
               * [OWWidgetBuilder](#owwidgetbuilder)
               * [createWidget](#createwidget)
            * [OWBiocImageBuilder](#owbiocimagebuilder)
            * [ToolDockEdit](#tooldockedit)
            * [makeToolDockCategories](#maketooldockcategories)
            * [workflowTools](#workflowtools)
         * [Organization of widget definition directory](#organization-of-widget-definition-directory)
         * [Organization of workflow directory](#organization-of-workflow-directory)
         * [List and description of included widgets](#list-and-description-of-included-widgets)
            * [Scripting widgets](#scripting-widgets)
               * [bash_utils](#bash_utils)
               * [bioc_R](#bioc_r)
               * [Java8](#java8)
               * [Perl](#perl)
               * [Python2](#python2)
               * [Python3](#python3)
            * [Jupyter widgets:](#jupyter-widgets)
               * [jupyter_base](#jupyter_base)
               * [jupyter_bioc](#jupyter_bioc)
               * [jupyter_sleuth](#jupyter_sleuth)
            * [RNA_seq:](#rna_seq)
               * [deseq2](#deseq2)
               * [DtoxSAnalysis](#dtoxsanalysis)
               * [DtoxSAlignment](#dtoxsalignment)
               * [kallistoIndex](#kallistoindex)
               * [kallistoQuant](#kallistoquant)
               * [starIndex](#starindex)
               * [starAlign](#staralign)
               * [startodeseq2](#startodeseq2)
               * [sleuth](#sleuth)
            * [Miscellaneous:](#miscellaneous-1)
               * [Directory](#directory)
               * [File](#file)
            * [User:](#user)
            * [Utilities:](#utilities)
               * [downloadURL](#downloadurl)
               * [fastqc](#fastqc)
               * [fastqDump](#fastqdump)
               * [gnumeric](#gnumeric)
         * [Description of json descriptors for widgets (Note that some of this may be outdated)](#description-of-json-descriptors-for-widgets-note-that-some-of-this-may-be-outdated)
         * [BwBase class](#bwbase-class)
            * [Keeping track of connections](#keeping-track-of-connections)
            * [Handling input signals](#handling-input-signals)
            * [Drawing an managing the GUI form](#drawing-an-managing-the-gui-form)
            * [Launching the executable with Docker](#launching-the-executable-with-docker)

# FAQ
## General

### Why should I use Bwb?

#### Biomedical scientists

The Bwb GUI is designed for non-programmers who want to use a workflow on their own data without worrying about installation and reproducibility. Bwb auto-installs the components and provides a very simple and intuitive GUI interface for modifying key parameters and accepting user files. We realize that for many users, interactive visualization is a large part of their analyses. Bwb supports Jupyter notebooks, Cytoscape and other software that have their own graphics and GUIs. The casual user can use familiar tools to customize the final stages of the analyses while maintaining a complete record and provenance of the entire pipeline which is essential for publication, sharing and reproducibility. Advanced users can swap in different parameter sets or even different modules just by dragging and dropping into an existing tested pipeline. A bash script can be created for the purpose of publication or to use in one of the many schedulers that support bash scripts such as SLURM/SGE/Torque-Maui.

#### Bioinformaticians

Bwb is also designed for bioinformaticians who support a group of users by analyzing data and installing and customizing workflows. For data analyses, bioinformaticians can more easily implement new pipelines and document existing ones using Bwb's GUI and support for Jupyter notebooks. Furthermore, Bwb pipelines can reduce the workload for bioinformaticians as users are able auto-install pre-tested workflows and adapt workflows by tweaking parameters on their own through a familiar interface. There is no need to re-implement a pipeline after a minor OS or package upgrade, or for new hardware. In addition, the widget building utilities allow the bioinformaticians to quickly customize the parameters and components that are exposed to the end user. Bwb also comes with widgets that support the major scripting languages used in Bioinformatics: Python, R, Perl, Bash and Java to allow for rapid customization, implementation and testing of new workflows. We provide a [tutorial](#tutorial---adding-a-python-script-to-a-bwb-workflow) and [video](https://www.youtube.com/watch?v=jtu-jCU2DU0) showing how to add a custom script to a Bwb pipeline. The export of Docker bash scripts allows for portable documentation of the workflows and execution on job schedulers, or for inclusion in custom scripts.

#### Software tool developers

A major motivation for our development of Bwb was that our own software tools were not being adopted or even tested by potential users. The barriers of installing and integrating new software into existing workflows are very high. By providing a GUI and an auto-installation method across different platforms, the adoption costs are greatly reduced. **Bwb is NOT meant to be a visual programming language** as we find this to be a cumbersome method for experienced coders while being too complicated to be easily adopted by users without a programming background. Instead, Bwb is designed to allow the developer a mechanism to provide a consistent and intuitive GUI for their software that largely consists of filling out a set of forms with minimal coding. In some cases, an additional wrapper script may be needed. Filling out the Bwb forms to skin an application is usually less work than the alternatives of writing a custom GUI, providing or conforming to a web-based API, writing, or dragging-and-dropping together a script to create a workflow using one or more workflow description languages. Our test cases are examples of ready-to-run modifiable workflows that self-install executables, dependencies and data. One possible use case is to add Bwb workflows to a GitHub to encourage users to try out new software.

## Usage

### How do I use Bwb on my own data files?

The instructions below are for local execution of Bwb. When Bwb is run on a cloud instance, the files will be saved to cloud instance file system. You will need to transfer files to and from the cloud. Tools such as sshfs, goofys, sftp/scp, or rsync can be used for this task. 

The mapping of local files to be used by Bwb workflows happens in the command line at launch time. For Windows, there is also an additional step of making the Windows directories accessible to the VM that is launching Docker. More about this later, but first let's talk about how to map your directories so that the Docker container can read from/write to them.
The -v option allows you to map one or more personal (local) directories to an internal directory so that the Bwb container can see them. Usually we map them to /data internally. The following start command for example maps the current directory to the /data directory inside the container. 

```
docker run --rm   -p 6080:6080 \
    -v  ${PWD}/:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
 ```
 whereas the following maps the home directory of myUser to /data
 ```
docker run --rm   -p 6080:6080 \
    -v  /home/myUser/:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
 ```
In other words, the Bwb container can read and write files to your local directory by reading and writing to the /data directory (or whatever directory you choose). This directory (/data) will be created automatically by Bwb. You will then be able to interact with files through the /data directory on Bwb. Bwb knows about these mappings or mountpoints and automatically handles all the filename mapping to any containers that it launches in the workflows by using these mountpoints.

More than one mapping is possible. The following maps the home directory of myUser to /data and the /opt/sequenceData directory on the host to /sequenceData in Bwb. Just make sure that the mappings are not contradictory or unexpected things will happen.

```
docker run --rm   -p 6080:6080 \
    -v  /home/myUser/:/data  \
    -v  /opt/sequenceData:/sequenceData \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```

For Windows there is the additional step of sharing a Windows folder with the VirtualBox or HyperV VM that is running Docker. Otherwise, the container will map an empty directory to the Docker container. For Windows 10 Pro instructions are [here](https://docs.docker.com/docker-for-windows/#shared-drives). For other Windows versions instructions are [here](https://medium.com/@Charles_Stover/fixing-volumes-in-docker-toolbox-4ad5ace0e572).

For example, if a the C://Users/myName folder is share with the name homefolder then
```
docker run --rm  -p 6080:6080 \
    -v  /homefolder:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```

would make the files and directories at C://Users/myName available to Bwb.

### How do I connect one widget to another?

Drag the mouse from the right side of the source widget to the left side of the sink widget. If they can be connected a dialog box should appear allowing you to choose which widgets to connect. This is shown in our [video](#tutorial---adding-a-python-script-to-a-bwb-workflow) at 5:49 to 6:05.

### How do I run Bwb on the cloud?
Bwb is a containerized mini webserver that can be run on any platform. To run it on the cloud requires you to make the IP and port accessible to the user. An example is given here for [AWS](#amazon-aws) interaction.

### What browser should I use with Bwb?

We recommend Chrome, only because most of our testing has been done using Chrome. However, any modern browser that has support for HTML5 is fine.

### Where are the sample workflows and datasets?
Bwb includes a set of sample workflows. These are found under the /workflows directory. Data are typically **NOT** included with the containers. This maximizes the portability of the containers for different workflows and makes them easier to download. Instead we use of the provided downloadURL widget to download files from an external source (e.g. a Google drive) for use with the containers. This is the mechanism that we use in all our examples. You can use our widget to download the data or look at the widget parameters to find the URL of the files and download them yourself. Then you can save these files if you wish and use them directly.

### Is it possible to use Bwb to run a batch of datasets?
Currently, this is possible in a couple of ways:

1) A wrapper script can be used inside the widget to distribute the computing to multiple threads. An example is [here](https://github.com/BioDepot/LINCS_RNAseq_cpp/blob/master/scripts/multibwa.sh).

2) Export the pipeline as a [bash script](#testing-and-exporting-workflows-as-a-bash-script) and modify the scripts to be used with a HPC scheduler such as SLURM, SGE or Torque-Maui. One could also use the script as the basis for a CWL or WDL descriptor for the workflow.

We do have a built-in job scheduler that is under development for Bwb. You can take a look at the latest upstream build with this at the dev branch.

### How do I add my own scripts to a Bwb pipeline?
We have provided basic widgets for Python, R, Perl, Bash, and Java. There is a [tutorial](#tutorial---adding-a-python-script-to-a-bwb-workflow) and a [video](https://www.youtube.com/watch?v=jtu-jCU2DU0)  showing how to add a script to a Bwb pipeline.

### How do I cut and paste into Bwb?
#### Browser version: 
There is a side arrow in the browser (noVNC) connected version of Bwb. Click on the arrow and a menu will show. Click on the clipboard icon. You can then cut and paste from your desktop to the clipboard and from the the clipboard into Bwb. The reverse is true too. When you are done you can hide the side menu. There are also icons for combined keystrokes such as control-c, control-v which are activated from the side menu. This gives a mechanism to override the browser mappings for these keystrokes. 
#### VNC client version:
If you are using a VNC client then you need to follow the instructions for that particular client. RealVNC, has very robust support for cut and paste, treating the windows inside Bwb like any other window. Also all the control-key combinations are supported without the need for a separate menu to enter them as there is no need to bypass the browser defaults.

## Common problems

### I'm having problems with windows

1. Check that virtualization is turned on. This may require booting into BIOS by restarting and pressing a function key (typically F1, F8 or F10).
2. If you are using VirtualBox (Docker toolbox) make sure that you allow Docker to install it i.e. you should uninstall Virtualbox if there is a previous installation before running the Docker installer.
3. If you are using a Docker version that uses a VM and you have upgraded VirtualBox, it is possible that the IP of the VM has changed from 192.168.99.100. You can run the following command to find the IP.
```
docker run --rm --net host alpine ip address
```
4. Make sure that you have read/write permissions to the directory that you are using to share files with Bwb and Docker. One method is to use your Desktop or a folder on your Desktop as the starting point for sharing files. For example when launching from Docker toolbox the starting command would be
```
docker run --rm   -p 6080:6080 \
    -v  /c/Users/Desktop:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```
### My window or desktop are too small

Note that changing the size of the desktop restarts the application and any work that is not saved will be lost.

If you are using a browser to connect to Bwb, zoom out until you have the desired window size. Then type ?auto after the url (e.g. localhost:6080/?auto) and press enter. The desktop will resize after about 30 seconds to the size of the new window. You can also enter the height and width in pixels for the desktop instead (e.g. localhost:6080?width=1920&height=1080)

If you are using a vnc client you can right-click on the canvas and choose resize from the dropdown menu. Enter the new width and height into the dialog box. The application will disconnect and restart with the new desktop size. This may take 30 seconds or so. After resizing, reconnect with the VNC client.

### STAR and Kallisto won't run
There are two common reasons:
1. These applications have RAM requirements. For the datasets in question, Kallisto requires about 7 GB of RAM and STAR requires about 30 GB of RAM. This amount of memory must be available on your host or the application will crash or run very slowly.

2. For systems running Docker using a Virtual Machine (VM) the VM is set to use a maximum amount of RAM. For Windows machines, this can be as low as 1 GB. The amount of RAM available to the VM must be adjusted. Instructions to adjust the VM size for the latest Docker versions are [here](https://docs.docker.com/docker-for-windows/#advanced) For earlier VirtualBox based installations (i.e. Windows 10 Home, older Macs), a video on how to do this is [here](https://www.youtube.com/watch?v=lLuIVGNfM4w).

### The Bwb container won't build on Windows when using the git repository
The two most common causes for this are the length of the internal filenames used by git and the difference between linefeeds used by Windows/DOS and Unix.

Try the following commands:

```
git config --global core.autocrlf input
git config --system core.longpaths true
git clone https://github.com/BioDepot/BioDepot-workflow-builder
```
The container should build correctly now.

## Miscellaneous
### How should I reference Bwb if I use it?
Please reference [Building Containerized Workflows Using the BioDepot-Workflow-Builder Cell Systems (2019)](https://www.cell.com/cell-systems/fulltext/S2405-4712(19)30276-5)

# MANUAL
## GENERAL INFORMATION

The BioDepot-workflow-builder (Bwb) can be used to build bioinformatics workflows by combining  interchangeable and encapsulated widgets, allowing researchers to easily implement and test new algorithms and observe how the outputs differ. Widgets call  Docker containers to execute software tools that could potentially be written in a different programming language, require different system configurations and/or be developed by different research groups.

Docker Image	: [https://hub.docker.com/r/biodepot/bwb/](https://hub.docker.com/r/biodepot/bwb/)

Source code	: [https://github.com/BioDepot/BioDepot-workflow-builder](https://github.com/BioDepot/BioDepot-workflow-builder)


### Overview: Running Bwb
1\. Install Docker.

2\. Start the container with Bwb by executing the following Docker command by typing into a window (Linux) or on the Docker command line (Windows/MacOS). For Windows, it may be necessary to run the Docker application as an Administrator. 

Linux/Mac
```bash
sudo docker run --rm   -p 6080:6080 \
    -v  ${PWD}/:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```

Windows
```bash
docker run --rm   -p 6080:6080 \
    -v  /c/users:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```

3\. Open a browser and connect to the Bwb container by typing the following url in the address bar of your browser:

   [http://localhost:6080](http://localhost:6080)

***NOTE:*** the vnc password is set to **bwb**

For cloud instances and remote servers use the IP of the instance or remote server instead of localhost.

For Windows and Macs the IP may vary depending on your setup.

Windows 10 Pro and newer Macs running native Docker will use the same localhost setup. Windows 7, 8 and 10 Home edition, and older Macs that use a Virtual machine (VM) will need to use the IP of the virtual machine instead of localhost - usually 192.168.99.100. In addition, for these VM setups, the available RAM of the VM limits the RAM that can be used (usually 1 or 2 GB only). The VM settings must be adjusted to increase the available memory for applications such as Kallisto (roughly 8 GB and STAR (roughly 32 GB for human datasets). 

4\. To quit the container, right click inside the browser and choose the QUIT container option. Alternatively, you can also stop it by finding the container id and stopping the container. Quitting the browser just closes the viewport to the container - it does not stop the container.

#### Using a VNC client

In addition to being able to connect with a browser it is also possible to use a dedicated VNC client. Real VNC [Free version](https://www.realvnc.com/en/connect/download/viewer/) is one that we use. Using a VNC client makes all the control key combinations available and cut-and-paste from outside Bwb into Bwb available without using the side menu as in the browser connection.

For a VNC connection you must add the flag
```
-p 5900:5900
```
to the Docker command to expose the port to communicate with the VNC client. Then connect your client to localhost:5900 if you are running Bwb locally or <ip>:5900 if you are running on the cloud. You will need to open the port if running on the cloud - follow the instructions [here](#on-the-cloud) but substituting 5900 for 6080. 

## Installing and starting Docker

### Linux
1\. Update your package manager index.

On Debian based distros such as Ubuntu the package manager is apt-get
```bash
sudo apt-get -y update
```
On Redhat based distros such as Fedora/Centos the package manager is dnf or yum  on older systems
```bash
sudo dnf -y update
```
2\. Install Docker.

Ubuntu:
```bash
sudo apt-get -y install docker-engine
```
Fedora/Centos:
```bash
sudo dnf -y install docker
```
3\.  Start the Docker daemon.
```bash
sudo service docker start
```
4\.  Verify docker is installed correctly.
```bash
sudo docker run hello-world
```

The last command downloads a test image and runs it in a container. When the container runs, it prints an informational message and then exits.

For more information please refer to:     

https://docs.docker.com/engine/installation/

### Linux virtual machine on Windows or MacOS

There are two viable routes for installing Docker on non-Linux platforms. One is installing a Linux VM using VirtualBox or another hypervisor and following the instructions for installing Docker on Linux. The other option, discussed in the next sections, is to use the Docker for Windows/Mac installers which create and manage the VM for you. The first approach requires learning and managing a full Linux distribution in a virtual environment. However, once the VM is correctly set up the Docker installation is simple and robust. This might be the best option for older operating systems or where the Windows user is unable or does not want to run Hyper-V. An (older) example of how to do this is given [here](https://wiki.cyverse.org/wiki/display/HDFDE/Installing+VirtualBox,+Ubuntu,+and+Docker)


### Docker for Mac

1\. Download the Docker package -  [Docker for Mac](https://download.docker.com/mac/stable/Docker.dmg)

2\. To install Docker: double-click Docker.dmg to open the installer, then drag Moby the whale to the Applications folder.		

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image1.png)

3\. To start Docker: double-click Docker.app in the Applications folder. (In the example below, the Applications folder is in ""grid"" view mode.)

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image13.png)


You will be asked to authorize Docker.app with your system password after you launch it. Privileged access is needed to install networking components and links to the Docker apps. The whale in the top status bar indicates that Docker is running, and accessible from a terminal.

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image16.png)


4\. By default, Docker will limit the memory usage to 2 GB. Given that most Bioinformatics workflows are computationally intensive, some of the tasks may require a higher memory usage. To change the memory allocation, go to `Docker Preferences (Right Click on the docker Icon) -> Preferences -> Advanced`, and adjust the memory allocation as needed. We recommend allowing Docker engine to use at least 10 GB of memory or more. 

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image25.png)

The second option relies on Docker to provision the VM and is simpler.  

#### Docker for Windows
1\. To install Docker:

For Windows 10 Pro (with HyperV) download the package - [Docker for Windows](https://desktop.docker.com/win/stable/Docker%20Desktop%20Installer.exe)

For other versions of Windows, the older toolbox version that uses VirtualBox will need to be installed which is available [here](https://github.com/docker/toolbox/releases)

 * Go to the folder where the installation file (Installer.exe) is saved and run (double-click) the installation file.
 * Click the installer link to download.
 * Follow the install wizard to accept the license, authorize the installer, and proceed with the install.
 * When it completes, the installer reports it was successful.
 * Click the finish button to complete the installation.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image21.png)

2\. For DockerToolbox installations on Windows 10 Home Edition, we recommend that you install the latest version of VirtualBox (6.0.10) available [here](https://download.virtualbox.org/virtualbox/6.0.10/VirtualBox-6.0.10-132072-Win.exe). VirtualBox will ask you whether you wish to install the extensions, which you should.

3\.  To start Docker:
* Search for Docker, select the app in the search results, and click it (or hit Return).

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image11.png)
* When the whale in the status bar stays steady, Docker is up-and-running, and accessible from any terminal window.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image20.png)


* If the whale is hidden in the Notifications area, click the up arrow on the taskbar to show it. To learn more, see [Docker Settings](https://docs.docker.com/docker-for-windows/#docker-settings).
* If you just installed the app, you also get a popup success message with suggested next steps, and a link to this documentation.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image15.png)


4\. By default, Docker for Windows limits the memory usage to 2 GB. Given that most Bioinformatics workflows are computationally intensive, some of the tasks may require a higher memory usage. To change the memory allocation, go to `Docker Preferences (Right Click on the docker Icon) -> Preferences -> Advanced`, and adjust the memory allocation as needed. We recommend allowing Docker engine to use at least 10 GB of memory or more.

![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image26.png)

5\. For those using the Docker version that uses VirtualBox, start the Docker application as an Administrator. To adjust the available memory, the user must launch Oracle VirtualBox, stop the Virtual machine and adjust the Systems settings to give the machine more RAM. The default is just 1 GB.

6\. To allow for sharing of files in Windows systems, the directories must be made available to either HyperV in Windows Pro (instructions [here](https://docs.docker.com/docker-for-windows/#shared-drives) ) or VirtualBox other Windows versions (instructions [here](https://medium.com/@Charles_Stover/fixing-volumes-in-docker-toolbox-4ad5ace0e572) ).

### On The Cloud

On the cloud, BwB can also be run on any cloud instance. Please refer to the Linux and Windows instructions to install Docker on the cloud instance.


#### Amazon AWS

1\.  Login to your console and create a new EC2 Linux instance.

2\.  Select the configuration and click on ""Configure Instance Details"".

3\.  Continue configuring your instance. You will click on ""Next:Add Storage"" then ""Next Add Tags"" and then ""Next Configure Security Group""

4\.  When you reach the ""Step 6: Configure Security Group"" page click on the ""Add Rule"" button on the lower left of the screen

5\.  We will the new rule that starts with ""Custom TCP"" that should appear

6\.  Change the Port Range (third box) from 0 to 6080

7\.  Click on the drop down menu that says ""Custom"" (under ""Source"" right next to the ""Port Range"") and choose ""My IP"" if you want to restrict access to Bwb to the computer that you are on or ""Anywhere"" if you wish to access Bwb from any computer

8\.  Now click on ""Review and Launch""

9\.  Copy the public url of the instance.

10\.  SSH into the instance by typing the following command into the terminal:

(Type the commands in the directory where the ssh key of the AWS instance was downloaded)
```bash
 # Update demo.pem with your ssh key name
 chmod 400 demo.pem
 ssh -i demo.pem ubuntu@public-dns-of-aws-instance
```
11\.  After you are logged in use the instructions [here](#linux) to install Docker on Linux.

12\. Then you can start Bwb
```bash
sudo docker run --rm   -p 6080:6080 \
    -v  ${PWD}/:/data  \
    -v  /var/run/docker.sock:/var/run/docker.sock \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --privileged --group-add root \
    biodepot/bwb
```
13\. Open a browser and then type <instance public url>:6080


## Starting Bwb

After you have installed Docker on your machine, you are now ready to start your Bwb session to create and execute Docker container workflows. Bwb comes inside its own Docker container so it is first necessary to launch Docker as shown in the previous sections depending on which platform is being used.

Then run the following commands on the command prompt / terminal.

1\.  Download the docker image containing Bwb.

```bash
docker pull biodepot/bwb:latest
```
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/image2.png)

Alternatively, you can build the image from the GitHub repo:

On Linux and Macs
```bash
git clone https://github.com/BioDepot/BioDepot-workflow-builder.git
cd BioDepot-workflow-builder
sudo docker build -t biodepot/bwb:latest .
```
On Windows (using Git for Windows)

Right-click on the git bash icon and choose to run as an administrator:
```bash
git config --system core.longpaths true
git clone https://github.com/BioDepot/BioDepot-workflow-builder.git
cd BioDepot-workflow-builder
docker build -t biodepot/bwb:latest .

```
2\.  Start the Bwb container.

For Linux/MacOS
```bash
sudo docker run --rm -p 6080:6080 -v ${PWD}:/data -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/.X11-unix:/tmp/.X11-unix  --privileged --group-add root biodepot/bwb
```
For Windows
```bash
docker run --rm -p 6080:6080 -v /c/users:/data -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/.X11-unix:/tmp/.X11-unix  --privileged --group-add root biodepot/bwb
```

This command will launch a mini-webserver and start a windowing environment inside the container. The Bwb application is automatically launched upon running the container and appears as a maximized window on the Desktop inside the container. In the above command we have set the port to be 6080. For Linux/MacOS the current directory is mapped to the /data directory inside the container. For Windows, by default the C://Users directory is made available for Docker and we map this to the /data directory in side the container. Other mappings are [possible](#How do I use Bwb on my own data files)  However, all this is hidden from view until the user connects to the container using a browser.

To access the container open up a browser window and type in the IP of the container, and port that it is listening to, into the address bar. For a local installation using Linux, the IP of the container is localhost or 127.0.0.1 so the user would type localhost:6080 into the address bar of the browser. For a remote installation, the IP is the IP of the server.

<a name=""findip""></a>

For Macs and Windows machines using VirtualBox, the local IP is usually [192:168:99:100](http://192:168:99:100:6080). If that does not work, you can find the IP with the following command in a terminal if using Linux/MacOS, or in the Docker window if using Windows.

```bash
docker ps
```
Alternatively, you can run a simple docker command:
```
docker run --rm --net host alpine ip address
```

More information about finding Docker IP is available here: [https://docs.docker.com/machine/reference/ip](https://docs.docker.com/machine/reference/ip/)

## The Bwb/fluxbox work environment

### Graphics support for containerized apps

The Bwb no-vnc container launches a mini-webserver that is accessed using your browser. The server uses fluxbox [http://fluxbox.org/], a compact windows manager to provide a graphical user interface similar to Windows or the MacOS. Fluxbox provides full graphical support using X11 to render the graphics internally on the server.  Bwb uses the GUIdock-X11 system to allow containerized apps (i.e. Jupyter, gnumeric)  to export graphic output to the server's internal screen.  The noVNC protocol is then used to transfer the internally rendered screen to your browser and HTML5 commands draw the graphics on your browser.


### Basic window manipulations

The Bwb application is started automatically upon starting the Docker container. The window can be minimized, maximized/restored and closed using the buttons in the upper left-hand corner. These are the same buttons available in standard Windows, MacOS and Linux windowing systems. The window can also be resized by clcking on the middle button to unmaximize and then dragging the lower right-hand corner.

Clicking on the left minimize button of the window hides the window and reveals the background. The window can be restored by clicking on the panels in the lower toolbar. Clicking on the right close button closes the application. It, however, does not quit the container.

### Application menu

If we minimize or close the window we will see the background screen. Right clicking on the background brings up an application menu. For the basic Bwb container, there are 3 menu options: the Bwb app, a terminal to enter system commands, and the quit container option.

### Multiple Bwb instances and workspaces

You can launch multiple instances of Bwb which will appear in separate windows.  There are 4 separate workspaces that are available which act as independent screens. Clicking on the panel at the left of the bottom toolbar switches between the workspaces. Cut and paste is supported between different windows within the browser window (i.e. originating from the container).

### Interaction with host windowing system

Note that the fluxbox windowing system is inside the browser window. You still have access to whatever windowing system you are using on your host machine. If your browser closes or go to another url, nothing happens to Bwb - the browser merely provides a viewport to the server in Bwb container. Refreshing or reconnecting the browser to the container IP allows you to interact with Bwb again. Only by using the quit option from the fluxbox application menu, or by using Docker to terminate the container, can you actually quit.  Cut and paste is not yet available from windows in the user's host machine to windows in the browser, though this feature will be added soon.

## Bwb application

### Bwb window with kallisto-sleuth-jupyter workflow showing main features
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/maximized-figure.png)

### Bwb fluxbox desktop with Bwb minimized
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/minimized-figure.png)

### Overview

Bioinformatics analytical pipelines are comprised of multiple modules. The output from one module becomes the input processed by the next module in the pipeline. Scripting languages such as Python, R, Perl, and bash are used to connect workflows and to generate the final output, often in the form of tables and graphics.

In Bwb, we use Bwb workflows and widgets to represent and execute analytical pipelines. Each execution module in a pipeline is represented by a graphical icon which we call a widget.  Output from one widget can be connected to the input of the next, to form a graphical flowchart representing the analytical pipeline which can be saved or executed. A Bwb workflow representing a pipeline, thus consists of a set of widgets (individual execution modules) and a file that defines how their inputs and outputs are connected. Construction of Bwb workflows is accomplished by constructing, and connecting widgets using the Bwb interface described in the next sections.

### Tool Dock

When Bwb is started, the Bwb application window pops up. On the left-hand side of the application window is toolbox (Tool Dock) with multiple tabs (drawers), which contain different collections of widgets. Clicking on the tab expands the toolbox drawer to reveal the contents. Drawers are organized by function. Bwb comes with a set of ready-to-use widgets. These are all linked to containers available on our BioDepot repositiory on Docker hub. Any workflows constructed with these widgets will automatically download the necessary containers the first time that they are run and require no installation.


Users can also create their own drawers. A new drawer is created whenever a workflow is loaded. Also, widgets can be added (and removed) using the Tool Dock editor available from the menu bar. (See the section on editing the Tool Dock.)

Note that different drawers in the Tool Dock can have widgets with the same name. For the included stock widgets, these are identical. However, they can represent different versions due to the ease with which widgets can be customized. For user workflows we use different colors to visually distinguish these customized widgets from stock widgets.

The Tool Dock can be minimized using the button on the top right-hand side.

A miniature version of the Tool Dock is accessible by right clicking in the canvas section to the right of the toolbox.

### Editing the Tool Dock

Widgets and drawers can be added and deleted from the Tool Dock by choosing the ""Edit Tool Dock"" item from the menu. Before any of the changes are reflected, the user must also choose to refresh the settings.

### Interacting with widgets

To interact with a widget, it is first dragged from the Tool Dock onto the canvas. Clicking on a widget brings up the widget UI window with tabs for parameter entry, an output console and a tool bar with options to control the execution of the widget. Right-clicking on the widget and choosing the edit widget item brings up the widget definition window. The definition window contains a set of forms that define the parameters to be queried by the UI window, the Docker container to be used, and the commands to be run upon execution. Finally, dragging the mouse from the edge of one widget to another creates a link from the output of one widget to the input of the second widget. These actions are described further in the next sections.

#### Widget user interaction window

The Bwb interaction window pops up when a widget is double clicked. There are up to 3 tabs in each window: Required entries, optional entries and console.

##### Required parameters screen
Required entries are parameters that must be entered before the widget can execute. An example would be fastq files for an alignment widget.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/required.png)


##### Optional parameters screen
Additional optional entries are flags and parameters that are not required for program execution. When these are present, they are displayed by clicking on the optional entries tab.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/optional.png)

##### Console screen
Finally, clicking on the console tab brings up a window with the text output from the widget. This is useful for monitoring the progress of a widget and for debugging.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/console.png)

##### Execution bar
At the bottom of the UI window are a series of controls that affect the execution of the widget.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/executionBar.png)

###### Start
The start button starts the execution.
###### Stop
The stop button then becomes active and pressing it will terminate execution.
###### Export graphics
The export graphics box, if checked allows the widget to output interactive graphics to the Bwb screen. This is necessary for applications such as Jupyter and Cytoscape that have their own GUI.
###### Test mode
The test mode box, if checked, runs the widget and downstream widgets in test mode. In test mode, upon pressing the start button, the docker  commands are not executed but are generated and recorded in console windows. An option will also appear to allow the user to save the commands to an executable bash script that can be run without Bwb.
###### Run mode
The runmode menu controls how the widget will be executed. In manual mode, the default option, the widget can only be run by the user pressing the start button. Automatic mode means that the widget will run without user input, once all the required options are entered. The last run mode is the triggered run mode. The widget will start execution after one or more inputs are received *AND* all the required parameters are set. Manual start mode is typically used for widgets at the beginning of pipelines or in optional sections of the pipeline. Triggered mode is typically used in downstream widgets to allow widgets to sequentially process the data as it flows through the analytical pipeline.
###### Select triggers
The select triggers menu allows the user to specify which inputs will trigger execution. If more than one input is chosen, the widget will wait until all inputs are received before executing. This menu is only active if the triggered runmode is chosen.


#### Widget definition window

Right-clicking on the widget brings up the option to edit its definition parameters. Choosing the edit option edits the present widget. Choosing the new option edits a new widget. The same options are also available from the main menu. Upon entering the edit widget mode, a window pops up with multiple tabs described next:

##### General tab
The general tab allows the user to enter general information about the widget.
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_gen.png)

The entries are:
###### description
A description of the widget's function. When the user mouses over a widget in the Tool Dock, this text will appear in the help box below the Tool Dock.
###### docker_image_name
The name of the Docker container that is used.
###### docker image tag
The image tag for the Docker container. The default tag for any container is 'latest', which, despite the name, is not necessarily the most recent container. Bwb separates the software version, major dependencies, and date, with underscores to provide a detailed, yet readable, versioning tag.
###### priority
Determines the order of appearance in the Tool Dock drawer.
###### icon
The icon used for the widget.

##### Inputs tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_inputs.png)
The input section allows the user to specify the name of the inputs accepted by the widget. These are variable names that can also be assigned to parameters and outputs. Currently, the callback option is not used. When an input name is also a parameter name, the value of the parameter will be determined by the value received by the input if it is connected to the output of another widget.

##### Outputs tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_outputs.png)
The output section allows the user to specify the names of outputs that will be sent when the widget has finished executing.

##### Volumes tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_volumes.png)
Volumes allow the user to map a user volume to a container volume. This allows the workflows to operate on data that is on the host system. The Bwb container already has one mapped volume and by default this is passed to the workflow containers. For example, the default mapping is that the current host directory where Bwb is launched is accessed through the /data mountpoint in the Bwb container. By default, all workflow containers will also be able to access the host directory through the /data mountpoint.

The volumes tab allows the user to enter a variable name and an internal container volume or mountpoint. The user is then queried (using the parameters section) for the local directory that is to be mapped to the internal container volume.

##### Ports tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_ports.png)
Similar to the volumes tab except the widget can query a host port to map to an internal port.

##### Parameters tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_parms.png)

Different flags and environment variables to be queried and be entered in this section. The name box is the internal variable name. This can also be an output, input, volume, or port variable defined in the previous section that the widget wants the user to input. The type of the variable determines the manner of entry. For example, a file type will bring up a line for manual entry and a button to browse for files. A boolean type will bring up a check box in the UI window. There is an optional flag field. This can be a single -, -- or any string that appears before the value that is entered. The variable can be an argument with no flag. Arguments and flags are passed in the command line. The value can also be passed to the container as an environment variable as well. The entry of a value for the variable can be optional.

Individual parameters are entered using the + button. This will add the parameter to the box where they can be dragged to change the order, deleted using the x button, or edited.

##### Command tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_command.png)
The command tab displays the command that is executed upon in the docker container. A command will be followed by the flags and arguments specified in the parameters section, in order from top to bottom. Arguments always appear at the end of the command. It is also possible to specify a specific order using the _bwb{<variable>} notation. Multiple lines are possible - these are joined by the && operator to form a single command (in bash...)

For example the command:
```
rm -f Counts/*
Rscript Programs/analyze.R _bwb{ConfigurationFile}
```
will generate the following command:
```
rm -f Counts/* && Rscript Programs/analyze.R <ConfigurationFile> <flags> <arguments>
```
##### Docker tab
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_docker.png)
The Docker tab contains information about the Dockerfiles and build commands used to construct the container. Currently, this is mainly for documenting the provenance of the container. However, we will be adding the option of generating the containers from this section rather than downloading the container from a repo.

Currently, buttons exist to copy a Dockerfile to the widget directory (Add Dockerfile) or delete Dockerfiles (Clear).
There is also a button to bring up the BiocImageBuilder utility which facilitates the building of Docker containers.

##### Widget definition save options and further customization by editing the .py file
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/def_bar.png)

At the bottom of the window are the save options. To understand the save options, let's first explain the files generated by the Bwb widget builder after saving the definition. The files generated for widget mywidget are
```
mywidget/mywidget.attr
mywidget/mywidget.states
mywidget/mywidget.json
mywidget/mywdiget.py
```
mywidget.attr is a json file that saves the variables entered into the definition window.
mywidget.states  is a json file that saves the actual state of the form so that when the definition window is opened again, the user can resume where they left off.
mywidget.json is a json file that stores the variables which are actually read in by the UI window and is derived from the attr and states files.
mywidget.py is the code that is executed to implement the widget. It reads in the mywidget.json file to display the UI window and execute the command in the docker container when the form is filled. Note that the values for the form are not stored with the widget but with the workflow .ows file described later.

Originally in the OrangeML setup, the mywidget.py code was written manually. Most of the boilerplate code has been automated using the mywidget.json file and type based interfaces. The input and output control routines have also been automated but this is an area where some customized code maybe required, for example to process internal variables into a form that will be output.

To accommodate custom code, there are 3 save options:
###### Save mode: Overwrite

Overwrites the existing python file with the newly generated one. This is used when there is no customized code.
###### Save mode: Merge
Will only overwrite the boilerplate code for reading values from the json file (the first 80% of the code). Any code that appears after this first part of the script is untouched.
###### Save mode: Data
Will not touch the python file. Will only generate the json files.

Finally, in addition to the 'save', and 'save as' buttons, there is a 'load' button.
###### Load button
Will load the attrs and state from another widget - this allows the user to use a pre-existing widget as a template or starting point.

### Building workflows from widgets

#### TLDR;

A quick summary of the steps to construct a workflow:

1\. Drag desired widgets from Tool Dock onto the canvas

2\. Save the workflow and check of merge all widgets box

3\. Load the saved workflow and a drawer of workflow widgets will appear in the Tool Dock

4\. Edit the widget definitions to define which parameters will be queried, what command and which container will be run

5\. Connect the widgets by dragging from the right side of the widget (output) to the left side of the next widget to form the pipeline

6\. Enter all the values for the different parameters

7\. Save the workflow

#### Workflow structure

The workflow is stored in a single directory. This directory contains widgets specific to the workflow, the icon, and a stub \__init__.py  python script used to load the workflow into Bwb. An .ows file saves the connections between the widgets and all of the parameter values or settings in xml format. This is different information than the json files for each of the widgets, which store information defining which parameters are queried and what/how the widgets execute based on these parameters, but not the values of the parameters themselves. Workflows can also use widgets that are not stored locally in the directory. This can be useful for building and prototyping new workflows or when the workflow is meant to be a dependent version to be automatically updated as widgets get changed in other workflows. Upon saving, there is the option of merging all widgets. This will make copies of all widgets outside the workflow, resolve any names that are identical and update the .ows and other files. All widgets will then appear in a separate Tool Dock drawer when the workflow is loaded.

For most cases, we recommend that you merge all widgets that are to be used, before modifying them in a workflow for maximum encapsulation and reproducibility.

**N.B. Changes to widgets and workflows may not be reflected immediately as Bwb must reload in order to incorporate new or changed routines. This is done as much as possible automatically but if it does not, use the reset settings option from the File Menu or the Load Workflow option to reload the workflow. **

#### Connecting widgets
Widgets are connected by dragging the cursor from the right side of the source widget to the left side of the destination widget. This transfers the output of the source widget to the destination widget. When there are several possible connections, Bwb will choose one. Double clicking on the link will allow the user to edit this choice and select which inputs are to be connected to which output. An output can be connected to multiple inputs, but inputs in Bwb by default currently accept only one output. This may be changed in the future - especially for triggers.

##### Input from connections override user input
When an input is connected to an output - the output value will become the input value. This value will override any user input. Accordingly, the form for that input will be grayed out and inaccessible.

##### Connections to triggers are used to control the execution of widgets
One of the major uses of connections is to control the execution of widgets in workflows. A widget that has one or more inputs that are connected can use these as execution triggers using the run mode at the bottom of the destination widget. The widget will not execute until all required parameters are entered and all the inputs that are triggers receive input. This allows for a widget to wait until another widget has finished processing. For example, the kallisto quant widget in the kallisto-sleuth demo workflow is triggered by the indexFile produced by the kallisto index widget and the output of the fastq download widget. It will wait until the index file is ready and the fastq files are downloaded before proceeding.

#### Saving workflows
The 'Save workflow as' option under the File menu will bring up a dialog box. The dialog box will ask for the workflow name which will also be the name of the directory where the widgets and other workflow files are stored. The parent directory where the workflow directory will reside also needs to be inputted. Optionally, a color and icon can be provided. Otherwise, the current icon and color of the workflow are used or, in the case of new workflows, default icons and colors will be used. Finally, there is a check box to merge all widgets. If this is left unchecked, the workflow will not maintain a separate copy of the widgets in its directory. The workflow will function but will use off-the-shelf widgets provided by Bwb or another loaded workflow. Checking the box clones all the widgets into the workflow directory. Note that different drawers can carry their own version of a widget. These will be assigned different names if there is a conflict before being copied into the workflow.

#### Loading and executing a workflow
To load a workflow go to the File menu at the top left corner of the Bwb window and choose the 'Load Workflow' option. A file dialog will pop up, allowing you to choose the workflow to load. The workflow is a directory.

Demo workflows that come with Bwb are in the /workflows directory.

To execute a workflow, double-click on a widget and manual start from that widget by hitting the blue start button. When that widget has finished executing, it will send output to connected widgets. If these widgets are triggered by the output, they will then execute (as long as all the required parameters and other other trigger signals have been received). Workflows can also be started from any widget if the required files and parameters are present. For example,  an alignment pipeline can skip the indexing steps if the index has already been generated or downloaded.

#### Testing and exporting workflows as a bash script
A test mode is also provided for testing. Checking the test mode box before hitting the start button causes the widget and downstream connected widgets to output the docker commands to the console rather than executing them. This allows the user to check whether the triggers are set and the necessary parameters are entered without needing to run a lengthy workflow. In addition, the user will be prompted for a file to save the docker commands as a bash script. The script is a record of the actual docker commands that are run when the workflow is executed.

The bash script is portable and can be run without Bwb with 3 caveats:

1\. It may be necessary to give the save file run permissions.

2\. The file paths are those of the host system - if the script is run elsewhere these will need to be altered.

3\. The graphics are set to screen1 which is used by Bwb. Bwb must be active for graphics support, in which case the graphics will appear inside Bwb even if the script is run on the host. Alternatively, the user can change the command to use screen0 and follow the recipes given in our GuiDock-X11 paper.

## Demo workflows

Four demo workflows are included with the Bwb container. They are found in the */workflows* directory

### DToxS demo
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/dtox_wf.png)
This is a workflow used for processing UMI (Unique Molecular Identifier) barcoded RNA-seq data. This is one of the first workflows that we converted for Bwb. The first downloadURL widget downloads the fastq files, directories and support files needed for the workflow. Upon completion, it signals the DetoxS alignment widget. This widget calls bwa using a bash script and two python scripts. These are the original scripts used by DToxS. The alignment on full files would take overnight to run so our sample files are shortened versions of the true reads. However, the short files are too short to give any detectable differential expression in the subsequent steps. Therefore, in this demo, we have a second downloadURL widget which downloads the SAM files produced by bwa on the complete files. These are fed to the DtoxS analyses which consist of a series of R scripts that use the edgeR package to identify differentially expressed genes. The top 40 most confidently predicted differentially expressed genes are then displayed by gnumeric, an open-source spreadsheet program.

### Kallisto-sleuth demo
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/kallisto_wf.png)
This workflow is a popular RNA-seq workflow using Kallisto to pseudo-align the reads and sleuth to determine which transcripts are differentially expressed. The workflow starts with a downloadURL widget that downloads the necessary directory structure and files that are used by sleuth to translate the transcript names to gene names. This widget also downloads a file that describes which data are in the control group and which data are from the treatment group. Upon completion, the widget signals a second downloadURL widget to download the human genomic sequence that will be used by the Kallisto index to create the indices needed for alignment. The first widget also signals the fastqDump widget to download 6 paired-end reads in 12 fastq files. These are the data obtained from 6 samples that will be analysed. The widget is set to only download the first 10000 spots to allow the demo to complete in a few minutes. The Kallisto align widget is triggered when both Kallisto index and fastqDump are finished, i.e. it will start running after the index is made and the files are downloaded. Kallisto-widget is a bash wrapper around the Kallisto pseudo-alignment program. The bash wrapper sends multiple pairs of paired-end reads to the Kallisto pseudo-alignment executable. Kallisto then produces a series of directories that contain estimates of the abundance of the reads at each transcript. Sleuth is an R script that uses a model based on the observed abundances to determine whether a gene is differentially expressed and obtains a p-value. The sleuth widget itself is a bash script which generates an R script that calls sleuth with the parameters given by the user. The resulting p-values are then output to a file which is read by gnumeric, an open-source spreadsheet, that displays the results on the screen.

### Kallisto-sleuth with Jupyter demo
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/kallisto_jup_wf.png)

Accompanying video link, https://www.youtube.com/watch?v=jtu-jCU2DU0

The Kallisto-sleuth workflow with Jupyter demo is identical to the Kallisto-sleuth workflow except that, instead of wrapping sleuth in a bash script and outputting the results using gnumeric, a Jupyter notebook is used to run and display the results. The first notebook widget runs nbconvert which runs the code in the notebook and generates a filled notebook with the results, including a graph of the expression of the top differentially expressed gene. This widget triggers a second jupyter widget which opens the filled notebook. The second widget is run in a container with firefox, which automatically opens the notebook at the end of the workflow. The user is free to interact with the code to change the analyses, or conduct further analyses, using the filled notebook as it is a fully functional dynamic instance of Jupyter.

### STAR demo
![](https://github.com/BioDepot/BioDepot-workflow-builder/raw/master/docs/images/star_wf.png)

STAR aligner is another popular RNA-seq aligner. Here we have paired it with DESEQ2 to perform the differential analyses. The pipeline is very similar in structure to the kallisto-sleuth pipeline. A downloadURL widget downloads the directory structure which then signals the download of the human genome and the calculation of indices. The fastqDump widget downloads the fastq files. STAR align waits for the downloads to complete and the index to be formed. Like Kallisto, this is wrapped in a bash script to allow STAR to run on multiple pairs of paired-end reads. A small widget runs a bash script then rearranges the output columns into a form that DESEQ2 can read. DESEQ2 is R-based and, like the sleuth widget, uses a bash script to pre-process the parameters and construct an R script. Gnumeric displays the final output as in the Kallisto-sleuth demo.

## Tutorial - Adding a Python script to a Bwb workflow

Accompanying video link, https://www.youtube.com/watch?v=r_03_UG1mBg

The aims of this workflow are to demonstrate how to build a widget for a custom Python script and modify and customize an existing workflow. In this tutorial we will write a script to call cutadapt (a Python app for removing adapters from reads) and insert it into the kallisto-jupyter demo workflow to trim the reads before alignment.


### Overview

The basic steps will be to:

1\. Add the Python2 widget to the Kallisto-jupyter workflow

2\. Customize the Python2 widget to query the additional parameters needed by the cutadapt script

3\. Create Docker image

4\. Write a short custom script locally to manage multiple files

5\. Connect widget into the workflow

6\. Test and run the new workflow


### Add Python2 widget to the kallisto-sleuth-jupyter workflow

1\. From the File menu choose 'Load Workflow'. Navigate to the */workflows* directory Click on the Demo_kallisto_jupyter directory and hit the 'Choose' button in the bottom right corner of the dialog. After a second or two, a new drawer should appear in the Tool Dock and the workflow should appear on the main canvas.

2\. Click on the 'Scripting' tab in the Tool Dock on the left-hand side of the window. A set of programming widgets will appear.

3\. Drag the 'Python2' widget onto the main Canvas. A purple 'Python2' icon should appear on the canvas next to the original light green workflow.

4\. Choose 'Save workflow as' from the 'File' menu. A 'Save Workflow' file dialog should pop up.

- Change the workflow name to Demo_kallisto_jupyter_cutadapt.
- Then click on the blue folder button next to 'Workflow Parent Directory'. This will bring up a 'Locate Directory' navigation window. This should start up in your local files. Use the new folder button (third from the right in the top right corner of the navigation window) to make a new folder and label it ""tutorial"". Click 'Choose' in the lower right-hand corner.
- Change the color of the new workflow to something else by clicking on the color wheel button next to 'Change Workflow Color'.
- Check the 'Merge all widget types' checkbox. This will clone the 'Python2' widget and the original Kallisto widgets and put them into a new workflow.
- Click OK on the Save workflow. This saves the new workflow to /data/tutorial/Demo_kallisto_Jupyter_cutadapt and loads the new workflow.

 The new workflow widgets should appear in the Tool Dock and the new cloned workflow should appear on the canvas. This will be apparent by the color change.

### Rename and customize the Python2 widget

1\. Right-click on the Python2 widget. A menu should pop up. Choose the 'Edit Widget' option to bring up the Widget Defintion window. Hit the rename button and change the name to cutadapt_demo. This will rename the widget definition, i.e. the base name of the widget in the drawer in the ToolDock

2\. The Python2 definition window should have 8 tabs. If the window is too narrow, not all the tabs will be visible. Either resize the window by dragging on the lower right corner or use the arrows in the top right to scroll the content. The 'General' tab should be the active one by default. Make the following changes:

**NB (do not enter quotes)**
-  description:  'Cutadapt trimming'
-  docker_image_name: 'biodepot/cutadapt-demo'
-  docker_image_tag: 1.0

The description will appear in the lower left-hand help window under the Tool Dock when we hover over the widget, to let other users know what the widget does. The Docker image and tag fields tell the widget which container to use. We will build that container later.

3\. Click on the Inputs Tab. Under the big white text box there is a set of data entry boxes followed by an add button (file icon with plus sign) and a delete button (file icon with x sign) which should be inactive. Enter 'OutputDir' in the 'Name' box and then click on the add button. The new entry should be visible in the text box. To edit an entry if there is a mistake, click on the entry in the text box. The boxes at the bottom will be filled with the values from the entry and can be edited and saved by clicking the add button. The delete button should become active and allow you to delete the entry.

What we have done is defined 'outputDir' as an input. This is because the fastq download widget outputs the output directory upon finishing and we want to be able to receive that signal. However we also want to be able to use that information by passing it to our script. We will do this in the next step.

4\. Click on the Parameters tab. Do the following:

- Create an entry for 'OutputDir' by putting ""OutputDir"" in the 'name' text box, choosing 'file' from the 'Type' text box, checking the 'flag' box, entering *-d* followed by a space as the value, and pressing 'enter'. Check the 'label' checkbox, and enter ""fastq directory:"" into the 'label' text box. Then press the 'add' button.

- Create an entry for 'quality' by putting ""quality"" in the 'name' text box, choosing 'int' from the 'Type' box, checking the 'flag' checkbox, and entering *-q* followed by a space, as the value for the flag. Enter ""Mininum quality"" in the label box. Enter ""10"" in the 'default' box. Then press the 'add' button.

- Create an entry for 'minLength' by putting ""minLength"" in the 'name' text box, choosing 'int' from the 'Type' box, checking the 'flag' checkbox, and entering *-m* followed by a space, as the value for the flag. Enter ""Mininum read length"" in the label box. Enter ""50"" in the 'default' box. Then press the 'add' button.

- Modify the the 'inputFile' entry by clicking on the inputFile entry. Then at the bottom uncheck the 'Argument' box. Check the 'flag' checkbox and enter a single space for the value of the flag. Click on the 'save' button (file icon with plus sign).

We have now created three new entry forms. One for 'outputDir', which is where the fastq file reside, one for the minimum quality of the reads, and one for the minimum read length. We also changed the 'inputFile' entry (which is the python script) to have a blank flag instead of being an argument. Bwb will place all parameters after the command in the order in which they appear in the list. However, arguments will be placed after all flags. The bare command is visible in the Command tab and is simply 'python'.

So with our changes the widget will execute:

```
python <blank flag><scriptName> -d <outputDir> -q <quality> -m <minLength>
```

In reality, we would also have parameters for the file with adapter sequences and possibly other flags instead of this relatively simple example.

### Creating the Docker image


The Dockerfile used is:

```
#from https://github.com/frol/docker-alpine-python2/blob/master/Dockerfile - the Dockerfile used for Python2 widget
FROM alpine:3.7
RUN apk add --no-cache python && \
    python -m ensurepip && \
    rm -r /usr/lib/python*/ensurepip && \
    pip install --upgrade pip setuptools && \
    rm -r /root/.cache

#added these lines for cutadapt
RUN apk add --no-cache gcc gzip && \
    pip install cutadapt && \
    apk del gcc python-dev libc-dev && \
    rm -r /root/.cache

```

The widget is built starting from the Dockerfile used for the python2 widget. This widget was built using the compact alpine Linux distro which uses apk as a package manager (analogous to apt-get, yum, and dnf in other distros). We add one extra command line (the slashes concatenate the lines into a single command) which installs the gcc compiler suite and dev libraries python-dev libc-dev needed by pip to install cutadapt. The gzip library is installed as well to support decompression of gzipped fastq files. *pip* is then called to install *cutadapt* and the dev tools removed as they are not necessary to run cutadapt. The entire procedure is done in one command to avoid Docker spawning intermediate containers that are take up space in the final image.

To add and build this Dockerfile:

1\. Click on the Docker tab of the Python2 definition window. This is the rightmost tab and may require scrolling to see it.

2\. Click on the blue folder button on  'Add Dockerfile' line and navigating to /tutorialFiles/Dockerfile. Then press the add button (rightmost button) to add the Dockerfile. A message should appear confirming that the fie has been added.

3\. Click on the blue launch button to launch the image builder. Resize it by dragging the right corner and scroll to the bottom to see the open button.

4\. Click on the 'open' button. It should start in the Python2 widgets Dockerfiles directory. The Dockerfile should be there. Open the file.

5\. Enter biodepot/cutadapt-demo:1.0 into the Image Name box and press the build button.

### Save widget and load the workflow again

Hit the bottom save button to save all the changes to the widget. The workflow should automatically reload to update the changes.

### Creating or copying the wrapper Python script

The wrapper script is needed to call cutadapt on different sets of paired-end files. It is provided in /tutorialFiles/cutadapt_multi.py or you can paste the code into a local editor to be saved on your local file system. The code is also available from the github repository.

The script is:
```python
import os
from glob import glob
from optparse import OptionParser

def runCutAdapt(file1,file2,flags):
    print (""cutadapt {} -o tmp/{} -p tmp/{} {} {}"".format(flags,file1,file2,file1,file2))
    os.system(""cutadapt {} -o tmp/{} -p tmp/{} {} {}"".format(flags,file1,file2,file1,file2))

#get options - we are interested in -d for directory -q for quality -m for minimum length
parser = OptionParser()
parser.add_option(""-d"")
parser.add_option(""-q"")
parser.add_option(""-m"")
(options, args) = parser.parse_args()

flags =""-q {} -m {}"".format(options.q,options.m)
#change directory to output directory
os.chdir(options.d)

#we use the fact that for our naming convention the paired end files will be nicely grouped in pairs
files=sorted(glob('SRR*.gz'))

#make a temporary directory
if not os.path.exists('tmp'):
    os.makedirs('tmp')

#run cutadapt on pairs
for i in range(0,len(files)/2):
    runCutAdapt(files[2*i],files[2*i+1],flags)

#copy the filtered files and remove the temp directory
os.system(""cp -r tmp/* . && rm -r tmp"")
```
The script should be stored locally because files stored in the Bwb container files system (those not accessed via /data or other mountpoint) are lost when the container terminates. In addition, although the script is in the Bwb container, the widget will not be able to see it unless we add a volume mapping or simply move the file to our local system.

To do this:
1\. Click on the orange right arrow at the bottom of the browser window to change the workspace to 'workspace 2'. There should be a screen with no windows.
2\. Right click and choose the 'Terminal' option.

3\. Enter the following command (assuming that you used the default /data mountpoint):
```bash
cp /tutorialFiles/cutadapt_multi.py /data/tutorial/.
```
### Connecting the widget to the workflow
 1\. Right-click on the link between the Download fastq files and kallistoQuant widgets and chose the 'Remove' option.

 2\. Click on the right side of the Download fastq file widget and drag the mouse to the left-hand side of the cutadapt widget.

 3\. When the link dialog pops up, click on the 'Clear all' button in the lower left-hand corner. Click on the OutputDir Box on the left and drag the mouse to the OutputDir Box on the right. Click 'OK'.

 4\. Click on the right side of the cutadapt widget and drag to the left side of the kallistoQuant widget. When the 'Dialog box' appears, hit the 'Clear all' button and connect the OutputDir of cutadapt to the trigger of kallistoQuant.

 5\. Double click on the Cutadapt widget. Enter the following:

 	Script: /data/tutorial/cutadapt_multi.py
 	RunMode: Triggered
 	
6\. Click on 'Select Triggers' and choose 'OutputDir'.

### Running and testing the workflow
The workflow is ready to be run by double-clicking on the 'Download sleuth directory' widget and pressing start. If you wish to make sure that all the connections and parameters are correct, then check the test mode box before pressing start. This will cause the Docker commands to be generated but not run. Instead the commands will be output to the console. In addition, a prompt will appear to allow the option of saving the commands as a bash script representation of the workflow. This script can be executed outside of Bwb and will give the same results (and errors) as running the workflow using the normal Bwb interface.


## Appendices

### How Bwb executes workflows
#### TLDR;

Bwb takes values from the widget forms, generates and executes a Docker command for the widget, and passes signals and data to downstream widgets to trigger their execution.

Bwb takes the values from the forms and generates a Docker command (or set of commands when there are multiple commands) for each widget. Pressing the start button in the widget UI window, executes the command as a Quicktime QProcess which is interruptable and can signal when it is finished. If the process finishes without error, output signals are emitted and passed to linked widgets using the OrangeML signal manager. Upon receiving a signal, the widget checks that all necessary parameters are set and if execution is also triggered (or is automatic once parameters are set), its execution starts.

### Organization of code

The Bwb container copies directories from the repository to the / directory of the container and installs the Python packages using pip. These directories are:

1\. orange3 - contains major routines of orange-ml

2\. coreutils - contains major routines of Bwb

3\. biodepot - mostly symbolic links, required so that Bwb widgets will appear in Orange's Tool Dock implementation

4\. widgets - where the included widget definitions for Bwb reside

5\. workflows - copies of the demo workflows are found here

6\. tutorialFiles - some files for the tutorial are found here

#### coreutils
The core code for Bwb is stored in coreutils directory of the Bwb repository and is loaded into the coreuitls.

##### BwBase
Each widget is an instantiation of the BwBase widget class which is a subclass extension of the original OWWidget class from Orange. The base class manages the forms and draws the UI for the widget. It also generates the basic command.

##### DockerClient
To run the Docker command, a DockerClient class is used. This DockerClient object is created when Bwb is started and used to have methods based on DockerPy. Now it has two main functions, one is to convert the widget commands to Docker cli commands and the other is to execute it. Execution is done using a subclass of QProcess which attaches output of the Docker command to the console display, manages STOP interrupts from the user and signals when the process has completed or aborted. The DockerClient is also responsible for running the workflow in test mode and generating a bash script.

##### OWWidgetBuilder
This class is responsible for editing and saving the widget definitions. Originally this was a separate widget - hence it is also a subclass of the OWWidget class of Orange.

##### createWidget
This is responsible for auto-creation of the python script for the widget.

#### OWBiocImageBuilder
This subclass also started out as a widget and is now optionally called by widget builder to provide a UI to facilitate building Docker images.

#### ToolDockEdit
Code for editing the Tool Dock.

#### makeToolDockCategories
Basic code for manipulation of the Tool Dock.

#### workflowTools
Code for loading and saving workflows.

### Organization of widget definition directory
Widgets are stored as a directory. Each widget consists of the 3 json files and one python file as described before. In addition, there is an icon directory which contains the icon file (png, svg, jpg) and a Dockerfiles directory, which (optionally) contains the Dockerfiles and scripts necessary to build the widget.

### Organization of workflow directory
Workflows are also stored as a directory. There is an XML .ows file which stores the graph of widgets, and the parameter values for each widget. This is the original format from OrangeML. There is an icon directory and a widgets directory that store the icon and widgets used by OrangeML.

### List and description of included widgets

#### Scripting widgets
Scripting widgets take as a required parameter the script to be run. Only basic libraries come pre-installed with the container.
##### bash_utils
Very light container with alpine linux and a few utilities (curl, wget, gzip, xterm) for bash scripts. 5 MB in size compared to 112 MB for a base ubuntu container
##### bioc_R
Bioconductor and R are installed. Ubuntu is the operating system as it is the base test system used by Bioconductor.
##### Java8
The java 8 engine is installed and uses alpine
##### Perl
Alpine Linux with Perl
##### Python2
Alpine and Python 2.7
##### Python3
Alpine and Python 3.6

#### Jupyter widgets:
All jupyter widgets take as a required parameter an ipynb notebook file which is executed. The export graphics checkbox must be checked if the notebook is to be used interactively. Base operating system is ubuntu. Firefox is used to interact with Jupyter.
##### jupyter_base
The basic vanilla jupyter widget with Python kernel. It takes a Jupyter notebook .ipynb file as an input. Most external libraries will have to be installed by the notebook.
##### jupyter_bioc
Bioconductor and the R kernel have been installed. It can be further customized so that packages are pre-installed in the container using the  BiocImagebuilder utility
##### jupyter_sleuth
Sleuth and the R kernel has been installed.
#### RNA_seq:
Widgets used for RNA-seq are here
##### deseq2
deseq2 wrapped in a shell script in order to pass the parameter values
##### DtoxSAnalysis
Runs the shell script used by DToxS for their UMI RNA-seq analysis. The shell script organizes data and calls EdgeR to perform the differential expression analyses.
##### DtoxSAlignment
Runs the shell script used by DToxS for their UMI RNA-seq alignment. Calls two Python scripts that use bwa for alignment
##### kallistoIndex
Generates the indices needed for kallisto. It calls kallisto with the index command. A shell script is used to pass multiple files and paired-end reads to kallisto.
##### kallistoQuant
Performs pseudoalignment quantitation using kallisto. It calls kallisto quant.
##### starIndex
Calls STAR aligner with index runmode to generate indices for STAR aligner
##### starAlign
Calls STAR aligner to perform alignment and quantitation. A shell script is used to pass multiple files and paired-end reads to STAR.
##### startodeseq2
A simple script to arrange the column output of STAR for input to deseq2
##### sleuth
A wrapper shell script that passes the parameters to sleuth.
#### Miscellaneous:
Some utility widgets are kept here that are useful for testing inputs and outputs.
##### Directory
Prompts user to choose a directory. Sends it using a second button.
##### File
Prompts user to choose a directory. Sends it using a second button.
#### User:
User defined widgets go here.
#### Utilities:
##### downloadURL
Downloads a file given a URL. A shell script uses curl/wget and gzip/bzip to fetch and decompress the files. Additional logic is used to download files from google drives.
##### fastqc
Invokes fastqc. The interactive graphics mode is also supported.
##### fastqDump
Used to download fastq files from GEO. Contains the SRA toolkit. A shell script passes parameters to the fastqDump utility of SRA tools.
##### gnumeric
Calls the gnumeric open-source spreadsheet. The use graphics option should be checked to use the UI.


### Description of json descriptors for widgets (Note that some of this may be outdated)

The json file describes a dict structure in Python which will be read into the dict *data* in the widget python script.

There are 17 primary fields to the object

**'name' :**  <*str*> -name of widget

**'category' :** <*str*> -may be used in future to group widgets by function

**'icon' :** <*str*> -path of icon file to be used by bwb

**'priority' :**  <*int*>  -priority of widget - used to determine which widget to evaluate first when there are multiple outputs

**'want_main_area' :** <*bool*> -use only if custom code needs second drawing area

**'docker_image_name' :** <*str*> - name of docker image to launch

**'docker_image_tag' :**  <*str*> tag of docker image e.g. latest

**'persistent_settings' :** <*str* or *list*> - 'all', 'none' or list of settings values that will be saved and restored upon launch

**'command' :** <*str*> the command that will be launched

**'groups' :** <*dict*> - group parameters together - currently not used but will be used to provide support for linked entries such as exclusive checkboxes
```python
	{
	  <str> - name of  group : [ <str>] -list of attribute to be grouped
	  'type ': <str> - reserved for indicating if group be treated as xor, or linked to a checkbox etc.
	  'callback' : <str> - callback for custom handling of group
	}
```
**'inputs'  :** <*OrderedDict*> -attributes that can obtain values from other widgets
```python
	{<str >- attribute to be input> : <dict>
	   {'type' : <str>,
	     'callback' : <str>,
	   }
	}
```
**'outputs'  :** <*OrderedDict*> -attributes that store outputs to other widgets
```python
       {<str> - attribute to be output : <dict>
           {'type' : <str>}
       }
```
**'volumeMappings'  :** <*list*> -mappings from container to host paths
```
       [ <dict>
         {'conVolume' : <str> - path of container volume to be mapped :
           'attr' : <str> - attr which stores the path of the host container
           'default' : <str> default value of the host path
         }
       ]
```
**'requiredParameters' :** <*list*> -list of attributes or groups that are required to be entered or obtained from inputs
**'parameters'** : <*Ordered dict*> - parameters and arguments to be entered
```Python
      {<str> - attr where parameter value is stored : <dict>
         {'flags' :  [<list of str>] -one or morecommand line flags used to specify the parameter - if empty then it is an argument - if None then it is not on command line
           'label' : <str> -used by GUI as short label to indicate what is to be entered
           'type;' : <str> -type of the value to be entered
           'default' : <depends on 'type' field> default value used to initialize - otherwise None
           'env' :<str> -environment variable in the container that is to be assigned this value
           'gui' : <one of 'FileDir', 'Ledit', 'Spin', 'bool'> - tells Bwb to use a specific gui element instead of the default one based on 'type'
          }
       }
```

Open a new instance of Bwb and you should see the new widget in the toolbox.
A sample json file is in the /biodepot/orangebiodepot/json directory

### BwBase class

The abstraction of the widget details is handled by the Bwb class which is responsible for the following tasks:

1\. Keeping track of connections

2\. Handling input signals

3\. Drawing the GUI form

4\. Calling the Docker API to launch the executable

#### Keeping track of connections
The current engine does not send a signal to a widget if the output connections are modified so currently, only connections to inputs are kept. This is done using the inputConnections object which is an instance of the custom ConnectionDict class. This class is a simple extension of the basic dict class in Python, with add, remove and isConnected methods.

#### Handling input signals
A signal is sent to an input when it is connected to an output, disconnected from an output or when a signal is sent from a connected output. The current engine requires that each input have a separate callback - there is no way around this using lambdas or partial functions due to how Orange handles signals. Bwb handles this by having a wrapper function pass the identity of the input along with the signal to the **handleInputs** method of Bwb.

#### Drawing and managing the GUI form
There are 4 main elements that are drawn:

1\. Status box - error messages and progress and other informational messages are displayed here

2\. Required parameters - the forms for the parameters required to run the widget are here

3\. Optional parameters - a list of optional parameters is placed here

4\. Execution box - whether the widget is run automatically, manually, or semi-automatically depending on runTriggers is set here

The master method is the 'drawGUI' method which draws the Status box and calls **drawRequiredElements**, **drawOptionalElements**, and **drawExec** to draw the other elements. It then calls **checkTrigger** to see if it should start running the Docker process.

**drawRequiredElements** and **drawOptionalElements**, loop through the parameters, initialize the value of the attributes, and send them to one of **drawCheckbox,** **drawSpin,** **drawLedit,** **drawFileDirElements** depending on the type of the parameter or whether a specific element was specified by the 'gui' field. There is also a **drawFilesBox** method element for entering and rearranging multiple elements. All optional elements are drawn with a checkbox, and inactive unless the checkbox is checked. All elements that are inputs are only active when there is no input from another widget that is responsible for defining a value. Elements of the same type are grouped together. This is done by mostly by the **bgui** object which is an instance of the **BwbGuiElement** class. 'bgui' keeps track of all of the gui elements associated with a particular attribute and activates or inactivates them.

The different draw routines are described next:

**drawCheckbox:** Used for booleans. Checked means value is True - otherwise False. The checkbox provided by the Orange **gui** class is used

**drawLedit :** A single line used for text entry and for non-integers. The **bwbLedit ** method is used instead of the Orange **gui** method to specify the layout of the button, label and line elements so that they line up correctly.

**drawSpin :** Used for entering integers - default range is 1 to 128 - for higher ranges the user should specify use of a drawLedit with the 'gui' keyword in the 'parameters' dict . The Orange **gui** code is used for this element.

**drawFileDirElements** : A single line with a label and browse button used for entering directories, file, or a list of files. The browse button is used to choose a directory of a file. When in files mode, the button appends to a list of files. A clear button is provided that clears the line. The browse functions are managed by the **browseFileDir** method. All paths returned by the browser are relative to the Bwb container filesystem (where /data is the usual portal to the user filesystem). These paths are later converted to hostpaths before execution.

**drawFilesBox** : This provides a box for entry of multiple items, not just files and directories. The items are entered using a line edit field at the bottom of the box. Entries are added or subtracted using the buttons next to the field. For files and directories an additional browse button is provided to traverse the files. Entries in the box can be re-ordered by dragging and dropping. This is necessary, for example, for paired-end reads where the order of the file arguments can matter depending on the application.

**drawExec** : Elements in this box control the execution of the software encapsulated by the widget. There is always a start button which will start the run. A comboBox allows the user to choose between manual, automatic and triggered execution modes. In manual mode, execution only starts after the user pushes the start button. In automatic mode execution will start when all the required parameters have been entered. If there are inputs that are not required, then the user may choose to have additional triggers. In other words, the user may choose to wait for one or more signals to be received on these inputs before starting the execution. When trigger mode is chosen, a menu is enabled to allow the user to choose the inputs that are to be used as triggers. When in trigger mode, all the required parameters must be set and signals must be set (has received a signal that is not 0).

The idea of triggers is to allow for previous requisite steps to signal that they are finished and that the widget should start. This can happen even if the output on the output stream is not used by the widget. For example, an alignment widget may produce a set of SAM files in a directory and output the directory. Rather than forcing the analysis widget to have a separate input to receive the input, the directory output may be connected to the generic trigger input instead.

Currently only one output can be connected to a trigger. However, support is being added so that multiple signals can be connected to the trigger and the user can choose whether to begin when any of the signals are received or when all the signals are received.

Currently, when the job is launched, all the gui elements are disabled and get re-enabled after the job terminates. Support will be added to allow the user to interrupt the widget and also for messages to be sent to the console window and automatically logged.

**N.B.** For the most part, once the elements are drawn, they stay drawn - only the states change depending on what signals are received either from the user or from another widget.

#### Launching the executable with Docker
Once the widget has decided to launch the executable, it calls the startJob method. The **startJob** method will check that the required parameters have been set. It will then check that all the container volumes that are specified in the **volumeMappings** field of the json file are mapped to host directories. If these criteria are met, then the command is generated from the parameters with the **generateCmdFromData** and **generateCmdFromBash** methods. Some of the parameters may be passed as environment variables as well if specified in the json file. This is handled by the **getEnvironmentVariables** method. The commands, environment variables and volumemappings are passed to docker run routines which call dockerpy to pull the container if it is not present and then run it.

Directory paths are a bit complicated as there are 3 different file systems. There is the container filesytem that is being launched, the host system (i.e. the laptop or cloud instance) and also the filesytem used by the Bwb container. The file browsers used by the Bwb GUI use the bwb directory paths. These are converted to host paths for the volume mappings. There can be multiple possible mappings - the shortest mapping is used.

Support will be added for an additional automap mode that will automatically map the file system the user has provided to Bwb to the same paths in the container launched by the widget.

",2023-07-07 15:50:24+00:00
biodsl,BioDSL,maasha/BioDSL,BioDSL (pronounced Biodiesel) is a Domain Specific Language for creating bioinformatic analysis workflows.,,False,15,2022-07-27 10:28:43+00:00,2014-01-30 10:17:56+00:00,3,4,1,0,,,GNU General Public License v2.0,1114,v1.0.2,34,2015-11-18 22:09:40+00:00,,2015-11-18 22:09:40+00:00,"BioDSL (pronounced Biodiesel) is a Domain Specific Language for creating
bioinformatic analysis workflows. A workflow may consist of several pipelines
and each pipeline consists of a series of steps such as reading in data from a
file, processing the data in some way, and writing data to a new file.

BioDSL is build on the same principles as [Biopieces](www.biopieces.org), where
data records are passed through multiple commands each with a specific task. The
idea is that a command will process the data record if this contains the
relevant attributes that the command can process. E.g. if a data record contains
a sequence, then the command [reverse_seq](reverse_seq) will reverse that
sequence.

# Installation

The recommended way of installing BioDSL is via Ruby’s gem package manager:

`$ gem install BioDSL`

For those commands which are wrappers around third-party tools, such as Usearch,
Mothur and SPAdes, you will have to install these and make the executables
available in your `$PATH`.

# Getting started

BioDSL is implemented in Ruby making use of Ruby’s powerful metaprogramming
facilities. Thus, a workflow is basically a Ruby script containing one or more
pipelines.

Here is a test script with a single pipeline that reads all FASTA entries from
the file `input.fna`, selects all records with a sequence ending in `ATC`, and
writing those records as FASTA entries to the file `output.fna`:

```
#!/usr/bin/env ruby

require 'BioDSL'

BD.new.
read_fasta(input: ""input.fna"").
grab(select: ""ATC$"", keys: :SEQ).
write_fasta(output: ""output.fna"").
run
```

Save the test script to a file `test.biodsl` and execute on the command line:

```
$ ruby test.biodsl
```

# Combining multiple pipelines

This script demonstrates how multiple pipelines can be created and combined. In
the end two pipelines are run, one consisting of p1 + p2 and one consisting of
p1 + p3. The first pipeline run will produce a histogram plot of sequence length
from sequences containing the pattern `ATCG`, and the other pipeline run will
produce a plot with sequences length distribution of sequences not matching
`ATCG`.

```
#!/usr/bin/env ruby

require 'BioDSL'

p1 = BD.new.read_fasta(input: ""test.fna"")
p2 = BD.new.grab(keys: :SEQ, select: ""ATCG"").
     plot_histogram(key: :SEQ_LEN, terminal: :png, output: ""select.png"")
p3 = BD.new.grab(keys: :SEQ, reject: ""ATCG"").
     plot_histogram(key: :SEQ_LEN, terminal: :png, output: ""reject.png"")
p4 = p1 + p3

(p1 + p2).write_fasta(output: ""select.fna"").run
p4.write_fasta(output: ""reject.fna"").run
```

# Running pipelines in parallel

This script demonstrates how to run multiple pipelines in parallel using 20 CPU
cores. Here we filter pair-end FASTQ entries from a list of samples described in
the file `samples.txt` which contains three tab separated columns: sample name,
a forward read file path, and a reverse read file path.

```
#!/usr/bin/env ruby

require 'BioDSL'
require 'csv'

samples = CSV.read(""samples.txt"")

Parallel.each(samples, in_processes: 20) do |sample|
  BD.new.
  read_fastq(input: sample[1], input2: sample[2], encoding: :base_33).
  grab(keys: :SEQ, select: ""ATCG"").
  write_fastq(output: ""#{sample[0]}_filted.fastq.bz2"", bzip2: true).
  run
end
```

# Ruby one-liners

It is possible to execute BioDSL pipelines on the command line:

```
ruby -r BioDSL -e 'BD.new.read_fasta(input: ""test.fna"").plot_histogram(key: :SEQ_LEN).run'
```

And to save typing we may use the alias `bd` which is set like this on the
command line:

```
$ alias bd='ruby -r BioDSL'
```

It may be a good idea to save that alias in your `.bashrc` file.

Now it is possible to run a BioDSL pipeline on the command line like this:

```
$ bd -e 'BD.new.read_fasta(input: ""test.fna"").plot_histogram(key: :SEQ_LEN).run'
```

# Using the Interactive Ruby interpreter

Here we demonstrate the use of Ruby's `irb` shell:

```
$ irb -r BioDSL --noinspect
irb(main):001:0> p = BD.new
=> BD.new
irb(main):002:0> p.read_fasta(input: ""input.fna"")
=> BD.new.read_fasta(input: ""input.fna"")
irb(main):003:0> p.grab(select: ""ATC$"", keys: :SEQ)
=> BD.new.read_fasta(input: ""input.fna"").grab(select: ""ATC$"", keys: :SEQ)
irb(main):004:0> p.write_fasta(output: ""output.fna"")
=> BD.new.read_fasta(input: ""input.fna"").grab(select: ""ATC$"", keys: :SEQ).write_fasta(output: ""output.fna"")
irb(main):005:0> p.run
=> BD.new.read_fasta(input: ""input.fna"").grab(select: ""ATC$"", keys: :SEQ).write_fasta(output: ""output.fna"").run
irb(main):006:0>
```

Again, it may be a good idea to save an alias `alias biodsl=""irb -r BioDSL --noinspect""` to your `.bashrc` file. Thus, we can use the new `biodsl` alias to chain commands directly:

```
$ biodsl
irb(main):001:0> BD.new.read_fasta(input: ""input.fna"").grab(select: ""ATC$"", keys: :SEQ).write_fasta(output: ""output.fna"").run(progress: true)
=> BD.new.read_fasta(input: ""input.fna"").grab(select: ""ATC$"", keys: :SEQ).write_fasta(output: ""output.fna"").run(progress: true)
irb(main):002:0>
```

# History file

A history file is kept in `$USER/.BioDSL_history` and each time run is called a history entry is added to this file:

```
BD.new.read_fasta(input: ""test_big.fna"", first: 100).plot_histogram(key: :SEQ_LEN).run
BD.new.read_fasta(input: ""test_big.fna"", first: 100).plot_histogram(key: :SEQ_LEN).run
BD.new.read_fasta(input: ""test_big.fna"", first: 10).plot_histogram(key: :SEQ_LEN).run
BD.new.read_fasta(input: ""test_big.fna"").plot_histogram(key: :SEQ_LEN).run
BD.new.read_fasta(input: ""test_big.fna"", first: 1000).plot_histogram(key: :SEQ_LEN).run
```

Thus it is possible to redo the last pipeline by pasting the line in irb or a Ruby one-liner.

# Log and History

All BioDSL events are logged to `~/.BioDSL_log`.

BioDSL history is saved to `~/.BioDSL_history`.

# Features

## Progress

Show nifty progress table with commands, records read and emittet and time.

`BD.new.read_fasta(input: ""input.fna"").dump.run(progress: true)`

## Verbose

Output verbose messages from commands and the run status.

```
BD.new.read_fasta(input: ""input.fna"").dump.run(verbose: true)
```

## Debug

Output debug messages from commands using these.

```
BD.new.read_fasta(input: ""input.fna"").dump.run(debug: true)
```

## E-mail notification

Send an email when run is complete.

```
BD.new.read_fasta(input: ""input.fna"").dump.run(email: bill@hotmail.com, subject: ""Script done!"")
```

## Reports

Create an HTML report of the run stats for a pipeline:

```
BD.new.read_fasta(input: ""input.fna"").dump.run(report: ""status.html"")
```

## Output directory

All output files from commands are put in a specified directory:

```
BD.new.read_fasta(input: ""input.fna"").dump.run(output_dir: ""Results"")
```

## Configuration File

It is possible to pre-set options in a configuration file located in your `$HOME`
directory called `.BioDSLrc`. Thus if an option is not already set, its value
will fall back to the one set in the configuration file. The configuration file
contains three whitespace separated columns:

  * Command name
  * Option
  * Option value

Lines starting with `#` are considered comments and are ignored.

An example:

```
maasha@mel:~$ cat ~/.BioDSLrc
uchime_ref   database   /home/maasha/Install/QIIME1.8/data/rdp_gold.fa
uchime_ref   cpus       20
```

On compute clusters it is necessary to specify the max processor count, which
is otherwise determined as the number of cores on the current node. To override
this add the following line:

```
pipeline   processor_count   1000
```

It is also possible to change the temporary directory from the systems default
by adding the following line:

```
pipeline   tmp_dir   /home/projects/ku_microbio/scratch/tmp
```

# Available BioDSL commands

  * [add_key]                          (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AddKey)
  * [align_seq_mothur]                 (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AlignSeqMothur)
  * [analyze_residue_distribution]     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AnalyzeResidueDistribution)
  * [assemble_pairs]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AssemblePairs)
  * [assemble_seq_idba]                (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AssembleSeqIdba)
  * [assemble_seq_ray]                 (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AssembleSeqRay)
  * [assemble_seq_spades]              (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/AssembleSeqSpades)
  * [classify_seq]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ClassifySeq)
  * [classify_seq_mothur]              (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ClassifySeqMothur)
  * [clip_primer]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ClipPrimer)
  * [cluster_otus]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ClusterOtus)
  * [collapse_otus]                    (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/CollapseOtus)
  * [collect_otus]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/CollectOtus)
  * [complement_seq]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ComplementSeq)
  * [count]                            (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Count)
  * [degap_seq]                        (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/DegapSeq)
  * [dereplicate_seq]                  (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/DereplicateSeq)
  * [dump]                             (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Dump)
  * [filter_rrna]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/FilterRrna)
  * [genecall]                         (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Genecall)
  * [grab]                             (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Grab)
  * [index_taxonomy]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/IndexTaxonomy)
  * [mean_scores]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/MeanScores)
  * [merge_pair_seq]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/MergePairSeq)
  * [merge_table]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/MergeTable)
  * [merge_values]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/MergeValues)
  * [plot_heatmap]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/PlotHeatmap)
  * [plot_histogram]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/PlotHistogram)
  * [plot_matches]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/PlotMatches)
  * [plot_residue_distribution]        (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/PlotResidueDistribution)
  * [plot_scores]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/PlotScores)
  * [random]                           (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Random)
  * [read_fasta]                       (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ReadFasta)
  * [read_fastq]                       (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ReadFastq)
  * [read_table]                       (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ReadTable)
  * [reverse_seq]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/ReverseSeq)
  * [slice_align]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/SliceAlign)
  * [slice_seq]                        (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/SliceSeq)
  * [sort]                             (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/Sort)
  * [split_pair_seq]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/SplitPairSeq)
  * [split_values]                     (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/SplitValues)
  * [trim_primer]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/TrimPrimer)
  * [trim_seq]                         (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/TrimSeq)
  * [uchime_ref]                       (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/UchimeRef)
  * [unique_values]                    (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/UniqueValues)
  * [usearch_global]                   (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/UsearchGlobal)
  * [write_fasta]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/WriteFasta)
  * [write_fastq]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/WriteFastq)
  * [write_table]                      (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/WriteTable)
  * [write_tree]                       (http://www.rubydoc.info/gems/BioDSL/1.0.2/BioDSL/WriteTree)

# Running the test suite

BioDSL have an extended set of unit tests that can be run after installing
development dependencies. First you need to install the bundler gem:

```
$ gem install bundler
```

Next you need to change to the source directory of BioDSL and run bundler to
download depending gems:

```
$ bundle install
```

And then you run the test suite by running `rake`:

```
$ rake
```

And the unit tests should all run, except those omitted because a third-party
executable was missing.

# Contributing

1. Fork it
1. Create your feature branch (git checkout -b my-new-feature)
1. Commit your changes (git commit -am 'Add some feature')
1. Push to the branch (git push origin my-new-feature)
1. Create new Pull Request
",2023-07-07 15:50:28+00:00
biomaj,biomaj,genouest/biomaj,BioMAJ,http://genouest.github.io/biomaj/,False,12,2022-05-31 06:45:02+00:00,2014-10-22 07:38:05+00:00,10,6,9,0,,,GNU Affero General Public License v3.0,828,3.1.23,45,2022-08-29 13:41:29+00:00,,2022-08-29 13:41:29+00:00,"BioMAJ3
=====

This project is a complete rewrite of BioMAJ and the documentation is available here : http://biomaj.genouest.org.

BioMAJ (BIOlogie Mise A Jour) is a workflow engine dedicated to data
synchronization and processing. The Software automates the update cycle and the
supervision of the locally mirrored databank repository.

Common usages are to download remote databanks (Genbank for example) and apply
some transformations (blast indexing, emboss indexing, etc.). Any script can be
applied on downloaded data. When all treatments are successfully applied, bank
is put in ""production"" on a dedicated release directory.
With cron tasks, update tasks can be executed at regular interval, data are
downloaded again only if a change is detected.

More documentation is available in wiki page.

BioMAJ is python 2 and 3 compatible until release 3.1.17.
After 3.1.17, only python 3 is supported.

Getting started
===============

Edit global.properties file to match your settings. Minimal conf are database connection and directories.

    biomaj-cli.py -h

    biomaj-cli.py --config global.properties --status

    biomaj-cli.py --config global.properties  --bank alu --update

Migration
=========

To migrate from previous BioMAJ 1.x, a script is available at:
https://github.com/genouest/biomaj-migrate. Script will import old database to
the new database, and update configuration files to the modified format. Data directory is the same.

Migration for 3.0 to 3.1:

Biomaj 3.1 provides an optional micro service architecture, allowing to separate and distributute/scale biomaj components on one or many hosts. This implementation is optional but recommended for server installations. Monolithic installation can be kept for local computer installation.
To upgrade an existing 3.0 installation, as biomaj code has been split into multiple components, it is necessary to install/update biomaj python package but also biomaj-cli and biomaj-daemon packages. Then database must be upgraded manually (see Upgrading in documentation).

To execute database migration:

    python biomaj_migrate_database.py

Application Features
====================

* Synchronisation:
  * Multiple remote protocols (ftp, ftps, http, local copy, etc.)
  * Data transfers integrity check
  * Release versioning using a incremental approach
  * Multi threading
  * Data extraction (gzip, tar, bzip)
  * Data tree directory normalisation
  * Plugins support for custom downloads

* Pre &Post processing :
  * Advanced workflow description (D.A.G)
  * Post-process indexation for various bioinformatics software (blast, srs, fastacmd, readseq, etc.)
  * Easy integration of personal scripts for bank post-processing automation

* Supervision:
  * Optional Administration web interface (biomaj-watcher)
  * CLI management
  * Mail alerts for the update cycle supervision
  * Prometheus and Influxdb optional integration
  * Optional consul supervision of processes


* Scalability:
  * Monolithic (local install) or microservice architecture (remote access to a BioMAJ server)
  * Microservice installation allows per process scalability and supervision (number of process in charge of download, execution, etc.)

* Remote access:
  * Optional FTP server providing authenticated or anonymous data access
  * HTTP access to bank files (/db endpoint, microservice setup only)

Dependencies
============

Packages:

* Debian: libcurl-dev, gcc
* CentOs: libcurl-devel, openldap-devel, gcc

 Linux tools: tar, unzip, gunzip, bunzip

Database:
 * mongodb (local or remote)

Indexing (optional):
 * elasticsearch (global property, use_elastic=1)

ElasticSearch indexing adds advanced search features to biomaj to find bank having files with specific format or type.
Configuration of ElasticSearch is not in the scope of BioMAJ documentation.
For a basic installation, one instance of ElasticSearch is enough (low volume of data), in such a case, the ElasticSearch configuration file should be modified accordingly:

    node.name: ""biomaj"" (or any other name)
    index.number_of_shards: 1
    index.number_of_replicas: 0

Installation
============

From source:

After dependencies installation, go in BioMAJ source directory:

    python setup.py install

From packages:

    pip install biomaj biomaj-cli biomaj-daemon

You should consider using a Python virtual environment (virtualenv) to install BioMAJ.

In tools/examples, copy the global.properties and update it to match your local
installation.

The tools/process contains example process files (python and shell).

Docker
======

You can use BioMAJ with Docker (osallou/biomaj-docker)

    docker pull osallou/biomaj-docker
    docker pull mongo
    docker run --name biomaj-mongodb -d mongo
    # Wait ~10 seconds for mongo to initialize
    # Create a local directory where databases will be permanently stored
    # *local_path*
    docker run --rm -v local_path:/var/lib/biomaj --link biomaj-mongodb:biomaj-mongodb osallou/biomaj-docker --help

Copy your bank properties in directory *local_path*/conf and post-processes (if any) in *local_path*/process

You can override global.properties in /etc/biomaj/global.properties (-v xx/global.properties:/etc/biomaj/global.properties)

No default bank property file or process are available in the container.

Examples are available at https://github.com/genouest/biomaj-data


Import bank templates
=====================

Once biomaj is installed, it is possible to import some bank examples with the biomaj client


    # List available templates
    biomaj-cli ... --data-list
    # Import a bank template
    biomaj-cli ... --data-import --bank alu
    # then edit bank template in config directory if needed and launch bank update
    biomaj-cli ... --update --bank alu

Plugins
=======

BioMAJ support python plugins to manage custom downloads where supported protocols
are not enough (http page with unformatted listing, access to protected pages, etc.).

Example of plugins and how to configure them are available on [biomaj-plugins](https://github.com/genouest/biomaj-plugins) repository.

Plugins can define a specific way to:

* retreive release
* list remote files to download
* download remote files

Plugin can define one or many of those features.

Basically, one defined in bank property file:

    # Location of plugins
    plugins_dir=/opt/biomaj-plugins
    # Use plugin to fetch release
    release.plugin=github
    # List of arguments of plugin function with key=value format, comma separated
    release.plugin_args=repo=osallou/goterra-cli

Plugins are used when related workflow step is used:

* release.plugin <= returns remote release
* remote.plugin <= returns list of files to download
* download.plugin <= download files from list of files

API documentation
=================

https://readthedocs.org/projects/biomaj/

Status
======

[![Build Status](https://travis-ci.org/genouest/biomaj.svg?branch=master)](https://travis-ci.org/genouest/biomaj)

[![Documentation Status](https://readthedocs.org/projects/biomaj/badge/?version=latest)](https://readthedocs.org/projects/biomaj/?badge=latest)

[![Code Health](https://landscape.io/github/genouest/biomaj/master/landscape.svg?style=flat)](https://landscape.io/github/genouest/biomaj/master)

Testing
=======

Execute unit tests

    python -m pytest -v tests/biomaj_tests.py

Execute unit tests but disable ones needing network access

    NETWORK=0 python -m pytest -v tests/biomaj_tests.py


Monitoring
==========

InfluxDB (optional) can be used to monitor biomaj. Following series are available:

* biomaj.banks.quantity (number of banks)
* biomaj.production.size.total (size of all production directories)
* biomaj.workflow.duration (workflow duration)
* biomaj.production.size.latest (size of latest update)
* biomaj.bank.update.downloaded_files (number of downloaded files)
* biomaj.bank.update.new (track updates)

*WARNING* Influxdb database must be created, biomaj does not create the database (see https://docs.influxdata.com/influxdb/v1.6/query_language/database_management/#create-database)

License
=======

A-GPL v3+

Remarks
=======

To delete elasticsearch index:

 curl -XDELETE '<http://localhost:9200/biomaj_test/'>

Credits
=======

Special thanks for tuco at Pasteur Institute for the intensive testing and new ideas.
Thanks to the old BioMAJ team for the work they have done.

BioMAJ is developped at IRISA research institute.
",2023-07-07 15:50:32+00:00
biomake,biomake,evoldoers/biomake,GNU-Make-like utility for managing builds and complex workflows,,False,88,2023-06-16 16:31:41+00:00,2011-09-10 00:48:26+00:00,9,11,4,6,v0.1.5,2018-06-29 17:00:57+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",423,v0.1.5,8,2018-06-29 17:00:57+00:00,2023-05-29 10:26:32+00:00,2022-01-07 21:42:32+00:00,"[![Build Status](https://travis-ci.org/evoldoers/biomake.svg?branch=master)](https://travis-ci.org/evoldoers/biomake)
[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

Biomake
=======

This is a [make](https://www.gnu.org/software/make/)-like utility for managing builds (or analysis workflows) involving multiple
dependent files.
It supports most of the functionality of GNU Make, along with neat extensions like
cluster-based job processing, multiple wildcards per target, MD5 checksums instead of timestamps,
and declarative logic programming in Prolog.

Indeed: [Prolog](https://en.wikipedia.org/wiki/Prolog).
No knowledge of the dark logical arts is necessary to use Biomake; the software can be
run directly off a GNU Makefile. However, if you know (or are prepared to
learn) a little Prolog, you can do a lot more.
Makefiles are logic programs: their power comes from combining a _declarative_ specification of dependencies
with _procedural_ shell scripts to build targets.
Prolog is a simple but expressive language for logic programming
that allows Makefile rules to be extended in sophisticated and flexible ways.

Getting Started
---------------

1. Install SWI-Prolog from http://www.swi-prolog.org

2. Get the latest biomake source from github. No installation steps are
required. Just add it to your path (changing the directory if necessary):

    `export PATH=$PATH:$HOME/biomake/bin`

3. Get (minimal) help from the command line:

    `biomake -h`

4. Create a 'Makefile' or a 'Makeprog' (see below)

Alternate installation instructions
-----------------------------------

If you want to install biomake system-wide, instead of adding it to your path, type `make install` (or `bin/biomake install`) in the top level directory of the repository.
This will copy the repository into `/usr/local/share` and create a symlink to `/usr/local/bin`.
(If you just want to create the symlink and leave the repository where it is, type `make symlink` instead.)

You can also try `make test` (or, equivalently, `biomake test`) to run the test suite.

The program can also be installed via the SWI-Prolog pack system.
Just start SWI and type:

    ?- pack_install('biomake').

Command-line
------------

    biomake [OPTIONS] [TARGETS]

Options
-------

```
-h,--help 
    Show help
-v,--version 
    Show version
-n,--dry-run,--recon,--just-print 
    Print the commands that would be executed, but do not execute them
-B,--always-make 
    Always build fresh target even if dependency is up to date
-f,--file,--makefile GNUMAKEFILE
    Use a GNU Makefile as the build specification [default: Makefile]
-p,--prog,--makeprog MAKEPROG
    Use MAKEPROG as the (Prolog) build specification [default: Makeprog]
-m,--eval,--makefile-syntax STRING
    Evaluate STRING as GNU Makefile syntax
-P,--eval-prolog,--makeprog-syntax STRING
    Evaluate STRING as Prolog Makeprog syntax
-I,--include-dir DIR
    Specify search directory for included Makefiles
--target TARGET
    Force biomake to recognize a target even if it looks like an option
-T,--translate,--save-prolog FILE
    Translate GNU Makefile to Prolog Makeprog syntax
-W,--what-if,--new-file,--assume-new TARGET
    Pretend that TARGET has been modified
-o,--old-file,--assume-old TARGET
    Do not remake TARGET, or remake anything on account of it
-k,--keep-going 
    Keep going after error
-S,--no-keep-going,--stop 
    Stop after error
-t,--touch 
    Touch files (and update MD5 hashes, if appropriate) instead of running recipes
-N,--no-dependencies 
    Do not test or rebuild dependencies
-D,--define Var Val
    Assign Makefile variables from command line
Var=Val 
    Alternative syntax for '-D Var Val'
-s,--quiet,--silent 
    Silent operation; do not print recipes as they are executed
--one-shell 
    Run recipes in single shell (loosely equivalent to GNU Make's .ONESHELL)
-y,--sync,--sync-dir URI
    Synchronize current working directory to a remote URI. If no --sync-exec is specified, S3-form URIs (s3://mybucket/my/path) are handled using the AWS CLI tool; other URIs will be passed to rsync.
-x,--sync-exec COMMAND
    Specify executable for --sync.
-H,--md5-hash 
    Use MD5 hashes instead of timestamps
-C,--no-md5-cache 
    Recompute MD5 checksums whenever biomake is restarted
-M,--no-md5-timestamp 
    Do not recompute MD5 checksums when timestamps appear stale
-Q,--queue-engine ENGINE
    Queue recipes using ENGINE (supported: poolq,sge,pbs,slurm,test)
-j,--jobs JOBS
    Number of job threads (poolq engine)
--qsub-exec PATH
    Path to qsub (sge,pbs) or sbatch (slurm)
--qdel-exec PATH
    Path to qdel (sge,pbs) or scancel (slurm)
--queue-args 'ARGS'
    Queue-specifying arguments for qsub/qdel (sge,pbs) or sbatch/scancel (slurm)
--qsub-args,--sbatch-args 'ARGS'
    Additional arguments for qsub (sge,pbs) or sbatch (slurm)
--qsub-use-biomake,--sbatch-use-biomake 
    Force qsub/sbatch to always call biomake recursively
--qsub-biomake-args,--sbatch-biomake-args 'ARGS'
    Arguments passed recursively to biomake by qsub/sbatch (default: '-N')
--qsub-header,--sbatch-header 'HEADER'
    Header for qsub (sge,pbs) or sbatch (slurm)
--qsub-header-file,--sbatch-header-file 'FILENAME'
    Header file for qsub (sge,pbs) or sbatch (slurm)
--qdel-args,--scancel-args 'ARGS'
    Additional arguments for qdel (sge,pbs) or scancel (slurm)
--flush,--qsub-flush <target or directory>
    Erase all jobs for given target/dir
-d 
    [developers] Print debugging messages. Equivalent to '--debug verbose'
--debug MSG
    [developers] Richer debugging messages. MSG can be verbose, bindrule, build, pattern, makefile, makeprog, md5...
--trace PREDICATE
    [developers] Print debugging trace for given predicate
--no-backtrace 
    [developers] Do not print a backtrace on error
```

Embedding Prolog in Makefiles
-----------------------------

Brief overview:

- Prolog can be embedded within `prolog` and `endprolog` directives
- `$(bagof Template,Goal)` expands to the space-separated `List` from the Prolog `bagof(Template,Goal,List)`
- Following the target list with `{target_goal}` causes the rule to match only if `target_goal` is satisfied. The target goal will be tested _before_ any dependencies are built. The special variable `TARGET`, if used, will be bound to the target filename (i.e. `$@`)
- Following the dependency list with `{deps_goal}` causes the recipe to be executed only if `deps_goal` is satisfied. The deps goal will be tested _after_ any dependencies are built (so it can examine the dependency files). The special variables `TARGET` and `DEPS`, if used, will be bound to the target and dependency-list (i.e. `$@` and `$^`, loosely speaking; except the latter is a true Prolog list, not encoded as a string with whitespace separators as in GNU Make)

Examples
--------

This assumes some knowledge of GNU Make and [Makefiles](https://www.gnu.org/software/make/manual/html_node/index.html).

Unlike makefiles, biomake allows multiple variables in pattern
matching. Let's say we have a program called `align` that compares two
files producing some output (e.g. biological sequence alignment, or
ontology alignment). Assume our file convention is to suffix "".fa"" on
the inputs.  We can write a `Makefile` with the following:

    align-$X-$Y: $X.fa $Y.fa
        align $X.fa $Y.fa > $@

Now if we have files `x.fa` and `y.fa` we can type:

    biomake align-x-y

Prolog extensions allow us to do even fancier things with logic.
Specifically, we can embed arbitrary Prolog, including both database facts and
rules. We can use these rules to control flow in a way that is more
powerful than makefiles.

Let's say we only want to run a certain program when the inputs match a certain table in our database.
We can embed Prolog in our Makefile as follows:

    prolog
    sp(mouse).
    sp(human).
    sp(zebrafish).
    endprolog

    align-$X-$Y: $X.fa $Y.fa {sp(X),sp(Y)}
        align $X.fa $Y.fa > $@

The lines beginning `sp` between `prolog` and `endprolog` define the set of species that we want the rule to apply to.
The rule itself consists of 4 parts:

 * the target (`align-$X-$Y`)
 * the dependencies (`$X.fa` and `$Y.fa`)
 * a Prolog goal, enclosed in braces (`{sp(X),sp(Y)}`), that is used as an additional logic test of whether the rule can be applied
 * the command (`align ...`)

In this case, the Prolog goal succeeds with 9 solutions, with 3
different values for `X` and `Y`. If we type...

    biomake align-platypus-coelacanth

...it will not succeed, even if the .fa files are on the filesystem. This
is because the goal `{sp(X),sp(Y)}` cannot be satisfied for these two values of `X` and `Y`.

To get a list of all matching targets,
we can use the special BioMake function `$(bagof...)`
which wraps the Prolog predicate [bagof/3](http://www.swi-prolog.org/pldoc/man?predicate=bagof/3).
The following example also uses the Prolog predicates
[format/2](http://www.swi-prolog.org/pldoc/man?predicate=format/2)
and
[format/3](http://www.swi-prolog.org/pldoc/man?predicate=format/3),
for formatted output:

~~~~
prolog

sp(mouse).
sp(human).
sp(zebrafish).

ordered_pair(X,Y) :- sp(X),sp(Y),X@<Y.

make_filename(F) :-
  ordered_pair(X,Y),
  format(atom(F),""align-~w-~w"",[X,Y]).

endprolog

all: $(bagof F,make_filename(F))

align-$X-$Y: $X.fa $Y.fa { ordered_pair(X,Y),
                           format(""Matched ~w <-- ~n"",[TARGET,DEPS]) },
    align $X.fa $Y.fa > $@
~~~~

Now if we type...

    biomake all

...then all non-identical ordered pairs will be compared
(since we have required them to be _ordered_ pairs, we get e.g. ""mouse-zebrafish"" but not ""zebrafish-mouse"";
the motivation here is that the `align` program is symmetric, and so only needs to be run once per pair).

In these examples, the goals between braces are tested _after_ the dependencies.
This means that any Prolog code in these braces can safely examine the dependency files
(for example, you could constrain a rule to apply only if a dependency file was below a certain size,
or in a certain file format).
You can also place a Prolog goal (in braces) between the target list and the colon;
it will then be tested after the target name has been matched,
but _before_ trying to build any dependencies.
In such a goal, you can use the `TARGET` variable but not the `DEPS` variable.

Programming directly in Prolog
------------------------------

If you are a Prolog wizard who finds embedding Prolog in Makefiles too cumbersome, you can use a native Prolog-like syntax.
Biomake looks for a Prolog file called `Makeprog` (or `Makespec.pro`) in your
current directory. (If it's not there, it will try looking for a
`Makefile` in GNU Make format. The following examples describe the
Prolog syntax.)

Assume you have two file formats, "".foo"" and "".bar"", and a `foo2bar`
converter.

Add the following rule to your `Makeprog`:

    '%.bar' <-- '%.foo',
        'foo2bar $< > $@'.

Unlike makefiles, whitespace is irrelevant. However, you
do need the quotes, and remember the closing ""."",
as this is Prolog syntax.

If you prefer to stick with GNU Make syntax,
the above `Makeprog` is equivalent to the following `Makefile`:

    %.bar: %.foo
    	   foo2bar $< > $@

To convert a pre-existing file ""x.foo"" to ""x.bar"" type:

    biomake x.bar

Let's say we can go from a .bar to a .baz using a `bar2baz`
converter. We can add an additional rule:

    '%.baz' <-- '%.bar',
        'bar2baz $< > $@'.

Now if we type...

    touch x.foo
    biomake x.baz

...we get something like the following output:

~~~~
% Checking dependencies: x.baz <-- [x.bar]
%  Checking dependencies: x.bar <-- [x.foo]
%   Nothing to be done for x.foo
%  Target x.bar not materialized - build required
 foo2bar x.foo > x.bar
%  x.bar built
% Target x.baz not materialized - build required
bar2baz x.bar > x.baz
% x.baz built
~~~~

The syntax in the makeprog above is designed to be similar to the automatic variable syntax
already used in makefiles. You can bypass this and use Prolog
variables. The following form is functionally equivalent:

    '$(Base).bar' <-- '$(Base).foo',
        'foo2bar $(Base).foo > $(Base).bar'.

The equivalent `Makefile` would be this...

    $(Base).bar: $(Base).foo
    	foo2bar $(Base).foo > $(Base).bar

...although strictly speaking, this is only equivalent if you are using Biomake;
GNU Make's treatment of this Makefile isn't quite equivalent, since unbound variables
don't work the same way in GNU Make as they do in Biomake
(Biomake will try to use them as wildcards for pattern-matching,
whereas GNU Make will just replace them with the empty string - which is also the default behavior
for Biomake if they occur outside of a pattern-matching context).

Following the GNU Make convention, variable names must be enclosed in
parentheses unless they are single letters.

Automatic translation to Prolog
-------------------------------

You can parse a GNU Makefile (including Biomake-specific extensions, if any)
and save the corresponding Prolog syntax using the `-T` option
(long-form `--translate`).

Here is the translation of the Makefile from the previous section (lightly formatted for clarity):

~~~
sp(mouse).
sp(human).
sp(zebrafish).

ordered_pair(X,Y):-
 sp(X),
 sp(Y),
 X@<Y.

make_filename(F):-
 ordered_pair(X,Y),
 format(atom(F),""align-~w-~w"",[X,Y]).

""all"" <-- ""$(bagof F,make_filename(F))"".

""align-$X-$Y"" <--
 [""$X.fa"",""$Y.fa""],
 {ordered_pair(X,Y),
  format(""Matched ~w <-- ~n"",[TARGET,DEPS])},
 ""align $X.fa $Y.fa > $@"".
~~~

Note how the list of dependencies in the second rule, which contains more than one dependency (`$X.fa` and `$Y.fa`), is enclosed in square brackets, i.e. a Prolog list (`[""$X.fa"",""$Y.fa""]`).
The same syntax applies to rules which have lists of multiple targets, or multiple executables.

The rule for target `all` in this translation involves a call to the Biomake function `$(bagof ...)`,
but (as noted) this function is just a wrapper for the Prolog `bagof/3` predicate.
The automatic translation is not smart enough to remove this layer of wrapping,
but we can do so manually, yielding a clearer program:

~~~
sp(mouse).
sp(human).
sp(zebrafish).

ordered_pair(X,Y):-
 sp(X),
 sp(Y),
 X@<Y.

make_filename(F):-
 ordered_pair(X,Y),
 format(atom(F),""align-~w-~w"",[X,Y]).

""all"", {bagof(F,make_filename(F),DepList)} <-- DepList, {true}.

""align-$X-$Y"" <--
 [""$X.fa"",""$Y.fa""],
 {ordered_pair(X,Y),
  format(""Matched ~w <-- ~n"",[TARGET,DEPS])},
 ""align $X.fa $Y.fa > $@"".
~~~

Make-like features
------------------

Biomake supports most of the functionality of GNU Make, including
- different [flavors of variable](https://www.gnu.org/software/make/manual/make.html#Flavors) (recursive, expanded, etc.)
- various ways of [setting variables](https://www.gnu.org/software/make/manual/html_node/Setting.html)
- [appending to variables](https://www.gnu.org/software/make/manual/html_node/Appending.html)
- [multi-line variables](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html)
- [automatic variables](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html) such as `$<`, `$@`, `$^`, `$(@F)`, etc.
- [substitution references](https://www.gnu.org/software/make/manual/html_node/Substitution-Refs.html)
- [computed variable names](https://www.gnu.org/software/make/manual/html_node/Computed-Names.html)
- [all the text functions](https://www.gnu.org/software/make/manual/html_node/Text-Functions.html)
- [all the filename functions](https://www.gnu.org/software/make/manual/html_node/File-Name-Functions.html)
- [the shell function](https://www.gnu.org/software/make/manual/html_node/Shell-Function.html)
- [user-defined functions](https://www.gnu.org/software/make/manual/html_node/Call-Function.html)
- [errors and warnings](https://www.gnu.org/software/make/manual/html_node/Make-Control-Functions.html)
- many of the same [command-line options](https://www.gnu.org/software/make/manual/html_node/Options-Summary.html)
- [conditional syntax](https://www.gnu.org/software/make/manual/html_node/Conditionals.html) and [conditional functions](https://www.gnu.org/software/make/manual/html_node/Conditional-Functions.html)
- the [include](https://www.gnu.org/software/make/manual/html_node/Include.html) directive
- [wildcards in dependency lists](https://www.gnu.org/software/make/manual/html_node/Wildcards.html)
- [phony targets](https://www.gnu.org/software/make/manual/html_node/Phony-Targets.html)
- various other quirks of GNU Make syntax e.g. single-line recipes, forced rebuilds

Currently unsupported features of GNU Make
------------------------------------------

The following features of GNU Make are not (yet) implemented:

- [Order-only prerequisites](https://www.gnu.org/software/make/manual/html_node/Prerequisite-Types.html)
- [Directory search](https://www.gnu.org/software/make/manual/html_node/Directory-Search.html)
- Many of the [special built-in targets](https://www.gnu.org/software/make/manual/html_node/Special-Targets.html), with some exceptions:
    - `.PHONY` is implemented
    - `.SILENT` is implemented
    - `.NOTPARALLEL` is implemented
    - `.ONESHELL` is implemented
    - `.IGNORE` is implemented
    - `.DELETE_ON_ERROR` is implemented
    - `.SECONDARY` is implicit and `.INTERMEDIATE` is unsupported: Biomake never removes intermediate files (unless `.DELETE_ON_ERROR` is specified)
    - `.PRECIOUS` is implicit for all targets
    - `.SECONDEXPANSION` is implicit
    - `.SUFFIXES` is unsupported (or implicit with no dependencies), since suffix rules are unsupported
    - other special targets not mentioned in the above list are not supported (they'll just be parsed as regular targets, i.e. ignored)
- [Multiple rules per target](https://www.gnu.org/software/make/manual/html_node/Multiple-Rules.html)
- [Static pattern rules](https://www.gnu.org/software/make/manual/html_node/Static-Pattern.html)
- [Double-colon rules](https://www.gnu.org/software/make/manual/html_node/Double_002dColon.html)
- [Suffix rules](https://www.gnu.org/software/make/manual/html_node/Suffix-Rules.html)
- Modifiers in recipe lines are only partially supported:
    - The [+ sign to force execution during dry runs](https://www.gnu.org/software/make/manual/html_node/Instead-of-Execution.html) is _not_ supported
    - The [- sign to suppress errors in recipes](https://www.gnu.org/software/make/manual/html_node/Errors.html) _is_ supported
    - The [@ sign to execute recipe lines silently](https://www.gnu.org/software/make/manual/html_node/Echoing.html) _is_ supported
- The [export](https://www.gnu.org/software/make/manual/html_node/Variables_002fRecursion.html) keyword is supported, but ""unexport"" and ""override"" are _not_ supported
- [Target-specific variable assignments](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html) (see issue [#79](https://github.com/evoldoers/biomake/issues/79))

Please [submit a GitHub issue](https://github.com/evoldoers/biomake/issues) if any of these are important to you.

Other differences from GNU Make
-------------------------------

There are slight differences in the way variables are expanded, which arise from the fact that Biomake
treats variable expansion as a post-processing step (performed at the last possible moment) rather than a pre-processing step (which is how GNU Make does it - at least partly).

Specifically, Biomake parses the Makefile, reading all variable and recipe declarations into memory, and only when the build begins are variables expanded.
The only exception to this is when variables are used in [conditional syntax](https://www.gnu.org/software/make/manual/html_node/Conditionals.html),
to control which parts of the Makefile are actually read:
these variables are expanded at parse-time.

In contrast, GNU Make expands variables in dependency lists at parse time (along with conditional syntax),
but expands variables in recipe bodies later.

This can cause differences between GNU and Biomake in situations where variables change value throughout the Makefile.
These situations are usually counter-intuitive anyway, as the following example illustrates.
This Makefile, which might naively be expected to print `hello everybody`,
in fact prints `hello world` when run with `make test`, but `goodbye world` when run with `biomake test`:

~~~~
A = hello
B = everybody

test: $A
	@echo $B

A = goodbye
B = world

hello goodbye:
	@echo $@
~~~~

This example gets even more counterintuitive if we wrap the `test` recipe with conditional syntax.
It still gives the same results, though: `hello world` when run with `make test`, and `goodbye world` when run with `biomake test`.

~~~~
A = hello
B = everybody

ifeq ($B,everybody)
test: $A
	@echo $B
else
test:
	@echo Curioser and curioser
endif

A = goodbye
B = world

hello goodbye:
	@echo $@
~~~~

Another consequence is that, when using Biomake, variable expansions must be aligned with the overall syntactic structure; they cannot span multiple syntactic elements.
As a concrete example, GNU Make allows this sort of thing:

~~~~
RULE = target: dep1 dep2
$(RULE) dep3
~~~~

which (in GNU Make, but not biomake) expands to

~~~~
target: dep1 dep2 dep3
~~~~

That is, the expansion of the `RULE` variable spans both the target list and the start of the dependency list.
To emulate this behavior faithfully, Biomake would have to do the variable expansion in a separate preprocessing pass - which would mean we couldn't translate variables directly into Prolog.
We think it's worth sacrificing this edge case in order to maintain the semantic parallel between Makefile variables and Prolog variables, which allows for some powerful constructs.

The implementation of [conditional syntax](https://www.gnu.org/software/make/manual/html_node/Conditionals.html)
(`ifeq`, `ifdef` and the like) similarly requires that syntax to be aligned with the overall structure:
you can only place a conditional at a point where a variable assignment, recipe, or `include` directive could go
(i.e. at the top level of the `Makefile` grammar).
Conditional syntax _is_ implemented as a preprocessing step.

Unlike GNU Make, Biomake does not offer domain-specific language extensions in [Scheme](https://www.gnu.org/software/guile/)
(even though this is one of the cooler aspects of GNU Make), but you can program it in Prolog instead - it's quite hackable.

Detailed build logic
--------------------

The build logic for biomake should _usually_ yield the same results as GNU Make, though there may be subtle differences.
The GNU Make [algorithm](https://www.gnu.org/software/make/manual/html_node/Implicit-Rule-Search.html) differs in the details.

Before attempting to build a target `T` using a rule `R`, Biomake performs the following steps:
- It tries to match the target name `T` to one of the target names in `R`
- It tests whether the Prolog _target goal_ (if there is one) is satisfied
- It checks whether there is a _theoretical path_ to all the dependencies. A theoretical path to a dependency `D` exists if either of the following is true:
    - There is a rule that could be used to build `D`, the target goal for that rule is satisfied, and there is a theoretical path to all the dependencies of that rule;
    - File `D` already exists, and the only applicable rules to rebuild `D`, if any exist at all, are wildcard (pattern) rules; that is, there are no rules that _explicitly and uniquely_ rebuild `D`.
- It attempts to build all the dependencies
- It tests whether the Prolog _deps goal_ (if there is one) is satisfied
- It tests whether the target is stale. Details depend on the various options:
    - Command-line options for marking targets as stale or new (`-W`, `-B`, `-o`) can override any of the following behavior
    - If using the queueing engine, or if doing a dry-run (`-n`), targets are flagged as stale if any of their dependency tree has been rebuilt (or submitted to the queue for a rebuild);
    - If using MD5 signatures (and _not_ the queueing engine), a target is stale if its MD5 checksum appears to be out of date;
    - If using MD5 _and_ queues, the MD5 signature will not be checked until the queueing engine executes the job (which is guaranteed to happen after any dependencies are rebuilt). Otherwise the dependencies might change after the MD5 checksum was tested. This is accomplished by wrapping the recipe script with a recursive call to biomake; so biomake has to be available on the worker machines, and not just the cluster head. (The same is true, incidentally, when using a cluster to execute any rule that has a Prolog deps goal: the submitted job is wrapped by biomake, in order that the goal can be tested after the dependencies are built.)
    - Otherwise (no queues and no MD5), Biomake looks at the file timestamps and/or the dependency tree.

If any of these tests fail, Biomake will backtrack and attempt to build the target using a different rule, or a different pattern-match to the same rule.
If all the tests pass, Biomake will commit to using the rule, and will attempt to execute the recipe using the shell (or the queueing engine).

Note that the target goal is tested multiple times (to plan theoretical build paths) and so should probably not have side effects.
The deps goal is tested later, and only once for every time the rule is bound, so it is a bit safer for the deps goal to have side effects.

Failure during execution of the recipe (or execution of any recipes in the dependency tree) will never cause Biomake to backtrack; it will either halt, or (if the `-k` command-line option was specified) soldier on obliviously.

Arithmetic functions
--------------------

Biomake provides a few extra functions for arithmetic on lists:

- `$(iota N)` returns a space-separated list of numbers from `1` to `N`
- `$(iota S,E)` returns a space-separated list of numbers from `S` to `E`
- `$(add X,L)` adds `X` to every element of the space-separated list `L`
- `$(multiply Y,L)` multiplies every element of the space-separated list `L` by `Y`
- `$(divide Z,L)` divides every element of the space-separated list `L` by `Z`

MD5 hashes
----------

Instead of using file timestamps, which are fragile (especially on networked filesystems),
Biomake can optionally use MD5 checksums to decide when to rebuild files.
Turn on this behavior with the `-H` option (long form `--md5-hash`).

Biomake uses the external program `md5` to do checksums (available on OS X), or `md5sum` (available on Linux).
If neither of these are found, Biomake falls back to using the SWI-Prolog md5 implementation;
this does however require loading the entire file into memory (which may be prohibitive for large files).

Queues
------

To run jobs in parallel, locally or on a cluster, you need to specify a queueing engine
using the `-Q` option (long form `--queue-engine`). Note that, unlike with GNU Make, multi-threading is not activated
simply by specifying the number of threads with `-j`; you need `-Q` as well.

There are several queueing engines currently supported:

- `-Q poolq` uses an internal thread pool for running jobs in parallel on the same machine that `biomake` is running on
- `-Q sge` uses [Sun Grid Engine](https://en.wikipedia.org/wiki/Oracle_Grid_Engine) or compatible (e.g. [Open Grid Scheduler](http://gridscheduler.sourceforge.net/))
- `-Q pbs` uses [PBS](https://en.wikipedia.org/wiki/Portable_Batch_System)
- `-Q slurm` uses [Slurm](https://slurm.schedmd.com/)
- `-Q test` just runs the jobs synchronously. Used for testing purposes only

For Sun Grid Engine, PBS and Slurm, the paths to the relevant job control executables, and any arguments to those executables
(such as the name of the queue that jobs should be run on), can be controlled using various command-line arguments.
In particular, the `--qsub-args` command-line option (applying to all recipes)
and the `QsubArgs` Prolog variable (on a per-recipe basis, in the target goal)
can be used to pass parameters such as the queue name.

Here's an example of using `QsubArgs`:

~~~~
my_target { QsubArgs = '--cores-per-socket=4' } : my_dependency
    do_something >$@
~~~~

Note that `QsubArgs` has to be set in the target goal, not the deps goal
(since the job is submitted to the queueing engine before the dependencies are guaranteed to have been built).

Similarly, you can use the `QsubHeader` variable (or the `--qsub-header` command-line option) to add header lines to the wrapper script that is submitted to the queue engine
(for example, to provide queue configuration directives),
or you can use `QsubHeaderFile` (or `--qsub-header-file`) to specify the filename of a header file to include.

The names of these Prolog variables for fine-grained queue configuration (`QsubArgs`, `QsubHeader`, `QsubHeaderFile`) are the same for Slurm as for SGE and PBS,
even though the batch submission command for Slurm is `sbatch` and not `qsub`.

More
----

Ideas for future development:

* a web-based build environment (a la Galaxy)
* semantic web enhancement (using NEPOMUK file ontology)
* using other back ends and target sources (sqlite db, REST services)
* cloud-based computing
* metadata
",2023-07-07 15:50:35+00:00
bionode,bionode-watermill,bionode/bionode-watermill,💧Bionode-Watermill: A (Not Yet Streaming) Workflow Engine,https://bionode.gitbooks.io/bionode-watermill/content/,False,36,2021-12-03 11:08:44+00:00,2016-06-20 17:13:21+00:00,11,19,4,2,,,MIT License,454,v0.5.1,7,2017-08-28 16:25:24+00:00,,2017-08-29 17:12:32+00:00,"<p align=""center"">
  <a href=""http://bionode.io"">
    <img height=""200"" width=""200"" title=""bionode"" alt=""bionode logo"" src=""https://rawgithub.com/bionode/bionode/master/docs/bionode-logo.min.svg""/>
  </a>
  <br/>
  <a href=""http://bionode.io/"">bionode.io</a>
</p>

# bionode-watermill

> Bionode-watermill: A (Not Yet Streaming) Workflow Engine

[![npm](https://img.shields.io/npm/v/bionode-watermill.svg?style=flat-square)](http://npmjs.org/package/bionode-watermill)
[![Travis](https://img.shields.io/travis/bionode/bionode-watermill/master.svg?label=master&style=flat-square)](https://travis-ci.org/bionode/bionode-watermill)
[![Travis](https://img.shields.io/travis/bionode/bionode-watermill/dev.svg?label=dev&style=flat-square)](https://travis-ci.org/bionode/bionode-watermill)
[![Codecov private](https://img.shields.io/codecov/c/github/bionode/bionode-watermill.svg?style=flat-square)](https://github.com/bionode/bionode-watermill/tree/master)
[![Dependencies](http://img.shields.io/david/bionode/bionode-watermill.svg?style=flat-square)](http://david-dm.org/bionode/bionode-watermill)
[![npm](https://img.shields.io/npm/dt/bionode-watermill.svg?style=flat-square)](https://www.npmjs.com/package/bionode-watermill)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg?style=flat-square)](https://gitter.im/bionode/bionode-watermill)

## Table of Contents

* [What is bionode-watermill](#what-is-bionode-watermill)
    * [Main features](#main-features)
    * [Who is this tool for?](#who-is-this-tool-for)
* [Installation](#installation)
* [Documentation](#documentation)
* [Tutorial](#tutorial)
* [Example pipelines](#example-pipelines)
* [Why bionode-watermill?](#why-bionode-watermill)
* [Contributing](#contributing)

## What is bionode-watermill

**Bionode-watermill** is a workflow engine that lets you assemble and run 
bioinformatic pipelines with ease and less overhead. Bionode-watermill 
pipelines are 
essentially node.js scripts in which [tasks](docs/BeginnerWalkthrough.md#task) are the modules that will be 
assembled in the final *pipeline* using [orchestrators](docs/BeginnerWalkthrough.md#orchestrators).

### Main features

* Lots of **modularity** - tasks can be recycled as many times as you want!
* **Reusability** - tasks can be reused many times within and between pipelines.
* **Automated Input/Output handling** - no need to worry about input/output 
location, bionode-watermill does that for you.
* Ability to **run programs using Unix shell** - As demonstrated by `myTask`.
 So, there is no need to reinvent the wheel, you can use your previous 
 scripts and programs within bionode-watermill framework.
* **Node.js integration** - not explored here, but you can use javascript 
alongside with bionode-watermill tasks and pipelines and even inside tasks 
instead of Unix commands.
* [Streamable tasks](docs/Task.md#streamable-tasks-potential) (still not 
implemented - Issue [#79](https://github.com/bionode/bionode-watermill/issues/79))

### Who is this tool for?

Bionode-watermill is for **biologists** who understand it is important to 
experiment with sample data, parameter values, and tools. Compared to other 
workflow systems, the ease of swapping around parameters and tools is much 
improved, allowing you to iteratively compare results and construct more 
confident inferences. Consider the ability to construct your own 
[Teaser](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0803-1) 
for *your data* with a *simple syntax*, and getting utmost performance out of the box.


Bionode-watermill is for **programmers** who desire an efficient and 
easy-to-write methodology for developing complex and dynamic data pipelines, 
while handling parallelization as much as possible. Bionode-watermill is an npm 
module, and is accessible by anyone willing to learn a little JavaScript. This 
is in contrast to other tools which develop their own DSL 
(domain specific language), which is not useful outside the tool. By leveraging 
the npm ecosystem and JavaScript on the client, Bionode-watermill can be built 
upon for inclusion on web apis, modern web applications, as well as native 
applications through [Electron](http://electron.atom.io/). Look forward to 
seeing Galaxy-like applications backed by a completely configurable Node API.


## Installation

Local installation:

```npm install bionode-watermill```

Global installation:

```npm install bionode-watermill -g```

## Documentation

Our documentation is available [here](https://bionode.gitbooks.io/bionode-watermill/content/). 
There you may find how to **use** bionode-watermill to construct and **run** 
your 
pipelines. Moreover, you will also find the description of the API to help 
anyone 
willing to **contribute**.


## Tutorial

- [Try bionode-watermill tutorial!](https://github.com/bionode/bionode-watermill-tutorial)

## Example pipelines

- [Toy pipeline with shell/node](https://github.com/bionode/bionode-watermill/blob/master/examples/pipelines/pids/pipeline.js)
- [Simple capitalize task](https://github.com/bionode/bionode-watermill/blob/master/examples/pipelines/capitalize/capitalize.js)
- [Simple SNP calling](https://github.com/bionode/bionode-watermill/blob/master/examples/pipelines/variant-calling-simple/pipeline.js)
- [SNP calling with filtering and fork](https://github.com/bionode/bionode-watermill/blob/master/examples/pipelines/variant-calling-filtered/pipeline.js)
- [Mapping with bowtie2 and bwa (with tutorial)](https://github.com/bionode/bionode-watermill/tree/master/examples/pipelines/two-mappers)

## Why bionode-watermill?

[This blog post](https://jmazz.me/blog/NGS-Workflows)
compares the available tools to deal with NGS workflows, explaining the 
advantages of each one, including **bionode-watermill**.


## Contributing
We welcome all kinds of contributions at all levels of experience, please 
refer to 
the [Issues section](https://github.com/bionode/bionode-watermill/issues). 
Also, you can allways reach us on [gitter](https://gitter.im/bionode/bionode-watermill).

### Feel free to submit your pipeline to us

Just make a PR for us, that adds a pipeline under `./examples/pipelines/`. 
You can check some of the already existing examples [here](examples/pipelines).
",2023-07-07 15:50:39+00:00
biopet,biopet,biopet/biopet,Biopet docs,http://biopet-docs.readthedocs.io/en/latest/,False,16,2023-06-26 15:36:15+00:00,2015-05-13 15:06:18+00:00,3,8,11,7,v0.9.0,2017-04-25 16:17:10+00:00,Other,7007,v0.9.0,19,2017-04-25 16:17:10+00:00,,2018-08-02 09:03:56+00:00,"# Welcome to Biopet


## Introduction

Biopet (Bio Pipeline Execution Toolkit) is the main pipeline development framework of the LUMC Sequencing Analysis Support Core team. It contains our main pipelines and some of the command line tools we develop in-house. It is meant to be used in the main [SHARK](https://humgenprojects.lumc.nl/trac/shark) computing cluster. While usage outside of SHARK is technically possible, some adjustments may need to be made in order to do so.

Full documentation is here: [Biopet documentation](http://biopet-docs.readthedocs.io/en/latest/)

## Quick Start

### Running Biopet in the SHARK cluster

Biopet is available as a JAR package in SHARK. The easiest way to start using it is to activate the `biopet` environment module, which sets useful aliases and environment variables:

~~~
$ module load biopet/v0.8.0
~~~

With each Biopet release, an accompanying environment module is also released. The latest release is version 0.4.0, thus `biopet/v0.4.0` is the module you would want to load.

After loading the module, you can access the biopet package by simply typing `biopet`:

~~~
$ biopet
~~~

This will show you a list of tools and pipelines that you can use straight away. You can also execute `biopet pipeline` to show only available pipelines or `biopet tool` to show only the tools. What you should be aware of, is that this is actually a shell function that calls `java` on the system-wide available Biopet JAR file.

~~~
$ java -jar <path/to/current/biopet/release.jar>
~~~

The actual path will vary from version to version, which is controlled by which module you loaded.

Almost all of the pipelines have a common usage pattern with a similar set of flags, for example:

~~~
$ biopet pipeline <pipeline_name> -config <path/to/config.json> -qsub -jobParaEnv BWA -retry 2
~~~

The command above will do a *dry* run of a pipeline using a config file as if the command would be submitted to the SHARK cluster (the `-qsub` flag) to the `BWA` parallel environment (the `-jobParaEnv BWA` flag). We also set the maximum retry of failing jobs to two times (via the `-retry 2` flag). Doing a good run is a good idea to ensure that your real run proceeds smoothly. It may not catch all the errors, but if the dry run fails you can be sure that the real run will never succeed.

If the dry run proceeds without problems, you can then do the real run by using the `-run` flag:

~~~
$ biopet pipeline <pipeline_name> -config <path/to/config.json> -qsub -jobParaEnv BWA -retry 2 -run
~~~

It is usually a good idea to do the real run using `screen` or `nohup` to prevent the job from terminating when you log out of SHARK. In practice, using `biopet` as it is is also fine. What you need to keep in mind, is that each pipeline has their own expected config layout. You can check out more about the general structure of our config files [here](docs/config.md). For the specific structure that each pipeline accepts, please consult the respective pipeline page.

## Testing

Our code is tested at our local Jenkins installation for every change. We are using a [JenkinsFile](Jenkinsfile) in our repository to do this.


## Contributing to Biopet

Biopet is based on the Queue framework developed by the Broad Institute as part of their Genome Analysis Toolkit (GATK) framework. The current Biopet release is based on the GATK 3.5 release.

We welcome any kind of contribution, be it merge requests on the code base, documentation updates, or any kinds of other fixes! The main language we use is Scala, though the repository also contains a small bit of Python and R. Our main code repository is located at [https://github.com/biopet/biopet](https://github.com/biopet/biopet/issues), along with our issue tracker.

For more information please go to our [Developer documentation](http://biopet-docs.readthedocs.io/en/develop/developer/getting-started/)

## About

Go to the [about page](docs/about.md)

## License

See: [License](docs/license.md)
",2023-07-07 15:50:43+00:00
biopig,biopig,JGI-Bioinformatics/biopig,,,False,9,2020-10-28 01:02:38+00:00,2012-02-23 20:32:15+00:00,6,36,6,0,,,Other,305,v2.0,2,2015-08-12 22:24:05+00:00,,2017-09-19 01:00:22+00:00,"BioPig

BioPig Copyright (c) 2015, The Regents of the University of California, through Lawrence Berkeley National Laboratory 
(subject to receipt of any required approvals from the U.S. Dept. of Energy).  All rights reserved.
 
If you have questions about your rights to use or distribute this software, please contact 
Berkeley Lab's Technology Transfer Department at  TTD@lbl.gov referring to "" BioPig (LBNL Ref CR-3209).""
 
NOTICE.  This software was developed under funding from the U.S. Department of Energy.  As such, the U.S. Government 
has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license 
in the Software to reproduce, prepare derivative works, and perform publicly and display publicly.  Beginning five (5)
 years after the date permission to assert copyright is obtained from the U.S. Department of Energy, and subject to 
 any subsequent five (5) year renewals, the U.S. Government is granted for itself and others acting on its behalf a 
 paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, prepare derivative works, 
 distribute copies to the public, perform publicly and display publicly, and to permit others to do so.

This project provides additional apache pig commands and data loaders/writers specific
for biologicial sequence data.  Specifically, biopig supports the following functionality:
  - load fasta and fastq files
  - filter sequence data by id, header or size
  - generate kmers
  - wrappers for common external programs (Blast, Newbler, velvet, cap3, etc)

With biopig, users can write data parallel analysis tools that get executed on a hadoop
map/reduce cluster such as NERSC's Magellan or AWS's Elastic MapReduce.

=======================================

to build

=======================================

To build the software, you need to have maven.  install maven and then build using

   % mvn package -Dmaven.test.skip=true

this will produce a JAR file in biopig/target/biopig-<version>-all.jar


=======================================

to use

=======================================

see ./apps for examples.


questions/comments can be emailed to us at biopig [at] jgi-psf [dot] org
",2023-07-07 15:50:47+00:00
bioqueue,BioQueue,liyao001/BioQueue,A novel pipeline framework to accelerate bioinformatics analysis,,False,28,2023-02-11 07:23:58+00:00,2017-03-12 06:46:59+00:00,9,6,2,3,1.0.313,2019-09-12 04:37:18+00:00,Apache License 2.0,442,v1.0,5,2017-03-25 07:43:29+00:00,,2023-03-19 18:51:43+00:00,"# BioQueue
[![document](https://readthedocs.org/projects/bioqueue/badge/?version=latest ""document"")](https://bioqueue.readthedocs.io/en/latest/?badge=latest)

BioQueue is a researcher-facing bioinformatic platform preferentially to improve the efficiency and robustness of analysis in bioinformatics research by estimating the system resources required by a particular job. At the same time, BioQueue also aims to promote the accessibility and reproducibility of data analysis in biomedical research. Implemented by Python **3.x**, BioQueue can work in both POSIX compatible systems (Linux, Solaris, OS X, etc.) and Windows.
## Get started

`pip` is the default package manager for BioQueue, so before installing BioQueue, please make sure that you have [pip](https://pip.pypa.io/en/stable/) installed.

### Prerequisites

BioQueue can store data on SQLite, which means users can set up BioQueue without an extra database software. However, to achieve a higher performance, **we suggest users to install MySQL or PostgreSQL**. If you want to set up MySQL or PostgreSQL on your machine, you can visit the wiki page. 
### 1. Download and setup the BioQueue project

First of all, you will need to clone the project from Github (Or you can download BioQueue by open [this link](https://github.com/liyao001/BioQueue/zipball/master)).
```
git clone https://github.com/liyao001/BioQueue.git
Or
wget https://github.com/liyao001/BioQueue/zipball/master
```
Then navigate to the project's directory, and run `install.py` script (All dependent python packages will be automatically installed):
```
cd BioQueue
python install.py
```
When running `install.py`, this script will ask you a few questions include:
 1. CPU cores: The amount of CPU to use. Default value: all cores on that machine.
 2. Memory (Gb): The amount of memory to use. Default value: all physical memory on that machine.
 3. Disk quota for each user(Gb, default value: all disk space on that machine).

If you decide to run BioQueue with MySQL or PostgreSQL, the script will ask a few more questions:
 1. Database host: If you install MySQL server on your own machine, enter `localhost` or `127.0.0.1`.
 2. Database user: user name of the database.
 3. Database password: password of the database.
 4. Database name: Name of the data table.
 5. Database port: `3306` by default for MySQL

Then the script will interact with you and create a super user account for the platform.

### 2. Start the queue

Run the `bioqueue.py` script in the `BioQueue/worker` folder
```
python worker/bioqueue.py
```

### 3. Start webserver

```
python manage.py runserver 0.0.0.0:8000
```
This will start up the server on `0.0.0.0` and port `8000`, so BioQueue can be accessed over the network. If you want access BioQueue only in local environment, remove `0.0.0.0:8000`.

## Useful information

* To stop the queue, the webserver or the ftp server, just hit `Ctrl-c` in the terminal from which BioQueue is running.
* To get a better performance, moving the webserver to [Apache](http://bioqueue.readthedocs.io/en/latest/faq.html#use-bioqueue-with-apache-in-production-environment) or [nginx](https://nginx.org) is a good idea.

## Screenshot

 ![](status_page.png)

## Citation

1. Yao, L., Wang, H., Song, Y. & Sui, G. BioQueue: a novel pipeline framework to accelerate bioinformatics analysis. *Bioinformatics* 33, 3286–3288 (2017). [doi:10.1093/bioinformatics/btx403](https://doi.org/doi:10.1093/bioinformatics/btx403)",2023-07-07 15:50:52+00:00
bioshake,bioshake,PapenfussLab/bioshake,Bioinformatics pipelines with Haskell and Shake,,False,55,2023-03-31 16:24:26+00:00,2017-02-28 02:35:49+00:00,7,6,3,0,,,ISC License,116,,0,,,2019-08-13 23:11:15+00:00,"# Bioshake: Bioinformatics pipelining with Shake

Bioshake is a bioinformatics workflow tool extending the Shake build system.

Pipelines are defined in terms of
_stages_ that connect together into simple streams. Here's an example pipeline:

    Input :-> align :-> mappedOnly :-> sortBam :-> deDup :-> out [""aligned.bam""]

Each stage takes its input from the previous stage and generates
output for the stage that follows. Some features of Bioshake pipelines are

-   There is no need to mention files between
    the stages: intermediate files are automatically named. The `out` stage 
    simply copies its input to explicitly named files. 
-   Each stage has strong type guarantees. For example, if a stage
    requires a sorted BAM file, it will be a type error to construct a pipeline that
    feeds it an unsorted BAM file. This allows many pipeline errors to be caught at
    compile time rather than runtime.
    
## Quickstart

This repository includes [Nix](http://nixos.org/nix) expressions for easy
building. To use, install Nix, clone this repository, then a shell containing
the necessary GHC environment for building Bioshake can be dropped into with
""nix-shell"". To compile and execute the example pipeline, the following commands
can be used:

 
    nix-shell --command 'ghc -o examples/simple examples/simple.lhs -O'
    cd examples
    nix-shell -p bwa samtools platypus --command ""./simple *.fq""

Note that the repository must be cloned with `git lfs` installed for
the example data to be downloaded, see: [examples/README](examples/README).
## How to use

Begin with something like this:

    main = bioshake threads shakeOptions $ do ...
    
This initialises bioshake with a certain number of threads and with some Shake
options. 

The (simplified) type signature of the `bioshake` function  is

    bioshake :: Int -> ShakeOptions -> Rules () -> IO ()
    
and so the `do` clause corresponds to a Shake `Rules ()` object; 
Bioshake pipelines are compiled down to Shake `Rules ()` and so can be partially
interleaved with them. (In Shake, a `Rules a` object is a (collection of) build rules that return data of type `a`.) 

Within the `Rules ()` block, a pipeline looks like

      compileRules $ do
        compile $ 
          Input :-> align :-> mappedOnly :-> sortBam :-> deDup :-> out [""aligned.bam""]
          
        ...

The `compileRules` and `compile` functions have the (again simplified) types

    compileRules :: Compiler () -> Rules ()
    compile :: Compilable a => a -> Compiler () 

So `compile` takes `Compilable` things (the pipeline in this case) and
makes something suitable for `compileRules`, which generates the actual shake
`Rules`.

A Bioshake pipeline must instantiate `Compilable`, but pipelines 
(and stages) are not themselves `Compiler`s. Unlike vanilla Shake -- in which
each stage of a build is represented by an object of type `Rules a` -- 
in Bioshake, there is no concrete datatype representing a stage; rather, 
each stage is the only inhabitant of its type, and the whole functionality 
of a stage is determined by the typeclasses that this type instantiates.
More on this below.

### Direct execution

In our pipeline, the stages `align`, `mappedOnly`, `sortBam`, and
`deDup` come from different modules such as `Bioshake.BWA` and
`Bioshake.Samtools`, or their cluster equivalents `Bioshake.Cluster.BWA` and
`Bioshake.Cluster.Samtools`. 
Each module in bioshake represents a tool, such as an aligner; for example, 
`Bioshake.BWA` is the module that defines alignment stages using the BWA aligner.
When a pipeline is executed using this module, the tool is directly invoked on
the machine that is executing bioshake. Many of these tools are multithreaded,
and thus you need to declare how many threads to use by default:

    instance Default Threads where def = Threads threads

This is overridable for specific stages using the `$~` operator. For example, if
we wanted to restrict the above pipeline to only 5 threads during alignment we
could write:

    Input :-> align $~ Threads 5 :-> mappedOnly :-> sortBam :-> deDup :-> out [""aligned.bam""]
    
Note that bioshake will not overcommit resources: the first argument to
`bioshake` defines the maximum number of concurrent threads at any given time.

### Cluster submission

Stages can be submitted to a cluster instead of executing directly. In this
case, the `Bioshake.Cluster.*` modules are used. Continuing the BWA example, to
submit BWA jobs to a cluster instead of executing directly you would import
`Bioshake.Cluster.BWA`. In this case, the default cluster configuration must be
defined:

    instance Default Config where def = Config [Queue ""somequeue"", Mem (gb 20),
    CPUs 42]
    
There are a several configuration options that can be defined:

1. `Mem`: the maximum memory used by the stage
2. `CPUs`: the maximum number of CPUs
3. `Walltime`: maximum walltime allowed
4. `Queue`: the queue to submit to
5. `Module`: a module to load before execution

### Writing your own stages

As mentioned above, in ordinary Shake, each stage of a build is represented by an object of
type `Rules a`. By contrast, in Bioshake, each stage inhabits its own type, and
the functionality of a stage is determined by the typeclasses 
that its type instantiates. Many of these instances can be automatically generated
by Template Haskell. The constructor `:->`, which is used to concatenate successive
stages into a pipeline, is isomorphic to `(,)`.

Here is an example showing how to create your own stage. We will
create a stage called `Magic` that takes a BAM file as input and
produces a VCF as output.

First, create a datatype to represent the stage:

    data Magic c = Magic c deriving Show
    
The `c` type variable is a configuration type which is used by bioshake to represent
threaded/cluster configuration. You can have other polymorphic types, but the
last type must be the configuration type. Next, write a function that executes
the tool:

    buildMagic (Magic c) (paths -> [input]) [out] =
      run ""/some/tool"" [""-a"", ""with-flags""] [input] [out]
  
Here `run` is pretty much like a restricted version of shake's `cmd`. Now use
template haskell to do the rest:

    $(makeSingleTypes ''Magic [''IsVCF] [])
    $(makeSingleThread ''Magic [''IsBAM] 'buildMagic)

The `makeSingleTypes` macro declares that `Magic` maps each input file to an output file (in
this case `buildMagic` expects a single input and maps it to a single output).
The first list in the template function declares properties about the _output_
of the stage: the first must be a `IsEXT` tag, which declares the type of output
and the extension. The second list is a bunch of transitive tags: if this tags
hold for the input, then they will hold for the output too.

`makeSingleThread` declares how to build `Magic` things. The list is a bunch of
tags that must hold for the _input_ files, and the second parameter is just the
function to build output files given inputs.

### The Input stage

The first stage `input` is probably the most confusing part about getting a
pipeline running. Stages generate some output that is fed to the next stage in
the pipeline, hence the first stage has to ""generate"" some output files, which
are just the input files on disk. We need some data structure to represent input
files on the disk:

    data Input = Input deriving Show
    
First is to declare the paths ""output"" from the stage. As the input stage, we
just return a list of input files:

    instance Pathable Input where
      paths _ = [""sample_R1.fastq.gz"", ""sample_R2.fastq.gz""]

We've assumed the inputs here are paired end reads, hence there are two fastq
files. This is not sufficient as we must declare our inputs have certain
properties: it is compilable to shake rules, that the reads are from paired end
sequencing, that they are fastq files, and furthermore that they should be
referenced against some genome:
    
    instance Compilable Input
    instance PairedEnd Input
    instance IsFastQ Input
    instance Referenced In where
      getRef _ = ""/path/to/hg38.fa""
      name _ = ""hg38""

## Further documentation

There is an example pipeline in the repository and haddock documentation
at <https://papenfusslab.github.io/bioshake/>.
",2023-07-07 15:50:56+00:00
bioworkbench,bioworkbench,mmondelli/bioworkbench,,,False,0,2018-07-05 17:52:31+00:00,2018-03-28 12:10:13+00:00,1,1,1,5,v1.0.4,2018-07-05 17:51:09+00:00,Apache License 2.0,32,v1.0.4,5,2018-07-05 17:51:09+00:00,,2018-07-05 17:51:09+00:00,"# BioWorkbench

This repository contains a Docker recipe for the construction of BioWorkbench, a high-performance framework for managing and analyzing bioinformatics experiments.

Each experiment comprises a set of activities to be executed in a chained way, also known as workflows. Thus, they were modeled and defined using the Swift, a scientific workflow management system that allows them to run in high-performance computing environments.

The Docker recipe contains all the programs and configurations required to run the following workflows:

* SwiftGecko: https://github.com/mmondelli/swift-gecko

* SwiftPhylo: https://github.com/mmondelli/swift-phylo

* RASflow: https://github.com/mmondelli/rasflow

Please refer to the links above for a better description of each of the experiments, as well as instructions for their execution.

To build an image from the Dockerfile you can run (assuming you are inside the directory where the file is located):

```
docker build -t bioworkbench:latest .
```

To run a container for this image, you can run:

```
docker run --rm -p 3837:3838 --name bio bioworkbench:latest
```

And with this you can access a web interface to analyze the provenance of the workflows executed through the address:
http://localhost:3837/workbench

This interface provides a menu where it is necessary to inform which workflow will be analyzed (Script name). When choosing a date range (Date range), the field below (Script ID) will list all the identifiers of each execution of this workflow performed during the reported period. The user must then choose one of these options and click on the Update button. The analyses presented on the right side will be updated according to the chosen workflow execution.

To run the experiments and other commands in the running container, you can use (from another terminal):

```
docker exec -ti bio /bin/bash
```

This Dockerfile is also configured for automatic build in [this Docker hub repository](https://hub.docker.com/r/malumondelli/bioworkbench). Thus, you can use the pull command to get the BioWorkbench image:

```
docker pull malumondelli/bioworkbench
```

",2023-07-07 15:50:59+00:00
bistro,bistro,pveber/bistro,A library to build and execute typed scientific workflows,,False,44,2022-05-03 17:34:49+00:00,2014-01-19 15:30:29+00:00,9,8,5,7,v0.6.0,2021-11-14 09:23:10+00:00,Other,1167,v0.6.0,7,2021-11-14 09:22:05+00:00,2023-06-07 09:15:27+00:00,2023-06-07 09:15:20+00:00,"# bistro: build and run distributed workflows

`bistro` is an [OCaml](http://ocaml.org) library to build and run
computations represented by a collection of interdependent scripts, as
is often found in applied research (especially computational
biology).

**Features**:
- build complex and composable workflows declaratively
- simple and lightweight wrapping of new components
- resume-on-failure: if something fails, fix it and the workflow will
  restart from where it stopped
- distributed workflow execution
- development-friendly: when a script is modified, bistro
  automatically finds out what needs to be recomputed
- automatic naming of generated files
- static typing: detect file format errors at compile time!

The library provides a datatype to represent scripts (including
metadata and dependencies), an engine to run workflows and a
standard library providing components for popular tools (although
mostly related to computational biology and unix for now).

Questions, suggestions or contributions are welcome, please file an
[issue](https://github.com/pveber/bistro/issues) as needed.

## Documentation

A
[manual](http://bistro.readthedocs.io/en/latest/getting-started.html)
is available, but feel free to file issues if something is unclear or
missing. There is also a
[generated API documentation](http://pveber.github.io/bistro/).

## Installation

Detailed instructions are available in the
[manual](http://bistro.readthedocs.io/en/latest/getting-started.html). In
a nutshell, `bistro` can be installed using
[opam](http://opam.ocaml.org/). You need a recent (at least 4.03.0)
installation of OCaml. Once this is done, simply type

```
opam install bistro
```

to install the library, or:

```
opam pin add -y bistro --dev-repo
```
to get the current development version.

",2023-07-07 15:51:03+00:00
bonobo,bonobo,python-bonobo/bonobo,Extract Transform Load for Python 3.5+,https://www.bonobo-project.org/,False,1554,2023-07-03 04:38:44+00:00,2016-12-09 04:03:23+00:00,141,59,30,0,,,Apache License 2.0,981,0.7.0rc3,40,2019-07-20 13:32:58+00:00,2023-07-03 04:38:44+00:00,2021-03-10 15:44:00+00:00,"==========
🐵  bonobo
==========

Data-processing for humans.

.. image:: https://img.shields.io/pypi/v/bonobo.svg
    :target: https://pypi.python.org/pypi/bonobo
    :alt: PyPI

.. image:: https://img.shields.io/pypi/pyversions/bonobo.svg
    :target: https://pypi.python.org/pypi/bonobo
    :alt: Versions

.. image:: https://readthedocs.org/projects/bonobo/badge/?version=master
    :target: http://docs.bonobo-project.org/
    :alt: Documentation

.. image:: https://travis-ci.org/python-bonobo/bonobo.svg?branch=master
    :target: https://travis-ci.org/python-bonobo/bonobo
    :alt: Continuous Integration (Linux)

.. image:: https://ci.appveyor.com/api/projects/status/github/python-bonobo/bonobo?retina=true&branch=master&svg=true
    :target: https://ci.appveyor.com/project/hartym/bonobo?branch=master
    :alt: Continuous Integration (Windows)

.. image:: https://codeclimate.com/github/python-bonobo/bonobo/badges/gpa.svg
   :target: https://codeclimate.com/github/python-bonobo/bonobo
   :alt: Code Climate

.. image:: https://img.shields.io/coveralls/python-bonobo/bonobo/master.svg
    :target: https://coveralls.io/github/python-bonobo/bonobo?branch=master
    :alt: Coverage

Bonobo is an extract-transform-load framework for python 3.5+ (see comparisons with other data tools).

Bonobo uses plain old python objects (functions, generators and iterators), allows them to be linked together in a directed graph, and then executed using a parallelized strategy, without having to worry about the underlying complexity.

Developers can focus on writing simple and atomic operations, that are easy to unit-test by-design, while the focus of the
framework is to apply them concurrently to rows of data.

One thing to note: write pure transformations and you'll be safe.

Bonobo is a young rewrite of an old python2.7 tool that ran millions of transformations per day for years on production.
Although it may not yet be complete or fully stable (please, allow us to reach 1.0), the basics are there.

----

*Bonobo is under heavy development, we're doing our best to keep the core as stable as possible while still moving forward. Please allow us to reach 1.0 stability and our sincere apologies for anything we break in the process (feel free to complain on issues, allowing us to correct breakages we did not expect)*

----

Homepage: https://www.bonobo-project.org/ (`Roadmap <https://www.bonobo-project.org/roadmap>`_)

Documentation: http://docs.bonobo-project.org/

Contributing guide: http://docs.bonobo-project.org/en/latest/contribute/index.html

Issues: https://github.com/python-bonobo/bonobo/issues

Slack: https://bonobo-slack.herokuapp.com/

Release announcements: http://eepurl.com/csHFKL

----

Made with ♥ by `Romain Dorgueil <https://twitter.com/rdorgueil>`_ and `contributors <https://github.com/python-bonobo/bonobo/graphs/contributors>`_.

.. image:: https://img.shields.io/pypi/l/bonobo.svg
    :target: https://pypi.python.org/pypi/bonobo
    :alt: License


",2023-07-07 15:51:07+00:00
bpipe,bpipe,ssadedin/bpipe,Bpipe - a tool for running and managing bioinformatics pipelines,http://docs.bpipe.org/,False,216,2023-06-06 17:18:28+00:00,2014-10-20 04:31:15+00:00,55,18,15,16,0.9.12,2023-05-07 05:50:10+00:00,Other,2341,0.9.12,20,2023-05-07 05:50:10+00:00,2023-07-04 08:29:05+00:00,2023-07-04 08:28:43+00:00,"Welcome to Bpipe  ![Tests](https://github.com/ssadedin/bpipe/actions/workflows/ci-build.yml/badge.svg)
=================

<style type='text/css'> .col-md-3 { display: none; } </style>
 
Bpipe provides a platform for running data analytic workgflows that consist of a series of processing stages - known as 'pipelines'. Bpipe has special features to help with
specific challenges in Bioinformatics and computational biology.

* May 2023 - New! [Bpipe 0.9.12](https://github.com/ssadedin/bpipe/releases/tag/0.9.12) released!
* [Documentation](https://docs.bpipe.org)
* [Mailing List](https://groups.google.com/forum/#!forum/bpipe-discuss) (Google Group)

Bpipe has been published in [Bioinformatics](http://bioinformatics.oxfordjournals.org/content/early/2012/04/11/bioinformatics.bts167.abstract)! If you use Bpipe, please cite:

  _Sadedin S, Pope B & Oshlack A, Bpipe: A Tool for Running and Managing Bioinformatics Pipelines, Bioinformatics_

**Example**
  
```groovy
 hello = {
    exec """"""
        echo ""hello world"" > $output.txt
    """"""
 }
 run { hello }
```

**Why Bpipe?**

Many people working in data science end up running jobs as custom shell (or similar)
scripts.  While this makes running them easy it has a lot of limitations. 
By turning your shell scripts into Bpipe scripts, here are some of the features
you can get:

  * **Dependency Tracking** - Like `make` and similar tools, Bpipe knows what you already did and won't do it again
  * **Simple definition of tasks to run** - Bpipe runs shell commands almost as-is : super low friction between what works in your command line and what you need to put into your script
  * **Transactional management of tasks** - commands that fail get outputs cleaned up, log files saved and the pipeline cleanly aborted.  No out of control jobs going crazy.
  * **Automatic Connection of Pipeline Stages** -  Bpipe manages the file names for input and output of each stage in a systematic way so that you don't need to think about it.  Removing or adding new stages ""just works"" and never breaks the flow of data.
  * **Job Management** - know what jobs are running, start, stop, manage whole workflows with simple commands
  * **Easy Parallelism** - split jobs into many pieces and run them all in parallel whether on a cluster, cloud or locally. Separate configuration of parallelism from the definition of the tasks.
  * **Audit Trail** - keeps a journal of exactly which commands executed, when and what their inputs and outputs were.
  * **Integration with Compute Providers** - pure Bpipe scripts can run unchanged whether locally, on
  your server, or in cloud or traditional HPC back ends such as Torque, SLURM GridEngine or others.
  * **Deep Integretation Options** - Bpipe integrates well with other systems: receive alerts to tell you when your pipeline finishes or even as each stage completes, call REST APIs, send messages to queueing systems and easily use any type of integration available within the Java ecosystem.
  * See how Bpipe compares to [similar tools](https://docs.bpipe.org/Overview/ComparisonToWorkflowTools/)

**Ready for More?**

Take a look at the [Overview](https://docs.bpipe.org/Overview/Introduction/) to
see Bpipe in action, work through the [Basic Tutorial](https://docs.bpipe.org/Tutorials/Hello-World/) 
for simple first steps, see a step by step example of a [realistic
pipeline](http://docs.bpipe.org/Tutorials/RealPipelineTutorial/) made using Bpipe, or 
take a look at the [Reference](http://docs.bpipe.org) to see all the documentation.
",2023-07-07 15:51:11+00:00
braneframework,brane,onnovalkering/brane,Programmable Orchestration of Applications and Networking,,True,3,2023-02-08 12:02:44+00:00,2020-04-24 13:03:04+00:00,8,2,2,5,v0.4.1,2021-08-16 10:19:10+00:00,Apache License 2.0,667,v0.4.1,5,2021-08-16 10:19:10+00:00,,2022-12-16 19:57:11+00:00,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/onnovalkering/brane/master/contrib/assets/logo.png"" alt=""logo"" width=""250""/>
  <h3 align=""center"">Programmable Orchestration of Applications and Networking</h3>
</p>

----

<span align=""center"">

  [![Audit status](https://github.com/onnovalkering/brane/workflows/Audit/badge.svg)](https://github.com/onnovalkering/brane/actions)
  [![CI status](https://github.com/onnovalkering/brane/workflows/CI/badge.svg)](https://github.com/onnovalkering/brane/actions)
  [![License: Apache-2.0](https://img.shields.io/github/license/onnovalkering/brane.svg)](https://github.com/onnovalkering/brane/blob/master/LICENSE)
  [![Coverage status](https://coveralls.io/repos/github/onnovalkering/brane/badge.svg)](https://coveralls.io/github/onnovalkering/brane)
  [![Release](https://img.shields.io/github/release/onnovalkering/brane.svg)](https://github.com/onnovalkering/brane/releases/latest)
  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3890928.svg)](https://doi.org/10.5281/zenodo.3890928)

</span>

:warning: The development and maintanence has been handed over to the [EPI project](https://enablingpersonalizedinterventions.nl), please see [this](http://github.com/epi-project/brane) fork.

## Introduction

Regardless of the context and rationale, running distributed applications on geographically dispersed IT resources often comes with various technical and organizational challenges. If not addressed appropriately, these challenges may impede development, and in turn, scientific and business innovation. We have designed and developed Brane to support implementers in addressing these challenges. Brane makes use of containerization to encapsulate functionalities as portable building blocks. Through programmability, application orchestration can be expressed using intuitive domain-specific languages. As a result, end-users with limited or no programming experience are empowered to compose applications by themselves, without having to deal with the underlying technical details.

See the [documentation](docs/) for more information, or check out our [conference paper](https://doi.org/10.1109/eScience51609.2021.00056) for the scientific context:

```
O. Valkering, R. Cushing and A. Belloum (2021)
Brane: A Framework for Programmable Orchestration of Multi-Site Applications 
IEEE 17th International Conference on eScience pp. 277-282
```

## Contributing
If you're interrested in contributing, please read the [code of conduct](.github/CODE_OF_CONDUCT.md) and [contributing](.github/CONTRIBUTING.md) guide.

Bug reports and feature requests can be created in the [issue tracker](https://github.com/onnovalkering/brane/issues).

## Development
The latest version of [Rust](https://www.rust-lang.org), and the following system dependencies must be installed (assuming Ubuntu 20.04):

- build-essential
- cmake
- docker-compose
- docker.io
- libssl-dev
- musl-tools
- pkg-config

### Builds
To compile and test the complete project:
```
$ cargo build
$ cargo test
```

To build optimized versions of the binaries (`brane` and `branelet`):
```shell
$ make build-binaries
```

To build optimized versions of the services (Docker images):
```shell
$ make build-services
```
",2023-07-07 15:51:15+00:00
briefly,briefly,bloomreach/briefly,Briefly - A Python Meta-programming Library for Job Flow Control,,False,103,2023-06-26 14:46:50+00:00,2015-03-02 18:52:14+00:00,29,12,2,1,1.0,2015-03-03 19:32:27+00:00,Apache License 2.0,6,1.0,1,2015-03-03 19:32:27+00:00,2023-06-26 14:46:50+00:00,2015-06-05 22:43:47+00:00,"Briefly Job Flow Control
========================

[![Join the chat at https://gitter.im/bloomreach/briefly](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/bloomreach/briefly?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
Chou-han Yang (chyang@bloomreach.com)
(Version 1.0)

Overview
--------

Briefly is a Python based meta programming library designed to manage complex workflows with various type of tasks, such as Hadoop (local, Amazon EMR, or Qubole), Java processes, and shell commands, with a minimal and elegant [Hartman pipeline](http://en.wikipedia.org/wiki/Hartmann_pipeline) syntax. Briefly provides resource management for job execution i.e. infrastructure and operational logic so that developers can focus on their job logic.

At [BloomReach](http://www.bloomreach.com), we run thousands of hadoop jobs everyday on clusters with hundreds of machines. We use Briefly to manage our critical pipelines, and to maintain the robustness and efficiency of data processing jobs.

#### Supported types of processes

  * Simple python process
  * Local hadoop process
  * Amazon Elastic Mapreduce hadoop process
  * Qubole hadoop process
  * Java process
  * Shell process

Each process is capable of defining its own dependencies and declares its own output. The library provides a lot of default wrappers to help start off with minimal customization.

#### Features

  - Use of a [Hartman pipeline](http://en.wikipedia.org/wiki/Hartmann_pipeline) to create job flow.
  - Resource management for multiple Hadoop clusters ([Amazon EMR](http://aws.amazon.com/elasticmapreduce/), [Qubole](http://www.qubole.com/)) for parallel execution, also allowing customized Hadoop cluster.
  - Individual logs for each process to make debugging easier.
  - Fully resumable pipeline with customizable execution check and error handling.
  - Encapsulated local and remote filesystem (s3) for unified access.
  - Automatic download of files from s3 for local process and upload of files to s3 for remote process with [s4cmd](https://github.com/bloomreach/s4cmd).
  - Automatic fail/retry logic for all failed processes.
  - Automatic price upgrades for EMR clusters with spot instances.
  - Timeout for Hadoop jobs to prevent long-running clusters.
  - Dynamic cluster size adjustment for EMR steps (to be implemented)

External and Third-Party Library Requirements
---------------------------------------------

  - s4cmd (>=1.5.20): [https://github.com/bloomreach/s4cmd](https://github.com/bloomreach/s4cmd)
  - boto (>=2.30.0): [https://github.com/boto/boto](https://github.com/boto/boto)
  - Qubole SDK (>=1.4.0): [https://github.com/qubole/qds-sdk-py](https://github.com/qubole/qds-sdk-py)

Required libraries will be installed automatically with setup.py.

Installation
------------

Clone briefly from github

```
git clone https://github.com/bloomreach/briefly.git
```

Install python package

```
cd briefly
python setup.py install
```

Getting Started
---------------

### Your First Briefly Pipeline

```python
from briefly import *
from briefly.common import *

objs = Pipeline(""My first pipeline"")
prop = objs.prop

@simple_process
def dump(self):
  '''Dump all data from the source node'''
  for line in self.read():
    self.write(line)
    print line,

target = objs.source(prop.input) | sort() | dump()
objs.run(target)
```

Run your pipeline:

```shell
mkdir build
python pipeline.py -Dinput=demo.txt
```

You will see the sorted output of the file:

```
Configuring targets...
Ann     5003
Jonh    5001
Lee     5004
Smith   5002
Releasing resources...
1 total target(s), 1 executed.
```

Note that if you run it again, you won't see anything because Briefly checks the source and target date and see there is no need to generate the target again.

```
Configuring targets...
Releasing resources...
1 total target(s), 1 executed.
```

You just need to delete the content of build directory to execute it again.

### Create a Propery File for Options

As the pipeline grows more complex, we will have more configurations. It makes sense to put all pipeline configurations into a file (or multiple files). We can create demo.conf:

```
my_input = ""demo.txt""
some_opt1 = [""1"", ""2"", ""3""]
some_opt2 = {""a"": 1, ""b"": 2, ""c"": 3}
some_opt3 = true
```

The value of each option can be any valid JSON object. See next chapter for full usage on property files and how to read values out from it. Now you can execute again with *-p* option:

```
python pipeline.py -pdemo.conf
```

Multiple configuration files can be passed in and the configuration passed in later will overwrite the one load previously:

```
python pipeline.py -p demo.conf -p foo.conf -p bar.conf -Dopt1=xxx -Dopt2=yyy
```

### Chain to hadoop process in pipeline

OK, so now we have some basic pipelines running. We can add more complex processes such as hadoop process. We are going to use Hadoop's Word Count in hadoop-examples.jar. First, we have to set a property in demo.conf:

```
my_input = ""demo.txt""

# This tells Briefly to run hadoop locally. Valid options are local, emr, and qubole
hadoop.runner = ""local""
```

Now we can chain the pipeline with our first hadoop job:

```python
from briefly import *
from briefly.common import *

objs = Pipeline(""My first hadoop pipeline"")
prop = objs.prop

@simple_process
def dump(self):
  for line in self.read():
    self.write(line)
    print line,

@simple_hadoop_process
def word_count(self):
  self.config.hadoop.jar = 'hadoop-examples.jar' # path to your local jar
  self.config.defaults(
    main_class = 'wordcount', # This is special for hadoop-examples.jar. Use full class name instead.
    args = ['${input}', '${output}']
  )

target = objs.source(prop.my_input) | sort() | dump()
target2 = objs.source(prop.my_input) | word_count() | dump()

objs.run(target, target2)
```

Run it again, we will see the output:

```txt
Configuring targets...
Ann 5003
Jonh  5001
Lee 5004
Smith 5002
5001  1
5002  1
5003  1
5004  1
Ann 1
Jonh  1
Lee 1
Smith 1
Releasing resources...
2 total target(s), 2 executed.
```

Full log for each process can be found in build directory. You can found the main execution log in build/execute.log:

```txt
Running pipeline: My first pipeline
Configuring targets...
 - sort-6462031860a6f17b : executing
 - word_count-7f55d503e26321d7 : executing
 - sort-6462031860a6f17b : done
 - dump-ea94f33ba627c5f6 : executing
 - dump-ea94f33ba627c5f6 : done
 - word_count-7f55d503e26321d7 : done
 - dump-7f06c703ee1089fb : executing
 - dump-7f06c703ee1089fb : done
Releasing resources...
2 total target(s), 2 executed.
```

Congratulations! Now that you've completed the demo pipeline, it is easy to go from here to build more complex pipelines. Things you can do:

  * Create multiple property files for test, staging, and production runs
  * Put your files on Amazon S3
  * Change hadoop.runner to use Amazone EMR or Qubole
  * And more...

Pipeline Basics
---------------

Briefly pipelines always bind to a property collection. The collection provides basic settings and environment for the entire pipeline. You can set the default value and combine with the properties loaded from *-p* command line parameter to override. See properties section for more details.

Internally, Briefly creates a DAG (directed acyclic graph) to resolve the dependencies between processes.
Every function-like process in Briefly is actually a node class factory. So all the executions are deferred until you call

```python
objs.run(targets)
```

### Pipeline Creation

To create a new pipeline is simple:

```python
objs = Pipeline(""Name of your pipeline"")
prop = objs.prop
```

The pipeline constructor will also generate a new property collection, from which you can get or set values.

### Node Executions

Each process you chain in the pipeline will be augmented to a class by the decorator, such as *@simple_process*. Different types of process require different initialization. For example, a function with *@simple_hadoop_process* will just let you configure all the necessary parameters to invoke hadoop process instead of actually processing data inside the function.

The *Node* class in Briefly looks like:

```python
class Node(object):
  ...
  def configure(self):
    '''Main configuration method.
       Setup the dependencies and create hash id here.
       Child class can override this method to modify the job flow.
    '''
    ...

  def check(self):
    '''Check if we can skip the execution.
       Child class should override this function to provide
       customized check.
    '''
    ...

  def execute(self):
    '''Execute this node. Child class should override.'''
    ...
```

  * `configure()` -- setup a node for execution, rewire process flow and setup necessary parameters.
  * `check()` -- to verify if a node has been executed before by checking file timestamps, therefore can be skipped.
  * `execute()` -- actual implementation of the process execution.

### Phases of execution

There are 3 major phases for pipeline constructions and execution:

  * `DAG construction` - Follow the code execution order of the Python program, your job flow will be constructed with a DAG as internal representation. This phase runs in single thread.
  * `Node Configure` - Check if each node meets the dependency requirements for execution. Also rewire the DAG for automatic file transfer if needed. This phase runs in single thread.
  * `Node Execute` - Execute all processes with the order and precedence with the DAG. This runs with multiple threads. Each node execution will hold a thread during execution.

### Default Properties

All Briefly system configurations have default values set in **defaults.py**. This file also provides good references of what options provided by Briefly.

Property Collection
-------------------

As a job flows get more complicated, we may want to have more options to control a job flow. More and more control options and parameters will be included into the property collection. Briefly properties are designed to make complex configuration simple.

#### Features
  * Python name binding - Use python symbol name such as prop.your_options instead of `prop['your_options']`, although both methods are provided.
  * Hierarchical name space - You can have embedded collection, such as `prop.some_group.some_settings`.
  * Stackable properties - Multiple configuration files can be combined, providing a common configuration with a special one-off configuration for override.
  * Multiple value types - Augment JSON as values for parsing. This allows other platforms such as Java to read and parse configuration easily.
  * String substitution with lazy evaluation - String substitution is needed especially for path composition, such as `'s3://${your_bucket}/${your_option}/somewhere'`. This is basically the pointer between properties, which reduce the requirement of string concatenation.
  * Simple setup utility function - Take advantage of Python's keyword parameter of a function. You can simply do `prop.set(opt1=xxx, opt2=yyy)`.

### Create and Set Properties

The constructor or the Properties class can be used to set values:

```python
prop = Properties(
  a = 3,
  b = 'xxx',
  c = Properties(
    d = 5
  )
)
```

Then you can retrieve the values with several different type of method:

```python
print prop.a # ==> 3
print prop['a'] # ==> 3
print prop.c.d # ==> 5
print prop['c.d'] # ==> 5
```

Use *set()* method to set multiple values:

```python
prop.set(
  x = [1, 2, 3],
  y = true
)
```

You can also set individual properties:

```python
prop.a = 100
prpp.new_entry = 200
prop['a'] = 50
prop['c.d'] = 'abcd'
prop['new_entr2'] = 'new'
```

Or use string substitution for lazy evaluation:

```python
prop.path = '${b}/ddd/${c.d}'
print prop.path # ==> 'xxx/ddd/5'
```

### Load and Save Properties

The same properties can be loaded from or saved to a config file. The format will be,

```
opt1 = <<JSON value>>
opt2 = <<JSON value>>

group1.op1 = <<JSON value>>
group1.op2 = <<JSON value>>
```

You can also use `@import` directive to import other property files:

```
@import = ""other.conf""

foo = ""xxx""
bar = ""yyy""
```

TODO: Note that during serialization, Briefly is not going to create subgroups for now. So it is required to provide default values before load from configuration file:

```python
prop.defaults(
  opt1 = 0,
  opt2 = 0,
  group1 = Properties(
    opt1 = 0,
    opt2 = 0
  )
)
```

Unrecognized JSON string will be seen as raw strings. For example:

```
opt1 = foo bar
opt2 = 'foo bar'
```

So those values will be:

```python
print prop.opt1 # ==> ""foo bar""
print prop.opt2 # ==> ""'foo bar'""
```

Noted that JSON string always use double quote. So single quote is not valid (TODO: This may be changed).

To save a property collection back to a file simply use *save()*:

```python
prop.save('foo.conf')
```

Execution and Flow Control
--------------------------

You can use Briefly to create very complex job flows. But remember, execution will be deferred until you invoke `run()`. So during the job flow construction, you may decide to change the job flow topology based on some conditions or checks. But after the execution starts, Briefly will control all the execution assuming the job flow is already fixed. Changing the job flow topology during execution is not recommended and my cause unknown execution results or errors.

### Node Alias

All process nodes need to have a source node. The pipeline object can be used as a dummy source node if the leading process doesn't have any dependencies:

```python
target = objs | first_process() | second_process()
```

The variable `target` is going to point to the last execution node, which is `second_process`.

Each execution node can be assigned to a variable as an alias:

```python
target = objs | first_process()
target = target | second_process()
```

which is equivalent to previous topology. Sometimes, it is cleaner and more readable to create alias for each steps.

### Node Branch

Alias can also be used to create branches:

```python
target = objs | first_process()
foo = target | foo_process()
bar = target | bar_process()
```

When executing the job flow with multiple available nodes, Briefly will try to execute with the creation order. Therefore, `foo_process()` will have higher precedence than `bar_process()`. Of course, if multiple threads are available, they will be executed at the same time.

### Process Dependencies

To create dependencies between nodes, you can simply chain them. When you have multiple dependencies, you can just pass into the node creators as parameters:

```python
foo = objs | foo_process()
bar = bar | bar_process()
target = combine_process(foo, bar)
```

This way, Briefly will detect the parameters are actual nodes for execution, so `combine_process` needs to wait for the completion of `foo_process` and `bar_process`.

### Node Hash

Every process node in Briefly requires a unique hash id so that it can be used to create output files and logs. It also uses the timestamps of the files to check if a process can be skipped. Briefly also use node hash to identify same process from the same sources to avoid duplicate executing the same process against the same set of data:

```python
input = objs.source(prop.input_file) | preprocess()
target1 = input | sort() | dump()
target2 = input | sort() | dump()
```

In this case, Briefly will identify the `sort` process in two pipelines have the same sources, therefore, it is not necessary to execute `sort` process twice.

Wrappers
--------

### What are Wrappers?

To make job flow construction easier, Briefly also provides a set of wrappers to help create process node class instead of defining node classes over and over again in Python. Wrappers leverages Python's decorator syntax. Those functions will be augmented to a node class:

```python
@simple_process
def dump(self):
  '''Dump all data from the source node'''
  for line in self.read():
    self.write(line)
    print line,
```

will be augmented to (conceptually):

```python
class dump(process.SimpleProcess):
  def do_execute(self):
   for line in self.read():
     self.write(line)
     print line,
```

The definition of `@simple_process` can be found in **wrappers.py**

```python
def simple_process(func):
  '''A simple local running process.
     The wrapped function is the do_execute() of the process.
  '''
  class process_wrapper(process.SimpleProcess):
    def do_execute(self):
      func(self, *self.args, **self.kargs)

  process_wrapper.__name__ = func.__name__
  return process_wrapper
```

### Hadoop Wrapper

To invoke external hadoop process, you have to prepare several config parameters:

  * Jar file to execute
  * Main class name
  * Arguments for the job

So `@simple_hadoop_process` wrapped these inside a function so you can provide enough information for the execution:

```python
@simple_hadoop_process
def my_hadoop_job(self):
  self.config.hadoop.jar = 'hadoop-uber-jar.jar'
  self.config.defaults(
    main_class = 'com.bloomreach.HadoopJob',
    args = ['${input}', '${output}', 'foo', 'bar']
  )
```

will be augmented to (conceptually):

```python
class my_hadoop_job(hadoop.HadoopProcess):
  def configure(self):
    self.config.hadoop.jar = 'hadoop-uber-jar.jar'
    self.config.defaults(
      main_class = 'com.bloomreach.HadoopJob',
      args = ['${input}', '${output}', 'foo', 'bar']
    )
```

The actual invocation to hadoop cluster depends on the property `hadoop.runner`. So it can be used to run hadoop locally, remotely on Amazon EMR, or on Qubole.

### Java Wrapper

To execute Java process, you can set the corresponding properties for java process:

```
java.runner = ""/path/to/java""
java.max_process = 3
```

`runner` set the path to the java binary, and `max_process` controls number of concurrent Java process allowed for execution.

Then simply use `@simiple_java_process` to customize the parameters

```python
@simple_java_process
def my_java_job(self):
  self.config.defaults(
    main_class = 'com.bloomreach.SomeClass'
    args = ['args', '${input}', '${output}']
  )
```

Use `${input}` to reference to the input file from the source if you need it. And optionally, use `${output}` to save the final output to a file if the Java process doesn't output directly to console.

### Shell Wrapper

To execute shell commands, simply use `@simple_shell_process`:

```python
@simple_shell_process
def list_file(self):
  self.config.defaults(
    cmd = '/bin/ls', # binary of the executable
    timeout = 60,
    args = ['/tmp']
  )
```

Use `${output}` in `args` if the process doesn't output to console directly. Set timeout to make sure that the shell process ends correctly in given time period, otherwise, it will fail and retry. More common shell process examples can be found in **common.py**.

For some shell script with a lot of input variables, you can also sent entire property collection with environment varialbe. In this case, your shell script can easily access all the properties from Briefly process:

```python
@simple_shell_process
def shell_script_with_env(self):
  # Complex parameters going to be sent
  vars = Properties()
  vars.param1 = 'xxx'
  vars.param2 = 'yyy'

  self.config.defaults(
    cmd = 'path/to/your/script.sh',
    env = dict(os.environ.items() + vars.get_data().items()),
    args = ['arg1', 'arg2']
  )
```

Therefore, you can access `${param1}` and `${param2}` directly in the shell script.

You can also dynamically running a shell by returning a shell script from the method:

```python
@simple_shell_process
def run_dynamic_shell(self):
  self.config.defaults(
    args = ['${output}']
  )
  return '''\
#!/bin/bash
echo ""Hello, world"" > $1
'''
```

Hadoop Runner
-------------

In order to control very large hadoop clusters on Amazon EMR, Briefly also manages the clusters internally. So the complex operational issues can be isolated from the job flow logic. To control the execution of hadoop jobs, there are several properties to be set:

```
# Where to execute hadoop, valid values can be ['local', 'emr', 'qubole']
hadoop.runner = ""emr""

# Location to the jar file. Use S3 path to run hadoop on EMR or Qubole.
hadoop.jar = ""your_hadoop.jar""

# Default output path.
hadoop.root = ""path/to/default/hadoop/output""
```

### Running Hadoop Locally

As the example shown in the first section. We can set the hadoop runner to local:

```
hadoop.runner = ""local""
```

Then Briefly will invoke hadoop directly on the executing machine.

### Amazon EMR

To execute your hadoop job on Amazon EMR, you need couple of properties setup:

```
hadoop.runner = ""emr""

# Max number of concurrent EMR clusters to be created
emr.max_cluster = 10

# Instances groups for each cluster
emr.instance_groups = [[1, ""MASTER"", ""m2.2xlarge""], [9, ""CORE"", ""m2.2xlarge""]]

# Name of your EMR cluster
emr.cluster_name = ""my-emr-cluster""

# A unique name for the project for cost tracking purpose
emr.project_name = ""my-emr-project""

# Where EMR is going to put yoru log
emr.log_uri = ""s3://log-bucket/log-path/""

# EC2 key pairs if you with to login into your EMR cluster
emr.keyname = ""ec2-keypair""

# Spot instnace price upgrade strategy. The multipliers to the EC2 ondemand price you want
# to bid against the spot instances. 0 means use ondemand instances.
emr.price_upgrade_rate = [1.5, 2.0, 0]
```

You also need to set the keys for your EC2 access:

```
ec2.key = ""your_ec2_key""
ec2.secret = ""your_ec2_secret""
```

Not that the number max of EMR clusters will be bounded by the number of threads you have. Therefore, you need to increase number of threads along with `emr.max_cluster`:

```
run_threads = 16
```

Please refer to **defaults.py** for all configurations you need to run hadoop on Amazon EMR.

### Qubole

To execute your hadoop job on Qubole, you need couple of properties setup:

```
hadoop.runner = ""qubole""

# Qubole execution api token
qubole.api_token = ""qubole_token""

# EC2 access keys
qubole.aws_access_key_id = ""your_ec2_key""
qubole.aws_secret_access_key = ""your_ec2_secret""

# Max concurrent clusters
qubole.max_cluster = 10

# Max jobs per cluster
qubole.max_job_per_cluster = 1

# Project name
qubole.project = ""your-qubolep-project""

# Qubole cluster settings
qubole.hadoop_settings = { \
  ""master_instance_type"": ""m2.2xlarge"", \
  ""slave_instance_type"": ""m2.2xlarge"", \
  ""initial_nodes"": 1, \
  ""max_nodes"": 9}
```

Common Utility Processes
------------------------

Please import the utility processes from **common.py** separately.

#### cat(*sources)

Concatenate all output from all sources.

```python
foo = objs.source(prop.source1) | foo_process()
bar = objs.source(prop.source2) | bar_process()
target = cat(foo, bar)
```

#### head(limit=10)

Cut first few list from the source.

```python
target = objs.source(prop.source1) | head(limit=20)
```

#### cut(column=1, sep='\t')

Cut out specific columns from the source.

```python
target = objs.source(prop.source1) | cut(5)
```

#### sort(key='1', numeric=False, reverse=False)

Sort the source input.

```python
target = objs.source(prop.source1) | sort(reverse=True)
```

#### uniq()

Uniquify the source. The source will be sorted automatically.

```python
target = objs.source(prop.source1) | uniq()
```

#### uniq_only()

Select unique lines from the source. The source will be sorted automatically.

```python
target = objs.source(prop.source1) | uniq_only()
```

#### dup_only()

Select duplicated lines from the source. The source will be sorted automatically.

```python
target = objs.source(prop.source1) | dup_only()
```

#### uniq_count(sep='\t')

Generate the count for each unique line from source. The source will be sorted automatically.

```python
target = objs.source(prop.source1) | uniq_count()
```

#### sample(limit=10)

Reservior sampler from the source.

```python
target = objs.source(prop.source1) | sample(limit=20)
```

#### filter(predicate, inverse=False)

Filter is similar to `grep` command, but provide more versatile predicate. The predicate can be:

  * `Python function` - return True or False with given line as input.
  * `String` - Look for a substring in lines.
  * `Regular Expression` - Use regular expression to match lines.

```python
target = objs.source(prop.source1) | filter(re.compile('$www\..*\.com'))
```

#### count()

Count the number of lines in the source

```python
target = objs.source(prop.source1) | count()
```

#### diff(diff)

Compare source input with another. Invoke `diff` command.

```python
foo = objs.source(prop.source1)
target = objs.source(prop.source2).diff(foo)
```

#### join(other, f1='1', f2='1', flags=[])

Join two inputs with `join` command. Both sources will be sorted automatically.

```python
foo = objs.source(prop.source1)
target = objs.source(prop.source2).join(foo)
```

Contributors
------------

  * Chou-han Yang (chyang@bloomreach.com) - Main developer/Project owner
  * Shao-Chuan Wang (shaochuan.wang@bloomreach.com)
  * Yiran Wang (yiran.wang@bloomreach.com)
  * Kazuyuki Tanimura (kazu@bloomreach.com)
  * Sumeet Khullar (sumeet@bloomreach.com)
  * Prateek Gupta (prateek@bloomreach.com)
  * Amit Kumar (amit.kumar@bloomreach.com)
  * Ching-lun Lin (ching-lun.lin@bloomreach.com)
  * Viksit Gaur (viksit@bloomreach.com)

More Information
----------------

   Bloomreach Engineering Blog: [http://engineering.bloomreach.com/briefly-python-dsl-scale-mapreduce-pipelines/](http://engineering.bloomreach.com/briefly-python-dsl-scale-mapreduce-pipelines/)

License
-------

  Apache License Version 2.0
  http://www.apache.org/licenses/LICENSE-2.0
",2023-07-07 15:51:18+00:00
butler,butler,yakneens/butler,Butler is a framework for running scientific workflows on public and academic clouds.,,False,68,2023-01-04 17:54:31+00:00,2015-08-27 11:49:53+00:00,19,9,3,3,,,,1058,v0.0.3-alpha,3,2017-10-12 18:24:26+00:00,,2020-02-10 08:51:44+00:00,"|health| |docs| |gitter|

.. |build-status| image:: https://img.shields.io/travis/rtfd/readthedocs.org.svg?style=flat
    :alt: build status
    :scale: 100%
    :target: https://travis-ci.org/rtfd/readthedocs.org

.. |docs| image:: https://readthedocs.org/projects/butler/badge/?version=latest
    :alt: Documentation Status
    :scale: 100%
    :target: http://butler.readthedocs.io/en/latest/?badge=latest
    
.. |health| image:: https://landscape.io/github/llevar/butler/master/landscape.svg?style=flat
	:target: https://landscape.io/github/llevar/butler/master
	:alt: Code Health
   
.. |coverage| image:: https://coveralls.io/repos/github/llevar/butler/badge.svg?branch=master
	:target: https://coveralls.io/github/llevar/butler?branch=master

.. |gitter| image:: https://badges.gitter.im/butler-cloud/Lobby.svg
   	:alt: Join the chat at https://gitter.im/butler-cloud/Lobby
   	:target: https://gitter.im/butler-cloud/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. image:: docs/images/butler_logo_with_text.png
 
.. docs-include-start-marker

 .. image:: images/butler_logo_with_text.png
 
############################################################
A Framework for large-scale scientific analysis on the cloud
############################################################


.. _Terraform: http://terraform.io
.. _Saltstack: https://saltstack.com/
.. _Apache Airflow: https://airflow.incubator.apache.org/
.. _Grafana: https://grafana.com/
.. _Influxdb: https://www.influxdata.com/
.. _PostgreSQL: https://www.postgresql.org/
.. _Celery: http://www.celeryproject.org/
.. _Elasticsearch: https://www.elastic.co/
.. _Consul: https://www.consul.io
.. _CWL: http://www.commonwl.org/
.. _BWA: http://bio-bwa.sourceforge.net/
.. _freebayes: https://github.com/ekg/freebayes
.. _Pindel: http://gmt.genome.wustl.edu/packages/pindel/
.. _Delly: https://github.com/dellytools/delly
.. _R: https://cran.r-project.org/
.. _Documentation: http://butler.readthedocs.io
.. _PCAWG: https://dcc.icgc.org/pcawg
.. _EOSC: http://eoscpilot.eu/
.. _source repository: https://github.com/llevar/butler
.. _keynote: https://youtu.be/n5W3p3hN_bQ
.. _paper: https://www.nature.com/articles/s41587-019-0360-3

===============
What is Butler?
===============

Butler is a collection of tools whose goal is to aid researchers in carrying out scientific analyses on a multitude of cloud computing platforms (AWS, Openstack, Google Compute Platform, Azure, and others). 
Butler is based on many other Open Source projects such as - `Apache Airflow`_, Terraform_, Saltstack_, Grafana_, InfluxDB_, PostgreSQL_, Celery_, Elasticsearch_, Consul_, and others. 

Butler aims to be a *comprehensive* toolkit for analysing scientific data on clouds. To achieve this goal it provides functionality in four broad areas:

* **Provisioning** - Creation and teardown of clusters of Virtual Machines on various clouds.
* **Configuration Management** - Installation and configuration of software on Virtual Machines.
* **Workflow Management** - Definition and execution of distributed scientific workflows at scale.
* **Operations Management** - A set of tools for maintaining operational control of the virtualized environment as it performs work.

You can use Butler to create and execute workflows of arbitrary complexity using Python, or you can quickly wrap and execute tools that ship as Docker containers, or are described with the 
Common Workflow Language (CWL_). Butler ships with a number of ready-made workflows that have been developed in the context of large-scale cancer genomics, including:

* Genome Alignment using BWA_ 
* Germline and Somatic SNV detection and genotyping using freebayes_, Pindel_, and other tools
* Germline and Somatic SV detection and genotyping using Delly_
* Variant filtering
* R_ data analysis

A typical Butler deployment looks like this:

.. image:: docs/images/embassy_butler_deployment_architecture.png

It can look like a bit of a tangle but is actually fairly simple: The Salt Master configures and installs software, 
the Tracker schedules workflows and puts them into a RabbitMQ queue keeping track of their state in a database, 
a fleet of Workers pick up workflow tasks and execute them, the Monitoring Server harvests logs and metrics
from everything and visualizes them on graphical dashboards. That's about it. Many more details about how
everything works can be found in the Documentation_.



================
Who uses Butler?
================

* The **Pan Cancer Analysis of Whole Genomes Project** (PCAWG_) - used Butler to run cancer genomics workflows on 2800+ high-coverage whole genome samples (725 TB of data) on Openstack.
* The **European Open Science Cloud Pilot Project** (EOSC_) - using Butler to run cancer genomics workflows on multiple platforms (Openstack, AWS).
* The **Pan Prostate Cancer Group** - using Butler to run cancer genomics workflows on 2000+ whole genome prostate cancer samples on Openstack. 

===============
Getting Started
===============

To get started with Butler you need the following:

* A target cloud computing environment.
* Some data.
* An analysis you want to perform (programs, scripts, etc.).
* The Butler `source repository`_.

The general sequence of steps you will use with Butler is as follows:

* Install Terraform_ on your local machine
* Clone the Butler Github repository
* Populate cloud provider credentials
* Select deployment parameters (VM flavours, networking and security settings, number of workers, etc.)
* Deploy Butler cluster onto your cloud provider
* Use Saltstack_ to configure and deploy all of the necessary software that is used by Butler (this is highly automated)
* Register some workflows with your Butler deployment
* Register and configure an analysis (what workflow do you want to run on what data)
* Launch your analysis
* Monitor the progress of the analysis and the health of your infrastructure using a variety of dashboards

.. docs-include-end-marker

==========
Next Steps
==========
Head over to the Documentation_ to learn about how to use Butler for your project.

Watch the keynote_ presentation by Sergei Yakneen from the de.NBI Cloud Computing Summer School in Giessen, Germany, from June 2017 that describes Butler.

Read the Butler paper_ in Nature Biotechnology.

.. image:: docs/images/de.NBI_Bild.png
   :scale: 20%


",2023-07-07 15:51:23+00:00
cadence,cadence,uber/cadence,"Cadence is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous long-running business logic in a scalable and resilient way.",https://cadenceworkflow.io,False,6986,2023-07-07 05:10:41+00:00,2017-02-21 16:10:18+00:00,706,1494,116,72,v1.0.0,2023-03-17 18:30:01+00:00,MIT License,3337,v1.0.0,83,2023-03-17 18:30:01+00:00,2023-07-07 13:50:58+00:00,2023-06-30 01:10:20+00:00,"# Cadence
[![Build Status](https://badge.buildkite.com/159887afd42000f11126f85237317d4090de97b26c287ebc40.svg?theme=github&branch=master)](https://buildkite.com/uberopensource/cadence-server)
[![Coverage Status](https://coveralls.io/repos/github/uber/cadence/badge.svg)](https://coveralls.io/github/uber/cadence)
[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://t.uber.com/cadence-slack)

This repo contains the source code of the Cadence server and other tooling including CLI, schema tools, bench and canary. 

You can implement your workflows with one of our client libraries. 
The [Go](https://github.com/uber-go/cadence-client) and [Java](https://github.com/uber-java/cadence-client) libraries are officially maintained by the Cadence team, 
while the [Python](https://github.com/firdaus/cadence-python) and [Ruby](https://github.com/coinbase/cadence-ruby) client libraries are developed by the community.

You can also use [iWF](https://github.com/indeedeng/iwf) as a DSL framework on top of Cadence. 

See Maxim's talk at [Data@Scale Conference](https://atscaleconference.com/videos/cadence-microservice-architecture-beyond-requestreply) for an architectural overview of Cadence.

Visit [cadenceworkflow.io](https://cadenceworkflow.io) to learn more about Cadence. Join us in [Cadence Documentation](https://github.com/uber/cadence-docs) project. Feel free to raise an Issue or Pull Request there.

### Community 
* [Github Discussion](https://github.com/uber/cadence/discussions)
  * Best for Q&A, support/help, general discusion, and annoucement 
* [StackOverflow](https://stackoverflow.com/questions/tagged/cadence-workflow)
  * Best for Q&A and general discusion
* [Github Issues](https://github.com/uber/cadence/issues)
  * Best for reporting bugs and feature requests
* [Slack](http://t.uber.com/cadence-slack)
  * Best for contributing/development discussion 
  
## Getting Started

### Start the cadence-server

To run Cadence services locally, we highly recommend that you use [Cadence service docker](docker/README.md) to run the service.
You can also follow the [instructions](./CONTRIBUTING.md) to build and run it. 

Please visit our [documentation](https://cadenceworkflow.io/docs/operation-guide/) site for production/cluster setup.

### Run the Samples

Try out the sample recipes for [Go](https://github.com/uber-common/cadence-samples) or [Java](https://github.com/uber/cadence-java-samples) to get started.

### Use [Cadence CLI](https://cadenceworkflow.io/docs/cli/) 

Cadence CLI can be used to operate workflows, tasklist, domain and even the clusters.

You can use the following ways to install Cadence CLI:
* Use brew to install CLI: `brew install cadence-workflow`
  * Follow the [instructions](https://github.com/uber/cadence/discussions/4457) if you need to install older versions of CLI via homebrew. Usually this is only needed when you are running a server of a too old version.
* Use docker image for CLI: `docker run --rm ubercadence/cli:<releaseVersion>`  or `docker run --rm ubercadence/cli:master ` . Be sure to update your image when you want to try new features: `docker pull ubercadence/cli:master `
* Build the CLI binary yourself, check out the repo and run `make cadence` to build all tools. See [CONTRIBUTING](CONTRIBUTING.md) for prerequisite of make command.
* Build the CLI image yourself, see [instructions](docker/README.md#diy-building-an-image-for-any-tag-or-branch)
  
Cadence CLI is a powerful tool. The commands are organized by **tabs**. E.g. `workflow`->`batch`->`start`, or `admin`->`workflow`->`describe`.

Please read the [documentation](https://cadenceworkflow.io/docs/cli/#documentation) and always try out `--help` on any tab to learn & explore.  
  
### Use Cadence Web

Try out [Cadence Web UI](https://github.com/uber/cadence-web) to view your workflows on Cadence.
(This is already available at localhost:8088 if you run Cadence with docker compose)


## Contributing

We'd love your help in making Cadence great. Please review our [contribution guide](CONTRIBUTING.md).

If you'd like to propose a new feature, first join the [Slack channel](http://t.uber.com/cadence-slack) to start a discussion and check if there are existing design discussions. Also peruse our [design docs](docs/design/index.md) in case a feature has been designed but not yet implemented. Once you're sure the proposal is not covered elsewhere, please follow our [proposal instructions](PROPOSALS.md).

## Other binaries in this repo

#### Bench/stress test workflow tools
See [bench documentation](./bench/README.md). 

#### Periodical feature health check workflow tools(aka Canary)
See [canary documentation](./canary/README.md).

#### Schema tools for SQL and Cassandra
The tools are for [manual setup or upgrading database schema](docs/persistence.md)  
 
  * If server runs with Cassandra, Use [Cadence Cassandra tool](tools/cassandra/README.md) 
  * If server runs with SQL database, Use [Cadence SQL tool](tools/sql/README.md) 

The easiest way to get the schema tool is via homebrew.

`brew install cadence-workflow` also includes `cadence-sql-tool` and `cadence-cassandra-tool`. 
 * The schema files are located at `/usr/local/etc/cadence/schema/`.
 * To upgrade, make sure you remove the old ElasticSearch schema first: `mv /usr/local/etc/cadence/schema/elasticsearch /usr/local/etc/cadence/schema/elasticsearch.old && brew upgrade cadence-workflow`. Otherwise ElasticSearch schemas may not be able to get updated.
 * Follow the [instructions](https://github.com/uber/cadence/discussions/4457) if you need to install older versions of schema tools via homebrew. 
 However, easier way is to use new versions of schema tools with old versions of schemas. 
 All you need is to check out the older version of schemas from this repo. Run `git checkout v0.21.3` to get the v0.21.3 schemas in [the schema folder](/schema). 
 
   
## License

MIT License, please see [LICENSE](https://github.com/uber/cadence/blob/master/LICENSE) for details.

",2023-07-07 15:51:26+00:00
cascading,cascading,cwensel/cascading,Cascading is a feature rich API for defining and executing complex and fault tolerant data processing flows locally or on a cluster.,http://cascading.wensel.net/,False,337,2023-05-30 16:08:12+00:00,2009-02-04 17:52:06+00:00,228,35,28,0,,,Other,2514,wip-366,832,2012-12-11 15:37:13+00:00,2023-06-14 22:10:00+00:00,2022-08-31 03:10:32+00:00,"# Cascading

Thanks for using [Cascading](https://cascading.wensel.net/).

## Cascading 4.5

Cascading 4.5 includes a few major changes and additions from prior major releases:

* Support for Apache Hadoop 3. and Apache Tez 0.10.x
* Removed Apache Hadoop 2.x and Apache Tez 0.8.x support

Cascading 4 includes a few major changes and additions from prior major releases:

* Local mode support improved for production use
* Moved the website to https://cascading.wensel.net/
* Changed the Maven group name to `net.wensel` (from `cascading`)
* Moved to GitHub Packages (WIPs) and Maven Central (releases)
* Added native JSON support via the `cascading-nested-json` sub-project
* Removed `cascading-xml` sub-project
* Removed Apache Hadoop 1.x support
* Imported deprecated Apache Parquet classes to retain support and provide enhancements

## General Information:

For current WIP releases, go to: https://github.com/cwensel?tab=packages&repo_name=cascading

For project documentation and community support, visit: [cascading.wensel.net](https://cascading.wensel.net/)

The project includes nine Cascading jar files:

* `cascading-core-x.y.z.jar`                   - all Cascading Core class files
* `cascading-expression-x.y.z.jar`             - all Cascading Janino expression operations class files
* `cascading-nested-json-x.y.z.jar`            - all Cascading JSON operations
* `cascading-nested-x.y.z.jar`                 - all Cascading base classes for nested data-type operations
* `cascading-local-x.y.z.jar`                  - all Cascading Local in-memory mode class files
* `cascading-local-kafka-x.y.z.jar`            - all Cascading Local support for Apache Kafka
* `cascading-local-neo4j-x.y.z.jar`            - all Cascading Local support for Neo4j
* `cascading-local-s3-x.y.z.jar`               - all Cascading Local support for AWS S3
* `cascading-local-splunk-x.y.z.jar`           - all Cascading Local support for Splunk
* `cascading-local-hadoop3-io-x.y.z.jar`       - all Cascading Local in-memory mode class files used with Hadoop
* `cascading-hadoop3-common-x.y.z.jar`         - all Cascading Hadoop 3.x common class files
* `cascading-hadoop3-io-x.y.z.jar`             - all Cascading Hadoop 3.x HDFS and IO related class files
* `cascading-hadoop3-mr1-x.y.z.jar`            - all Cascading Hadoop 3.x MapReduce mode class files
* `cascading-hadoop3-tez-x.y.z.jar`            - all Cascading Hadoop 3.x Tez mode class files
* `cascading-hadoop3-tez-stats-x.y.z.jar`      - all Cascading Tez YARN timeline server class files
* `cascading-hadoop3-parquet-x.y.z.jar`        - all Cascading Parquet class files
* `cascading-hadoop3-parquet-thrift-x.y.z.jar` - all Cascading Parquet Thrift class files

These class jars, along with, tests, source and javadoc jars, are all available via the Maven repository.

Local mode is where the Cascading application will run locally in memory without cluster distribution. This
implementation has minimal to no robustness in low memory situations, by design.

Hadoop 3.x MR1 mode is for running on Hadoop 3.x releases.

Hadoop 3.x Tez mode is where the Cascading application should run on an Apache Tez *DAG* cluster.

As of Cascading 4.x, all above jar files are built against Java 1.8. Prior versions of Cascading are built against Java
1.7 and 1.6.

## Local Mode

Local mode has been much improved for production use in applications that do not need to run distributed across a
cluster. Specifically in applications that trivially parallelize and run within AWS Lambda or Batch applications.

See https://github.com/cwensel/cascading-local for a collection of local mode integrations.

Note this project will merge into Cascading in then next minor release.

## Extensions, the SDK, and DSLs

There are a number of projects based on and extensions to Cascading available.

Visit https://cascading.org/ for links. As these projects are updated to depend on 4.x, we will update the main site.

Note many projects built and released against Cascading 3.x will work without modification with Cascading 4.x.

## Versioning

Cascading stable releases are always of the form `x.y.z`, where `z` is the current maintenance release.

`x.y.z` releases are maintenance releases. No public incompatible API changes will be made, but in an effort to fix
bugs, remediation may entail throwing new Exceptions.

`x.y` releases are minor releases. New features are added. No public incompatible API changes will be made on the core
processing APIs (Pipes, Functions, etc), but in an effort to resolve inconsistencies, minor semantic changes may be
necessary.

It is important to note that *we do reserve to make breaking changes to the new query planner API through the 4.x
releases*. This allows us to respond to bugs and performance issues without issuing new major releases.

All releases will be maintained here:
[https://github.com/cwensel/cascading](https://github.com/cwensel/cascading)

WIP (work in progress) releases are fully tested builds of code not yet deemed fully stable. On every build by our
continuous integration servers, the WIP build number is increased. Successful builds are then tagged and published.

The WIP releases are always of the form `x.y.z-wip-n`, where `x.y.z` will be the next stable release version the WIP
releases are leading up to. `n` is the current successfully tested build.

The source, working branches, and tags for all WIP releases can be found here:
[https://github.com/cwensel/cascading](https://github.com/cwensel/cascading)

When a WIP is deemed stable and ready for production use, it will be published as a `x.y.z` release, and made available
from Maven Central.

## Writing and Running Tests

Comprehensive tests should be written against the `cascading.PlatformTestCase`.

When running tests built against the PlatformTestCase, the local cluster can be disabled (if enabled by the test)
by setting:

    -Dtest.cluster.enabled=false

From Gradle, to run a single test case:

    > ./gradlew :cascading-hadoop3-mr1:platformTest --tests=*.FieldedPipesPlatformTest -i

or a single test method:

    > ./gradlew :cascading-hadoop3-mr1:platformTest --tests=*.FieldedPipesPlatformTest.testNoGroup -i

## Debugging the Planner

When running tests, set the following

    -Dtest.traceplan.enabled=true

If you are on Mac OS X and have installed GraphViz, dot files can be converted to pdf on the fly. To enable, set:

    -Dutil.dot.to.pdf.enabled=true

Optionally, for stand alone applications, statistics and tracing can be enabled selectively with the following
properties:

* `cascading.planner.stats.path` - outputs detailed statistics on time spent by the planner
* `cascading.planner.plan.path` - basic planner information
* `cascading.planner.plan.transforms.path` - detailed information for each rule

## Contributing and Reporting Issues

See __CONTRIBUTING.md__ at https://github.com/Cascading/cascading.

## Using with Maven/Ivy

It is strongly recommended developers pull Cascading from Maven Central.

Alternatively, see GitHub Packages for latest WIP releases:

* https://maven.pkg.github.com/cwensel/cascading

When creating tests, make sure to add any of the relevant above dependencies to your `test` scope or equivalent
configuration along with the `cascading-platform` dependency.

Note the `cascading-platform` compile dependency has no classes, you must pull the tests dependency with the
`tests` classifier.

Source and Javadoc artifacts (using the appropriate classifier) are also available through Maven.

Note that `cascading-hadoop3-mr1`, and `cascading-hadoop3-tez` have a `provided` dependency on the Hadoop jars so that
it won't get sucked into any application packaging as a dependency, typically.

## Building and IDE Integration

For most cases, building Cascading is unnecessary as it has been pre-built, tested, and published to our Maven
repository (above).

To build Cascading, run the following in the shell:

```bash
> git clone https://github.com/cwensel/cascading.git
> cd cascading
> ./gradlew build
```

## Using with Apache Hadoop

First confirm you are using a supported version of Apache Hadoop by checking the
[Compatibility](https://cascading.wensel.net/support/compatibility/) page.

To use Cascading with Hadoop, we suggest stuffing `cascading-core` and `cascading-hadoop3-mr1`, jar files and all
third-party libs into the `lib` folder of your job jar and executing your job via
`$HADOOP_HOME/bin/hadoop jar your.jar <your args>`.

For example, your job jar would look like this (via: `jar -t your.jar`)

```bash
/<all your class and resource files>
/lib/cascading-core-x.y.z.jar
/lib/cascading-hadoop3-common-x.y.z.jar
/lib/cascading-hadoop3-mr1-x.y.z.jar
/lib/cascading-hadoop3-io-x.y.z.jar
/lib/cascading-expression-x.y.z.jar
/lib/cascading-nested-json-x.y.z.jar
/lib/<cascading third-party jar files>
```

Hadoop will unpack the jar locally and remotely (in the cluster) and add any libraries in `lib` to the classpath. This
is a feature specific to Hadoop.

## History and Status

Cascading was started in 2007 by Chris K Wensel.

After a series of acquisitions, it was left unsupported and unmaintained by the copyright, domain name, and GitHub
organization owners.

Chris has since continued his noodling with Cascading and has been pushing changes to the original repo.

Cascading remains under the Apache License v2.0.",2023-07-07 15:51:32+00:00
cctools,cctools,cooperative-computing-lab/cctools,"The Cooperative Computing Tools (cctools) enable large scale distributed computations to harness hundreds to thousands of machines from clusters, clouds, and grids.",http://ccl.cse.nd.edu,False,116,2023-06-12 21:09:15+00:00,2013-03-13 19:39:45+00:00,106,19,63,94,release/7.6.0,2023-07-06 18:30:37+00:00,Other,10952,tutorial/XSEDE-2013,316,2013-07-19 19:24:44+00:00,2023-07-07 15:48:25+00:00,2023-07-06 20:01:43+00:00,"
The Cooperative Computing Tools (cctools) is a software
package for enabling large scale distributed computing
on clusters, clouds, and grids.  It is used primarily for
attacking large scale problems in science and engineering.

You can read more about this software at:
   http://ccl.cse.nd.edu/software

The software is developed by members of the Cooperative
Computing Lab at the University of Notre Dame, led by
Prof. Douglas Thain.  The file CREDITS lists the many
people that have contributed to the software over the years.

For complete instructions on how to use the software, see here:
   https://cctools.readthedocs.io/en/latest/

CCTools can be installed via Conda with:

    conda install -y -c conda-forge ndcctools

You can also install via Spack or build from source.
Full installation instructions are available here:
   https://cctools.readthedocs.io/en/latest/install/

If you are impatient, the following commands will build
and install the software in your home directory:

   ./configure --prefix ${HOME}/cctools
   make clean
   make install
   export PATH=${PATH}:${HOME}/cctools/bin
   chirp_status

------------------------------------------------------------
This software package is
Copyright (c) 2003-2004 Douglas Thain
Copyright (c) 2005-2019 The University of Notre Dame
This software is distributed under the GNU General Public License.
See the file COPYING for details.
------------------------------------------------------------
This product includes software developed by and/or derived
from the Globus Project (http://www.globus.org/)
to which the U.S. Government retains certain rights.
------------------------------------------------------------
This product includes code derived from the RSA Data
Security, Inc. MD5 Message-Digest Algorithm.
------------------------------------------------------------
This product includes public domain code for the
SHA1 algorithm written by Peter Gutmann, David Ireland,
and A. M. Kutchman.
------------------------------------------------------------
This product includes the source code for the MT19937-64
Mersenne Twister pseudorandom number generator, written by 
Makoto Matsumoto and Takuji Nishimura.


",2023-07-07 15:51:36+00:00
cgat-core,cgat-core,cgat-developers/cgat-core,Core functionality of the CGAT code,,False,30,2022-03-29 10:29:00+00:00,2018-02-09 08:38:20+00:00,12,7,9,17,v0.6.15,2023-02-08 20:50:01+00:00,MIT License,567,v0.6.15,28,2023-02-08 20:50:01+00:00,2023-07-04 09:15:52+00:00,2023-02-08 20:50:01+00:00,"
![CGAT-core](https://github.com/cgat-developers/cgat-core/blob/master/docs/img/CGAT_logo.png)
----------------------------------------

<p align=""left"">
	<a href=""https://readthedocs.org/projects/cgat-core/badge/?version=latest"", alt=""Documentation"">
		<img src=""https://readthedocs.org/projects/cgat-core/badge/?version=latest"" /></a>
	<a href=""https://travis-ci.org/cgat-developers/cgat-core"", alt=""Travis"">
		<img src=""https://img.shields.io/travis/cgat-developers/cgat-core.svg"" /></a>
	<a href=""https://twitter.com/cgat_oxford?lang=en"", alt=""Twitter followers"">
		<img src=""https://img.shields.io/twitter/url/http/shields.io.svg?style=social&logo=twitter"" /></a>
	<a href=""https://twitter.com/cgat_oxford?lang=en"", alt=""Twitter followers"">
		<img src=""https://img.shields.io/twitter/url/http/shields.io.svg?style=social&logo=twitter"" /></a>
</p>

----------------------------------------

CGAT-core is a workflow management system that allows users to quickly and reproducibly build scalable
data analysis pipelines. CGAT-core is a set of libraries and helper functions used to enable researchers
to design and build computational workflows for the analysis of large-scale data-analysis. 

Documentation for CGAT-core can be accessed at [read the docs](http://cgat-core.readthedocs.io/en/latest/) 

Used in combination with CGAT-apps, we have demonstrated the functionality of our
flexible implementation using a set of well documented, easy to install and easy to use workflows, 
called [CGAT-flow](https://github.com/cgat-developers/cgat-flow) ([Documentation](https://www.cgat.org/downloads/public/cgatpipelines/documentation/)).

CGAT-core is open-sourced, powerful and user-friendly, and has been continually developed 
as a Next Generation Sequencing (NGS) workflow management system over the past 10 years.


Installation
============

The following sections describe how to install the [cgatcore](https://cgat-core.readthedocs.io/en/latest/index.html) framework. For instructions on how to install
our other repos, CGAT-apps (scripts) and CGAT-flow (workflows/pipelines), please follow these instructions [here](https://www.cgat.org/downloads/public/cgatpipelines/documentation/InstallingPipelines.html).

The preferred method to install the cgatcore is using conda, by following the instructions on [read the docs](https://cgat-core.readthedocs.io/en/latest/getting_started/Installation.html). However, there are a few other methods to install cgatcore, including pip and our own bash script installer.

Linux vs OS X
=============

* ulimit works as expected in Linux but it does not have an effect on OS X. [Disabled](https://github.com/cgat-developers/cgat-core/commit/d4d9b9fb75525873b291028a622aac70c44a5065) ulimit tests for OS X.

* ssh.connect times out in OSX. Exception [caught](https://github.com/cgat-developers/cgat-core/commit/d4d9b9fb75525873b291028a622aac70c44a5065)

* Linux uses /proc/meminfo and OS X uses [vm_stat](https://github.com/cgat-developers/cgat-core/compare/bb1c75df8f42...575f0699b326)

* Currently our testing framework is broken for OSX, however we are working to fix this. However, we dont envisage any issues running the code at present.

",2023-07-07 15:51:40+00:00
cgat-ruffus,ruffus,cgat-developers/ruffus,CGAT-ruffus is a lightweight python module for running computational pipelines,,False,172,2023-05-21 21:12:00+00:00,2013-01-06 22:20:24+00:00,34,12,17,5,v2.8.4,2020-04-24 15:06:47+00:00,MIT License,1308,v2.8.4,7,2020-04-24 15:06:47+00:00,2023-05-21 21:11:59+00:00,2021-07-12 08:12:28+00:00,"===========
CGAT-ruffus
===========


***************************************
Overview
***************************************

The ruffus module is a lightweight way to add support
for running computational pipelines.

Computational pipelines are often conceptually quite simple, especially
if we breakdown the process into simple stages, or separate **tasks**.

Each stage or **task** in a computational pipeline is represented by a python function
Each python function can be called in parallel to run multiple **jobs**.

Ruffus was originally designed for use in bioinformatics to analyse multiple genome
data sets.

More recently, we have extended the functionality of CGAT-ruffus to include cluster integration (Currently
support SGE, SLURM and PBS-pro/Torque), paramaterisation, logging, database integration
and conda environment switching. `CGAT-core <https://github.com/cgat-developers/cgat-core>`_ code and `documentation <https://cgat-core.readthedocs.io/en/latest/>`_.

***************************************
Documentation
***************************************

Ruffus documentation can be found `here <https://cgat-ruffus.readthedocs.io/en/latest/>`_ ,
with `installation notes <https://cgat-ruffus.readthedocs.io/en/latest/installation.html>`_ , and
an `in-depth manual <https://cgat-ruffus.readthedocs.io/en/latest/tutorials/new_tutorial/manual_contents.html>`_ .

However, to utilise the full power of this workflow management system we recomend
using `CGAT-core <https://github.com/cgat-developers/cgat-core>`_ (`documentation <https://cgat-core.readthedocs.io/en/latest/>`_).

***************************************
Background
***************************************

The purpose of a pipeline is to determine automatically which parts of a multi-stage
process needs to be run and in what order in order to reach an objective (""targets"")

Computational pipelines, especially for analysing large scientific datasets are
in widespread use.
However, even a conceptually simple series of steps can be difficult to set up and
to maintain, perhaps because the right tools are not available.

***************************************
Design
***************************************

The ruffus module has the following design goals:

* Simplicity. Can be picked up in 10 minutes
* Elegance
* Lightweight
* Unintrusive
* Flexible/Powerful

***************************************
Features
***************************************

Automatic support for

* Managing dependencies
* Parallel jobs
* Re-starting from arbitrary points, especially after errors
* Display of the pipeline as a flowchart
* Reporting

***************************************
A Simple example
***************************************

Use the **@transform(...)** python decorator before the function definitions:

  .. code-block:: python

    from ruffus import *

    # make 10 dummy DNA data files
    data_files = [(prefix + "".fastq"") for prefix in range(""abcdefghij"")]
    for df in data_files:
        open(df, ""w"").close()


    @transform(data_files, suffix("".fastq""), "".bam"")
    def run_bwa(input_file, output_file):
        print ""Align DNA sequences in %s to a genome -> %s "" % (input_file, output_file)
        # make dummy output file
        open(output_file, ""w"").close()


    @transform(run_bwa, suffix("".bam""), "".sorted.bam"")
    def sort_bam(input_file, output_file):
        print ""Sort DNA sequences in %s -> %s "" % (input_file, output_file)
        # make dummy output file
        open(output_file, ""w"").close()

    pipeline_run([sort_bam], multithread = 5)


the ``@transform`` decorator indicate that the data flows from the ``run_bwa`` function to ``sort_bwa`` down
the pipeline.

********
Usage
********

Each stage or **task** in a computational pipeline is represented by a python function
Each python function can be called in parallel to run multiple **jobs**.

1. Import module::

        import ruffus


1. Annotate functions with python decorators

2. Print dependency graph if you necessary

    - For a graphical flowchart in ``jpg``, ``svg``, ``dot``, ``png``, ``ps``, ``gif`` formats::

        pipeline_printout_graph (""flowchart.svg"")

    This requires ``dot`` to be installed

    - For a text printout of all jobs ::

        pipeline_printout(sys.stdout)


3. Run the pipeline::

    pipeline_run(list_of_target_tasks, verbose = NNN, [multithread | multiprocess = NNN])
",2023-07-07 15:51:44+00:00
chipster,chipster,chipster/chipster,Chipster is a user-friendly analysis software for high-throughput data.,,True,34,2023-01-28 01:59:19+00:00,2013-03-04 11:03:05+00:00,13,21,16,0,,,MIT License,8824,vm-3.11.2,246,2017-01-12 08:48:30+00:00,,2020-12-17 14:59:24+00:00,"Chipster v3 has reached end-of-life
-----------------------------------

Up to version 3.x Chipster was a desktop application requiring Java Web Start. 
Chipster version 4 is a web application which runs on your browser. 

Here are some links to get started with the new Chipster v4:
- Main website: https://chipster.csc.fi
- Installation instructions: https://github.com/chipster/chipster-openshift/tree/k3s/k3s
- Source code: https://github.com/chipster

Chipster Source Package
-----------------------

Out-of-the-box this project can be built with Ant and used with Eclipse.
Source package structure follows Maven layout, but this is not a Maven project. 
Unlike in Maven, xternal dependencies are located in ""ext"" directory.

This is a version of Chipster which is supposed to be running with Chipster Jog Manager component.

For license, see LICENSE.TXT.


Building
--------

For building options, use ""ant -p"" in project root directory.

For complete build, use ""ant"". Please note that you need to have keystore 
available because client JAR needs to be signed. Any key will do, so 
you can use ""keytool"" to generate your signing key.

To build a freshly checked out project directory, go to command line and
in the project directory give following command:

keytool -genkey -alias chipster -keystore keystore.ks -storepass chipster -validity 1825

Fill in your data to keytool command parameters. You might want to use your organisations name 
also as your own name as it is shown by Java Web Start. Use the keystore password
also as a key password.

Next, issue command ""ant"", and the project will be built and packages will be available in ""dist""
directory.

If you do not want to place your keystore in the project directory, you can create
a file called alternative-keystore-path.txt and place path to your keystore into it.
Do not enter a newline after the path! The build script will read that file and
use the alternative path into keystore. This is strongly recommended if you do also 
commits into the public version repository, because placing a private keystore file into 
project directory creates a risk of publishing it by accident.
",2023-07-07 15:51:49+00:00
chronos,chronos,mesos/chronos,Fault tolerant job scheduler for Mesos which handles dependencies and ISO8601 based schedules,http://mesos.github.io/chronos/,False,4363,2023-07-07 12:49:05+00:00,2013-02-13 22:59:14+00:00,542,293,96,11,2.5.0,2017-06-28 18:23:05+00:00,Apache License 2.0,1140,v3.0.2,17,2017-02-24 02:36:02+00:00,2023-07-07 12:49:05+00:00,2018-09-06 16:47:59+00:00,"# Chronos [![Build Status](https://travis-ci.org/mesos/chronos.svg?branch=master)](https://travis-ci.org/mesos/chronos)
Chronos is a replacement for `cron`. It is a distributed and fault-tolerant scheduler that runs on top of [Apache Mesos][mesos] that can be used for job orchestration.  It supports custom Mesos executors as well
as the default command executor. Thus by default, Chronos executes `sh`
(on most systems bash) scripts.

Chronos can be used to interact with systems such as Hadoop (incl. EMR), even if the Mesos agents on which execution happens do not have Hadoop installed. Included wrapper scripts allow transfering files and executing them on a remote machine in the background and using asynchronous callbacks to notify Chronos of job completion or failures. Chronos is also natively able to schedule jobs that run inside Docker containers.

Chronos has a number of advantages over regular cron.
It allows you to schedule your jobs using [ISO8601][ISO8601] repeating interval notation, which enables more flexibility in job scheduling. Chronos also supports the definition of jobs triggered by the completion of other jobs. It supports arbitrarily long dependency chains.

*The easiest way to use Chronos is to use [DC/OS](https://dcos.io/get-started/) and install chronos via the universe.*


## Features

* Web UI
* ISO8601 Repeating Interval Notation
* Handles dependencies
* Job Stats (e.g. 50th, 75th, 95th and 99th percentile timing, failure/success)
* Job History (e.g. job duration, start time, end time, failure/success)
* Fault Tolerance (leader/follower)
* Configurable Retries
* Multiple Workers (i.e. Mesos agents)
* Native Docker support

## Documentation and Support

Chronos documentation is available on the [Chronos GitHub pages site](https://mesos.github.io/chronos/).

Documentation for installing and configuring the full Mesosphere stack including Mesos and Chronos is available on the [Mesosphere website](https://docs.mesosphere.com).

For questions and discussions around Chronos, please use the Google Group ""chronos-scheduler"":
[Chronos Scheduler Group](https://groups.google.com/forum/#!forum/chronos-scheduler).

If you'd like to take part in design research and test new features in Chronos before they're released, please add your name to Mesosphere's [UX Research](http://uxresearch.mesosphere.com) list.

## Packaging

Mesosphere publishes Docker images for Chronos to Dockerhub, at <https://hub.docker.com/r/mesosphere/chronos/>.

## Contributing

Instructions on how to contribute to Chronos are available on the [Contributing](http://mesos.github.io/chronos/docs/contributing.html) docs page.

## License

The use and distribution terms for this software are covered by the
Apache 2.0 License (http://www.apache.org/licenses/LICENSE-2.0.html)
which can be found in the file LICENSE at the root of this distribution.
By using this software in any fashion, you are agreeing to be bound by
the terms of this license.
You must not remove this notice, or any other, from this software.

## Contributors

* Florian Leibert ([@flo](http://twitter.com/flo))
* Andy Kramolisch ([@andykram](https://github.com/andykram))
* Harry Shoff ([@hshoff](https://twitter.com/hshoff))
* Elizabeth Lingg

## Reporting Bugs

Please see the [support page](http://mesos.github.io/chronos/support.html) for information on how to report bugs.

[ISO8601]: http://en.wikipedia.org/wiki/ISO_8601 ""ISO8601 Standard""
[mesos]: https://mesos.apache.org/ ""Apache Mesos""
",2023-07-07 15:51:52+00:00
ckan,ckan,ckan/ckan,"CKAN is an open-source DMS (data management system) for powering data hubs and data portals. CKAN makes it easy to publish, share and use data. It powers catalog.data.gov, open.canada.ca/data, data.humdata.org among many other sites.",https://ckan.org/,False,3869,2023-07-06 15:38:08+00:00,2011-11-10 18:42:17+00:00,1899,198,278,2,ckan-2.10.1,2023-05-24 11:38:37+00:00,Other,24880,demo-0.2,114,2012-08-01 09:06:31+00:00,2023-07-07 15:12:38+00:00,2023-07-06 13:07:26+00:00,"CKAN: The Open Source Data Portal Software
==========================================

.. image:: https://img.shields.io/badge/license-AGPL-blue.svg?style=flat
    :target: https://opensource.org/licenses/AGPL-3.0
    :alt: License

.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat
    :target: http://docs.ckan.org
    :alt: Documentation
.. image:: https://img.shields.io/badge/support-StackOverflow-yellowgreen.svg?style=flat
    :target: https://stackoverflow.com/questions/tagged/ckan
    :alt: Support on StackOverflow

.. image:: https://circleci.com/gh/ckan/ckan.svg?style=shield
    :target: https://circleci.com/gh/ckan/ckan
    :alt: Build Status

.. image:: https://coveralls.io/repos/github/ckan/ckan/badge.svg?branch=master
    :target: https://coveralls.io/github/ckan/ckan?branch=master
    :alt: Coverage Status

.. image:: https://badges.gitter.im/gitterHQ/gitter.svg
    :target: https://gitter.im/ckan/chat
    :alt: Chat on Gitter

**CKAN is the world’s leading open-source data portal platform**.
CKAN makes it easy to publish, share and work with data. It's a data management
system that provides a powerful platform for cataloging, storing and accessing
datasets with a rich front-end, full API (for both data and catalog), visualization
tools and more. Read more at `ckan.org <http://ckan.org/>`_.


Installation
------------

See the `CKAN Documentation <http://docs.ckan.org>`_ for installation instructions.


Support
-------
If you need help with CKAN or want to ask a question, use either the
`ckan-dev`_ mailing list, the `CKAN chat on Gitter`_, or the `CKAN tag on Stack Overflow`_ (try
searching the Stack Overflow and ckan-dev `archives`_ for an answer to your
question first).

If you've found a bug in CKAN, open a new issue on CKAN's `GitHub Issues`_ (try
searching first to see if there's already an issue for your bug).

If you find a potential security vulnerability please email security@ckan.org,
rather than creating a public issue on GitHub.

.. _CKAN tag on Stack Overflow: http://stackoverflow.com/questions/tagged/ckan
.. _archives: https://groups.google.com/a/ckan.org/g/ckan-dev
.. _GitHub Issues: https://github.com/ckan/ckan/issues
.. _CKAN chat on Gitter: https://gitter.im/ckan/chat


Contributing to CKAN
--------------------

For contributing to CKAN or its documentation, see
`CONTRIBUTING <https://github.com/ckan/ckan/blob/master/CONTRIBUTING.md>`_.

Mailing List
~~~~~~~~~~~~

Subscribe to the `ckan-dev`_ mailing list to receive news about upcoming releases and
future plans as well as questions and discussions about CKAN development, deployment, etc.

Community Chat
~~~~~~~~~~~~~~

If you want to talk about CKAN development say hi to the CKAN developers and members of
the CKAN community on the public `CKAN chat on Gitter`_. Gitter is free and open-source;
you can sign in with your GitHub, GitLab, or Twitter account.

The logs for the old `#ckan`_ IRC channel (2014 to 2018) can be found here:
https://github.com/ckan/irc-logs.

Wiki
~~~~

If you've figured out how to do something with CKAN and want to document it for
others, make a new page on the `CKAN wiki`_ and tell us about it on the
ckan-dev mailing list or on Gitter.

.. _ckan-dev: https://groups.google.com/a/ckan.org/forum/#!forum/ckan-dev
.. _#ckan: http://webchat.freenode.net/?channels=ckan
.. _CKAN Wiki: https://github.com/ckan/ckan/wiki
.. _CKAN chat on Gitter: https://gitter.im/ckan/chat


Copying and License
-------------------

This material is copyright (c) 2006-2023 Open Knowledge Foundation and contributors.

It is open and licensed under the GNU Affero General Public License (AGPL) v3.0
whose full text may be found at:

http://www.fsf.org/licensing/licenses/agpl-3.0.html
",2023-07-07 15:51:57+00:00
claimed,component-library,claimed-framework/component-library,The goal of CLAIMED is to enable low-code/no-code rapid prototyping style programming to seamlessly CI/CD into production. ,,False,2207,2023-07-07 12:41:44+00:00,2018-05-21 17:13:20+00:00,3961,86,23,2,,,Apache License 2.0,1541,v0.2rc0,2,2022-11-30 15:03:21+00:00,2023-07-07 14:34:29+00:00,2023-06-23 21:43:54+00:00,"[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6718/badge)](https://bestpractices.coreinfrastructure.org/projects/6718)
[![GitHub](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/claimed-framework/component-library/issues)



# CLAIMED - The Component Library for AI, Machine Learning, ETL, and Data Science

**TL;DR**
- set of re-usable coarse-grained components (just a bunch of code)
- think of tasks, not functions (e.g., read from a database, transform data, train model, deploy model, store result to cloud object storage)
- write once, runs everywhere: Kubeflow, Apache Airflow, CLI, KNative, Docker, Kubernetes
- orchestrate with anything: shell script, Kubeflow, Airflow, Argo, Tekton
- persistence layer / queue agnostic: Cloud Object Storage, file systems, PVC, Kafka, MQTT
- just use Python - no other skills required (no Kubeflow component YAML, maven, Java)
- 1st class citizen in JupyterLab and the Elyra Pipeline Editor (creating a low code / no code IDE for data science)
- upstream repository to IBM Watson Studio Pipelines contributed components in IBM Cloud Pak for Data

You also might wanna have a look at our [FAQ](FAQ.md)


CLAIMED is a component library for artificial intelligence, machine learning, ""extract, transform, load"" processes, and data science. The goal is to enable low-code/no-code rapid prototyping style programming to seamlessly CI/CD into production. The library provides ready-made components for various business domains, supports multiple computer languages, works on different data flow editors and command line tools, and runs on various execution engines including Kubernetes, KNative, Kubeflow, Airflow or plain docker. To demonstrate its utility, we constructed a workflow composed exclusively of this library's components. To display the capabilities of this library, we made use of a publicly available Computed Tomography (CT) scan dataset [covidata]. We created a deep learning model, which is supposed to classify exams as either COVID-19 positive or negative. We built the pipeline with Elyra's Pipeline Visual Editor, with support for local, Airflow, and Kubeflow execution [https://arxiv.org/abs/2103.03281](https://arxiv.org/abs/2103.03281).


![Low Code / No Code pipeline creation tool for data science](https://github.com/IBM/claimed/raw/master/images/elyra_pipeline.png)
*Low Code / No Code pipeline creation tool for data science*

 **Bring the latest and greatest libraries to the hands of everybody.**

![AIX360/LIME highlights a poor deep learning covid classification model looking at bones only](https://github.com/IBM/claimed/raw/master/images/elyra_lime.png)
*AIX360/LIME highlights a poor deep learning covid classification model looking at bones only*

Components of this library can be exported as:
1. Kubeflow pipeline components
2. Apache Airflow components
3. Standalone graphical components for the Elyra pipeline editor
4. Standalone components to be run from the command line
5. Standalone components to be run as docker containers
6. Standalone components to be run as Kubernetes Service
7. Standalone components to be run as KNative Application or Job
8. Components to consume from or publish to Queue Managers like Kafka or MQTT
9. Components deployed to Kubernets wrapped into DAPR (as service or message consumer/producer)

![Visually create pipelines from notebooks and run everywhere](https://github.com/IBM/claimed/raw/master/images/elyra_graphical_export.png)
*Visually create pipelines from notebooks and run them everywhere*

Each notebook is following a similar format.

1. The first cell contains a description of the component itself.
2. The second cell installs all dependencies using pip3.
3. The third cell imports all dependencies.
4. The fourth cell contains a list of dependencies, input parameters, and return values as Python comments
5. The fifth cell reads the input parameters from environment variables.


![Export notebooks and files as runtime components for different engines](https://github.com/IBM/claimed/raw/master/images/elyra_cli_export.png)
*Export notebooks and files as runtime components for different engines*


To learn more on how this library works in practice, please have a look at the following [video](https://www.youtube.com/watch?v=FuV2oG55C5s)

## Related work
[Ploomber](https://github.com/ploomber/ploomber)

[Orchest](https://www.orchest.io/)

[covidata] Joseph Paul Cohen et al. *COVID-19 Image Data Collection: Prospective Predictions Are the Future*, arXiv:2006.11988, 2020

## Getting Help

We welcome your questions, ideas, and feedback. Please create an [issue](https://github.com/claimed-framework/component-library/issues) or a [discussion thread](https://github.com/claimed-framework/component-library/discussions).
Please see [VULNERABILITIES.md](VULNERABILITIES.md) for reporting vulnerabilities.

## Contributing to CLAIMED
Interested in helping make CLAIMED better? We encourage you to take a look at our 
[Contributing](CONTRIBUTING.md) page.

## License
This software is released under Apache License v2.0
",2023-07-07 15:52:01+00:00
cloudcomputecannon,cloud-compute-cannon,Autodesk/cloud-compute-cannon,Cloud Compute Cannon is a tool aimed at scientists and more general users who want to use cheap cloud providers (such as Amazon) to perform large scale computes (number crunching).,,True,18,2023-01-27 19:15:49+00:00,2016-04-28 00:13:24+00:00,9,8,2,0,,,,636,,0,,,2018-04-28 19:47:15+00:00,"# Cloud Compute Cannon [![Build Status](https://travis-ci.org/dionjwa/cloud-compute-cannon.svg?branch=master)](https://travis-ci.org/dionjwa/cloud-compute-cannon)

## TOC:

 - [API](docs/API.md)
 - [ARCHITECTURE](docs/ARCHITECTURE.md)
 - [DEVELOPERS](docs/DEVELOPERS.md)
 - [DEPLOYMENT](docs/DEPLOYMENT.md)
 - [ENVIRONMENT VARIABLES](src/haxe/ccc/compute/shared/ServerConfig.hx)
 - [LOGS](docs/LOGS.md)
 - [ROADMAP](docs/ROADMAP.md)

Cloud Compute Cannon (CCC) is a stack that provides an HTTP API for running arbitrary compute jobs that run in docker containers.

The stack runs locally or in AWS (other compute providers coming soon).


Cloud Compute Cannon allows you to create a server (that scales) that provides a REST API that allows callers to run *any* docker image.

This means that the Cloud-Compute-Cannon (CCC) server allow you to run anything on your server: Python scripts, R statistics analysis, deep learning algorithms, C++ simulations

Cloud Compute Cannon is a tool aimed at scientists and more general users who want to use cheap cloud providers (such as Amazon) to perform large scale computes (number crunching). It aims to lower some of the biggest barriers and learning curves in getting data and custom code running on distributed cloud infrastructure. It can be run both as a command-line tool, or as a server for integrating into other tools via a REST API/websockets.

Use cases:

 - Simulating molecular dynamics
 - Numerical simulations, data crunching
 - Server infrastructure for scalable computation

Cloud Compute Cannon is designed to do one thing well: run docker-based compute jobs on any cloud provider (or your local machine) reliably, with a minimum or user intervention, and scale machines up and down as needed. Its feature set is purposefully limited, it is designed to be used standalone, or as a component in more complex tools, rather than be extended itself.

## License

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",2023-07-07 15:52:06+00:00
clouddoe,CloudDOE,CSCLabTW/CloudDOE,"CloudDOE is a user friendly software package to deploy, operate and extend a MapReduce-based bioinformatics environment, which is collectively denoted as a CloudDOE Cloud.",http://clouddoe.iis.sinica.edu.tw/,False,1,2017-08-22 06:42:59+00:00,2014-02-14 07:32:29+00:00,1,3,1,0,,,Apache License 2.0,153,,0,,,2014-03-25 10:19:23+00:00,"CloudDOE
========

CloudDOE is a user friendly software package to deploy, operate and extend a
MapReduce-based bioinformatics environment, which is collectively denoted as a
CloudDOE Cloud. A CloudDOE Cloud consists of a Hadoop MapReduce computing
framework and specific bioinformatics data analysis tools, e.g., CloudBurst,
CloudBrush, and CloudRS.

CloudDOE is distributed under Apache License 2.0. It consists of a set of BASH
scripts and binary files, and three Java wizards:

1. Deploy wizard provides easy deployment of the Hadoop MapReduce computing
framework for an administrator. This wizard installs JRE 1.6 and Hadoop version
0.20.203, and also starts the Hadoop services automatically.

2. Operate wizard aims at providing users with choices and execution of the
supported MapReduced-based bioinformatics applications from its dashboard.

3. Extend wizard allows a CloudDOE Cloud administrator to extend the
functionalities by maintaining software information of the dashboard, which may
used by the Operate wizard.

Users can read README_USER for prerequisites of CloudDOE. Developers need to read
README_DEVELOPER before compiling and building CloudDOE.

For more informaion, please visit the project website at:
http://clouddoe.iis.sinica.edu.tw/
",2023-07-07 15:52:10+00:00
cloudgene,cloudgene,genepi/cloudgene,A framework to build Software As A Service (SaaS) platforms for data analysis pipelines.,http://www.cloudgene.io,False,32,2023-06-22 20:02:49+00:00,2018-01-29 11:39:56+00:00,16,2,7,52,v2.6.2,2023-06-20 09:20:34+00:00,GNU Affero General Public License v3.0,1050,v3.0.0-beta2,55,2022-05-24 10:16:45+00:00,2023-06-23 19:39:05+00:00,2023-06-20 09:20:34+00:00,"Cloudgene
=========

[![Build Status](https://travis-ci.org/genepi/cloudgene.svg?branch=master)](https://travis-ci.org/genepi/cloudgene)
[![codecov](https://codecov.io/gh/genepi/cloudgene/branch/master/graph/badge.svg)](https://codecov.io/gh/genepi/cloudgene)
[![GitHub release](https://img.shields.io/github/release/genepi/cloudgene.svg)](https://GitHub.com/genepi/cloudgene/releases/)


A framework to build Software As A Service (SaaS) platforms for data analysis pipelines.

## Features

- :wrench: **Build** your analysis pipeline in your favorite language or use Hadoop based technologies (MapReduce, Spark, Pig)
- :page_facing_up: **Integrate** your analysis pipeline into Cloudgene by writing a simple [configuration file](http://docs.cloudgene.io/developers/introduction/)
- :bulb: **Get** a powerful web application with user management, data transfer, error handling and more
- :star: **Deploy** your application with one click to any Hadoop cluster or to public Clouds like Amazon AWS
- :cloud: **Provide** your application as SaaS to other scientists and handle thousands of jobs like a pro
- :earth_americas: **Share** your application and enable everyone to clone your service to its own hardware or private cloud instance

## Requirements

You will need the following things properly installed on your computer.

* [Java 8 or higher](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
* [Hadoop](http://hadoop.apache.org/) (Optional)
* [Docker](https://www.docker.com/) (Optional)
* MySQL Server (Optional)


## Installation

You can install Cloudgene via our install script:

```sh
mkdir cloudgene
cd cloudgene
curl -s install.cloudgene.io | bash
```

Test the installation with the following command:

```sh
./cloudgene version
```

We provide a [Docker image](https://github.com/genepi/cloudgene-docker) to get a full-working Cloudgene instance in minutes without any installation.


## Getting started

The *hello-cloudgene* application can be installed by using the following command:

```sh
./cloudgene github-install lukfor/hello-cloudgene
```

The webserver can be started with the following command:

```sh
./cloudgene server
```

The webservice is available on http://localhost:8082. Please open this address in your web browser and enter as username `admin` and as password `admin1978` to login.

Click on *Run* to start the application.

![image](docs/images/hello-cloudgene-saas.png)


A job can be started by filling out the form and clicking on the blue submit button. The *hello-cloudgene* application displays several inspiring quotes:

![image](docs/images/hello-cloudgene-saas-results.png)


The documentation is available at http://docs.cloudgene.io

More examples can be found in [genepi/cloudgene-examples](https://github.com/genepi/cloudgene-examples).

## Cloudgene and Genomics

See Cloudgene in action:

- [Michigan Imputation Server](https://imputationserver.sph.umich.edu)
- [mtDNA Server](https://mtdna-server.uibk.ac.at)
- [Laser Server](https://laser.sph.umich.edu)

## Developing

More about how to build Cloudgene from source can be found [here](https://github.com/genepi/cloudgene/blob/master/DEVELOPING.md).

## Contact

- Lukas Forer @lukfor
- Sebastian Schönherr @seppinho

## License

Cloudgene is licensed under [AGPL-3.0](https://opensource.org/licenses/AGPL-3.0).
",2023-07-07 15:52:13+00:00
cloudslang,cloud-slang,CloudSlang/cloud-slang,"CloudSlang Language, CLI and Builder",http://cloudslang.io,False,225,2023-05-30 10:31:01+00:00,2014-12-21 16:32:24+00:00,80,33,69,378,cloudslang-2.0.5,2021-07-22 05:20:18+00:00,Apache License 2.0,4826,travis01,534,2018-01-31 11:45:44+00:00,2023-07-07 10:53:41+00:00,2023-07-07 10:53:38+00:00,"CloudSlang
==========

[![Join the chat at https://gitter.im/CloudSlang/cloud-slang](https://badges.gitter.im/CloudSlang/cloud-slang.svg)](https://gitter.im/CloudSlang/cloud-slang?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

CloudSlang is a YAML based language for writing human-readable workflows for the CloudSlang Orchestration Engine .
This project includes the CLI to trigger flows.

[![Build Status](https://travis-ci.org/CloudSlang/cloud-slang.svg?branch=master)](https://travis-ci.org/CloudSlang/cloud-slang)
[![Coverage Status](https://coveralls.io/repos/CloudSlang/cloud-slang/badge.svg?branch=master)](https://coveralls.io/r/CloudSlang/cloud-slang?branch=master)
[![Code Climate](https://codeclimate.com/github/CloudSlang/cloud-slang/badges/gpa.svg)](https://codeclimate.com/github/CloudSlang/cloud-slang)

#### Getting started:

###### Pre-Requisite: Java JRE >= 7

1. Download the CloudSlang CLI file named cslang-cli-with-content:
    + [Stable release](https://github.com/CloudSlang/cloud-slang/releases/latest)
    + [Latest snapshot](https://github.com/CloudSlang/cloud-slang/releases/)
2. Extract it.
3. Go to the folder /cslang/bin/
4. Run the executable :
  - For Windows : cslang.bat
  - For Linux : bash cslang
5. Run a simple example print text flow:  run --f ../content/io/cloudslang/base/print/print_text.sl --i text=first_flow

#### Documentation

All documentation is available on the [CloudSlang website](http://www.cloudslang.io/#/docs).

#### What's New

See what's new [here](CHANGELOG.md).

#### Get Involved

Read our contributing guide [here](CONTRIBUTING.md).

Contact us at support@cloudslang.io.

#### Building and Testing from Source

###### Pre-Requisites:

1. Maven version >= 3.0.3
2. Java JDK version >= 7

###### Steps

1. ```git clone``` the source code
2. ```mvn clean install```
3. Run the CLI executable from cloudslang-cli\target\cslang\bin

### CloudSlang Docker Image
Just use:

``` docker pull cloudslang/cloudslang ```

And run it using:

``` docker run -it cloudslang/cloudslang ```

### CloudSlang npm Package
###### cslang-cli
> The CloudSlang command line interface.

Install this globally and you'll have access to the `cslang` command anywhere on your system.

```shell
npm install -g cloudslang-cli
```

Now you can just use the `cslang` command anywhere
```shell
cslang
```

###### Pre-Requisites
Node.js & Java installed.

cslang-cli page in the [npm repository](https://www.npmjs.com/package/cslang-cli).
",2023-07-07 15:52:17+00:00
clusterflow,clusterflow,ewels/clusterflow,A pipelining tool to automate and standardise bioinformatics analyses on cluster environments.,https://ewels.github.io/clusterflow/,True,95,2023-05-06 12:17:54+00:00,2014-05-16 09:33:05+00:00,26,13,7,6,v0.5,2017-04-10 09:45:46+00:00,GNU General Public License v3.0,859,v0.5,6,2017-04-10 09:45:46+00:00,2023-04-11 14:25:48+00:00,2023-04-08 20:51:08+00:00,"# <img src=""docs/assets/Cluster_Flow_logo.png"" width=""400"" title=""Cluster Flow"">

### A user-friendly bioinformatics workflow tool

---

# Cluster Flow is now archived

_This project is no longer under active maintenance. You're welcome to use it, but no updates or bug fixes will be posted. We recommend using [Nextflow](https://nextflow.io/) together with [nf-core](https://nf-co.re/) instead._

_Many thanks to everyone who used and supported Cluster Flow over the years._

---


[![Build Status](https://img.shields.io/travis/ewels/clusterflow.svg?style=flat-square)](https://travis-ci.org/ewels/clusterflow)
[![Gitter](https://img.shields.io/badge/gitter-%20join%20chat%20%E2%86%92-4fb99a.svg?style=flat-square)](https://gitter.im/ewels/clusterflow)
[![DOI](https://img.shields.io/badge/DOI-10.12688%2Ff1000research.10335.2-lightgrey.svg?style=flat-square)](http://dx.doi.org/10.12688/f1000research.10335.2)

**Find Cluster Flow documentation with information and examples at
[https://ewels.github.io/clusterflow/](https://ewels.github.io/clusterflow/)**

---

Cluster Flow is a pipelining tool to automate and standardise
bioinformatics analyses on high-performance cluster environments.
It is designed to be easy to use, quick to set up and flexible to configure.

Cluster Flow is written in Perl and works by launching jobs to a cluster
(can also be run locally). Each job is a stand-alone Perl executable wrapper
around a bioinformatics tool of interest.

Modules collect extensive logging information and Cluster Flow e-mails
the user with a summary of the pipeline commands and exit codes upon completion.

## Installation
You can find stable versions to download on the
[releases page](https://github.com/ewels/clusterflow/releases).

You can get the development version of the code by cloning this repository:
```
git clone https://github.com/ewels/clusterflow.git
```

Once downloaded and extracted, create a `clusterflow.config` file in the
script directory, based on `clusterflow.config.example`.

Next, you need to add the main `cf` executable to your `PATH`. This can be done
as an environment module, with a symlink to `bin` or by adding to your `~/.bashrc`
file.

Finally, run the setup wizard (`cf --setup`) and genomes wizard (`cf --add_genome`) and
you're ready to go! See the [installation docs](docs/installation.md) for more
information.

## Usage
Pipelines are launched by naming a pipeline or module and the input files. A simple
example could look like this:
```bash
cf sra_trim *.fastq.gz
```

Most pipelines need reference genomes, and Cluster Flow has built in reference
genome management. Parameters can be passed to modify tool behaviour.

For example, to run the `fastq_bowtie` pipeline (FastQC, TrimGalore! and Bowtie)
with Human data, trimming the first 6bp of read 1, the command would be:

```bash
cf --genome GRCh37 --params ""clip_r1=6"" fastq_bowtie *.fastq.gz
```

Additional common Cluster Flow commands are as follows:
```bash
cf --genomes     # List available reference genomes
cf --pipelines   # List available pipelines
cf --modules     # List available modules
cf --qstat       # List running pipelines
cf --qdel [id]   # Cancel jobs for a running pipeline
```


## Supported Tools
Cluster Flow comes with modules and pipelines for the following tools:

| Read QC & pre-processing     | Aligners / quantifiers  | Post-alignment processing                               | Post-alignment QC                                                                                               |
| ---------------------------- | ----------------------- | ------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| [FastQ Screen](fastqscreen)  | [Bismark](bismark)      | [bedtools](bedtools) (`bamToBed`, `intersectNeg`)       | [deepTools](deeptools) (`bamCoverage`, `bamFingerprint`)                                                        |
| [FastQC](fastqc)             | [Bowtie 1](bowtie1)     | [subread featureCounts](featurecounts)                  | [MultiQC](multiqc)                                                                                              |
| [TrimGalore!](trimgalore)    | [Bowtie 2](bowtie2)     | [HTSeq Count](htseq_count)                              | [phantompeaktools](phantompeaktools) (`runSpp`)                                                                 |
| [SRA Toolkit](sratoolkit)    | [BWA](bwa)              | [Picard](picard) (`MarkDuplicates`)                     | [Preseq](preseq)                                                                                                |
|                              | [HiCUP](hicup)          | [Samtools](samtools) (`bam2sam`, `dedup`, `sort_index`) | [RSeQC](rseqc) (`geneBody_coverage`, `inner_distance`, `junction_annotation`, `junction_saturation`, `read_GC`) |
|                              | [HISAT2](hisat2)        |                                                         |                                                                                                                 |
|                              | [Kallisto](kallisto)    |                                                         |                                                                                                                 |
|                              | [STAR](star)            |                                                         |                                                                                                                 |
|                              | [TopHat](tophat)        |                                                         |                                                                                                                 |

## Citation
Please consider citing Cluster Flow if you use it in your analysis.

> **Cluster Flow: A user-friendly bioinformatics workflow tool [version 2; referees: 3 approved].** <br/>
> Philip Ewels, Felix Krueger, Max Käller, Simon Andrews <br/>
> _F1000Research_ 2016, **5**:2824 <br/>
> doi: [10.12688/f1000research.10335.2](http://dx.doi.org/10.12688/f1000research.10335.2)

```
@article{Ewels2016,
author = {Ewels, Philip and Krueger, Felix and K{\""{a}}ller, Max and Andrews, Simon},
title = {Cluster Flow: A user-friendly bioinformatics workflow tool [version 2; referees: 3 approved].},
journal = {F1000Research},
volume = {5},
pages = {2824},
year = {2016},
doi = {10.12688/f1000research.10335.2},
URL = { + http://dx.doi.org/10.12688/f1000research.10335.2}
}
```

## Contributions & Support
Contributions and suggestions for new features are welcome, as are bug reports!
Please create a new [issue](https://github.com/ewels/clusterflow/issues).
Cluster Flow has extensive
[documentation](https://ewels.github.io/clusterflow/docs) describing how to write new modules
and pipelines.

There is a chat room for the package hosted on Gitter where you can discuss
things with the package author and other developers:
https://gitter.im/ewels/clusterflow

If in doubt, feel free to get in touch with the author directly:
[@ewels](https://github.com/ewels) (phil.ewels@scilifelab.se)

## Contributors
Project lead and main author: [@ewels](https://github.com/ewels)

Code contributions from:
[@s-andrews](https://github.com/s-andrews),
[@FelixKrueger](https://github.com/FelixKrueger),
[@stu2](https://github.com/stu2),
[@orzechoj](https://github.com/orzechoj)
[@darogan](https://github.com/darogan)
and others. Thanks for your support!

## License
Cluster Flow is released with a GPL v3 licence. Cluster Flow is free software: you can
redistribute it and/or modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, either version 3 of the License, or (at your
option) any later version. For more information, see the licence that comes bundled with
Cluster Flow.

[bedtools]:          http://bedtools.readthedocs.io/en/latest/
[bismark]:           http://www.bioinformatics.babraham.ac.uk/projects/bismark/
[bowtie1]:           http://bowtie-bio.sourceforge.net/index.shtml
[bowtie2]:           http://bowtie-bio.sourceforge.net/bowtie2/index.shtml
[bwa]:               http://bio-bwa.sourceforge.net/
[deeptools]:         https://deeptools.github.io/
[fastqscreen]:       http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/
[fastqc]:            http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
[featurecounts]:     http://bioinf.wehi.edu.au/featureCounts/
[hicup]:             http://www.bioinformatics.babraham.ac.uk/projects/hicup/
[hisat2]:            http://ccb.jhu.edu/software/hisat2/index.shtml
[htseq_count]:       http://www-huber.embl.de/HTSeq/doc/count.html
[kallisto]:          https://pachterlab.github.io/kallisto/
[multiqc]:           http://multiqc.info
[phantompeaktools]:  https://code.google.com/archive/p/phantompeakqualtools/
[picard]:            https://broadinstitute.github.io/picard/
[preseq]:            http://smithlabresearch.org/software/preseq/
[rseqc]:             http://rseqc.sourceforge.net/
[samtools]:          http://www.htslib.org/
[sratoolkit]:        https://github.com/ncbi/sra-tools
[star]:              https://github.com/alexdobin/STAR
[tophat]:            http://ccb.jhu.edu/software/tophat/index.shtml
[trimgalore]:        http://www.bioinformatics.babraham.ac.uk/projects/trim_galore/

",2023-07-07 15:52:22+00:00
clusterjob,clusterjob,monajemi/clusterjob,ClusterJob: An automated system for painless and reproducible massive computational experiments,http://clusterjob.org,False,19,2022-12-08 23:49:47+00:00,2015-02-27 04:41:24+00:00,8,7,2,0,,,"BSD 3-Clause ""New"" or ""Revised"" License",517,v0.0.3,4,2017-03-18 03:25:37+00:00,,2019-08-22 01:02:01+00:00,"# ClusterJob
Clusterjob, hereafter CJ, is an experiment management system (EMS) for data science. CJ is 
written mainly in perl and allows submiting computational jobs to clusters in a hassle-free and reproducible manner.
CJ produces 'reporoducible' computational packages for academic publications at no-cost. CJ project started in 2013 at Stanford University by Hatef Monajemi and his PhD advisor David L. Donoho with the goal of encouraging  more efficient and reproducible research paradigm. 
CJ is currently under active development. Current implementation allows submission of MATLAB,Python and R jobs. 
The code for R works partially for serial jobs only. In the future versions, we hope to include other data science 
programming languages such as Julia. 

You can read more about CJ on http://clusterjob.org

You can find CJ book project at https://github.com/monajemi/CJ-book  


# key contributors:

1. Hatef Monajemi
2. Bekk Blando 
3. David Donoho
4. Vardan Papyan


# How to cite ClusterJob

```
@article{clusterjob,
Author = {H.~Monajemi and D.~L.~Donoho},
Month = March,
Url= {https://github.com/monajemi/clusterjob},
Title = {ClusterJob: An automated system for painless and reproducible massive computational experiments},
Year = 2015}


@article{MMCEP17,
title = {Making massive computational experiments painless},
author = {H.~Monajemi  and D.~L.~Donoho and V.~Stodden},
journal={Big Data (Big Data), 2016 IEEE International Conference on},
year={2017},
month={February},
}



@article{Monajemi19,
title = {Ambitious data science can be painless},
author = {H.~Monajemi and R.~Murri and E.~Yonas and P.~Liang and V.~Stodden and D.L.~Donoho},
note={arXiv:1901.08705},
year={2019},
}
```



Copyright 2015 Hatef Monajemi (monajemi@stanford.edu)


",2023-07-07 15:52:25+00:00
collectiveknowledge,ck,mlcommons/ck,"MLCommons CM is a technology-agnostic language to automate experiments, facilitate reproducible research, and accelerate production deployments across rapidly evolving software, hardware, models, and data.  It powers MLCommons CK platform for collaborative benchmarking, optimization, and co-design of Pareto-efficient AI/ML systems and applications",https://access.cKnowledge.org,False,528,2023-07-04 08:19:14+00:00,2014-11-05 17:14:43+00:00,87,51,18,94,cm-v1.5.0,2023-07-05 19:29:01+00:00,Apache License 2.0,5385,v1.9.4,95,2017-10-27 13:00:40+00:00,2023-07-07 14:59:42+00:00,2023-07-07 14:59:40+00:00,"[![PyPI version](https://badge.fury.io/py/cmind.svg)](https://pepy.tech/project/cmind)
[![Python Version](https://img.shields.io/badge/python-3+-blue.svg)](https://github.com/mlcommons/ck/tree/master/cm/cmind)
[![License](https://img.shields.io/badge/License-Apache%202.0-green)](LICENSE.md)

[![CM test](https://github.com/mlcommons/ck/actions/workflows/test-cm.yml/badge.svg)](https://github.com/mlcommons/ck/actions/workflows/test-cm.yml)
[![CM script automation features test](https://github.com/mlcommons/ck/actions/workflows/test-cm-script-features.yml/badge.svg)](https://github.com/mlcommons/ck/actions/workflows/test-cm-script-features.yml)

### Documentation and the Getting Started Guide

[Table of contents](docs/README.md)

### About

***We deeply believe in the power of open science and open source to solve the world's most challenging problems.
   That's why we are developing a common automation and reproducibility language to help connect academia and industry,
   facilitate reproducible research and bridge the growing gap between research and production - 
   [see our ACM REP'23 keynote for more details](https://doi.org/10.5281/zenodo.8105339)!***

The ""Collective Knowledge"" project (CK) is motivated by the [feedback from researchers and practitioners](https://learning.acm.org/techtalks/reproducibility)
while reproducing results from more than 150 research papers and validating them in the real world - 
there is a need for a common and technology-agnostic language
that can facilitate reproducible research and simplify technology transfer to production
across diverse and rapidly evolving software, hardware, models, and data.
It consists of the following sub-projects:

* [Collective Mind automation language (MLCommons CM)](cm) 
  is intended to help researchers and practitioners
  describe all the steps required to reproduce their experiments across any software, hardware, and data
  in a common and technology-agnostic way.
  It is powered by Python, JSON and/or YAML meta descriptions, and a unified CLI.
  CM can automatically generate unified README and synthesize unified containers with a common API
  while reducing all the tedious, manual, repetitive, and ad-hoc efforts to validate research projects in production.
  It is used in the same way in native environments, Python virtual environments, and containers.

  See a few real-world examples of using the CM scripting language:
  - [README to reproduce published IPOL'22 paper](cm-mlops/script/app-ipol-reproducibility-2022-439)
  - [README to reproduce MLPerf RetinaNet inference benchmark at Student Cluster Competition'22](docs/tutorials/sc22-scc-mlperf.md)
  - [Auto-generated READMEs to reproduce official MLPerf BERT inference benchmark v3.0 submission with a model from the Hugging Face Zoo](https://github.com/mlcommons/submissions_inference_3.0/tree/main/open/cTuning/code/huggingface-bert/README.md)
  - [Auto-generated Docker containers to run and reproduce MLPerf inference benchmark](cm-mlops/script/app-mlperf-inference/dockerfiles/retinanet)

* [Collective Mind scripts (MLCommons CM scripts)](cm-mlops/script) 
  provide a low-level implementation of the high-level and technology-agnostic CM language.

* [Collective Knowledge platform (MLCommons CK playground)](platform) 
  aggregates [reproducible experiments](https://access.cknowledge.org/playground/?action=experiments) 
  in the CM format, connects academia and industry to 
  [organize benchmarking, reproducibility, replicability and optimization challenges]( https://github.com/mlcommons/ck/tree/master/cm-mlops/challenge ),
  and help developers and users select Pareto-optimal end-to-end applications and systems based on their requirements and constraints
  (cost, performance, power consumption, accuracy, etc).


### Collaborative development

This open-source technology is being developed by the public
[MLCommons task force on automation and reproducibility](docs/taskforce.md)
led by [Grigori Fursin](https://cKnowledge.org/gfursin) and
[Arjun Suresh](https://www.linkedin.com/in/arjunsuresh).
The goal is to connect academia and industry to develop, benchmark, compare, synthesize, 
and deploy Pareto-efficient AI and ML systems and applications 
(optimal trade off between performance, accuracy, power consumption, and price)
in a unified, automated and reproducible way while slashing all development and operational costs.

* Join our [public Discord server](https://discord.gg/JjWNWXKxwT).
* Join our [public conf-calls](https://docs.google.com/document/d/1zMNK1m_LhWm6jimZK6YE05hu4VH9usdbKJ3nBy-ZPAw).
* Check our [news](docs/news.md).
* Check our [presentation](https://doi.org/10.5281/zenodo.7871070) and [Forbes article](https://www.forbes.com/sites/karlfreund/2023/04/05/nvidia-performance-trounces-all-competitors-who-have-the-guts-to-submit-to-mlperf-inference-30/?sh=3c38d2866676) about our development plans.
* Read about our [CK concept (previous version before MLCommons)](https://arxiv.org/abs/2011.01149).

### Copyright

2021-2023 [MLCommons](https://mlcommons.org)

### License

[Apache 2.0](LICENSE.md)

### Acknowledgments

This project is currently supported by [MLCommons](https://mlcommons.org), [cTuning foundation](https://www.linkedin.com/company/ctuning-foundation),
[cKnowledge](https://www.linkedin.com/company/cknowledge) and [individual contributors](https://github.com/mlcommons/ck/blob/master/CONTRIBUTING.md).
We thank [HiPEAC](https://hipeac.net) and [OctoML](https://octoml.ai) for sponsoring initial development.
",2023-07-07 15:52:29+00:00
combusti/o,combustio,jarlebass/combustio,COMBUSTI/O,,False,4,2016-12-17 15:55:37+00:00,2016-05-28 11:33:41+00:00,1,2,1,0,,,MIT License,1,,0,,,2016-05-28 11:35:44+00:00,"# COMBUSTI/O
## Abstractions facilitating parallel execution of programs implementing common I/O patterns in a pipelined fashion as workflows in Spark

Build and clean application running the following commands from the project root directory:
- <code>$ sbt assembly</code>
- <code>$ sbt clean clean-files</code>

Examples on how to run the application can be found in the [scripts](script/) folder.
",2023-07-07 15:52:34+00:00
communitydatalicenseagreement(cdla),CDLA,lfai/CDLA,Community Data License Agreements: Data license agreements that could enable sharing of data similar to what we have with open source software.,https://cdla.io/,False,4,2021-03-31 14:35:36+00:00,2018-11-27 19:48:28+00:00,1,3,1,0,,,Other,10,,0,,,2018-12-05 16:26:04+00:00,"# Community Data License Agreements 

Open source software communities have shown the power of open
collaboration building some of the world’s most important software
assets together. There are communities also looking to collaboratively
build datasets that can be shared and developed in a very similar
model to software. For example, machine learning and AI systems
require vast amounts of training data. Governments are looking for
ways to establish public-private sharing of data. The challenge is
that intellectual property systems around the world treat data
different than software. Our common OSI-approved licenses do not work
well applied to data.

Our communities wanted to develop data license agreements that could
enable sharing of data similar to what we have with open source
software. The result is a large scale collaboration on two licenses
for sharing data under a legal framework which we call the Community
Data License Agreement (CDLA).

There are two initial CDLA licenses. The CDLA-Sharing license was
designed to embody the principles of copyleft in a data license. In
general, if someone shares their data, the CDLA-Sharing agreement puts
terms in place to ensure that downstream recipients can use and modify
that data, and are also required to share their changes to the data.
The CDLA-Permissive agreement is similar to permissive open source
licenses in that the publisher of data allows anyone to use, modify
and do what they want with the data with no obligations to share any
of their changes or modifications.

These two licenses establish the framework for collaborative sharing
of data that we have seen proven to work in open source software
communities. The context document should be helpful for understanding
the framework to apply the CDLA. We encourage communities and
organizations seeking to share data to review the Community Data
License Agreements and see if they fit your needs and use cases.

Please visit https://cdla.io/ for additional details and resources.
",2023-07-07 15:52:37+00:00
compi,compi,sing-group/compi,Java Framework for Computational Pipelines,http://sing-group.org/compi,False,7,2023-03-08 09:49:33+00:00,2017-05-22 18:05:35+00:00,0,6,4,19,v1.5.2,2022-09-02 12:18:12+00:00,Apache License 2.0,285,v1.5.2,20,2022-09-02 12:18:02+00:00,2023-04-14 17:48:50+00:00,2022-09-02 12:18:02+00:00,"![Compi Logo](artwork/logo.png)
# Compi: framework for portable computational pipelines [![license](https://img.shields.io/github/license/sing-group/compi)](https://github.com/sing-group/compi) [![release](https://img.shields.io/github/release/sing-group/compi.svg)](http://www.sing-group.org/compi) [![DOI](https://img.shields.io/badge/DOI-10.7717%2Fpeerj--cs.593-blue)](https://doi.org/10.7717/peerj-cs.593)

Compi is an application development framework for portable computational pipelines. A software pipeline is a chain of processing elements so that the output of each element is the input of the next.

There are many fields where computational pipelines constitute the main architecture of applications, such as big data analysis or bioinformatics.

Many pipelines combine third party tools along with custom made processes, conforming the final pipeline. Compi is the framework helping to create the final, portable application.

You can get more information at:

* Compi homepage: http://sing-group.org/compi
* Compi documentation: http://sing-group.org/compi/docs
* Compi Hub: http://sing-group.org/compihub
* Compi source code: https://github.com/sing-group/compi

# The Compi ecosystem

Compi is an ecosystem that comprises:

- `compi`: the workflow engine with a command-line user interface to control the pipeline execution.
- `compi-dk`: a command-line tool to help in the development and packaging of Compi-based applications.
- *Compi Hub*: a public repository of Compi pipelines that allows other users to discover, browse and reuse them easily.

# Install `compi` and `compi-dk`

## From binaries

Binaries for `compi` and `compi-dk` for Linux 64-bit systems are available here: https://www.sing-group.org/compi#downloads

Portable versions (*.tar.gz*) and self-extracted installers (*.bsx*) are available for both. `compi` distributions are self-contained and do not require any dependencies. `compi-dk` only requires Docker, which should be available for the `compi-dk build` command to work.

## Build from source

Alternatively, the compi project can be build to obtain the `compi` and `compi-dk` binaries.

To do so, just download or clone this project and run the following command (*Note*: requires Maven 3.x and Java 1.8): `mvn clean package -PcreateInstaller`

If the build succeeds, then:
- The `compi` and `compi-dk` builds will be available at `compi/cli/target/dist/` and `compi/dk/target/dist/`, respectively. Java is required to run these binaries. The `compi` also requires `envsubst` to be available at runtime and `compi-dk` requires Docker, which should be available for the *compi-dk build* command to work.
-  the `compi` and `compi-dk` Linux 64-bit builds will be available at `compi/cli/target/installer/` and `compi/dk/target/installer/`, respectively.

## Citing

Please, cite the following publication if you use Compi:
- H. López-Fernández; O. Graña-Castro; A. Nogueira-Rodríguez; M. Reboiro-Jato; D. Glez-Peña (2021) **Compi: a Framework for Portable and Reproducible Pipelines**. *PeerJ Computer Science*. Volume 7: e593. ISSN: 2376-5992 [![DOI](https://img.shields.io/badge/DOI-10.7717%2Fpeerj--cs.593-blue)](https://doi.org/10.7717/peerj-cs.593)
",2023-07-07 15:52:40+00:00
comps,compss,bsc-wdc/compss,"COMP Superscalar (COMPSs) is a framework which aims to ease the development and execution of applications for distributed infrastructures, such as Clusters, Grids and Clouds.",https://compss.bsc.es,False,36,2023-06-05 14:17:14+00:00,2018-03-05 16:44:51+00:00,17,10,17,11,v3.2,2023-05-17 07:26:11+00:00,Apache License 2.0,7294,v3.2,88,2023-05-17 07:26:11+00:00,2023-06-26 08:37:21+00:00,2023-06-01 20:36:47+00:00,"<!-- LOGOS AND HEADER -->
<h1 align=""center"">
  <br>
  <a href=""https://www.bsc.es/"">
    <img src=""files/logos/bsc_logo.png"" alt=""Barcelona Supercomputing Center"" height=""60px"">
  </a>
  <a href=""https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/"">
    <img src=""files/logos/COMPSs_logo.png"" alt=""COMP Superscalar"" height=""60px"">
  </a>
  COMPSs/PyCOMPSs Framework
</h1>

<p align=""center"">
  <a href='https://eu.badgr.com/public/assertions/DyM-w_V-SEKU64D90AsrEA'>
  <img src='https://api.eu.badgr.io/public/assertions/DyM-w_V-SEKU64D90AsrEA/image' alt=""SQAaaS silver badge achieved"" height=""75px""/>
  </a>
  <a href='https://compss.bsc.es/jenkins/job/COMPSs_Framework_Create_Release/'>
  <img src='https://compss.bsc.es/jenkins/buildStatus/icon?job=COMPSs_Framework_Create_Release' alt=""Build Status"">
  </a>
  <a href='https://compss-doc.readthedocs.io/en/stable/?badge=stable'>
  <img src='https://readthedocs.org/projects/compss-doc/badge/?version=stable' alt='Documentation Status' />
  </a>
  <a href=""https://doi.org/10.5281/zenodo.6362594"">
  <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.6362594.svg"" alt=""DOI"">
  </a>
  <a href='https://opensource.org/licenses/Apache-2.0'>
  <img src='https://img.shields.io/badge/License-Apache_2.0-blue.svg' alt='License'/>
  </a>
</p>

<p align=""center""><b>
    <a href=""https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/"">Website</a> •  
    <a href=""https://compss-doc.readthedocs.io/en/latest/"">Documentation</a> •
    <a href=""https://github.com/bsc-wdc/compss/releasess"">Releases</a> •
    <a href=""https://bit.ly/bsc-wdc-community"">Slack</a> •
    <a href=""mailto:support-compss@bsc.es"">&#9993</a>
</b></p>

COMP Superscalar (COMPSs) is a programming model which aims to ease the development
of applications for distributed infrastructures, such as Clusters, Grids and Clouds.
COMP Superscalar also features a runtime system that exploits the inherent parallelism
of applications at execution time.

<!-- SOURCES STRUCTURE -->
## Repository Structure

  * **builders**: Packages, scripts for local installations, scripts for supercomputers
   installation and package building scripts
  * **compss** : Programming Model, Bindings and Runtime source code
  * **dependencies** : Embeded dependencies
  * **files** : Dependency files (i.e. paraver configurations)
  * **tests** : Integration tests
  * **utils** : Misc utils (i.e. OVA scripts, Docker generation, Storage implementations)


## Supported Systems

COMPSs/PyCOMPSs fully supports Linux systems for x86_64, amd64, ppc64, arm64 and riscv64 architectures. OSX systems are also supported with some limitations. 


<!-- BUILDING COMPSS -->
## Building From Sources

Follow the next steps to build COMPSs in your current machine.

### 1. Install dependencies

For an updated list of COMPSs dependencies and how to install them for different systems visit the [dependencies section](https://compss-doc.readthedocs.io/en/latest/Sections/01_Installation/01_Dependencies.html) of the COMPSs documentation website.


### 2. Get GIT submodules

Before installing COMPSs you need to download the git submodules that contain its dependencies. To do that execute the following two commands at the root of the repository.

```
./submodules_get.sh
```

### 3. Build COMPSs

**Note**: Remember to install the COMPSs dependencies and to get the GIT submodules before trying to build COMPSs from sources.

* Building COMPSs for all users (not supported in OSX)

```
cd builders/
INSTALL_DIR=/opt/COMPSs/
sudo -E ./buildlocal [options] ${INSTALL_DIR}
```

* Building COMPSs for current user

```
cd builders/

INSTALL_DIR=$HOME/opt/COMPSs/
./buildlocal [options] ${INSTALL_DIR}
```
For OSX:
```
cd builders/
alias libtoolize=/usr/local/bin/glibtoolize
alias readlink=/usr/local/bin/greadlink

export LIBTOOL=`which glibtool`
export LIBTOOLIZE=`which glibtoolize`

INSTALL_DIR=$HOME/opt/COMPSs/
./buildlocal -K -T -M ${INSTALL_DIR}
```


Many COMPSs modules can be activated/deactivated during the build using different options in the `buildlocal` command. You may check the available options by running the following command:

```
cd builders
./buildlocal -h
```

<!-- RUNNING DOCKER TESTS -->
## Running docker tests

### 1. Install Docker and docker-py

Follow these instructions

 - [Docker for Mac](https://store.docker.com/editions/community/docker-ce-desktop-mac). Or, if you prefer to use [Homebrew](https://brew.sh/).
 - [Docker for Ubuntu](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1).
 - [Docker for Arch Linux](https://wiki.archlinux.org/index.php/Docker#Installation).

Add user to docker group to run docker as non-root user.

 - [Instructions](https://docs.docker.com/install/linux/linux-postinstall/).


### 2. Build the docker image

Run the following command at the root of the project to build the image that will used for testing. The command create an image named **compss** and install the current branch into the image.

```
docker build --target=ci -t compss .
```


### 3. Run the tests

To run the tests inside the docker image use the script found in `./tests/scripts/docker_main`. This command is a wrapper for the `./main` test command
so it has de the syntax and options. For example, you can run the first test without retrials as follows:

```
./docker_main -R -t 1
```

The docker main command creates a new docker container each time you run it (replacing the last one used). It copies the current framework inside it
and runs its tests. **Note**: the testing scripts assumes you have named the testing image `compss`.

**Please be aware that:**

* Code changes affecting the tests sources, config files (e.g. `local.cfg`, and scripts (like `./local`) __will be__ visible inside the newly created container.
* Code changes affecting the installation __will not be__ visible in the installation because framework is not reinstalled. To do that rebuild the docker image as explained in step 3.
* If you run the command once, the container will be available for manual inspection (such as logs). You can log into in issuing `docker exec --user jenkins -it compss_test bash` and use the CLI as usual.
* To delete the created image issue `docker rmi compss`
* To delete the compss_test container use `docker rm -f compss_test`.


<!-- CONTACT -->
## Support
For support please send and e-mail to support-compss@bsc.es

(c) Workflows and Distributed Computing Group (WDC) - Department of Computer Science (CS) - Barcelona Supercomputing Center (BSC)



",2023-07-07 15:52:44+00:00
conan2,Conan2,tburdett/Conan2,Conan2 lightweight workflow manager,,False,3,2018-03-13 13:20:34+00:00,2011-11-02 14:38:16+00:00,3,5,3,1,2.5-X-0.4.1,2013-07-09 10:12:29+00:00,,401,2.7.0-X,3,2014-02-06 18:00:34+00:00,,2013-07-05 14:49:35+00:00,"Conan2
======

Conan is an extremely light-weight workflow management application. It is designed to provide a user-friendly interface to interact with various components of the ""script-chaining"" types workflows that are typical in bioinformatics.

Description
===========

Conan was developed to handle the various loading scenarios and workflows involved in the ArrayExpress and the GXA databases. However, it is available as a standalone tool and can be customised to chain processes (for example, Java processes, perl or shell scripts) together in a resuable way. It is possible to install the Conan web application in your own environments and put together a new workflow with a minimum amount of development.

If you are interested in installing Conan and developing your own processes, see the developer documentation on the Github wiki.

Issues and support
==================

Document any known bugs, unexpected or unintuitive behaviour (gotchas), unimplemented features and other issues here.

Issues and feature requests
===========================

To request a new feature or if you think you've found a bug, please raise an issue in the Github Tracker

Support
=======

If you need help using this tool, please read the documentation in the Github wiki, or email Tony Burdett (tburdett@ebi.ac.uk).

Contact
=======

For more information or to get involved please email Tony Burdett (tburdett@ebi.ac.uk).

Acknowledgements
================

This tool is developed by Tony Burdett, Natalja Kurbatova, Emma Hastings, Adam Faulconbridge, Dan Mapleson, Rob Davey

Development of the original Conan workflow tool was paid for by EMBL-EBI core funding
",2023-07-07 15:52:49+00:00
consecution,consecution,robdmc/consecution,:trolleybus: A pipeline abstraction for Python,,False,169,2023-05-12 23:05:58+00:00,2015-12-03 19:19:32+00:00,15,9,1,1,0.2.0,2016-12-29 14:38:32+00:00,Other,141,0.2.0,1,2016-12-29 14:38:32+00:00,2023-05-12 23:05:58+00:00,2021-02-24 00:23:45+00:00,"Update (2/23/2021)
===
It looks like this README is slowly turning into a reference of all the projects in this space that I think are better than consecution.
Here is [metaflow](https://github.com/Netflix/metaflow), an offering from Netflix.


Update (9/21/2020)
===
Another library that I believe to be better than consecution is the [pypeln](https://cgarciae.github.io/pypeln/) project.  The way it allows for a different number of workers on each node of a pipeline is quite nice.  Additionally the ability to control whether each node is run using threads, processes, async, or sync is really useful.


Update (5/1/2020)
===
Since writing this, the excellent [streamz](https://streamz.readthedocs.io/en/latest/) package has been created.  Streamz
is the project I wish had existed back when I wrote this.  It is a much more capable implementation of the of the core 
ideas of consecution, and plays nicely with [dask](https://dask.org/) to achieve scale.  I have started using streamz in my work in place of consecution.

Consecution
===
[![Build Status](https://travis-ci.org/robdmc/consecution.svg?branch=develop)](https://travis-ci.org/robdmc/consecution)
[![Coverage Status](https://coveralls.io/repos/github/robdmc/consecution/badge.svg?branch=develop)](https://coveralls.io/github/robdmc/consecution?branch=add_docs)

Introduction
---
Consecution is:
  * An easy-to-use pipeline abstraction inspired by <a href=""http://storm.apache.org/releases/current/Tutorial.html""> Apache Storm Topologies</a>
  * Designed to simplify building ETL pipelines that are robust and easy to test
  * A system for wiring together simple processing nodes to form a DAG, which is fed with a python iterable
  * Built using synchronous, single-threaded execution strategies designed to run efficiently on a single core
  * Implemented in pure-python with optional requirements that are needed only for graph visualization
  * Written with 100% test coverage

Consecution makes it easy to build systems like this.

![Output Image](/images/etl_example.png?raw=true ""ETL Example"")


Installation
---
Consecution is a pure-python package that is simply installed with pip.  The only non-essential
requirement is the 
<a href=""http://www.graphviz.org/"">Graphviz</a> system package, which is only needed if you want to create
graphical representations of your pipeline.

<pre><code><strong>[~]$ pip install consecution</strong></code></pre>

Docker
---
If you would like to try out consecution on docker, check out consecution from github and navigate to the
`docker/` subdirectory.  From there, run the following.

* Build the consecution image: `docker_build.sh`
* Start a container: `docker_run.sh`
* Once in the container, run the example: `python simple_example.py`


Quick Start
---
What follows is a quick tour of consecution.  See the <a
href=""http://consecution.readthedocs.io/en/latest/"">API documentation</a> for
more detailed information.

### Nodes
Consecution works by wiring together nodes.  You create nodes by inheriting from the
`consecution.Node` class.  Every node must define a `.process()` method.  This method
contains whatever logic you want for processing single items as they pass through your
pipeline.  Here is an example of a node that simply logs items passing through it.
```python
from consecution import Node

class LogNode(Node):
    def process(self, item):
        # any logic you want for processing single item 
        print('{: >15} processing {}'.format(self.name, item))

        # send item downstream
        self.push(item)
```
### Pipelines
Now let's create a pipeline that wires together a series of these logging nodes.
We do this by employing the pipe symbol in  much the same way that you pipe data
between programs in unix.  Note that you must name nodes when you instantiate
them.
```python
from consecution import Node, Pipeline

# This is the same node class we defined above
class LogNode(Node):
    def process(self, item):
        print('{} processing {}'.format(self.name, item))
        self.push(item)

# Connect nodes with pipe symbols to create pipeline for consuming any iterable.
pipe = Pipeline(
    LogNode('extract') | LogNode('transform') | LogNode('load')
)
```
At this point, we can visualize the pipeline to verify that the topology is
what we expect it to be.  If you Graphviz installed, you can now simply type
one of the following to see the pipeline visualized.
```python
# Create a pipeline.png file in your working directory
pipe.plot()  

# Interactively display the pipeline visualization in an IPython notebook
# by simply making the final expression in a cell evaluate to a pipeline.
pipe
```
The plot command should produce the following visualization.

![Output Image](/images/etl1.png?raw=true ""Three Node ETL Example"")

If you don't have Graphviz installed, you can print the pipeline
object to get a text-based visualization.
```python
print(pipe)
```
This represents your pipeline as a series of pipe statements showing
how data is piped between nodes.
```
Pipeline
--------------------------------------------------------------------
  extract | transform
transform | load
--------------------------------------------------------------------
```


We can now process an iterable with our pipeline by running
```python
pipe.consume(range(5))
```
which will print the following to the console.
```
   extract processing 0
 transform processing 0
      load processing 0
   extract processing 1
 transform processing 1
      load processing 1
   extract processing 2
 transform processing 2
      load processing 2
```

### Broadcasting
Piping the output of a single node into a list of nodes will cause the single
node to broadcast its pushed items to every item in the list.  So, again, using
our logging node, we could construct a pipeline like this:
```python
from consecution import Node, Pipeline

class LogNode(Node):
    def process(self, item):
        print('{} processing {}'.format(self.name, item))
        self.push(item)

# pipe to a list of nodes to broadcast items
pipe = Pipeline(
    LogNode('extract') 
    | LogNode('transform') 
    | [LogNode('load_redis'), LogNode('load_postgres'), LogNode('load_mongo')]
)
pipe.plot()
pipe.consume(range(2))
```
The plot command produces this visualization

![Output Image](/images/broadcast.png?raw=true ""Broadcast Example"")

and consuming `range(2)` produces this output
```
      extract processing 0
    transform processing 0
   load_redis processing 0
load_postgres processing 0
   load_mongo processing 0
      extract processing 1
    transform processing 1
   load_redis processing 1
load_postgres processing 1
   load_mongo processing 1
```

### Routing
If you pipe to a list that contains multiple nodes and a single callable, then
consecution will interpret the callable as a routing function that accepts a
single item as its only argument and returns the name of one of the nodes in the
list.  The routing function will direct the flow of items as illustrated below.
```python
from consecution import Node, Pipeline

class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)
        
def parity(item):
    if item % 2 == 0:
        return 'transform_even'
    else:
        return 'transform_odd'

# pipe to a list containing a callable to achieve routing behaviour
pipe = Pipeline(
    LogNode('extract') 
    | [LogNode('transform_even'), LogNode('transform_odd'), parity] 
)
pipe.plot()
pipe.consume(range(4))
```
The plot command produces the following pipeline

![Output Image](/images/routing.png?raw=true ""Routing Example"")

and consuming `range(4)` produces this output
```
        extract processing 0
 transform_even processing 0
        extract processing 1
  transform_odd processing 1
        extract processing 2
 transform_even processing 2
        extract processing 3
  transform_odd processing 3
```


### Merging
Up to this point, we have the ability to create processing trees where nodes
can either broadcast to or route between their downstream nodes.  We can,
however, do more then this and create DAGs (Directed-Acyclic-Graphs).  Piping
from a list back to a single node will merge the output of all nodes in the
list together into the single downstream node like this.
```python
from consecution import Node, Pipeline

class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)
        
def parity(item):
    if item % 2 == 0:
        return 'transform_even'
    else:
        return 'transform_odd'

# piping from a list back to a single node merges items into downstream node
pipe = Pipeline(
    LogNode('extract') 
    | [LogNode('transform_even'), LogNode('transform_odd'), parity] 
    | LogNode('load')
)
pipe.plot()
pipe.consume(range(4))
```
The plot command produces the following pipeline

![Output Image](/images/dag.png?raw=true ""DAG Example"")

and consuming `range(4)` produces this output
```
        extract processing 0
 transform_even processing 0
           load processing 0
        extract processing 1
  transform_odd processing 1
           load processing 1
        extract processing 2
 transform_even processing 2
           load processing 2
        extract processing 3
  transform_odd processing 3
           load processing 3
```
### Managing Local State
Nodes are classes, and as such, you have the freedom to create any attribute you
want on a node.  You can actually define two additional methods on your nodes to
set up and tear down node-local state.  It is important to note the order of
execution here.  All nodes in a pipeline will execute their `.begin()` methods
in pipeline-order before any items are processed.  Each node will enter its
`.end()` method only after it has processed all items, and after all parent
nodes have finished their respective `.end()` methods.  Below, we've modified
our LogNode to keep a running sum of all items that pass through it and end by
printing their sum.
```python
from consecution import Node, Pipeline

class LogNode(Node):
    def begin(self):
        self.sum = 0
        print('{}.begin()'.format(self.name))

    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.sum += item
        self.push(item)

    def end(self):
        print('sum = {:d} in {}.end()'.format(self.sum, self.name))

# Identical pipeline to merge example above, but with modified LogNode
pipe = Pipeline(
    LogNode('extract') 
    | [LogNode('transform_even'), LogNode('transform_odd'), parity] 
    | LogNode('load')
)
pipe.consume(range(4))
```

Consuming `range(4)` produces the following output
```
extract.begin()
transform_even.begin()
transform_odd.begin()
load.begin()
        extract processing 0
 transform_even processing 0
           load processing 0
        extract processing 1
  transform_odd processing 1
           load processing 1
        extract processing 2
 transform_even processing 2
           load processing 2
        extract processing 3
  transform_odd processing 3
           load processing 3
sum = 6 in extract.end()
sum = 2 in transform_even.end()
sum = 4 in transform_odd.end()
sum = 6 in load.end()
```


### Managing Global State 
Every node object has a `.global_state` attribute that is shared globally across
all nodes in the pipeline.  The attribute is also available on the Pipeline
object itself.  The GlobalState object is a simple mutable python object whose
attributes can be mutated by any node.  It also remains accesible on the
Pipeline object after all nodes have completed.  Below is a simple example of
mutating and accessing global state.

```python
from consecution import Node, Pipeline, GlobalState

class LogNode(Node):
    def process(self, item):
        self.global_state.messages.append(
            '{: >15} processing {}'.format(self.name, item)
        )
        self.push(item)
        
# create a global state object with a messages attribute
global_state = GlobalState(messages=[])

# Assign the predefined global_state to the pipeline
pipe = Pipeline(
    LogNode('extract') | LogNode('transform') | LogNode('load'),
    global_state=global_state)
)
pipe.consume(range(3))

# print the content of the global state message list
for msg in pipe.global_state.messages:
    print msg
```

Printing the contents of the messages list produces
```
  extract processing 0
transform processing 0
     load processing 0
  extract processing 1
transform processing 1
     load processing 1
  extract processing 2
transform processing 2
     load processing 2
```

## Common Patterns
This section shows examples of how to implement some common patterns in
consecution.

### Map
Mapping with nodes is very simple. Just push an altered item downstream.
```python
from consecution import Node, Pipeline
class Mapper(Node):
    def process(self, item):
        self.push(2 * item)

class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)

pipe = Pipeline(
    LogNode('extractor') | Mapper('mapper') | LogNode('loader')
)

pipe.consume(range(3))
```
This will produce an output of
```
extractor processing 0
   loader processing 0
extractor processing 1
   loader processing 2
extractor processing 2
   loader processing 4
```

### Reduce
Reducing, or folding, is easily implemented by using the `.begin()`
and `.end()` methods to handle accumulated values.
```python
from consecution import Node, Pipeline
class Reducer(Node):
    def begin(self):
        self.result = 0
        
    def process(self, item):
        self.result += item
        
    def end(self):
        self.push(self.result)

class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)

pipe = Pipeline(
    LogNode('extractor') | Reducer('reducer') | LogNode('loader')
)

pipe.consume(range(3))
```
This will produce an output of
```
extractor processing 0
extractor processing 1
extractor processing 2
   loader processing 3
```

### Filter
Filtering is as simple as placing the push statement behind a conditional. All
items that don't pass the conditional will not be pushed downstream, and thus
silently dropped.
```python
from consecution import Node, Pipeline
class Filter(Node):
    def process(self, item):
        if item > 3:
            self.push(item)

class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)

pipe = Pipeline(
    LogNode('extractor') | Filter('filter') | LogNode('loader')
)

pipe.consume(range(6))
```
This produces an output of
```
extractor processing 0
extractor processing 1
extractor processing 2
extractor processing 3
extractor processing 4
   loader processing 4
extractor processing 5
   loader processing 5
```

### Group By
Consecution provides a specialized class you can inherit from to perform
grouping operations.  GroupBy nodes must define two methods: `.key(item)` and
`.process(batch)`.  The `.key` method should return a key from an item that is used
to identify groups.  Any time that key changes, a new group is initiated.  Like
Python's `itertools.groupby`, you will usually want the GroupByNode to process
sorted items.  The `.process` method functions exactly like the `.process`
method on regular nodes, except that instead of being called with items,
consecution will call it with a batch of items contained in a list.
```python
class LogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)

class Batcher(GroupByNode):
    def key(self, item):
        return item // 4
    
    def process(self, batch):
        sum_val = sum(batch)
        self.push(sum_val)
        
pipe = Pipeline(
    Batcher('batcher') | LogNode('logger') 
)

pipe.consume(range(16))
```
This produces an output of
```
logger processing 6
logger processing 22
logger processing 38
logger processing 54
```

### Plugin-Style Composition
Consecution forces you to think about problems in terms of how small processing
units are connected.  This separation between logic and connectivity can be
exploited to create flexible and reusable solutions.  Basically, you specify the
connectivity you want to use in solving your problem, and then plug in the
processing units later.  Breaking the problem up in this way allows you to swap
out processing units to acheive different objectives with the same pipeline.

```python
# This function defines a pipeline that can use swappable processing nodes.
# We don't worry about how we are going to do logging or aggregating.
# We just focus on how the nodes are connected.
def pipeline_factory(log_node, agg_node):
    pipe = Pipeline(
        log_node('extractor') | agg_node('aggregator') | log_node('result_logger')
    )
    return pipe


# Now we define a node for left-justified logging
class LeftLogNode(Node):
    def process(self, item):
        print('{: <15} processing {}'.format(self.name, item))
        self.push(item)

# And one for right-justified logging
class RightLogNode(Node):
    def process(self, item):
        print('{: >15} processing {}'.format(self.name, item))
        self.push(item)

# We can aggregate by summing
class SumNode(Node):
    def begin(self):
        self.result = 0
        
    def process(self, item):
        self.result += item
        
    def end(self):
        self.push(self.result)

# Or we can aggregate by multiplying
class ProdNode(Node):
    def begin(self):
        self.result = 1
        
    def process(self, item):
        self.result *= item
        
    def end(self):
        self.push(self.result)


# Now we plug in nodes to create a pipeline that left-prints sums
sum_pipeline = pipeline_factory(log_node=LeftLogNode, agg_node=SumNode)

# And a different pipeline that right prints products
prod_pipeline = pipeline_factory(log_node=RightLogNode, agg_node=ProdNode)

print 'aggregate with sum, left justified\n' + '-'*40
sum_pipeline.consume(range(1, 5))

print '\naggregate with product, right justified\n' + '-'*40
prod_pipeline.consume(range(1, 5))
```
This produces the following output
```
aggregate with sum, left justified
----------------------------------------
extractor       processing 1
extractor       processing 2
extractor       processing 3
extractor       processing 4
result_logger   processing 10

aggregate with product, right justified
----------------------------------------
      extractor processing 1
      extractor processing 2
      extractor processing 3
      extractor processing 4
  result_logger processing 24
```

# Aggregation Example
We end with a full-blown example of using a pipeline to aggregate data from a
csv file.  The data is contained in 
<a href=""https://raw.githubusercontent.com/robdmc/consecution/master/sample_data.csv"">
a csv file </a> that looks like this.

gender |age |spent
---    |--- |---
male   |11  |39.39
female |10  |34.72
female |15  |40.02
male   |19  |26.27
male   |13  |21.22
female |40  |23.17
female |52  |33.42
male   |33  |39.52
female |16  |28.65
male   |60  |26.74


Although there are much simpler ways of solving this problem, (e.g. with <a
href=""https://github.com/robdmc/consecution/blob/master/pandashells.md"">
Pandashells</a>)
we deliberately construct a complex topology just to illustrate how to achieve
complexity when it is actually needed.

The diagram below was produced from the code beneath it.  A quick glance at the
diagram makes it obvious how the data is being routed through the system.  The
code is heavily commented to explain features of the consecution toolkit.

![Output Image](/images/gender_age.png?raw=true ""Gender Age Pipeline"")

```python
from __future__ import print_function
from collections import namedtuple
from pprint import pprint
import csv
from consecution import Node, Pipeline, GlobalState

# Named tuples are nice immutable containers 
# for passing data between nodes
Person = namedtuple('Person', 'gender age spent')

# Create a pipeline that aggregates by gender and age
# In creating the pipeline we focus on connectivity and don't
# worry about defining node behavior.
def pipe_factory(Extractor, Agg, gender_router, age_router):
    # Consecution provides a generic GlobalState class.  Any object can be used
    # as the global_state in a pipeline, but the GlobalState object provides a
    # nice abstraction where attributes can be accessed either by dot notation
    # (e.g. global_state.my_attribute) or by dictionary notation (e.g.
    # global_state['my_attribute'].  Furthermore, GlobalState objects can be
    # instantiated with initialized attributes using key-word arguments as shown
    # here.
    global_state = GlobalState(segment_totals={})

    # Notice, we haven't even defined the behavior of these nodes yet.  They
    # will be defined later and are, for now, just passed into the factory
    # function as arguments while we focus on getting the topology right.
    pipe = Pipeline(
        Extractor('make_person') |
        [
            gender_router,
            (Agg('male') | [age_router, Agg('male_child'), Agg('male_adult')]),
            (Agg('female') | [age_router, Agg('female_child'), Agg('female_adult')]),
        ],
        global_state=global_state
    )

    # Nodes can be created outside of a pipeline definition
    adult = Agg('adult')
    child = Agg('child')
    total = Agg('total')

    # Sometimes the topology you want to create cannot easily be expressed
    # using the pipeline abstraction for wiring nodes together.  You can
    # drop down to a lower level of abstraction by explicitly wiring nodes 
    # together using the .add_downstream() method.
    adult.add_downstream(total)
    child.add_downstream(total)

    # Once a pipeline has been created, you can access individual nodes
    # with dictionary-like indexing on the pipeline.
    pipe['male_child'].add_downstream(child)
    pipe['female_child'].add_downstream(child)
    pipe['male_adult'].add_downstream(adult)
    pipe['female_adult'].add_downstream(adult)

    return pipe

# Now that we have the topology of our pipeline defined, we can think about the
# logic that needs to go into each node.  We start by defining a node that takes
# a row from a csv file and tranforms it into a namedtuple.
class MakePerson(Node):
    def process(self, item):
        item['age'] = int(item['age'])
        item['spent'] = float(item['spent'])
        self.push(Person(**item))

# We now define a node to perform our aggregations.  Mutable global state comes
# with a lot of baggage and should be used with care.  This node illustrates
# how to use global state to put all aggregations in a central location that
# remains accessible when the pipeline finishes processing.
class Sum(Node):
    def begin(self):
        # initialize the node-local sum to zero
        self.total = 0

    def process(self, item):
        # increment the node-local total and push the item down stream
        self.total += item.spent
        self.push(item)

    def end(self):
        # when pipeline is done, update global state with sum
        self.global_state.segment_totals[self.name] = round(self.total, 2)


# This function routes tuples based on their associated gender
def by_gender(item):
    return '{}'.format(item.gender)

# This function routes tuples based on whether the purchaser was an adult or
# child
def by_age(item):
    if item.age >= 18:
        return '{}_adult'.format(item.gender)
    else:
        return '{}_child'.format(item.gender)

# Here we plug our node definitions into our topology to create a fully-defined
# pipeline.
pipe = pipe_factory(MakePerson, Sum, by_gender, by_age)

# We can now visualize pipeline.
pipe.plot()

# Now we feed our pipeline with rows from the csv file
with open('sample_data.csv') as f:
    pipe.consume(csv.DictReader(f))

# The global_state is also available as an attribute on the pipeline allowing
# us to access it when the pipeline is finished.  This is a good way to ""return""
# an object from a pipeline.  Here we simply print the result.
print()
pprint(pipe.global_state.segment_totals)
```

And this is the result of running the pipeline with the sample csv file.
```
{'adult': 149.12,
 'child': 164.0,
 'female': 159.98,
 'female_adult': 56.59,
 'female_child': 103.39,
 'male': 153.14,
 'male_adult': 92.53,
 'male_child': 60.61,
 'total': 313.12}
```

As illustrated in the <a
href=""https://github.com/robdmc/consecution/blob/master/pandashells.md"">
Pandashells</a> example, this aggregation is actually much more simple to
implement in Pandas.  However, there are a couple of important caveats.

The Pandas solution must load the entire csv file into memory at once.  If you
look at the pipeline solution, you will notice that each node simply increments
its local sum and passes the data downstream.  At no point is the data
completely loaded into memory.  Although the Pandas code runs much faster due to
the highly optimized vectorized math it employes, the pipeline solution can
process arbitrarily large csv files with a very small memory footprint.

Perhaps the most exciting aspect of consecution is its ability to create
repeatable and testable data analysis pipelines.  Passing Pandas Dataframes
through a consecution pipeline makes it very easy to encapsulate any analysis
into a well-defined, repeatable process where each node manipulates a dataframe
in its prescribed way. Adopting this structure in analysis projects will
undoubtedly ease the transition from analysis/research into production.

___
Projects by [robdmc](https://www.linkedin.com/in/robdecarvalho).
* [Pandashells](https://github.com/robdmc/pandashells) Pandas at the bash command line
* [Consecution](https://github.com/robdmc/consecution) Pipeline abstraction for Python
* [Behold](https://github.com/robdmc/behold) Helping debug large Python projects
* [Crontabs](https://github.com/robdmc/crontabs) Simple scheduling library for Python scripts
* [Switchenv](https://github.com/robdmc/switchenv) Manager for bash environments
* [Gistfinder](https://github.com/robdmc/gistfinder) Fuzzy-search your gists
",2023-07-07 15:52:52+00:00
consonance,consonance,Consonance/consonance,"Core consonance utilities for scheduling, reporting on, and provisioning VMs for workflows",,False,14,2020-11-14 04:42:23+00:00,2015-04-05 14:05:02+00:00,5,11,6,22,1.1-rc.4,2016-08-02 18:16:10+00:00,GNU General Public License v3.0,753,brian_20150430,44,2015-04-30 21:51:39+00:00,,2018-06-27 21:48:23+00:00,"# Consonance

[![Build Status](https://travis-ci.org/Consonance/consonance.svg?branch=develop)](https://travis-ci.org/Consonance/Consonance)
[![Coverage Status](https://coveralls.io/repos/Consonance/consonance/badge.svg?branch=develop)](https://coveralls.io/r/Consonance/consonance?branch=develop)

## About

Consonance is a cloud orchestration tool for running Docker-based tools and CWL/WDL workflows available at [Dockstore](https://dockstore.org) on fleets of VMs running in clouds.  It allows you to schedule a set of Dockstore job orders, spinning up the necessary VMs on AWS, Microsoft Azure, or OpenStack via the Youxia library for provisioning cloud-based VMs, and then tearing them down after all work is complete.

We are currently at work on Consonance 2.0 which supports anything from Dockstore and allows users to submit a variety of jobs intended for a variety of instance types.

The latest unstable releases on develop support running tools/workflows from  Dockstore.

Consonance 1.0 is currently in maintenance mode and has also been integrated into a simplified workflow launcher Pancancer Workflow Launcher for AWS for external users and a more flexible but less well documented Pancancer Workflow Launcher for users internal to OICR.

The latest stable releases on master support the pancancer project https://icgc.org/working-pancancer-data-aws

See the Consonance [wiki](https://github.com/Consonance/consonance/wiki) for more information on this project.

## Dependencies

I'm showing how to install dependencies on a Mac so adapt to your system/OS.

### PostgreSQL

You need a PostgreSQL server to run the tests, on the Mac try [Postgres.app](http://postgresapp.com/).

Once you have this setup (using whatever technique is appropriate for your system) create the `queue_status` database.

    $bash> ""/Applications/Postgres.app/Contents/Versions/10/bin/psql"" -p5432 -d ""postgres""
    postgres=# create database queue_status;
    postgres=# create user queue_user;
    postgres=# grant all privileges  on database queue_status to queue_user;

Now load the schema:

    $bash> cat ~/gitroot/Consonance/consonance/consonance-arch/sql/schema.sql | ""/Applications/Postgres.app/Contents/Versions/10/bin/psql"" -p5432 -d ""queue_status""

### RabbitMQ

The integration tests require RabbitMQ, here's how to install
via home brew:

    $bash> brew install rabbitmq
    $bash> /usr/local/sbin/rabbitmq-server

### Consonance Config File

By default the client looks for `~/.consonance/config`. You need this file for
tests to pass:

    $bash> mkdir ~/.consonance/ && cp ./consonance-client/src/test/resources/config ~/.consonance/config

### Dockstore CLI

The integration tests (see below for how to trigger) will actually simulate the full lifecycle of a WDL workflow run using the Dockstore CLI.  See http://dockstore.org for how to install, you need the `dockstore` command dependencies in your path and properly configured in order for the full integration tests to pass.  Consonance is using the Dockstore CLI library so you only need to install cwltool in your path if you want to test CWL workflows.  Cromwell for WDL workflows is baked into the Dockstore CLI library.

### Docker

Related to the Dockstore CLI above, one of the WDL workflows used for testing requires Docker.  Make sure the system you are using for testing has Docker installed if you run the full integration tests.

## Building & Testing

The build uses maven (3.2.3, look at using [MVNVM](http://mvnvm.org/)), just run:

    $bash> mvn clean install

 To avoid tests (probably a bad idea!):

    $bash> mvn -Dmaven.test.skip=true clean install

Skip tests and use parallel build (see more info [here](https://zeroturnaround.com/rebellabs/your-maven-build-is-slow-speed-it-up/)):

    $bash> mvn -Dmaven.test.skip=true -T 1C install -pl consonance-integration-testing -am

This gives me a build time of 36 seconds vs. 1:21 min for `mvn clean install`

Now run the full integration tests (assumes you have RabbitMQ and PostgreSQL installed):

    $bash> cd consonance-integration-testing
    # run a particular test class
    $bash> mvn -Dtest=SystemMainIT test
    # or even a speicifc test method!
    $bash> mvn -Dtest=SystemMainIT#testGetConfiguration test
    # or all ITs
    $bash> cd ..
    $bash> mvn -B clean install -DskipITs=false

## Monitoring Integration Tests

### RabbitMQ

See http://localhost:15672/#/queues for your queues, username and password is guest by default.

## Releasing

See the [developer page](https://github.com/Consonance/consonance/wiki/developers) on our wiki.

## Installation

See the container-admin [README](container-admin/README.md) for information on quickly setting up Consonance via Docker-compose and an interactive bootstrap configuration script.

## Using

See the [quickstart guide](https://github.com/Consonance/consonance/wiki/quickstart) and the [non-quickstart guide](https://github.com/Consonance/consonance/wiki/non-quickstart-users) on our wiki.

## TODO

* test container-admin with pre-release locally with provisioning on AWS
* review and merge the pull request
* perform release
* test container-admin with release on AWS, should be a safe harbor
* then review/try WES support branch from Abraham",2023-07-07 15:52:57+00:00
coreflow,CoreFlow,pasculescu/CoreFlow,CoreFlow - management of proteomics and genomics analysis - Samuel Lunefeld Research Institute and LindingLab,,False,0,2014-02-06 20:02:41+00:00,2013-09-18 01:07:06+00:00,1,3,1,0,,,Other,13,,0,,,2014-02-06 20:01:41+00:00,"CoreFlow
========

CoreFlow - management of proteomics and genomics analysis - Lunefeld-Tanenbaum Research Institute and LindingLab

please see http://coreflow.mshri.on.ca for a running online version.

",2023-07-07 15:53:01+00:00
cortex,cortex,cortexlabs/cortex,Production infrastructure for machine learning at scale,https://cortex.dev,False,7939,2023-07-07 07:16:18+00:00,2019-01-24 04:43:14+00:00,617,147,20,63,v0.42.1,2022-09-23 17:28:56+00:00,Apache License 2.0,2327,v0.42.1,63,2022-09-23 17:28:56+00:00,2023-07-07 07:16:18+00:00,2023-03-04 05:19:44+00:00,"**[Docs](https://docs.cortexlabs.com)** • **[Slack](https://community.cortexlabs.com)**

<br>

<img src='https://cortex-public.s3.us-west-2.amazonaws.com/logo.png' height='32'>

<br>

Note: This project is no longer actively maintained by its original authors.

# Production infrastructure for machine learning at scale

Deploy, manage, and scale machine learning models in production.

<br>

## Serverless workloads

**Realtime** - respond to requests in real-time and autoscale based on in-flight request volumes.

**Async** - process requests asynchronously and autoscale based on request queue length.

**Batch** - run distributed and fault-tolerant batch processing jobs on-demand.

<br>

## Automated cluster management

**Autoscaling** - elastically scale clusters with CPU and GPU instances.

**Spot instances** - run workloads on spot instances with automated on-demand backups.

**Environments** - create multiple clusters with different configurations.

<br>

## CI/CD and observability integrations

**Provisioning** - provision clusters with declarative configuration or a Terraform provider.

**Metrics** - send metrics to any monitoring tool or use pre-built Grafana dashboards.

**Logs** - stream logs to any log management tool or use the pre-built CloudWatch integration.

<br>

## Built for AWS

**EKS** - Cortex runs on top of EKS to scale workloads reliably and cost-effectively.

**VPC** - deploy clusters into a VPC on your AWS account to keep your data private.

**IAM** - integrate with IAM for authentication and authorization workflows.
",2023-07-07 15:53:05+00:00
cosmos,COSMOS2,Mizzou-CBMI/COSMOS2,Python Scientific Pipeline Management System,,False,72,2023-04-24 16:04:42+00:00,2013-12-13 02:54:55+00:00,39,17,19,4,2.6.24,2018-06-05 18:43:21+00:00,GNU General Public License v3.0,1546,reqs,84,2015-10-19 05:41:57+00:00,2023-05-17 16:29:44+00:00,2023-05-17 16:29:42+00:00,".. image:: https://travis-ci.org/Mizzou-CBMI/COSMOS2.svg?branch=master
    :target: https://travis-ci.org/Mizzou-CBMI/COSMOS2

Documentation
==============

`http://mizzou-cbmi.github.io/COSMOS2/ <http://mizzou-cbmi.github.io/COSMOS2/>`_


Install
==========

From pip:

.. code-block:: python

    pip install cosmos-wfm

    # Optional, recommended for visualizing Workflows:
    sudo apt-get graphviz graphviz-dev  # or brew install graphviz for mac
    pip install pygraphviz # requires graphviz

From conda:

.. code-block:: python

    conda install cosmos-wfm -c ravelbio


Introduction
============
Cosmos is a python library for creating scientific pipelines that run on a distributed computing cluster.
It is primarily designed and used for machine learning and bioinformatics pipelines,
but is general enough for any type of distributed computing workflow and is also used in fields such as image processing.

Cosmos provides a simple python api to specify any job DAG using simple python code making it extremely flexible and intuitive
- you do *not* specify your DAG using json, CWL, groovy, or some other domain specific language (DSL).

Cosmos allows you to resume modified or failed workflows, uses SQL to store job information, and provides a web dashboard for monitoring and debugging.
It is different from libraries such as `Luigi <https://github.com/spotify/luigi>`__
or `Airflow <http://airbnb.io/projects/airflow/>`__ which also try to solve ETL problems such as scheduling recurring tasks and listening for events.

Cosmos is very focused on reproducible scientific pipelines, allowing it to have a very simple state.
There is a single process per Workflow which is a python script, and a single process per Task which is python function represented by an executable script.
When a Task fails, reproducing the exact environment of a Task is as simple as re-running the command script.  Since
the command script is a python script, you can also launch it with pdb (python -m ipdb log/stage/uid/command_attempt).

The same pipeline can also easily be run on a variety of compute infrastructure: locally, in the cloud, or on a grid computing cluster.

Cosmos is intended and useful for both one-off analyses and production software.
Users have analyzed >100 whole genomes (~50TB and tens of thousands of jobs) in a single Workflow without issue, and some of the largest
clinical sequencing laboratories use it for the production and R&D workflows.  We routinely use it to run workflows
consisting of 10s of thousands of Machine Learning jobs.

AWS Batch
__________

We've been using quite a bit of AWS Batch for the past year, and this is by far the most developed and supported DRM.
It's pretty hard to continue to support DRMs that we're not using in our day-to-day.  That is mostly left to the community
using Cosmos.  Support for a DRM is contained in a single class that people often tweak for their particular distributed computing environment,
see the classes in cosmos/job/drm, the interface only has a handful of methods that must be implemented.

Make sure to check out examples/ex_awsbatch.py for details about how to use the AWS Batch DRM.
Jobs submit and terminate much faster than any other DRM.  This is a great way to utilize cheap AWS spot
instances for your workflows for both machine learning and bioinformatics workflows.  Cosmos will automatically
resubmit jobs that fail due to a spot-instance termination.


History
___________
Cosmos was published as an Application Note in the journal `Bioinformatics <http://bioinformatics.oxfordjournals.org/>`_,
but has evolved a lot since its original inception.  If you use Cosmos
for research, please cite its `manuscript <http://bioinformatics.oxfordjournals.org/content/early/2014/06/29/bioinformatics.btu385>`_. 

Since the original publication, it has been re-written and open-sourced by the original author, in a collaboration between
`The Lab for Personalized Medicine <http://lpm.hms.harvard.edu/>`_ at Harvard Medical School,
the `Wall Lab <http://wall-lab.stanford.edu/>`_ at Stanford University, and
`Invitae <http://invitae.com>`_.  Invitae is a leading clinical genetic sequencing diagnostics laboratory where
Cosmos is deployed in production and has processed hundreds of thousands of samples.
It is also used by various research groups around the world; if you use it for cool stuff please let us know!

Features
_________
* Written in python which is easy to learn, powerful, and popular.  A researcher or programmer with limited experience can begin writing Cosmos workflows right away.
* Powerful syntax for the creation of complex and highly parallelized workflows.
* Reusable recipes and definitions of tools and sub workflows allows for DRY code.
* Keeps track of workflows, job information, resource utilization and provenance in an SQL database and log files.
* The ability to visualize all jobs and job dependencies as a convenient image.
* Monitor and debug running workflows, and a history of all workflows via a web dashboard.
* Alter and resume failed workflows.

Web Dashboard
_______________
.. figure:: docs/source/_static/imgs/web_interface.png
   :align: center
   
Multi-platform Support
+++++++++++++++++++++++
* Support for running pipelines locally
* Support for running pipelines on AWSBatch (new!)
* Support for running pipelines on DRMS such as SGE, LSF, SLURM and others via DRMAA.  Adding support for more DRMs is very straightforward.
* Supports for MySQL, PosgreSQL, Oracle, SQLite by using the SQLALchemy ORM.

Bug Reports
____________

Please use the `Github Issue Tracker <https://github.com/Mizzou-CBMI/Cosmos2/issues>`_.

Testing
__________
python setup.py test

.. code-block:: bash

    py.test

Building Docs
______________

In a python2.7 environment

.. code-block:: bash

    pip install ghp-import sphinx sphinx_rtd_theme
    cd docs
    make html
    cd build/html
    ghp-import -n ./ -p

Building Conda Package
________________________

.. code-block:: bash

    python devops.py release

    rm -rf cosmos-wfm
    conda skeleton pypi cosmos-wfm --version 2.13.4
    conda build cosmos-wfm
    anaconda upload /home/egafni/miniconda3/conda-bld/linux-64/cosmos-wfm-2.13.4-py38_0.tar.bz2 -u ravelbio

Cosmos Users
_________________

Please let us know if you're using Cosmos by sending a PR with your company or lab name and any relevant information.

* Ravel Biotechnology - A Biotech startup focused on early detection of disease
* `GenomeKey <https://github.com/Mizzou-CBMI/GenomeKey>`__ - A GATK best practices variant calling pipeline.
* `PV-Key  <https://github.com/Mizzou-CBMI/PvKey>`__ - Somatic Tumor/normal variant calling pipeline.
* `MC-Key <https://bitbucket.org/shazly/mcgk>`__ - Multi-cloud implementation of GenomeKey.
* `Invitae <http://invitae.com>`__ - Clinical NGS sequencing laboratory.  Utilizes Cosmos for production variant calling pipelines and R&D analysis.
* `NGXBIO <https://ngxbio.com/>`__ - NGS Sequencing as a Service.
* `EnGenome <https://engenome.com/en/>`__ - Bioinformatics and NGS Analysis.
* `Freenome <https://freenome.com>`__ - Liquid Biopsy Sequencing Laboratory, specializing in Machine Learning

Publications using Cosmos
__________________________

1) Elshazly H, Souilmi Y, Tonellato PJ, Wall DP, Abouelhoda M (2017) MC-GenomeKey: a multicloud system for the detection and annotation of genomic variants. BMC Bioinformatics, 18(1), 49.

2) Souilmi Y, Lancaster AK, Jung JY, Rizzo E, Hawkins JB, Powles R, Amzazi S, Ghazal H, Tonellato PJ, Wall DP (2015) Scalable and cost-effective NGS genotyping in the cloud. BMC Medical Genomics, 8(1), 64.

3) Souilmi Y., Jung J-Y., Lancaster AK, Gafni E., Amzazi S., Ghazal H., Wall DP., Tonellato, P. (2015). COSMOS: cloud enabled NGS analysis. BMC Bioinformatics, 16(Suppl 2), A2. doi: 10.1186/1471-2105- 16-S2- A2

4) Gafni E, Luquette LJ, Lancaster AK, Hawkins JB, Jung J-Y, Souilmi Y, Wall DP, Tonellato PJ: COSMOS: Python library for massively parallel workflows. Bioinformatics (2014) 30 (20): 2956-2958. doi: 10.1093/bioinformatics/btu385

5) Hawkins JB, Souilmi Y, Powles R, Jung JY, Wall DP, Tonellato PJ (2013) COSMOS: NGS Analysis in the Cloud. AMIA TBI. BMC Medical Genomics


Changelog
__________

2.13.0
+++++++

SQL Column added!  If you see this error:

    sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: task.status_reason

It's because this new version of cosmos is not backwards
compatible with these old databases.  This can be easily fixed by migrating the old database

To use cosmos 2.13.0 on old databases, you must add this new column.  Ex:

.. code-block:: bash

    sqlite cosmos.sqlite
    sqlite> alter table task add status_reason CHAR(255)

* added capability to reattempt jobs if and only if they died due to an AWSBatch spot instance failure.
  see examples/ex_awsbatch.py


2.12.0
++++++

* sped up AWS Batch DRM.  Can now submit many thousands of jobs (and terminate them) very quickly.

2.11.0
++++++++

* Dropped support for python2



2.5.1
++++++

API Change!

* Removed Workflow.run(int: max_attempts) and replaced with Cosmos.start(int: default_max_attempts)
* Added Workflow.add_task(int: max_attempts) to specify individual Task retry numbers


2.5.0
++++++

* Added support for python3

2.0.1
++++++
Some pretty big changes here, incurred during a hackathon at Invitae where a lot of feedback and contributions were received.  Primarily, the api was simplified and made
more intuitive.  A new Cosmos primitive was created called a Dependency, which we have found extremely useful for generalizing subworkflow recipes.
This API is now considered to be much more stable.

* Renamed Execution -> Workflow
* Reworked Workflow.add_task() api, see its docstring.
* Renamed task.tags -> task.params.
* Require that a task's params do not have keywords that do not exist in a task's functions parameters.
* Require that a user specify a task uid (unique identifer), which is now used for resuming instead of a Task's params.
* Created Cosmos.api.Dependency, which provides a way to specify a parent and input at the same time.
* Removed one2one, one2many, etc. helpers.  Found this just confused people more than helped.
* Various stability improvements to the drmaa jobmanager module

",2023-07-07 15:53:09+00:00
couler,couler,couler-proj/couler,"Unified Interface for Constructing and Managing Workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.",https://couler-proj.github.io/couler/index.html,False,830,2023-07-07 08:34:45+00:00,2020-08-18 17:41:29+00:00,81,25,31,5,v0.1.1rc8-stable,2021-04-07 20:13:09+00:00,Apache License 2.0,190,v0.1.1rc8,5,2021-03-23 14:06:58+00:00,2023-07-07 14:25:06+00:00,2023-07-03 20:23:09+00:00,"[![CI](https://github.com/couler-proj/couler/workflows/CI/badge.svg)](https://github.com/couler-proj/couler/actions?query=event%3Apush+branch%3Amaster)
[![Twitter](https://img.shields.io/badge/@CoulerProject--_.svg?style=social&logo=twitter)](https://twitter.com/CoulerProject)

# Couler

## What is Couler?

Couler aims to provide a unified interface for constructing and managing workflows on
different workflow engines, such as [Argo Workflows](https://github.com/argoproj/argo-workflows), [Tekton Pipelines](https://tekton.dev/), and [Apache Airflow](https://airflow.apache.org/).

Couler is included in [CNCF Cloud Native Landscape](https://landscape.cncf.io/) and [LF AI Landscape](https://landscape.lfai.foundation).

> Note that while one of ambitious goals of Couler is to support multiple workflow engines, Couler currently only supports Argo Workflows as the workflow orchestration backend. In addition, if you are looking for a Python SDK that provides access to all the available features from Argo Workflows, you might want to check out [the low-level Python SDK maintained by the Argo Workflows team](https://github.com/argoproj/argo-workflows/tree/master/sdks/python).

## Who uses Couler?

You can find a list of organizations who are using Couler in [ADOPTERS.md](ADOPTERS.md). If you'd like to add your organization to the list, please send us a pull request.

## Why use Couler?

Many workflow engines exist nowadays, e.g. [Argo Workflows](https://github.com/argoproj/argo-workflows), [Tekton Pipelines](https://tekton.dev/), and [Apache Airflow](https://airflow.apache.org/).
However, their programming experience varies and they have different level of abstractions
that are often obscure and complex. The code snippets below are some examples for constructing workflows
using Apache Airflow and [Kubeflow Pipelines](https://github.com/kubeflow/pipelines/).

<table>
<tr><th>Apache Airflow</th><th>Kubeflow Pipelines</th></tr>
<tr>
<td valign=""top""><p>

```python
def create_dag(dag_id,
               schedule,
               dag_number,
               default_args):
    def hello_world_py(*args):
        print('Hello World')

    dag = DAG(dag_id,
              schedule_interval=schedule,
              default_args=default_args)
    with dag:
        t1 = PythonOperator(
            task_id='hello_world',
            python_callable=hello_world_py,
            dag_number=dag_number)
    return dag

for n in range(1, 10):
    default_args = {'owner': 'airflow',
                    'start_date': datetime(2018, 1, 1)
                    }
    globals()[dag_id] = create_dag(
        'hello_world_{}'.format(str(n)),
        '@daily',
        n,
        default_args)
```

</p></td>
<td valign=""top""><p>

```python
class FlipCoinOp(dsl.ContainerOp):
    """"""Flip a coin and output heads or tails randomly.""""""
    def __init__(self):
        super(FlipCoinOp, self).__init__(
            name='Flip',
            image='python:alpine3.6',
            command=['sh', '-c'],
            arguments=['python -c ""import random; result = \'heads\' if random.randint(0,1) == 0 '
                       'else \'tails\'; print(result)"" | tee /tmp/output'],
            file_outputs={'output': '/tmp/output'})

class PrintOp(dsl.ContainerOp):
    """"""Print a message.""""""
    def __init__(self, msg):
        super(PrintOp, self).__init__(
            name='Print',
            image='alpine:3.6',
            command=['echo', msg],
        )

# define the recursive operation
@graph_component
def flip_component(flip_result):
    print_flip = PrintOp(flip_result)
    flipA = FlipCoinOp().after(print_flip)
    with dsl.Condition(flipA.output == 'heads'):
        flip_component(flipA.output)

@dsl.pipeline(
    name='pipeline flip coin',
    description='shows how to use graph_component.'
)
def recursive():
    flipA = FlipCoinOp()
    flipB = FlipCoinOp()
    flip_loop = flip_component(flipA.output)
    flip_loop.after(flipB)
    PrintOp('cool, it is over. %s' % flipA.output).after(flip_loop)
```

</p></td>
</tr>
</table>

Couler provides a unified interface for constructing and managing workflows that provides the following:

* Simplicity: Unified interface and imperative programming style for defining workflows with automatic construction of directed acyclic graph (DAG).
* Extensibility: Extensible to support various workflow engines.
* Reusability: Reusable steps for tasks such as distributed training of machine learning models.
* Efficiency: Automatic workflow and resource optimizations under the hood.

Please see the following sections for installation guide and examples.

## Installation

* Couler currently only supports Argo Workflows. Please see instructions [here](https://argoproj.github.io/argo-workflows/quick-start/#install-argo-workflows)
to install Argo Workflows on your Kubernetes cluster.
* Install Python 3.6+
* Install Couler Python SDK via the following command:

```bash
python3 -m pip install git+https://github.com/couler-proj/couler --ignore-installed
```
Alternatively, you can clone this repository and then run the following to install:

```bash
python setup.py install
```

## Try Couler with Argo Workflows

Click [here](https://katacoda.com/argoproj/courses/argo-workflows/python) to launch the interactive Katacoda environment and learn how to write and submit your first Argo workflow using Couler Python SDK in your browser!

## Examples

### Coin Flip

This example combines the use of a Python function result, along with conditionals,
to take a dynamic path in the workflow. In this example, depending on the result
of the first step defined in `flip_coin()`, the template will either run the
`heads()` step or the `tails()` step.

Steps can be defined via either `couler.run_script()`
for Python functions or `couler.run_container()` for containers. In addition,
the conditional logic to decide whether to flip the coin in this example
is defined via the combined use of `couler.when()` and `couler.equal()`.

```python
import couler.argo as couler
from couler.argo_submitter import ArgoSubmitter


def random_code():
    import random

    res = ""heads"" if random.randint(0, 1) == 0 else ""tails""
    print(res)


def flip_coin():
    return couler.run_script(image=""python:alpine3.6"", source=random_code)


def heads():
    return couler.run_container(
        image=""alpine:3.6"", command=[""sh"", ""-c"", 'echo ""it was heads""']
    )


def tails():
    return couler.run_container(
        image=""alpine:3.6"", command=[""sh"", ""-c"", 'echo ""it was tails""']
    )


result = flip_coin()
couler.when(couler.equal(result, ""heads""), lambda: heads())
couler.when(couler.equal(result, ""tails""), lambda: tails())

submitter = ArgoSubmitter()
couler.run(submitter=submitter)
```

### DAG

This example demonstrates different ways to define the workflow as a directed-acyclic graph (DAG) by specifying the
dependencies of each task via `couler.set_dependencies()` and `couler.dag()`. Please see the code comments for the
specific shape of DAG that we've defined in `linear()` and `diamond()`.

```python
import couler.argo as couler
from couler.argo_submitter import ArgoSubmitter


def job(name):
    couler.run_container(
        image=""docker/whalesay:latest"",
        command=[""cowsay""],
        args=[name],
        step_name=name,
    )


#     A
#    / \
#   B   C
#  /
# D
def linear():
    couler.set_dependencies(lambda: job(name=""A""), dependencies=None)
    couler.set_dependencies(lambda: job(name=""B""), dependencies=[""A""])
    couler.set_dependencies(lambda: job(name=""C""), dependencies=[""A""])
    couler.set_dependencies(lambda: job(name=""D""), dependencies=[""B""])

#   A
#  / \
# B   C
#  \ /
#   D
def diamond():
    couler.dag(
        [
            [lambda: job(name=""A"")],
            [lambda: job(name=""A""), lambda: job(name=""B"")],  # A -> B
            [lambda: job(name=""A""), lambda: job(name=""C"")],  # A -> C
            [lambda: job(name=""B""), lambda: job(name=""D"")],  # B -> D
            [lambda: job(name=""C""), lambda: job(name=""D"")],  # C -> D
        ]
    )


linear()
submitter = ArgoSubmitter()
couler.run(submitter=submitter)
```

Note that the current version only works with Argo Workflows but we are actively working on the design of the unified
interface that is extensible to additional workflow engines. Please stay tuned for more updates and we welcome
any feedback and contributions from the community.

## Community Blogs and Presentations

* [Introducing Couler: Unified Interface for Constructing and Managing Workflows, Argo Workflows Community Meeting](https://docs.google.com/presentation/d/11KVEkKQGeV3R_-nHdqlzQV2uOrya94ra6Ilm_k6RwE4/edit?usp=sharing)
* [Authoring and Submitting Argo Workflows using Python](https://blog.argoproj.io/authoring-and-submitting-argo-workflows-using-python-aff9a070d95f)
",2023-07-07 15:53:13+00:00
covalent,covalent,AgnostiqHQ/covalent,Pythonic tool for running machine-learning/high performance/quantum-computing workflows in heterogenous environments.,https://www.covalent.xyz,False,485,2023-07-06 13:10:51+00:00,2021-11-03 14:02:07+00:00,53,20,35,208,v0.220.0,2023-04-28 01:09:19+00:00,GNU Affero General Public License v3.0,719,v0.226.0-rc.0,279,2023-06-09 17:13:06+00:00,2023-07-07 08:40:42+00:00,2023-07-07 07:48:42+00:00,"&nbsp;

<div align=""center"">

<img src=""https://raw.githubusercontent.com/AgnostiqHQ/covalent/master/doc/source/_static/covalent_readme_banner.svg"" width=150%>

[![version](https://img.shields.io/github/v/tag/AgnostiqHQ/covalent?color=navy&include_prereleases&label=version&sort=semver)](https://github.com/AgnostiqHQ/covalent/blob/develop/CHANGELOG.md)
[![python](https://img.shields.io/pypi/pyversions/cova)](https://github.com/AgnostiqHQ/covalent)
[![tests](https://github.com/AgnostiqHQ/covalent/actions/workflows/tests.yml/badge.svg)](https://github.com/AgnostiqHQ/covalent/actions/workflows/tests.yml)
[![docs](https://readthedocs.org/projects/covalent/badge/?version=latest)](https://covalent.readthedocs.io/en/latest/?badge=latest)
[![codecov](https://codecov.io/gh/AgnostiqHQ/covalent/branch/master/graph/badge.svg?token=YGHCB3DE4P)](https://codecov.io/gh/AgnostiqHQ/covalent)
[![agpl](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0.en.html)

</div>



## 🤔  What is Covalent?

Covalent is a Pythonic workflow tool for computational scientists, AI/ML software engineers, and anyone who needs to run experiments on limited or expensive computing resources including quantum computers, HPC clusters, GPU arrays, and cloud services.

Covalent enables a researcher to run computation tasks on an advanced hardware platform – such as a quantum computer or serverless HPC cluster – using a single line of code.
<p align=""center"">
<img src=""./doc/source/_static/Executor_Gif_Full_List.gif"" width=""60%"" alt=""Covalent Executors""></img>
</p>

## 💭 Why Covalent?

Covalent overcomes computational and operational challenges inherent in AI/ML experimentation.

| **Computational challenges**                                                                                                                                                                                                                                                                                         | **Operational challenges**                                                                                                                                                                                                     |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <ul><li>Advanced compute hardware is expensive, and access is often limited – shared with other researchers, for example.</li><li>You'd like to iterate quickly, but running large models takes time.</li><li>Parallel computation speeds execution, but requires careful attention to data relationships.</li></ul> | <ul><li>Proliferation of models, datasets, and hardware trials.</li><li> Switching between development tools, including notebooks, scripts, and submission queues.</li><li>Tracking, repeating, and sharing results.</li></ul> |

<p align=""center"">
<img src=""./doc/source/_static/Cover-banner-readme.png"" width=""100%"" alt=""Covalent value""></img>
</p>

With Covalent, you:
- Assign functions to appropriate resources: Use advanced hardware (quantum computers, HPC clusters) for the heavy lifting and commodity hardware for bookkeeping.
- Test functions on local servers before shipping them to advanced hardware.
- Let Covalent's services analyze functions for data independence and automatically parallelize them.
- Run experiments from a Jupyter notebook (or whatever your preferred interactive Python environment is).
- Track workflows and examine results in a browser-based GUI.



## 📦 Installation

Covalent is developed using Python version 3.8 on Linux and macOS. The easiest way to install Covalent is using the PyPI package manager:

```console
pip install covalent
```

Refer to the [Quick Start](https://covalent.readthedocs.io/en/latest/getting_started/quick_start/index.html) guide for quick setup instructions, or to the [First Experiment](https://covalent.readthedocs.io/en/latest/getting_started/first_experiment/index.html) guide for a more thorough approach. For a full list of supported platforms, see the Covalent [compatibility matrix](https://covalent.readthedocs.io/en/latest/getting_started/compatibility.html).

## 🚀 Getting started

Ready to try it? Go to the [First Experiment](https://covalent.readthedocs.io/en/latest/getting_started/first_experiment/index.html) guide in the documentation.

For a more in-depth description of Covalent's features and how they work, see the [Concepts](https://covalent.readthedocs.io/en/latest/concepts/concepts.html) page in the documentation.

<table border=""0"">
 <tr>
    <td><b style=""font-size:30px"">📚 Know more !&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></td>
    <td><b style=""font-size:30px"">✍️ Tutorials and Examples&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></td>
 </tr>
 <tr>
    <td>

  - [What is Covalent?](https://www.covalent.xyz/what-is-covalent/)
  - [Covalent in the era of cloud-HPC](https://www.covalent.xyz/navigating-the-modern-hpc-landscape/)
  - [Basic Concepts of Covalent](https://covalent.readthedocs.io/en/latest/concepts/concepts.html)
  - [How does Covalent work?](#how-does-it-work)
</td>
    <td>

  - [Covalent with pytorch for classical machine learning](https://covalent.readthedocs.io/en/latest/tutorials/0_ClassicalMachineLearning/mnist_images/source.html)
  - [Covalent with pennylane for quantum machine learning](https://covalent.readthedocs.io/en/latest/tutorials/1_QuantumMachineLearning/quantum_embedding_kernel/source.html)
  - [Covalent with Qiskit for quantum-SVM](https://covalent.readthedocs.io/en/latest/tutorials/1_QuantumMachineLearning/classical_quantum_svm/source.html)
  - [Covalent with Dask for scaling Ensemble classification](https://covalent.readthedocs.io/en/latest/tutorials/1_QuantumMachineLearning/pennylane_ensemble_classification/source.html)
  - [Covalent for Deep Neural Network on AWS](https://covalent.readthedocs.io/en/latest/tutorials/machine_learning/dnn_comparison.html)
</td>
 </tr>
</table>


## How Does It Work?
<img src=""https://raw.githubusercontent.com/AgnostiqHQ/covalent/master/doc/source/_static/cova_archi.png"" align=""right"" width=""40%"" alt=""Covalent Architecture""/>
Covalent has three main components:

- A Python module containing an API that you use to build manageable workflows out of new or existing Python functions.
- A set of services that run locally or on a server to dispatch and execute workflow tasks.
- A browser-based UI from which to manage workflows and view results.

You compose workflows using the Covalent API by simply adding a single line of python decorator and submit them to the Covalent server. The server analyzes the workflow to determine dependencies between tasks, then dispatches each task to its specified execution backend. Independent tasks are executed concurrently if resources are available.

The Covalent UI displays the progress of each workflow at the level of individual tasks.

<details>
<summary> <h3> The Covalent API </h3> </summary>
The Covalent API is a Python module containing a small collection of classes that implement server-based workflow management. The key elements are two decorators that wrap functions to create managed *tasks* and *workflows*.

The task decorator is called an *electron*. The electron decorator simply turns the function into a dispatchable task.

The workflow decorator is called a *lattice*. The lattice decorator turns a function composed of electrons into a manageable workflow.
</details>
<details>
<summary> <h3>  Covalent Services </h3> </summary>
The Covalent server is a lightweight service that runs on your local machine or a server. A dispatcher analyzes workflows (lattices) and hands its component functions (electrons) off to executors. Each executor is an adaptor to a backend hardware resource. Covalent has a growing list of turn-key executors for common compute backends. If no executor exists yet for your compute platform, Covalent supports writing your own.
</details>
<details>
<summary> <h3>  The Covalent GUI </h3> </summary>
The Covalent user interface runs as a web server on the machine where the Covalent server is running. The GUI dashboard shows a list of dispatched workflows. From there, you can drill down to workflow details or a graphical view of the workflow. You can also view logs, settings, and result sets.
</details>

## 📚 Documentation

The official documentation includes tips on getting started, high-level concepts, tutorials, and the API documentation, and more. To learn more, see the [Covalent documentation](https://covalent.readthedocs.io/en/latest/).

## Troubleshooting

Solutions to common issues can be found in the [Troubleshooting Guide](https://covalent.readthedocs.io/en/latest/).

## ✔️  Contributing

To contribute to Covalent, refer to the [Contribution Guidelines](https://github.com/AgnostiqHQ/covalent/blob/master/CONTRIBUTING.md). We use GitHub's [issue tracking](https://github.com/AgnostiqHQ/covalent/issues) to manage known issues, bugs, and pull requests. Get started by forking the develop branch and submitting a pull request with your contributions. Improvements to the documentation, including tutorials and how-to guides, are also welcome from the community. For more more information on adding tutorials, check the [Tutorial Guidelines](https://github.com/AgnostiqHQ/covalent/blob/master/doc/TUTORIAL_GUIDELINES.md) Participation in the Covalent community is governed by the [Code of Conduct](https://github.com/AgnostiqHQ/covalent/blob/master/CODE_OF_CONDUCT.md).

## ⚓ Citation

Please use the following citation in any publications:

> https://doi.org/10.5281/zenodo.5903364

## 📃 License

Covalent is licensed under the GNU Affero GPL 3.0 License. Covalent may be distributed under other licenses upon request. See the [LICENSE](https://github.com/AgnostiqHQ/covalent/blob/master/LICENSE) file or contact the [support team](mailto:support@agnostiq.ai) for more details.


>For a detailed history of changes and new features, see the [Changelog](https://github.com/AgnostiqHQ/covalent/blob/master/CHANGELOG.md).
",2023-07-07 15:53:17+00:00
cpipe,cpipe,MelbourneGenomics/cpipe,The open source version of the Melbourne Genomics Health Alliance Exome Sequencing Pipeline,,False,32,2022-02-02 10:39:32+00:00,2015-04-02 07:28:28+00:00,14,14,4,18,v2.5.1,2017-07-17 01:42:48+00:00,Other,356,v2.5.1,20,2017-07-17 01:42:48+00:00,,2016-02-26 05:59:22+00:00,"Cpipe  
=======================

Cpipe is a clinically focused exome sequencing pipeline developed
by the Melbourne Genomics Health Alliance. Cpipe offers an industry
standard variant calling pipeline with a suite of additional features 
needed by diagnostic laboratories added on top.

To set up Cpipe, clone this repository and then run the install script:

    git clone https://github.com/MelbourneGenomics/cpipe.git
    cd cpipe
    ./pipeline/scripts/install.sh

For further instructions, take a look at the [User Guide](https://melbournegenomics.github.io/docs/Cpipe_User_Guide.pdf).

",2023-07-07 15:53:22+00:00
cromwell,cromwell,broadinstitute/cromwell,Scientific workflow engine designed for simplicity & scalability. Trivially transition between one off use cases to massive scale production environments,http://cromwell.readthedocs.io/,False,903,2023-07-07 02:15:29+00:00,2015-04-17 19:39:36+00:00,334,112,116,90,85,2023-02-16 19:36:49+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",5396,staging,96,2015-08-21 15:40:10+00:00,2023-07-07 14:06:36+00:00,2023-07-06 18:15:49+00:00,"[![codecov](https://codecov.io/gh/broadinstitute/cromwell/branch/develop/graph/badge.svg)](https://codecov.io/gh/broadinstitute/cromwell)

## Welcome to Cromwell

Cromwell is an open-source Workflow Management System for bioinformatics. Licensing is [BSD 3-Clause](LICENSE.txt).

The [Cromwell documentation has a dedicated site](https://cromwell.readthedocs.io/en/stable).

First time to Cromwell? Get started with [Tutorials](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/).

### Community

Thinking about contributing to Cromwell? Get started by reading our [Contributor Guide](CONTRIBUTING.md).

Cromwell has a growing ecosystem of community-backed projects to make your experience even better! Check out our [Ecosystem](https://cromwell.readthedocs.io/en/stable/Ecosystem/) page to learn more.

Talk to us:
- [Join the Cromwell Slack workspace](https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g) to discuss the Cromwell workflow engine.
- [Join the OpenWDL Slack workspace](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw) to discuss the evolution of the WDL language itself.
    - More information about WDL is available in [that project's repository](https://github.com/openwdl/wdl).  

### Capabilities and roadmap

Many users today run their WDL workflows in [Terra](https://app.terra.bio/), a managed cloud bioinformatics platform with built-in WDL support provided by Cromwell. See [here](https://support.terra.bio/hc/en-us/articles/360036379771-Get-started-running-workflows) for a quick-start guide.

Users with specialized needs who wish to install and maintain their own Cromwell instances can [download](https://github.com/broadinstitute/cromwell/releases) a JAR or Docker image. The development team accepts reproducible bug reports from self-managed instances, but cannot feasibly provide direct support.

[Cromwell's backends](https://cromwell.readthedocs.io/en/stable/backends/Backends/) receive development resources proportional to user demand. The team is actively developing for Google Cloud and Microsoft Azure (see [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure)). Maintenance of other backends is primarily community-based.

Cromwell [supports](https://cromwell.readthedocs.io/en/stable/LanguageSupport/) the WDL workflow language. Cromwell version 80 and above no longer support CWL.

CWL will be re-introduced at a later date in the [Terra platform](https://terra.bio/), using a solution other than Cromwell. See the blog post [""Terra’s roadmap to supporting more workflow languages""](https://terra.bio/terras-roadmap-to-supporting-more-workflow-languages/) for details.

### Security reports

If you believe you have found a security issue please contact `infosec@broadinstitute.org`.

### Issue tracking

Need to file an issue? Head over to [Github Issues](https://github.com/broadinstitute/cromwell/issues).

If you previously filed an issue in JIRA, the link is [here](https://broadworkbench.atlassian.net/jira/software/c/projects/CROM/issues). New signups are no longer accepted.

![Jamie, the Cromwell pig](docs/jamie_the_cromwell_pig.png)
",2023-07-07 15:53:26+00:00
cumulus,cumulus,Kitware/cumulus,A REST API for creating and using cloud clusters,,False,27,2022-07-27 10:48:10+00:00,2015-07-29 18:34:03+00:00,8,11,13,0,,,Apache License 2.0,1508,v1.0.0,8,2019-05-16 15:04:16+00:00,,2020-11-23 18:03:14+00:00,"# cumulus
A platform for building HPC workflows.

The goal of the project is to provide a platform for developing HPC workflows. Cumulus enables running workflows on traditional or on-demand clusters. It provides the ability to create compute resources on cloud computing platforms such as AWS, and then to provision MPI clusters on top of those resources using Ansible.

Job management is supported through traditional batch job schedulers. We also provide a workflow engine called TaskFlow to allow a collection of Celery tasks to be run as a workflow.

See [documentation](docs/README.md) for more details.



",2023-07-07 15:53:31+00:00
cuneiform,cuneiform,joergen7/cuneiform,Cuneiform distributed programming language,https://cuneiform-lang.org/,False,215,2023-05-29 12:02:01+00:00,2014-05-20 16:58:18+00:00,15,17,5,12,3.0.5,2022-05-27 07:48:04+00:00,Apache License 2.0,554,3.0.5,12,2022-05-27 07:48:04+00:00,2023-07-01 06:15:55+00:00,2023-07-01 06:15:39+00:00,"![Cuneiform: Data analysis open and general](priv/cuneiform_title.jpg)

[![hex.pm](https://img.shields.io/hexpm/v/cuneiform.svg?style=flat-square)](https://hex.pm/packages/cuneiform) [![Build Status](https://travis-ci.org/joergen7/cuneiform.svg?branch=master)](https://travis-ci.org/joergen7/cuneiform)

Cuneiform is a large-scale data analysis functional programming language. It is *open* because it easily integrates foreign tools and libraries, e.g., Python libraries or command line tools. It is *general* because it has the expressive power of a functional programming language while using the independence of sub-expressions to automatically parallelize programs. Cuneiform uses distributed Erlang to scalably run in cluster and cloud environments.

[cuneiform-lang.org](https://www.cuneiform-lang.org/)

## Usage

### Compiling

Having rebar3 available on your system, compile the project by entering

    rebar3 escriptize

### Displaying Cuneiform Help

Compiling the Cuneiform client using `rebar3 escriptize` creates an Erlang script file `_build/default/bin/cfl` which allows starting Cuneiform via the command line. 

To display a help text enter

    _build/default/bin/cfl --help


This will show the command line synopsis, which looks like the following:

    Usage: cfl [-v] [-h] [-n <n_wrk>] [-w <wrk_dir>] [-r <repo_dir>]
               [-d <data_dir>]

      -v, --version   Show cf_worker version.
      -h, --help      Show command line options.
      -n, --n_wrk     Number of worker processes to start. 0 means auto-detect 
                      available processors.
      -w, --wrk_dir   Working directory in which workers store temporary files.
      -r, --repo_dir  Repository directory for intermediate and output data.
      -d, --data_dir  Data directory where input data is located.

This script is self-contained and can be moved around to, e.g., `~/bin/cfl`. From here on we assume that the `cfl` script is accessible in your system path and that we can start it by just entering `cfl` instead of `_build/default/bin/cfl`.


#### Starting an Interactive Shell

You can start a shell and program Cuneiform interactively by starting it without any command line parameters like so:

    cfl

This will open a shell giving the following initial output, along with a number of status messages:

               @@WB      Cuneiform
              @@E_____
         _g@@@@@WWWWWWL  Type help for usage info
       g@@#*`3@B              quit to exit shell
      @@P    3@B
      @N____ 3@B         http://www.cuneiform-lang.org
      ""W@@@WF3@B         Jorgen Brandt

    1>

Note that starting Cuneiform like that will create a local instance of a Cuneiform that entails the scheduler service, a client service, and as many worker services as CPUs were detected on the host machine. To set up a distributed Cuneiform system, these services need to be started separately on multiple hosts as needed.


#### Running a Cuneiform Script

Alternatively, Cuneiform can be started by giving it a source file which will only output the final result of the computation. If your Cuneiform script is stored in `my_script.cfl` start it by entering

    cfl my_script.cfl

## Examples

A collection of self-contained Cuneiform examples is available under [joergen7/cuneiform-examples](https://github.com/joergen7/cuneiform-examples).

### Variable assignment

You can assign a value to a variable and retrieve a variable's content like so:

    let x : Str =
      ""foo"";

    x;

In the first line we assign the value `""foo""` to a variable named `x` declaring its type to be `Str`. In the last line we query the variable `x`.

### Booleans and Conditions

We can branch execution based on conditions using conditional statements. Conditionals are expressions.

    let x : Str =
      if true
      then
        ""bla""
      else
        ""blub""
      end;

    x;

The above command the conditional binding the string `""bla""` to the variable `x`. Then, we query the variable.

### Lists

We can construct list literals by enumerating their elements in square brackets and declaring the type of the list elements.

    let xs : [Bool] =
      [true, false, true, true : Bool];

    xs;

Here, we define the list `xs` whose elements are of type `Bool` giving four Boolean values of which only the second is `false`.

### Records and Pattern Matching

A record is a collection of fields that can be accessed via their labels. Literal records can be constructed like so:

    let r : <a : Str, b : Bool> =
      <a = ""blub"", b = false>;

    ( r|a );

We define a record `r` with two fields `a` and `b`, of types `Str` and `Bool` respectively. The field associated with `a` gets the value `""blub""` while the field associated with `b` gets the value `false`. In the last line we access the `a` field of the record `r`.

Alternatively, we can access record fields via pattern matching:

    let <a = z : Str> = r;
    z;

In the first line we associate the variable `z` with the field `a` of record `r`. In the second line we query the content of `z`.

### Native Function Definition

Defining native functions in Cuneiform is done by giving the function name, its signature, and a body expression in curly braces:

    def identity( x : Str ) -> Str {
      x
    }

    identity( x = ""bar"" );

In the first line we define the function `identity` which consumes an argument `x` of type `Str` and produces a return value of type `Str`. In the second line, the body expression is just the argument `x`. In the last line we call the function binding the argument `x` to the value `""bar""`.

### Foreign Function Definition

Defining foreign functions is done by giving the function name, its signature, the foreign language name, and the function body in mickey-mouse-eared curly braces.

    def greet( person : Str ) -> <out : Str> in Bash *{
      out=""Hello $person""
    }*

    greet( person = ""Peter"" );

The first line defines a foreign function `greet` taking one argument `person` of type `Str` and returning a tuple with a single field `out` of type `Str`. The foreign function body is given in Bash code. In the last line we call the foreign function, binding the argument `person` to the string value `""Peter""`.

### Iterating over Lists using For

To perform an operation on each element of a list, one can iterate using for:

    let xs : [Bool] =
      [true, false, true, true : Bool];

    for x : Bool <- xs do
      not x
      : Bool
    end;

Here, we define a list of four Booleans and negate each element.

### Aggregating Lists using Fold

We can aggregate over lists using fold:

    def add( a : Str, b : Str ) -> <c : Str> in Python *{
      c = int( a )+int( b )
    }*

    let xs : [Str] = [1, 2, 3 : Str];

    let sum : Str =
      fold acc : Str = 0, x : Str <- xs do
        ( add( a = acc, b = x )|c )
      end;

    sum;

Here, we first define the function `add` which lets us add two numbers in Python and then the string list `xs` containing the numbers from one to three. We aggregate the sum of the numbers in `xs` and store it the result in the variable `sum`. Lastly, we query the `sum` variable.

## System Requirements

- [Erlang](https://www.erlang.org) OTP 19.0 or higher
- [Rebar3](https://www.rebar3.org) 3.0.0 or higher

## Resources

- [cuneiform-lang.org](https://www.cuneiform-lang.org/). Official website of the Cuneiform programming language.
- [joergen7/cuneiform-examples](https://github.com/joergen7/cuneiform-examples). Collection of small, self-contained Cuneiform code examples.
- [joergen7/cre](https://github.com/joergen7/cre). A common runtime environment (CRE) for distributed workflow languages.
- [joergen7/cf_client](https://github.com/joergen7/cf_client). A Cuneiform client implementation.
- [joergen7/cf_worker](https://github.com/joergen7/cf_worker). A Cuneiform worker implementation.

## License

[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0.html)
",2023-07-07 15:53:35+00:00
cwlexec,cwlexec,yuch7/cwlexec,A new open source tool to run CWL workflows on LSF,,False,35,2022-06-14 14:24:11+00:00,2018-04-18 08:23:54+00:00,8,10,7,5,v0.2.2,2019-05-17 04:57:58+00:00,Other,181,v0.2.2,5,2019-05-17 04:57:58+00:00,,2021-06-08 07:20:03+00:00,"# cwlexec
[![Build Status](https://travis-ci.com/IBMSpectrumComputing/cwlexec.svg?branch=master)](https://travis-ci.com/IBMSpectrumComputing/cwlexec)

<a href=""https://www.youtube.com/watch?v=_jSTZMWtPAY""><img align=""right"" src=""cwlexec-introduction.png"" height=""224"" width=""320"" alt=""[Video] CWLEXEC: A new open source tool to run CWL workflows on LSF""></a>

`cwlexec` implements running CWL (Common Workflow Language) workflows on IBM Spectrum LSF. It is written in Java and tested for Java 8, with the following features:

* Tight integration with `IBM® Spectrum LSF`
* Leverages `LSF` features (such as native container support)
* Implements CWL draft-3 and v1.0 with a few exceptions (SoftwareRequirement, include directive, remote location in File/Directory specification)





## Install
Installing `cwlexec` is a simple process of downloading and extracting the package.

Before downloading the package, make sure you installed [IBM Spectrum LSF 10.1.0.3](https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_welcome/lsf_welcome.html) (or above) and Java Runtime Environment (version 8), and that you set the `JAVA_HOME` environment variable.

Download the latest release package from https://github.com/IBMSpectrumComputing/cwlexec/releases and extract the package.

```
tar xzvf cwlexec-0.2.2.tar.gz
```
Add the extracted directory `cwlexec-0.2.2` with the `cwlexec` command to the `PATH` environment variable.

## Run
Make sure that you sourced the LSF environment, then run `cwlexec` or `cwlexec -h` to view help.

The following is a typical command to run a CWL workflow:
```
cwlexec [options] workflow-description-location [input-settings-location]
```
## Build
You can build the package from source. Make sure that you have Maven installed

```
git clone https://github.com/IBMSpectrumComputing/cwlexec.git # Clone cwlexec repo
cd cwlexec         # Switch to source directory
mvn package        # build package
```
After the build, the `cwlexec-0.2.2.tar.gz` package is generated in the `target` directory.


## Test
```
cd cwlexec
mvn clean package # build package and run unit test
cd src/test/integration-test
./run.sh
```
All conformance test cases (127) are pased, except `src/test/integration-test/v1.0/envvar.cwl`, due to LSF limitation: LSF does not support propagating the `$HOME` variable.

### Run your conformance tests

For instructions on running conformance tests refer to https://github.com/common-workflow-language/common-workflow-language/blob/master/CONFORMANCE_TESTS.md

## Features
`cwlexec` has the following features:

### bsub options support
By default, cwlexec submits steps/jobs without any extra `bsub` options. cwlexec provides a separate configuration file in JSON format to be used for workflow execution `--exec-config|-c`. This enables users to specify LSF-specific options while keeping CWL definitions generic and portable.

```
cwlexec -c myconfig.json myflow.cwl myinput.yml
```

|Field  |Type      |Description    |
|-----  |---    |----------|
|queue	|String		|Specify the LSF queue option `–q <queue>` |
|project|	String|	Specify the LSF project option `–P <project>`|
|rerunnable|	Boolean|		Specify the LSF rerunnable option `-r`|
|app|	String|		Specify the LSF app option `–app <application>`|
|processors|String|Specify the the number of tasks in the LSF job, it is same as `bsub –n <the number of tasks in the job>`|
|res_req|	String|		Specify the LSF resource option `–R res_req`. Beware that this option will override the `ResourceRequirement` defined. If `res_req` is not specified in exec-config, LSF using following syntax for `ResourceRequirement` specification<br>coresMin:`bsub -n`<br>coresMax:`bsub –n coresMin,coresMax`<br>ramMin:`bsub –R mem>ramMin`<br>ramMax:`bsub –M ramMax`|

The configuration file suppports workflow level and step level settings:

__Workflow setting__: The options in this part are enabled for each workflow step. For example, if a user specifies a queue in this part, cwlexec adds the –q queue_name option for each step/job.

__Step setting__: The options in this part are enabled only for the current step/job. If the current step is a subworkflow, the options are enabled for each step in the subworkflow. 

If the same options appear in the workflow level and step level configuration, the step level setting overrides the workflow level settings.

Examples of execution configuration:

* Specify a queue and enable jobs to be rerunnable for all steps:
```
{
    ""queue"": ""high"",
    ""rerunnable"": true
}
```
* Specify a queue for all steps, specify an application profile for step1, and specify a resource requirement for step2:
```
{
    ""queue"": ""high"",
    ""steps"": {
        ""step1"": {
            ""app"": ""dockerapp""
        },
        ""step2"": {
            ""res_req"": ""select[type==X86_64] order[ut] rusage[mem=512MB:swp=1GB:tmp=500GB]""
        }
    }
}
```
* Specify a queue for all steps, enable the rerunnable option, specify resource requirements for mainstep, and specify the application profile for one subworkflow step:
```
{
    ""queue"": ""high"",
    ""steps"": {
        ""mainstep"": {
            ""rerunnable"": false,
            ""res_req"": ""select[type==X86_64] order[ut] rusage[mem=512MB:swp=1GB:tmp=500GB]""
        },
        ""subflow/step1"": {
            ""app"": ""dockerapp""
        }
    }
}
```

### Docker Support
Indicates that a workflow component should be run in a Docker container, and specifies how to fetch or build the image.

Before you start, ensure you configure the following for your environment:
>Docker Engine, Version 1.12, or later, must be installed on an LSF server host. The Docker daemon must be started on this host and can successfully start containers.

cwlexec has two ways to submit docker job in LSF: use the `bsub -app` option to submit a job to a docker application profile, or use `bsub –R <res_req>` to specify a docker 
resource and use ""docker run"" directly.

__Use Docker application profile to submit job__
The LSF administrator must complete the following configuration steps as a pre-requisite:
* Configure the application in your environment. For more details, refer to  https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_welcome/lsf_kc_docker.html
>Note:
>* Use `$LSB_CONTAINER_IMAGE` in your application configuration; do not hardcode your image. Configure your registry for your image. For example, image(register_server_path/$LSB_CONTAINER_IMAGE)
>* Specify your shell script for preparing Docker variables in CONTAINER, such as (`@/path/dockeroptions.sh`). This location should be in shard directory.
>* If \$LSB_CONTAINER_IMAGE is from Docker-register, configure your register_server_path to your image, such as  image(register_server_path/$LSB_CONTAINER_IMAGE)

* Create your `dockerOptions.sh` with the following content
```
#!/bin/bash
for OPTION in $LSB_CONTAINER_OPTIONS
do
    echo $OPTION
done
```
cwlexec passes volume mappings to the docker job through the `$LSB_CONTAINER_OPTIONS` environment variable, such as workdir, input, output, and \$HOME and envDef defined in EnvVarRequirements. You can add more options in `dockerOptions.sh` as needed, for example
```
…
echo --rm
echo --net=host
echo --ipc=host
…
```

The end user must specify the Docker application profile in `app` in the `exec-config` file, for example
```
app.json
{
    ""steps"": {
        ""step1"": {
            ""application"": ""dockerapp""
        }
    }
}
```
Run workflow
```
cwlexec –c app.json docker.cwl docker-job.yml
```

>Note: The docker image must be ready and can be pulled with `docker pull`.

__Specify docker resource to submit job__
The LSF administrator must complete the following configuration steps as pre-conditions:
* Make sure the job submission user is in the `docker` user group.
* If the Docker engine is installed on all LSF server hosts, end users can run Docker jobs without any configuration.
    ```
    cwlexec docker.cwl docker-job.yml
    ```
* If the Docker engine is not installed on all LSF server hosts, define the `docker` boolean resource on hosts that can run Docker jobs. For more details, refer to  
https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_docker/lsf_docker_prepare.html

End users must specify the docker resource in `res_req` in `exec-config` file, for example
```
res.json
{
    ""steps"": {
        ""step1"": {
            ""res_req"": ""docker""
        }
    }
}
```

Run workflow
```
./cwlexec –c res.json docker.cwl docker-job.yml
```
>Note: Since the job submission user must be in the docker user group, which is a security concern, use `bsub -app` to submit docker jobs. 

###	Interrupt an executing workflow
You can use `Ctrl+C` to interrupt an executing command. When a command captures this signal, the command exits with 130, and the executing workflow exits. The submitted jobs continue to run, but no new jobs are submitted.

### Rerun a workflow
The workflow exits as long as any step exits. You can rerun the exited workflow with the workflow ID and the workflow is rerun from the failed step.

`cwlexec -r|--rerun  [--debug] <workflow-id>`

When the workflow is rerun if the workflow has running jobs, the command prompts the user to kill the running jobs. 
```
The workflow has running jobs. Do you want to kill them before rerunning the workflow? (Y/N)
```
Choose ""Yes"" if you want to kill all running jobs before rerunning the workflow. Choose ""No"" and the command will exit and do nothing.

###	Post-failure script support
You can configure a post-failure script for the workflow. When a step is exited, the post-failure script executes to try to recover the job.

- The post-failure script can be configured at the step or flow level, just like the bsub options in the exec-config configuration file that works at different levels.

- When the script fails (that is, exits with a non-zero code), the exit code of the step will still be the exit code from the job, not the one from the script.

The following environment variables are passed to the post-failure script:

|Variable|Description|
|--------|-----------|
|CWLEXEC_JOB_ID | job ID|
|CWLEXEC_JOB_BSUB | bsub command|
|CWLEXEC_JOB_CMD | job command|
|CWLEXEC_JOB_CWD |job working directory|
|CWLEXEC_JOB_OUTDIR |job output directory|
|CWLEXEC_JOB_RESREQ |job resource requirement|
|CWLEXEC_RETRY_NUM |number of retry time|

The post-failure script:
* Create your post-failure script, for example, /path/recoverscript.sh
```
#!/bin/sh
JOB_ID=$CWLEXEC_JOB_ID
brequeue -aH $JOB_ID
bmod -Z ""job command"" $JOB_ID
bresume $JOB_ID
```
* Configure the post-failure script in exec_conf, for example, postscript.json
```
{
    ...
    ""post-failure-script"": {
        ""script"": ""/path/recoverscript.sh""
        ""timeout"": 10
        ""retry"": 3
    }
    ...
}
```
Field|Required|Description
---|---|---
Script| Yes| The absolute path of post-failure script
Timeout| No | The timeout of post-failure script. By default 10 seconds
Retry| No| The maximum retry times. By default retry 1 time

* Run your workflow with post-failure script support
```
cwlexec –c postscript.json workflow.cwl workflow-job.yml
```

###	List executed workflows
The `cwlexec --list|-l` command lists all your submitted workflow's information, and the `cwlexec --list|-l <workflow-id>` command displays a workflow information in detail.


|Field              |Description| 
|-------------------|-----------|
|ID                 | The unique identifier for this workflow |
|Name               | The name of workflow |
|Submit Time        | The time that the workflow is submitted |
|Start Time         | The time that the workflow is started to execute |
|End Time           | The time that the workflow is finished |
|Exit State         | The workflow exit state, DONE or EXITED |
|Exit Code          | 0~255 |
|Working Directory  | The workflow work directory |
|Output Directory   | The workflow output directory |
|CWL File           | The path for workflow description file |
|Input Setting Files| The path for workflow input settings file |

### Exit Code Definition
If all steps of the workflow are done and the workflow is successful, the workflow exit code is 0. By default, if a workflow step exit code is 0 and its outputs match the output schema, the step was treated as done; otherwise the step is treated as exited.

If a user defines the success code for a workflow step, the step exit code is in the successCodes, and its outputs match the output schema, the step is treated as done; otherwise the step is treated as exited.

If any step exits in a workflow, the workflow exits and the command exit code will be the exit code of the exited step. If the workflow exits, all submitted jobs continue to run, but no new jobs are submitted.


|Exit Code|Description|
|---------|-----------|
| 0       | The workflow is done |
| 33      | There is an unsupported feature in the workflow |
| 130     | User used Ctrl + C to interupt the workflow |
| 250     | The workflow input/output cannot be found |
| 251     | Fail to parse workflow |
| 252     | Fail to load workflow inputs |
| 253     | Fail to evaluate the expression in workflow |
| 254     | Fail to capture the workflow/step output after the workflow/step is done |
| 255     | System exception. For example, command arguments are wrong; the CWL workflow description file cannot be found; bsub/bwait command cannot be found |

## Implementation
Overview on how `cwlexec` is implemented

### Overview

cwlexec includes three packages:
- com.ibm.spectrumcomputing.cwl.model: defines the Java beans for CWL document
- com.ibm.spectrumcomputing.cwl.parser: parses CWL document to a Java object and binds the input settings to the parsed object
- com.ibm.spectrumcomputing.cwl.exec: executes the workflow

![Sequence Diagram](cwlexec-sequence-diagram.png)

### Working Directory

The workflow work directory is used to store intermediate files of the workflow execution. It must be a shared directory for the LSF cluster. 

Each workflow work directory is under the user specified `-w` work directory top. By default the top directory is `$HOME/cwl-workdir`. The work directory has the following structure:

```
WORKDIR_TOP
  |-workflow_id
      |- inputs
      |- ...
      |- step_id
      |    |- inputs
      |    |- ...
      |    |- output_id
      |    |- ...
      |- ...
  |- ...
```
The workflow id is a global unique id (UUID)

### Record the workflow execution states

Each workflow information and execution states will be recorded to an embedded database `HyperSQL`. For each cwlexec command user, the embedded database records are persisted to `$HOME/.cwlexec`

There are two tables that are used to persist the workflow records
![ER-Diagram](cwlexec-er-diagram.png)

### Workflow Execution

The execution sequence of a CWL workflow is as follows:

1. Parse the CWL document to yield a Java object and resolve the dependencies for each step.
2. Load the input settings and bind them for parsed object (if needed).
3. Evaluate the parsed object expressions.
4. Traverse the parsed object and submit the all of workflow steps.
    - CommandLineTool steps are handled in one of three ways:
        1. *Independent step*: Build the step command by step inputs and arguments first, then submit (`bsub`) the step with the command. Set the step to running, record the LSF job ID, and send a start event (include the step job id) to its main workflow.
        2. *A step that has dependencies and the dependencies are from the main workflow inputs*: Build the step command by step inputs, arguments and dependent main workflow inputs first, then submit (`bsub`) the step with the command. Set the step to running, record the LSF job ID, and send a start event (include the step job id) to its main workflow.
        3. *A step that has dependencies and the dependencies are from other workflow steps outputs*: Create a placeholder execution script (a shell script with blank content) for this step first, then submit (`bsub -H`) the step with the placeholder execution script. Set the step to waiting and record the LSF job ID.
    - If the step is a subworkflow, repeat the previous step.
    - If the step is a scatter, create a placeholder script (exit 0) for it, then submit this step (`bsub -H`). Set the step to waiting and record the LSF job ID. After the scatter is done, change the step state to done and send a start event to its main workflow, then resume (`bresume`) this step.
5. After the main workflow receives the step start event, it broadcasts the event to its waiting steps. When a step receives the start event, it checks its dependencies. If all the dependencies are ready (all dependencies corresponding start events are received), wait (`bwait -w`) for the ready dependencies. After the wait action is met, this step validates the dependencies' outputs. If all outputs are validated, build the command for this step by the outputs and fill the command to the corresponding placeholder script. The step then sends a done event for all of the dependencies' steps to its main workflow and this step is resumed (`bresume`). Finally, set this step to running and send a start event to its main workflow.
6. After the main workflow receives the step done event, the workflow counts the done steps. If all steps in the workflow are done, the workflow itself is done.
7. If any wait (`bwait`) action is failed, the step sends an exit event (include the exit step job ID) to its main workflow.
8. If any output validation is failed, the step sends an exit event (include the exit step job ID) to its main workflow.
9. After the main workflow receives the step exit event, the workflow is exited, and all of waiting steps are killed (`bkill`), but running jobs will continue to run.

## Community Contribution Requirement
Community contributions to this repository must follow the [IBM Developer's Certificate of Origin (DCO)](https://github.com/IBMSpectrumComputing/cwlexec/blob/master/IBMDCO.md) process, and only through GitHub Pull Requests:

 1. Contributor proposes new code to the community.

 2. Contributor signs off on contributions
    (that is, attaches the DCO to ensure contributor is either the code
    originator or has rights to publish. The template of the DCO is included in this package).

 3. IBM Spectrum LSF Development reviews the contribution to check for:
    i)  Applicability and relevancy of functional content
    ii) Any obvious issues

 4. If accepted, the contribution is posted. If rejected, work goes back to the contributor and is not merged.
",2023-07-07 15:53:39+00:00
cwltool,cwltool,common-workflow-language/cwltool,Common Workflow Language reference implementation,https://www.commonwl.org,False,312,2023-06-26 09:38:08+00:00,2015-10-07 13:03:05+00:00,213,49,107,142,3.1.20230624081518,2023-06-24 08:15:18+00:00,Apache License 2.0,4443,v1.17.20160820165726,230,2016-08-20 16:57:26+00:00,2023-07-06 19:22:46+00:00,2023-07-01 05:34:49+00:00,"#############################################################################################
``cwltool``: The reference reference implementation of the Common Workflow Language standards
#############################################################################################

|Linux Status| |Coverage Status| |Docs Status|

PyPI: |PyPI Version| |PyPI Downloads Month| |Total PyPI Downloads|

Conda: |Conda Version| |Conda Installs|

Debian: |Debian Testing package| |Debian Stable package|

Quay.io (Docker): |Quay.io Container|

.. |Linux Status| image:: https://github.com/common-workflow-language/cwltool/actions/workflows/ci-tests.yml/badge.svg?branch=main
   :target: https://github.com/common-workflow-language/cwltool/actions/workflows/ci-tests.yml

.. |Debian Stable package| image:: https://badges.debian.net/badges/debian/stable/cwltool/version.svg
   :target: https://packages.debian.org/stable/cwltool

.. |Debian Testing package| image:: https://badges.debian.net/badges/debian/testing/cwltool/version.svg
   :target: https://packages.debian.org/testing/cwltool

.. |Coverage Status| image:: https://img.shields.io/codecov/c/github/common-workflow-language/cwltool.svg
   :target: https://codecov.io/gh/common-workflow-language/cwltool

.. |PyPI Version| image:: https://badge.fury.io/py/cwltool.svg
   :target: https://badge.fury.io/py/cwltool

.. |PyPI Downloads Month| image:: https://pepy.tech/badge/cwltool/month
   :target: https://pepy.tech/project/cwltool

.. |Total PyPI Downloads| image:: https://static.pepy.tech/personalized-badge/cwltool?period=total&units=international_system&left_color=black&right_color=orange&left_text=Total%20PyPI%20Downloads
   :target: https://pepy.tech/project/cwltool

.. |Conda Version| image:: https://anaconda.org/conda-forge/cwltool/badges/version.svg
   :target: https://anaconda.org/conda-forge/cwltool

.. |Conda Installs| image:: https://anaconda.org/conda-forge/cwltool/badges/downloads.svg
   :target: https://anaconda.org/conda-forge/cwltool

.. |Quay.io Container| image:: https://quay.io/repository/commonwl/cwltool/status
   :target: https://quay.io/repository/commonwl/cwltool

.. |Docs Status| image:: https://readthedocs.org/projects/cwltool/badge/?version=latest
   :target: https://cwltool.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status

This is the reference implementation of the `Common Workflow Language open
standards <https://www.commonwl.org/>`_.  It is intended to be feature complete
and provide comprehensive validation of CWL
files as well as provide other tools related to working with CWL.

``cwltool`` is written and tested for
`Python <https://www.python.org/>`_ ``3.x {x = 6, 7, 8, 9, 10, 11}``

The reference implementation consists of two packages.  The ``cwltool`` package
is the primary Python module containing the reference implementation in the
``cwltool`` module and console executable by the same name.

The ``cwlref-runner`` package is optional and provides an additional entry point
under the alias ``cwl-runner``, which is the implementation-agnostic name for the
default CWL interpreter installed on a host.

``cwltool`` is provided by the CWL project, `a member project of Software Freedom Conservancy <https://sfconservancy.org/news/2018/apr/11/cwl-new-member-project/>`_
and our `many contributors <https://github.com/common-workflow-language/cwltool/graphs/contributors>`_.

.. contents:: Table of Contents

*******
Install
*******

``cwltool`` packages
====================

Your operating system may offer cwltool directly. For `Debian <https://tracker.debian.org/pkg/cwltool>`_, `Ubuntu <https://launchpad.net/ubuntu/+source/cwltool>`_,
and similar Linux distribution try

.. code:: bash

   sudo apt-get install cwltool

If you encounter an error, first try to update package information by using

.. code:: bash

   sudo apt-get update

If you are running macOS X or other UNIXes and you want to use packages prepared by the conda-forge project, then
please follow the install instructions for `conda-forge <https://conda-forge.org/#about>`_ (if you haven't already) and then

.. code:: bash

   conda install -c conda-forge cwltool

All of the above methods of installing ``cwltool`` use packages that might contain bugs already fixed in newer versions or be missing desired features.
If the packaged version of ``cwltool`` available to you is too old, then we recommend installing using ``pip`` and ``venv``

.. code:: bash

   python3 -m venv env      # Create a virtual environment named 'env' in the current directory
   source env/bin/activate  # Activate environment before installing `cwltool`

Then install the latest ``cwlref-runner`` package from PyPi (which will install the latest ``cwltool`` package as
well)

.. code:: bash

  pip install cwlref-runner

If installing alongside another CWL implementation (like ``toil-cwl-runner`` or ``arvados-cwl-runner``) then instead run

.. code:: bash

  pip install cwltool

MS Windows users
================

1. `Install Windows Subsystem for Linux 2 and Docker Desktop <https://docs.docker.com/docker-for-windows/wsl/#prerequisites>`_. 
2. `Install Debian from the Microsoft Store <https://www.microsoft.com/en-us/p/debian/9msvkqc78pk6>`_.
3. Set Debian as your default WSL 2 distro: ``wsl --set-default debian``.
4. Return to the Docker Desktop, choose ``Settings`` → ``Resources`` → ``WSL Integration`` and under ""Enable integration with additional distros"" select ""Debian"",
5. Reboot if you have not yet already.
6. Launch Debian and follow the Linux instructions above (``apt-get install cwltool`` or use the ``venv`` method)

Network problems from within WSL2? Try `these instructions <https://github.com/microsoft/WSL/issues/4731#issuecomment-702176954>`_ followed by ``wsl --shutdown``.

``cwltool`` development version
===============================

Or you can skip the direct ``pip`` commands above and install the latest development version of ``cwltool``:

.. code:: bash

  git clone https://github.com/common-workflow-language/cwltool.git # clone (copy) the cwltool git repository
  cd cwltool           # Change to source directory that git clone just downloaded
  pip install .[deps]  # Installs ``cwltool`` from source
  cwltool --version    # Check if the installation works correctly

Remember, if co-installing multiple CWL implementations, then you need to
maintain which implementation ``cwl-runner`` points to via a symbolic file
system link or `another facility <https://wiki.debian.org/DebianAlternatives>`_.

Recommended Software
====================

We strongly suggested to have the following installed:

* One of the following software container engines

  * `Podman <https://podman.io/getting-started/installation>`_
  * `Docker <https://docs.docker.com/engine/install/>`_
  * Singularity/Apptainer: See `Using Singularity`_
  * udocker: See `Using uDocker`_

* `node.js <https://nodejs.org/en/download/>`_ for evaluating CWL Expressions quickly
  (required for `udocker` users, optional but recommended for the other container engines).

Without these, some examples in the CWL tutorials at http://www.commonwl.org/user_guide/ may not work.

***********************
Run on the command line
***********************

Simple command::

  cwl-runner my_workflow.cwl my_inputs.yaml

Or if you have multiple CWL implementations installed and you want to override
the default cwl-runner then use::

  cwltool my_workflow.cwl my_inputs.yml

You can set cwltool options in the environment with ``CWLTOOL_OPTIONS``,
these will be inserted at the beginning of the command line::

  export CWLTOOL_OPTIONS=""--debug""

Use with boot2docker on macOS
=============================
boot2docker runs Docker inside a virtual machine, and it only mounts ``Users``
on it. The default behavior of CWL is to create temporary directories under e.g.
``/Var`` which is not accessible to Docker containers.

To run CWL successfully with boot2docker you need to set the ``--tmpdir-prefix``
and ``--tmp-outdir-prefix`` to somewhere under ``/Users``::

    $ cwl-runner --tmp-outdir-prefix=/Users/username/project --tmpdir-prefix=/Users/username/project wc-tool.cwl wc-job.json

Using uDocker
=============

Some shared computing environments don't support Docker software containers for technical or policy reasons.
As a workaround, the CWL reference runner supports using the `udocker <https://github.com/indigo-dc/udocker>`_
program on Linux using ``--udocker``.

udocker installation: https://indigo-dc.github.io/udocker/installation_manual.html

Run `cwltool` just as you usually would, but with ``--udocker`` prior to the workflow path:

.. code:: bash

  cwltool --udocker https://github.com/common-workflow-language/common-workflow-language/raw/main/v1.0/v1.0/test-cwl-out2.cwl https://github.com/common-workflow-language/common-workflow-language/raw/main/v1.0/v1.0/empty.json

As was mentioned in the `Recommended Software`_ section,

Using Singularity
=================

``cwltool`` can also use `Singularity <https://github.com/hpcng/singularity/releases/>`_ version 2.6.1
or later as a Docker container runtime.
``cwltool`` with Singularity will run software containers specified in
``DockerRequirement`` and therefore works with Docker images only, native
Singularity images are not supported. To use Singularity as the Docker container
runtime, provide ``--singularity`` command line option to ``cwltool``.
With Singularity, ``cwltool`` can pass all CWL v1.0 conformance tests, except
those involving Docker container ENTRYPOINTs.

Example

.. code:: bash

  cwltool --singularity https://github.com/common-workflow-language/common-workflow-language/raw/main/v1.0/v1.0/cat3-tool-mediumcut.cwl https://github.com/common-workflow-language/common-workflow-language/raw/main/v1.0/v1.0/cat-job.json

Running a tool or workflow from remote or local locations
=========================================================

``cwltool`` can run tool and workflow descriptions on both local and remote
systems via its support for HTTP[S] URLs.

Input job files and Workflow steps (via the `run` directive) can reference CWL
documents using absolute or relative local filesystem paths. If a relative path
is referenced and that document isn't found in the current directory, then the
following locations will be searched:
http://www.commonwl.org/v1.0/CommandLineTool.html#Discovering_CWL_documents_on_a_local_filesystem

You can also use `cwldep <https://github.com/common-workflow-language/cwldep>`_
to manage dependencies on external tools and workflows.

Overriding workflow requirements at load time
=============================================

Sometimes a workflow needs additional requirements to run in a particular
environment or with a particular dataset.  To avoid the need to modify the
underlying workflow, cwltool supports requirement ""overrides"".

The format of the ""overrides"" object is a mapping of item identifier (workflow,
workflow step, or command line tool) to the process requirements that should be applied.

.. code:: yaml

  cwltool:overrides:
    echo.cwl:
      requirements:
        EnvVarRequirement:
          envDef:
            MESSAGE: override_value

Overrides can be specified either on the command line, or as part of the job
input document.  Workflow steps are identified using the name of the workflow
file followed by the step name as a document fragment identifier ""#id"".
Override identifiers are relative to the top-level workflow document.

.. code:: bash

  cwltool --overrides overrides.yml my-tool.cwl my-job.yml

.. code:: yaml

  input_parameter1: value1
  input_parameter2: value2
  cwltool:overrides:
    workflow.cwl#step1:
      requirements:
        EnvVarRequirement:
          envDef:
            MESSAGE: override_value

.. code:: bash

  cwltool my-tool.cwl my-job-with-overrides.yml


Combining parts of a workflow into a single document
====================================================

Use ``--pack`` to combine a workflow made up of multiple files into a
single compound document.  This operation takes all the CWL files
referenced by a workflow and builds a new CWL document with all
Process objects (CommandLineTool and Workflow) in a list in the
``$graph`` field.  Cross references (such as ``run:`` and ``source:``
fields) are updated to internal references within the new packed
document.  The top-level workflow is named ``#main``.

.. code:: bash

  cwltool --pack my-wf.cwl > my-packed-wf.cwl


Running only part of a workflow
===============================

You can run a partial workflow with the ``--target`` (``-t``) option.  This
takes the name of an output parameter, workflow step, or input
parameter in the top-level workflow.  You may provide multiple
targets.

.. code:: bash

  cwltool --target step3 my-wf.cwl

If a target is an output parameter, it will only run only the steps
that contribute to that output.  If a target is a workflow step, it
will run the workflow starting from that step.  If a target is an
input parameter, it will only run the steps connected to
that input.

Use ``--print-targets`` to get a listing of the targets of a workflow.
To see which steps will run, use ``--print-subgraph`` with
``--target`` to get a printout of the workflow subgraph for the
selected targets.

.. code:: bash

  cwltool --print-targets my-wf.cwl

  cwltool --target step3 --print-subgraph my-wf.cwl > my-wf-starting-from-step3.cwl


Visualizing a CWL document
==========================

The ``--print-dot`` option will print a file suitable for Graphviz ``dot`` program.  Here is a bash onliner to generate a Scalable Vector Graphic (SVG) file:

.. code:: bash

  cwltool --print-dot my-wf.cwl | dot -Tsvg > my-wf.svg

Modeling a CWL document as RDF
==============================

CWL documents can be expressed as RDF triple graphs.

.. code:: bash

  cwltool --print-rdf --rdf-serializer=turtle mywf.cwl


Environment Variables in cwltool
================================

This reference implementation supports several ways of setting
environment variables for tools, in addition to the standard
``EnvVarRequirement``. The sequence of steps applied to create the
environment is:

0. If the ``--preserve-entire-environment`` flag is present, then begin with the current
   environment, else begin with an empty environment.

1. Add any variables specified by ``--preserve-environment`` option(s).

2. Set ``TMPDIR`` and ``HOME`` per `the CWL v1.0+ CommandLineTool specification <https://www.commonwl.org/v1.0/CommandLineTool.html#Runtime_environment>`_.

3. Apply any ``EnvVarRequirement`` from the ``CommandLineTool`` description.

4. Apply any manipulations required by any ``cwltool:MPIRequirement`` extensions.

5. Substitute any secrets required by ``Secrets`` extension.

6. Modify the environment in response to ``SoftwareRequirement`` (see below).


Leveraging SoftwareRequirements (Beta)
--------------------------------------

CWL tools may be decorated with ``SoftwareRequirement`` hints that cwltool
may in turn use to resolve to packages in various package managers or
dependency management systems such as `Environment Modules
<http://modules.sourceforge.net/>`__.

Utilizing ``SoftwareRequirement`` hints using cwltool requires an optional
dependency, for this reason be sure to use specify the ``deps`` modifier when
installing cwltool. For instance::

  $ pip install 'cwltool[deps]'

Installing cwltool in this fashion enables several new command line options.
The most general of these options is ``--beta-dependency-resolvers-configuration``.
This option allows one to specify a dependency resolver's configuration file.
This file may be specified as either XML or YAML and very simply describes various
plugins to enable to ""resolve"" ``SoftwareRequirement`` dependencies.

Using these hints will allow cwltool to modify the environment in
which your tool runs, for example by loading one or more Environment
Modules. The environment is constructed as above, then the environment
may modified by the selected tool resolver.  This currently means that
you cannot override any environment variables set by the selected tool
resolver. Note that the environment given to the configured dependency
resolver has the variable `_CWLTOOL` set to `1` to allow introspection.

To discuss some of these plugins and how to configure them, first consider the
following ``hint`` definition for an example CWL tool.

.. code:: yaml

  SoftwareRequirement:
    packages:
    - package: seqtk
      version:
      - r93

Now imagine deploying cwltool on a cluster with Software Modules installed
and that a ``seqtk`` module is available at version ``r93``. This means cluster
users likely won't have the binary ``seqtk`` on their ``PATH`` by default, but after
sourcing this module with the command ``modulecmd sh load seqtk/r93`` ``seqtk`` is
available on the ``PATH``. A simple dependency resolvers configuration file, called
``dependency-resolvers-conf.yml`` for instance, that would enable cwltool to source
the correct module environment before executing the above tool would simply be:

.. code:: yaml

  - type: modules

The outer list indicates that one plugin is being enabled, the plugin parameters are
defined as a dictionary for this one list item. There is only one required parameter
for the plugin above, this is ``type`` and defines the plugin type. This parameter
is required for all plugins. The available plugins and the parameters
available for each are documented (incompletely) `here
<https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html>`__.
Unfortunately, this documentation is in the context of Galaxy tool
``requirement`` s instead of CWL ``SoftwareRequirement`` s, but the concepts map fairly directly.

cwltool is distributed with an example of such seqtk tool and sample corresponding
job. It could executed from the cwltool root using a dependency resolvers
configuration file such as the above one using the command::

  cwltool --beta-dependency-resolvers-configuration /path/to/dependency-resolvers-conf.yml \
      tests/seqtk_seq.cwl \
      tests/seqtk_seq_job.json

This example demonstrates both that cwltool can leverage
existing software installations and also handle workflows with dependencies
on different versions of the same software and libraries. However the above
example does require an existing module setup so it is impossible to test this example
""out of the box"" with cwltool. For a more isolated test that demonstrates all
the same concepts - the resolver plugin type ``galaxy_packages`` can be used.

""Galaxy packages"" are a lighter-weight alternative to Environment Modules that are
really just defined by a way to lay out directories into packages and versions
to find little scripts that are sourced to modify the environment. They have
been used for years in Galaxy community to adapt Galaxy tools to cluster
environments but require neither knowledge of Galaxy nor any special tools to
setup. These should work just fine for CWL tools.

The cwltool source code repository's test directory is setup with a very simple
directory that defines a set of ""Galaxy  packages"" (but really just defines one
package named ``random-lines``). The directory layout is simply::

  tests/test_deps_env/
    random-lines/
      1.0/
        env.sh

If the ``galaxy_packages`` plugin is enabled and pointed at the
``tests/test_deps_env`` directory in cwltool's root and a ``SoftwareRequirement``
such as the following is encountered.

.. code:: yaml

  hints:
    SoftwareRequirement:
      packages:
      - package: 'random-lines'
        version:
        - '1.0'

Then cwltool will simply find that ``env.sh`` file and source it before executing
the corresponding tool. That ``env.sh`` script is only responsible for modifying
the job's ``PATH`` to add the required binaries.

This is a full example that works since resolving ""Galaxy packages"" has no
external requirements. Try it out by executing the following command from cwltool's
root directory::

  cwltool --beta-dependency-resolvers-configuration tests/test_deps_env_resolvers_conf.yml \
      tests/random_lines.cwl \
      tests/random_lines_job.json

The resolvers configuration file in the above example was simply:

.. code:: yaml

  - type: galaxy_packages
    base_path: ./tests/test_deps_env

It is possible that the ``SoftwareRequirement`` s in a given CWL tool will not
match the module names for a given cluster. Such requirements can be re-mapped
to specific deployed packages or versions using another file specified using
the resolver plugin parameter `mapping_files`. We will
demonstrate this using `galaxy_packages,` but the concepts apply equally well
to Environment Modules or Conda packages (described below), for instance.

So consider the resolvers configuration file.
(`tests/test_deps_env_resolvers_conf_rewrite.yml`):

.. code:: yaml

  - type: galaxy_packages
    base_path: ./tests/test_deps_env
    mapping_files: ./tests/test_deps_mapping.yml

And the corresponding mapping configuration file (`tests/test_deps_mapping.yml`):

.. code:: yaml

  - from:
      name: randomLines
      version: 1.0.0-rc1
    to:
      name: random-lines
      version: '1.0'

This is saying if cwltool encounters a requirement of ``randomLines`` at version
``1.0.0-rc1`` in a tool, to rewrite to our specific plugin as ``random-lines`` at
version ``1.0``. cwltool has such a test tool called ``random_lines_mapping.cwl``
that contains such a source ``SoftwareRequirement``. To try out this example with
mapping, execute the following command from the cwltool root directory::

  cwltool --beta-dependency-resolvers-configuration tests/test_deps_env_resolvers_conf_rewrite.yml \
      tests/random_lines_mapping.cwl \
      tests/random_lines_job.json

The previous examples demonstrated leveraging existing infrastructure to
provide requirements for CWL tools. If instead a real package manager is used
cwltool has the opportunity to install requirements as needed. While initial
support for Homebrew/Linuxbrew plugins is available, the most developed such
plugin is for the `Conda <https://conda.io/docs/#>`__ package manager. Conda has the nice properties
of allowing multiple versions of a package to be installed simultaneously,
not requiring evaluated permissions to install Conda itself or packages using
Conda, and being cross-platform. For these reasons, cwltool may run as a normal
user, install its own Conda environment and manage multiple versions of Conda packages
on Linux and Mac OS X.

The Conda plugin can be endlessly configured, but a sensible set of defaults
that has proven a powerful stack for dependency management within the Galaxy tool
development ecosystem can be enabled by simply passing cwltool the
``--beta-conda-dependencies`` flag.

With this, we can use the seqtk example above without Docker or any externally managed services - cwltool should install everything it needs
and create an environment for the tool. Try it out with the following command::

  cwltool --beta-conda-dependencies tests/seqtk_seq.cwl tests/seqtk_seq_job.json

The CWL specification allows URIs to be attached to ``SoftwareRequirement`` s
that allow disambiguation of package names. If the mapping files described above
allow deployers to adapt tools to their infrastructure, this mechanism allows
tools to adapt their requirements to multiple package managers. To demonstrate
this within the context of the seqtk, we can simply break the package name we
use and then specify a specific Conda package as follows:

.. code:: yaml

  hints:
    SoftwareRequirement:
      packages:
      - package: seqtk_seq
        version:
        - '1.2'
        specs:
        - https://anaconda.org/bioconda/seqtk
        - https://packages.debian.org/sid/seqtk

The example can be executed using the command::

  cwltool --beta-conda-dependencies tests/seqtk_seq_wrong_name.cwl tests/seqtk_seq_job.json

The plugin framework for managing the resolution of these software requirements
as maintained as part of `galaxy-tool-util <https://github.com/galaxyproject/galaxy/tree/dev/packages/tool_util>`__ - a small,
portable subset of the Galaxy project. More information on configuration and implementation can be found
at the following links:

- `Dependency Resolvers in Galaxy <https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html>`__
- `Conda for [Galaxy] Tool Dependencies <https://docs.galaxyproject.org/en/latest/admin/conda_faq.html>`__
- `Mapping Files - Implementation <https://github.com/galaxyproject/galaxy/commit/495802d229967771df5b64a2f79b88a0eaf00edb>`__
- `Specifications - Implementation <https://github.com/galaxyproject/galaxy/commit/81d71d2e740ee07754785306e4448f8425f890bc>`__
- `Initial cwltool Integration Pull Request <https://github.com/common-workflow-language/cwltool/pull/214>`__

Use with GA4GH Tool Registry API
================================

Cwltool can launch tools directly from `GA4GH Tool Registry API`_ endpoints.

By default, cwltool searches https://dockstore.org/ .  Use ``--add-tool-registry`` to add other registries to the search path.

For example ::

  cwltool quay.io/collaboratory/dockstore-tool-bamstats:develop test.json

and (defaults to latest when a version is not specified) ::

  cwltool quay.io/collaboratory/dockstore-tool-bamstats test.json

For this example, grab the test.json (and input file) from https://github.com/CancerCollaboratory/dockstore-tool-bamstats ::

  wget https://dockstore.org/api/api/ga4gh/v2/tools/quay.io%2Fbriandoconnor%2Fdockstore-tool-bamstats/versions/develop/PLAIN-CWL/descriptor/test.json
  wget https://github.com/CancerCollaboratory/dockstore-tool-bamstats/raw/develop/rna.SRR948778.bam


.. _`GA4GH Tool Registry API`: https://github.com/ga4gh/tool-registry-schemas

Running MPI-based tools that need to be launched
================================================

Cwltool supports an extension to the CWL spec
``http://commonwl.org/cwltool#MPIRequirement``. When the tool
definition has this in its ``requirements``/``hints`` section, and
cwltool has been run with ``--enable-ext``, then the tool's command
line will be extended with the commands needed to launch it with
``mpirun`` or similar. You can specify the number of processes to
start as either a literal integer or an expression (that will result
in an integer). For example::

  #!/usr/bin/env cwl-runner
  cwlVersion: v1.1
  class: CommandLineTool
  $namespaces:
    cwltool: ""http://commonwl.org/cwltool#""
  requirements:
    cwltool:MPIRequirement:
      processes: $(inputs.nproc)
  inputs:
    nproc:
      type: int

Interaction with containers: the MPIRequirement currently prepends its
commands to the front of the command line that is constructed. If you
wish to run a containerized application in parallel, for simple use
cases, this does work with Singularity, depending upon the platform
setup. However, this combination should be considered ""alpha"" -- please
do report any issues you have! This does not work with Docker at the
moment. (More precisely, you get `n` copies of the same single process
image run at the same time that cannot communicate with each other.)

The host-specific parameters are configured in a simple YAML file
(specified with the ``--mpi-config-file`` flag). The allowed keys are
given in the following table; all are optional.

+----------------+------------------+----------+------------------------------+
| Key            | Type             | Default  | Description                  |
+================+==================+==========+==============================+
| runner         | str              | ""mpirun"" | The primary command to use.  |
+----------------+------------------+----------+------------------------------+
| nproc_flag     | str              | ""-n""     | Flag to set number of        |
|                |                  |          | processes to start.          |
+----------------+------------------+----------+------------------------------+
| default_nproc  | int              | 1        | Default number of processes. |
+----------------+------------------+----------+------------------------------+
| extra_flags    | List[str]        | []       | A list of any other flags to |
|                |                  |          | be added to the runner's     |
|                |                  |          | command line before          |
|                |                  |          | the ``baseCommand``.         |
+----------------+------------------+----------+------------------------------+
| env_pass       | List[str]        | []       | A list of environment        |
|                |                  |          | variables that should be     |
|                |                  |          | passed from the host         |
|                |                  |          | environment through to the   |
|                |                  |          | tool (e.g., giving the       |
|                |                  |          | node list as set by your     |
|                |                  |          | scheduler).                  |
+----------------+------------------+----------+------------------------------+
| env_pass_regex | List[str]        | []       | A list of python regular     |
|                |                  |          | expressions that will be     |
|                |                  |          | matched against the host's   |
|                |                  |          | environment. Those that match|
|                |                  |          | will be passed through.      |
+----------------+------------------+----------+------------------------------+
| env_set        | Mapping[str,str] | {}       | A dictionary whose keys are  |
|                |                  |          | the environment variables set|
|                |                  |          | and the values being the     |
|                |                  |          | values.                      |
+----------------+------------------+----------+------------------------------+


Enabling Fast Parser (experimental)
===================================

For very large workflows, `cwltool` can spend a lot of time in
initialization, before the first step runs.  There is an experimental
flag ``--fast-parser`` which can dramatically reduce the
initialization overhead, however as of this writing it has several limitations:

- Error reporting in general is worse than the standard parser, you will want to use it with workflows that you know are already correct.

- It does not check for dangling links (these will become runtime errors instead of loading errors)

- Several other cases fail, as documented in https://github.com/common-workflow-language/cwltool/pull/1720

***********
Development
***********

Running tests locally
=====================

-  Running basic tests ``(/tests)``:

To run the basic tests after installing `cwltool` execute the following:

.. code:: bash

  pip install -rtest-requirements.txt
  pytest   ## N.B. This requires node.js or docker to be available

To run various tests in all supported Python environments, we use `tox <https://github.com/common-workflow-language/cwltool/tree/main/tox.ini>`_. To run the test suite in all supported Python environments
first clone the complete code repository (see the ``git clone`` instructions above) and then run
the following in the terminal:
``pip install ""tox<4""; tox -p``

List of all environment can be seen using:
``tox --listenvs``
and running a specific test env using:
``tox -e <env name>``
and additionally run a specific test using this format:
``tox -e py310-unit -- -v tests/test_examples.py::test_scandeps``

-  Running the entire suite of CWL conformance tests:

The GitHub repository for the CWL specifications contains a script that tests a CWL
implementation against a wide array of valid CWL files using the `cwltest <https://github.com/common-workflow-language/cwltest>`_
program

Instructions for running these tests can be found in the Common Workflow Language Specification repository at https://github.com/common-workflow-language/common-workflow-language/blob/main/CONFORMANCE_TESTS.md .

Import as a module
==================

Add

.. code:: python

  import cwltool

to your script.

The easiest way to use cwltool to run a tool or workflow from Python is to use a Factory

.. code:: python

  import cwltool.factory
  fac = cwltool.factory.Factory()

  echo = fac.make(""echo.cwl"")
  result = echo(inp=""foo"")

  # result[""out""] == ""foo""


CWL Tool Control Flow
=====================

Technical outline of how cwltool works internally, for maintainers.

#. Use CWL ``load_tool()`` to load document.

   #. Fetches the document from file or URL
   #. Applies preprocessing (syntax/identifier expansion and normalization)
   #. Validates the document based on cwlVersion
   #. If necessary, updates the document to the latest spec
   #. Constructs a Process object using ``make_tool()``` callback.  This yields a
      CommandLineTool, Workflow, or ExpressionTool.  For workflows, this
      recursively constructs each workflow step.
   #. To construct custom types for CommandLineTool, Workflow, or
      ExpressionTool, provide a custom ``make_tool()``

#. Iterate on the ``job()`` method of the Process object to get back runnable jobs.

   #. ``job()`` is a generator method (uses the Python iterator protocol)
   #. Each time the ``job()`` method is invoked in an iteration, it returns one
      of: a runnable item (an object with a ``run()`` method), ``None`` (indicating
      there is currently no work ready to run) or end of iteration (indicating
      the process is complete.)
   #. Invoke the runnable item by calling ``run()``.  This runs the tool and gets output.
   #. An output callback reports the output of a process.
   #. ``job()`` may be iterated over multiple times.  It will yield all the work
      that is currently ready to run and then yield None.

#. ``Workflow`` objects create a corresponding ``WorkflowJob`` and ``WorkflowJobStep`` objects to hold the workflow state for the duration of the job invocation.

   #. The WorkflowJob iterates over each WorkflowJobStep and determines if the
      inputs the step are ready.
   #. When a step is ready, it constructs an input object for that step and
      iterates on the ``job()`` method of the workflow job step.
   #. Each runnable item is yielded back up to top-level run loop
   #. When a step job completes and receives an output callback, the
      job outputs are assigned to the output of the workflow step.
   #. When all steps are complete, the intermediate files are moved to a final
      workflow output, intermediate directories are deleted, and the workflow's output callback is called.

#. ``CommandLineTool`` job() objects yield a single runnable object.

   #. The CommandLineTool ``job()`` method calls ``make_job_runner()`` to create a
      ``CommandLineJob`` object
   #. The job method configures the CommandLineJob object by setting public
      attributes
   #. The job method iterates over file and directories inputs to the
      CommandLineTool and creates a ""path map"".
   #. Files are mapped from their ""resolved"" location to a ""target"" path where
      they will appear at tool invocation (for example, a location inside a
      Docker container.)  The target paths are used on the command line.
   #. Files are staged to targets paths using either Docker volume binds (when
      using containers) or symlinks (if not).  This staging step enables files
      to be logically rearranged or renamed independent of their source layout.
   #. The ``run()`` method of CommandLineJob executes the command line tool or
      Docker container, waits for it to complete, collects output, and makes
      the output callback.

Extension points
================

The following functions can be passed to main() to override or augment
the listed behaviors.

executor
  ::

    executor(tool, job_order_object, runtimeContext, logger)
      (Process, Dict[Text, Any], RuntimeContext) -> Tuple[Dict[Text, Any], Text]

  An implementation of the top-level workflow execution loop should
  synchronously run a process object to completion and return the
  output object.

versionfunc
  ::

    ()
      () -> Text

  Return version string.

logger_handler
  ::

    logger_handler
      logging.Handler

  Handler object for logging.

The following functions can be set in LoadingContext to override or
augment the listed behaviors.

fetcher_constructor
  ::

    fetcher_constructor(cache, session)
      (Dict[unicode, unicode], requests.sessions.Session) -> Fetcher

  Construct a Fetcher object with the supplied cache and HTTP session.

resolver
  ::

    resolver(document_loader, document)
      (Loader, Union[Text, dict[Text, Any]]) -> Text

  Resolve a relative document identifier to an absolute one that can be fetched.

The following functions can be set in RuntimeContext to override or
augment the listed behaviors.

construct_tool_object
  ::

    construct_tool_object(toolpath_object, loadingContext)
      (MutableMapping[Text, Any], LoadingContext) -> Process

  Hook to construct a Process object (eg CommandLineTool) object from a document.

select_resources
  ::

    selectResources(request)
      (Dict[str, int], RuntimeContext) -> Dict[Text, int]

  Take a resource request and turn it into a concrete resource assignment.

make_fs_access
  ::

    make_fs_access(basedir)
      (Text) -> StdFsAccess

  Return a file system access object.

In addition, when providing custom subclasses of Process objects, you can override the following methods:

CommandLineTool.make_job_runner
  ::

    make_job_runner(RuntimeContext)
      (RuntimeContext) -> Type[JobBase]

  Create and return a job runner object (this implements concrete execution of a command line tool).

Workflow.make_workflow_step
  ::

    make_workflow_step(toolpath_object, pos, loadingContext, parentworkflowProv)
      (Dict[Text, Any], int, LoadingContext, Optional[ProvenanceProfile]) -> WorkflowStep

  Create and return a workflow step object.
",2023-07-07 15:53:43+00:00
cyclone,cyclone,caicloud/cyclone,Powerful workflow engine and end-to-end pipeline solutions implemented with native Kubernetes resources. https://cyclone.dev,,False,1060,2023-07-03 06:39:54+00:00,2016-10-17 04:19:35+00:00,171,49,30,77,v1.0.3-beta,2021-02-03 04:37:08+00:00,Apache License 2.0,1073,v1.2.0,85,2021-01-28 04:59:30+00:00,2023-07-06 11:53:00+00:00,2021-03-02 07:15:04+00:00,"# Cyclone

<p align=""center""><img src=""docs/images/logo.jpeg"" width=""200""></p>

[![Build Status](https://travis-ci.org/caicloud/cyclone.svg?branch=master)](https://travis-ci.org/caicloud/cyclone)
[![Go Report Card](https://goreportcard.com/badge/github.com/caicloud/cyclone?style=flat-square)](https://goreportcard.com/report/github.com/caicloud/cyclone)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2792/badge)](https://bestpractices.coreinfrastructure.org/projects/2792)
[![Coverage Status](https://coveralls.io/repos/github/caicloud/cyclone/badge.svg?branch=master)](https://coveralls.io/github/caicloud/cyclone?branch=master)
[![GoDoc](https://img.shields.io/badge/godoc-reference-blue.svg?style=flat-square)](https://godoc.org/github.com/caicloud/cyclone)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)

Cyclone is a powerful workflow engine and end-to-end pipeline solution implemented with native Kubernetes resources,
with no extra dependencies. It can run anywhere Kubernetes is deployed: public cloud, on-prem or hybrid cloud.

Cyclone is architectured with a low-level workflow engine that is application agnostic, offering capabilities like
workflow DAG scheduling, resource lifecycle management and most importantly, a pluggable and extensible framework for
extending the core APIs. Above which, Cyclone provides built-in support for high-level functionalities, with CI/CD
pipelines and AI DevOps being two notable examples, and it is possible to expand to more use cases as well.

With Cyclone, users end up with the flexibility of workflow orchestration and the usability of complete CI/CD and AI DevOps solutions.

## Features

- DAG graph scheduling: Cyclone supports DAG workflow execution
- Parameterization: stage (unit of execution) can be parameterized to maximize configuration reuse
- External integration: external systems like SCM, docker registry, S3 can be easily integrated with Cyclone
- Triggers: Cyclone supports cron and webhook trigger today, with upcoming support for other types of triggers
- Controllability: workflow execution can be paused, resumed, retried or cancelled
- Multi-cluster: workflow can be executed in different clusters from where Cyclone is running
- Multi-tenancy: resource manifests and workflow executions are grouped and isolated per tenant
- Garbage Collection: automatic resource cleanup after workflow execution
- Logging: logs are persisted and independent from workflow lifecycle, enabling offline inspection
- Built-in Pipeline: curated DAG templates and stage runtimes for running DevOps pipelines for both regular software and AI development
- Delegation Workload: Delegate some complicated stage to be executed in external systems, instead of Cyclone.

## Quick Start

> Cyclone has been tested with Kubernetes 1.12, 1.13 and 1.14.

Make sure [Helm](https://helm.sh/) with a version higher than **2.10** is installed ([install guide](https://helm.sh/docs/using_helm/#install-helm)), then install Cyclone with:

```bash
$ helm install --name cyclone --namespace cyclone-system ./manifests/cyclone-public
```

If you want to configure the installation or want to install from source code, please refer to [Cyclone Install Guide](docs/installation.md).

Then you can access Cyclone with kubectl or Cyclone web (http://<node-ip>:30022).

## Community

- **Slack**: Join [Cyclone Community](https://cycloneworkflow.slack.com/) for disscussions and posting questions. If you are not yet a member of Cyclone Slack, you may sign up [here](https://join.slack.com/t/cycloneworkflow/shared_invite/enQtNzc3NzY1MjY1MTY4LTNmZTQ2ZjQxOTM1ZDE0ZDJlMDhiOTk2YTU4MzdmYmVhNDE0NDYxMTk1ODYyNmRmNzkzNWRiYTMzYmI0ZWIxMWU).

## Aditional Tools

- [cycli](https://github.com/cd1989/cycli) Cyclone CLI interface

## Roadmap

[Cyclone Roadmap](./docs/ROADMAP.md)

## Contributing

If you are interested in contributing to Cyclone, please checkout [CONTRIBUTING.md](./CONTRIBUTING.md).
We welcome any code or non-code contribution!

## Licensing

Cyclone is licensed under the Apache License, Version 2.0. See [LICENSE](./LICENSE) for the full license text.
",2023-07-07 15:53:47+00:00
cylc,cylc-flow,cylc/cylc-flow,Cylc: a workflow engine for cycling systems. ,https://cylc.github.io,False,286,2023-06-14 11:03:36+00:00,2011-06-02 08:31:49+00:00,85,14,31,86,8.1.4,2023-05-04 12:37:09+00:00,GNU General Public License v3.0,17580,gh-pages.6.10.1,248,2016-05-22 23:46:18+00:00,2023-07-07 15:33:59+00:00,2023-07-06 07:48:58+00:00,"<div
  align=""center""
>
<img
  src=""https://raw.githubusercontent.com/cylc/cylc-admin/master/docs/img/cylc-logo.svg""
  width=""50%""
/>

[![PyPI](https://img.shields.io/pypi/v/cylc-flow.svg?color=yellow)](https://pypi.org/project/cylc-flow/)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/cylc-flow/badges/version.svg)](https://anaconda.org/conda-forge/cylc-flow)
[![chat](https://img.shields.io/matrix/cylc-general:matrix.org)](https://matrix.to/#/#cylc-general:matrix.org)
[![forum](https://img.shields.io/discourse/https/cylc.discourse.group/posts.svg)](https://cylc.discourse.group/)
[![Documentation](https://img.shields.io/website?label=documentation&up_message=live&url=https%3A%2F%2Fcylc.github.io%2Fcylc-doc%2Fstable%2Fhtml%2Findex.html)](https://cylc.github.io/cylc-doc/stable/html/index.html)

</div>

Cylc (pronounced silk) is a general purpose workflow engine that also
manages cycling systems very efficiently. It is used in production weather,
climate, and environmental forecasting on HPC, but is not specialized to those
domains.

### Quick Start


[Installation](https://cylc.github.io/cylc-doc/stable/html/installation.html) |
[Documentation](https://cylc.github.io/cylc-doc/stable/html/index.html)

```bash
# install cylc
conda install cylc-flow

# write your first workflow
mkdir -p ~/cylc-src/example
cat > ~/cylc-src/example/flow.cylc <<__CONFIG__
[scheduling]
    initial cycle point = 1
    cycling mode = integer
    [[graph]]
        P1 = """"""
            a => b => c & d
            b[-P1] => b
        """"""
[runtime]
    [[a, b, c, d]]
        script = echo ""Hello $CYLC_TASK_NAME""
__CONFIG__

# install and run it
cylc install example
cylc play example

# watch it run
cylc tui example
```

### The Cylc Ecosystem

- [cylc-flow](https://github.com/cylc/cylc-flow) - The core Cylc Scheduler for defining and running workflows.
- [cylc-uiserver](https://github.com/cylc/cylc-uiserver) - The web-based Cylc graphical user interface for monitoring and controlling workflows.
- [cylc-rose](https://github.com/cylc/cylc-rose) - Provides integration with [Rose](http://metomi.github.io/rose/).

### Migrating From Cylc 7

[Migration Guide](https://cylc.github.io/cylc-doc/stable/html/7-to-8/index.html)
| [Migration Support](https://cylc.discourse.group/c/cylc/7-to-8/13)

Cylc 8 can run most Cylc 7 workflows in compatibility mode with little to no
changes, go through the
[migration guide](https://cylc.github.io/cylc-doc/stable/html/7-to-8/index.html)
for more details.

Quick summary of major changes:

* Python 2 -> 3.
* Internal communications converted from HTTPS to ZMQ (TCP).
* PyGTK GUIs replaced by:
  * Terminal user interface (TUI) included in cylc-flow.
  * Web user interface provided by the cylc-uiserver package.
* A new scheduling algorithm with support for branched workflows.
* Command line changes:
  * `cylc run <id>` -> `cylc play <id>`
  * `cylc restart <id>` -> `cylc play <id>`
  * `rose suite-run` -> `cylc install; cylc play <id>`
* The core package containing Cylc scheduler program has been renamed cylc-flow.
* Cylc review has been removed, the Cylc 7 version remains Cylc 8 compatible.


### Citations & Publications

[![DOI](https://zenodo.org/badge/1836229.svg)](https://zenodo.org/badge/latestdoi/1836229)
[![JOSS](http://joss.theoj.org/papers/10.21105/joss.00737/status.svg)](https://doi.org/10.21105/joss.00737)
[![CISE](https://img.shields.io/website/https/ieeexplore.ieee.org/document/8675433.svg?color=orange&label=CISE&up_message=10.1109%2FMCSE.2019.2906593)](https://ieeexplore.ieee.org/document/8675433)

### Copyright and Terms of Use

[![License](https://img.shields.io/github/license/cylc/cylc-flow.svg?color=lightgrey)](https://github.com/cylc/cylc-flow/blob/master/COPYING)

Copyright (C) 2008-<span actions:bind='current-year'>2023</span> NIWA & British Crown (Met Office) & Contributors.

Cylc is free software: you can redistribute it and/or modify it under the terms
of the GNU General Public License as published by the Free Software Foundation,
either version 3 of the License, or (at your option) any later version.

Cylc is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
Cylc.  If not, see [GNU licenses](http://www.gnu.org/licenses/).

### Contributing

[![Contributors](https://img.shields.io/github/contributors/cylc/cylc-flow.svg?color=9cf)](https://github.com/cylc/cylc-flow/graphs/contributors)
[![Commit activity](https://img.shields.io/github/commit-activity/m/cylc/cylc-flow.svg?color=yellowgreen)](https://github.com/cylc/cylc-flow/commits/master)
[![Last commit](https://img.shields.io/github/last-commit/cylc/cylc-flow.svg?color=ff69b4)](https://github.com/cylc/cylc-flow/commits/master)

Contributions welcome:

* Read the [contributing](CONTRIBUTING.md) page.
* Development setup instructions are in the
  [developer docs](https://cylc.github.io/cylc-admin/#cylc-8-developer-docs).
* Involved change proposals can be found in the
  [admin pages](https://cylc.github.io/cylc-admin/#change-proposals).
* Touch base in the
  [developers chat](https://matrix.to/#/#cylc-general:matrix.org).
",2023-07-07 15:53:51+00:00
czid-dag,czid-dag,chanzuckerberg/czid-dag,Please see https://github.com/chanzuckerberg/czid-workflows for the latest version of CZ ID workflows.,https://idseq.net,True,26,2023-01-28 04:11:30+00:00,2018-06-02 00:58:48+00:00,12,16,19,4,v4.11.3,2020-06-26 20:06:09+00:00,MIT License,346,v4.11.8,40,2020-08-12 20:57:19+00:00,2023-05-22 08:37:52+00:00,2022-01-10 21:25:06+00:00,"# Historical CZ ID workflows
----------------------------
Please see https://github.com/chanzuckerberg/czid-workflows for CZ ID workflows. This repository is no longer maintained, and is made available for
historical purposes only.

----------------------------
This repo is formerly known as idseq-dag

#### Infectious Disease Sequencing Platform
CZ ID is a hypothesis-free global software platform that helps scientists identify pathogens in metagenomic sequencing data.

- **Discover** - Identify the pathogen landscape
- **Detect** - Monitor and review potential outbreaks
- **Decipher** - Find potential infecting organisms in large datasets

A collaborative open project of [Chan Zuckerberg Initiative](https://www.chanzuckerberg.com/) and [Chan Zuckerberg Biohub](https://czbiohub.org).

Check out our repositories:
- [idseq-web](https://github.com/chanzuckerberg/idseq-web) - Frontend portal
- [idseq-dag](https://github.com/chanzuckerberg/idseq-dag) - Bioinformatics pipeline and workflow engine (here)
- [idseq-cli](https://github.com/chanzuckerberg/idseq-cli) - Command line upload interface
- [idseq-bench](https://github.com/chanzuckerberg/idseq-bench) - Pipeline benchmarking tools

## CZID-DAG

czid_dag is the pipeline execution engine for CZ ID (see czid.org). It is a pipelining system that implements a directed acyclic graph (DAG) where the nodes (steps) correspond to individual python classes. The graph is defined using JSON.

The pipeline would be executed locally with local machine resources. idseq-dag could be installed inside a docker container and run inside the container. See the [Dockerfile](Dockerfile) for our setup.  More details could be found below.

## Requirements

Python 3.6 and up

## Installation

```
cd idseq-dag; pip3 install -e .
```

## Example

```
idseq_dag examples/host_filter_dag.json

```

` idseq_dag --help ` for more options

## Test

```
cd idseq-dag; python3 -m unittest

```

or `python3 -m unittest tests/<module_file> ` for testing individual modules.

## DAG Execution Details

### Composing an example dag json file
There are five basic elements of an IdSeq Dag
 - **name**: the name of the IdSeq Dag.
 - **output_dir_s3**: the s3 directory where the output files will be copied to.
 - **targets**: the outputs that are to be generated through dag execution. Each target consists of a list of files that will be copied to *output_dir_s3*
 - **steps**: the steps that will be executed in order to generate the targets. For each step, the following attributes can be specified:
   * *in*: the input targets
   * *out*: the output target
   * *module*: name of python module
   * *class*: name of python class that inherits from [PipelineStep](idseq_dag/engine/pipeline_step.py)
   * *additional_files*: additional S3 files required for dag execution, i.e. reference files.
   * *additional_attributes*: additional input parameters for the pipeline class
 - **given_targets**: the list of targets that are given. Given targets will be downloaded from S3 before the pipeline execution starts.

The following is an example dag for generating alignment output for idseq. The *host_filter_out* is given, and once downloaded, *gsnap_out* and *rapsearch2_out* steps will run in parallel. When *gsnap_out* and *rapsearch2_out* are both completed, *taxon_count_out* and *annotated_out* will be run simultaneously and the pipeline will be complete once everything is uploaded to S3.

```
{
  ""name"": ""alignment"",
  ""output_dir_s3"": ""s3://idseq-samples-prod/samples/12/5815/results_test"",
  ""targets"": {
    ""host_filter_out"": [
        ""gsnap_filter_1.fa""
          , ""gsnap_filter_2.fa""
          , ""gsnap_filter_merged.fa""
    ],
    ""gsnap_out"": [
      ""gsnap.m8"",
      ""gsnap.deduped.m8"",
      ""gsnap.hitsummary.tab"",
      ""gsnap_counts.json""
    ],
    ""rapsearch2_out"": [
      ""rapsearch2.m8"",
      ""rapsearch2.deduped.m8"",
      ""rapsearch2.hitsummary.tab"",
      ""rapsearch2_counts.json""
    ],
    ""taxon_count_out"": [""taxon_counts.json""],
    ""annotated_out"": [""annotated_merged.fa"", ""unidentified.fa""]
  },
  ""steps"": [
    {
      ""in"": [""host_filter_out""],
      ""out"": ""gsnap_out"",
      ""class"": ""PipelineStepRunAlignment"",
      ""module"": ""idseq_dag.steps.run_alignment_remotely"",
      ""additional_files"": {
        ""lineage_db"": ""s3://idseq-public-references/taxonomy/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/taxid-lineages.db"",
        ""accession2taxid_db"": ""s3://idseq-public-references/alignment_data/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/accession2taxid.db""

          ,""deuterostome_db"": ""s3://idseq-public-references/taxonomy/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/deuterostome_taxids.txt""

      },
      ""additional_attributes"": {
        ""alignment_algorithm"": ""gsnap"",
      }
    },
    {
      ""in"": [""host_filter_out""],
      ""out"": ""rapsearch2_out"",
      ""class"": ""PipelineStepRunAlignment"",
      ""module"": ""idseq_dag.steps.run_alignment_remotely"",
      ""additional_files"": {
        ""lineage_db"": ""s3://idseq-public-references/taxonomy/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/taxid-lineages.db"",
        ""accession2taxid_db"": ""s3://idseq-public-references/alignment_data/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/accession2taxid.db""

          ,""deuterostome_db"": ""s3://idseq-public-references/taxonomy/2018-02-15-utc-1518652800-unixtime__2018-02-15-utc-1518652800-unixtime/deuterostome_taxids.txt""

      },
      ""additional_attributes"": {
        ""alignment_algorithm"": ""rapsearch2"",
      }
    },
    {
      ""in"": [""gsnap_out"", ""rapsearch2_out""],
      ""out"": ""taxon_count_out"",
      ""class"": ""PipelineStepCombineTaxonCounts"",
      ""module"": ""idseq_dag.steps.combine_taxon_counts"",
      ""additional_files"": {},
      ""additional_attributes"": {}
    },
    {
      ""in"": [""host_filter_out"", ""gsnap_out"", ""rapsearch2_out""],
      ""out"": ""annotated_out"",
      ""class"": ""PipelineStepGenerateAnnotatedFasta"",
      ""module"": ""idseq_dag.steps.generate_annotated_fasta"",
      ""additional_files"": {},
      ""additional_attributes"": {}
    }
  ],
  ""given_targets"": {
    ""host_filter_out"": {
      ""s3_dir"": ""s3://idseq-samples-prod/samples/12/5815/results_test/2.4""
    }
  }
}

```

### Design

idseq-dag follows the KISS design principle. There are only two major components for dag execution:

 -  *[PipelineFlow](idseq_dag/engine/pipeline_flow.py)* validates the DAG, downloads the given targets, starts prefetching the additional files in a different thread, starts the pipeline steps in parallel and coordinates the execution.
 -  *[PipelineStep](idseq_dag/engine/pipeline_step.py)* waits for the input targets to be available, executes the run method, validates the output and uploads the files to S3.

Here is a quick example of a PipelineStep implementation for generating [taxon_count_out](master/idseq_dag/steps/combine_taxon_counts.py). Anyone can implement their own step by subclassing PipelineStep and implementing the *run* and *count_reads* method. *count_reads* is a method we use to count the output files. If you are not sure how to implement the count_reads function, just put a dummy function there as shown below.

In the *run* function, you need to make sure your implementation generates all the files specified in the `self.output_files_local()` list. Otherwise, the step will fail, which will trigger the whole pipeline to also fail.

By default, idseq-dag will only execute a step when the output target is not generated yet. You can turn off this caching mechanism with `--no-lazy-run` option with `idseq_dag` command.

```

import json
from idseq_dag.engine.pipeline_step import PipelineStep
import idseq_dag.util.command as command
import idseq_dag.util.count as count

class PipelineStepCombineTaxonCounts(PipelineStep):
    '''
    Combine counts from gsnap and rapsearch
    '''
    def run(self):
        input_files = []
        for target in self.input_files_local:
            input_files.append(target[3])
        output_file = self.output_files_local()[0]
        self.combine_counts(input_files, output_file)


    def count_reads(self):
        pass
    @staticmethod
    def combine_counts(input_json_files, output_json_path):
        taxon_counts = []
        for input_file_name in input_json_files:
            with open(input_file_name, 'r') as input_file:
                data = json.load(input_file)
                taxon_counts += data[""pipeline_output""][""taxon_counts_attributes""]
        output_dict = {""pipeline_output"": { ""taxon_counts_attributes"": taxon_counts}}
        with open(output_json_path, 'w') as output_file:
            json.dump(output_dict, output_file)

```

## Developers
When merging a commit to master, you need to increase the version number in `idseq_dag/__init__.py`:
  - if results are expected to change, increase the 2nd number
  - if results are not expected to change, increase the 3rd number.

## Generating reference indices
IDseq DAGs require the use of several indices prepared from files in NCBI. If you need to generate a new version of these indices, please refer to https://github.com/chanzuckerberg/idseq-pipeline, specifically the following commands:
  - host_indexing
  - gsnap_indexing
  - rapsearch_indexing
  - blacklist
  - lineages
  - curate_accession2taxid
  - curate_accessionid2seq
  - push_reference_update.
TODO: Move this code over to the idseq-dag repo.

## Release notes
Version numbers for this repo take the form X.Y.Z.
- We increase Z for a change that does not add or change results in any way. Example: adding a log statement.
- We increase Y for a change that adds results, changes results, or has the potential to change results. Example: changing a parameter in the GSNAP command.
- We increase X for a paradigm shift in how the pipeline is conceived. Example: adding a de-novo assembly step and then reassigning hits based on the assembled contigs.
Changes to X or Y force recomputation of all results when a sample is rerun using idseq-web. Changes to Z do not force recomputation when the sample is rerun - the pipeline will lazily reuse existing outputs in AWS S3.

When releasing a new version, please add a Git tag of the form `vX.Y.Z`.

- 4.11.8
  - Support local non-host alignment

- 4.11.7
  - Replace references to idseq-database with idseq-public-references

- 4.11.5
  - Unify remote and local non host alignment commands

- 4.11.4
  - Correctly count reads in empty file

- 4.11.3
  - Make STAR outputs deterministic by sorting

- 4.11.2
  - Disable lazy loading remote alignment chunks

- 4.11.1
  - Add support for running alignment locally.
  - Add fetch_from_s3 compatibility hack for transition to miniwdl handled downloads

- 4.11.0
  - Normalize error handling by raising a specific class of exceptions when expected errors occur
  - Simplify idseq_dag.util.count.reads() and remove invalid assumptions
  - Add unit tests for changed components
  - Fix unit test autodiscovery
  - Improve portability of existing unit tests and run them in CI

- 4.10.0
  - Add e-value threshold to require all internal alignments (short read and assembly-based) have e-value below threshold.

- 4.9.0
  - Update NCBI index databases to those downloaded on 2020-04-20.

- 4.8.0
  - Fix counting of reads that have a tax ID but do not have an accession to ensure all reads mapped to taxa only by assembly are counted in r value.

- 4.7.1
  - Fix occasional error in unidentified.fa counting.

- 4.7.0
  - Add a step-level entry point CLI.

- 4.6.0
  - Include non-unique reads in unidentified fastas for download.

- 4.5.0
  - Include non-unique reads in non-host fastqs for download.

- 4.4.2
  - Make the pipeline deterministic with hard coded seeded pseudo random number generation
  - Re-enable reusing previous chunks when re-running alignment.

- 4.4.1
  - Disable reusing previous chunks when re-running alignment. This was casing an error because the input to this step is non-determinsitic and cdhitdup requires all reads from the input to be present in the `cdhit_cluster_sizes` file.

- 4.4
  - When assigning contigs to their best-matching accessions, prioritize the accession that has the most best matches across all contigs when resolving ties.

- 4.3.8
  - Increase gsnap and rapsearch2 chunk size by 4x to reduce the number of batch jobs generated
  - Decreased alignment max chunks in flight from 32 to 16 to better balance chunk execution between large and small jobs

- 4.3.7
  - Remove old alignment code
  - Remove sqlite

- 4.3.6
  - Run alignment chunks first on spot then on on-demand after two failures

- 4.3.5
  - Check for filename collisions on input and output

- 4.3.4
  - Refactor call_hits_m8 to improve memory usage

- 4.3.3
  - Update pipeline stage status directly on s3 file for compatibility with SFN pipeline.

- 4.3.2
  - Clean up log statement for AMR bug.

- 4.3.1
  - Add compatibility for idseq-web environment name 'production'

- 4.3.0
  - Generate betacoronavirus fastq files for user download if use_taxon_whitelist is specified in the DAG.

- 4.2.4
  - Update RunAlignmentRemotely to name batch jobs with the chunk id, project id, and sample id
  - Update RunAlignmentRemotely to download results using boto3 rather than fetch_from_s3

- 4.2.3
  - Validate input step now properly rejects invalid gzip files.

- 4.2.2
  - Fix bug in phylo tree creation for organisms with an unknown superkingdom.

- 4.2.1
  - Switch RunAlignmentRemotely to distribute alignment chunks use AWS Batch instead of custom Autoscaling Group Logic logic

- 4.2.0
  - Apply deuterostome, blacklist, whitelist and human filters to contigs.

- 4.1.1
  - Removed `DAG_SURGERY_HACKS_FOR_READ_COUNTING` and related code.

- 4.1.0
  - Fix the behavior of blacklist and whitelist so that they are applied during compilation of taxon counts and not during identification of candidate accessions. This means that the pipeline now finds the global best taxon match for each read/contig first and only then excludes blacklisted/non-whitelisted taxa from the resulting counts. Previously, it was artifically restricting the search space by applying the blacklist and whitelist to candidate taxa upfront, thus reporting non-optimal matches for affected reads/contigs.

- 4.0.0
  - Switched to computing relative abundance values (r, rPM, contig r) without duplicate removal.

- 3.21.2
  - fix v4 counts for nonhuman hosts human filter steps; no effect in v3

- 3.21.1
  - bugfix affecting samples where SPADES crashed and we only have read alignment data

- 3.21.0
  - work around cdhitdup bug affecting unpaired reads that sometimes discards half the unique reads in an unpaired sample
  - set stage for 4.0 release by changing cdhit identity threshold to 100%
  - emit new files taxon_count_with_dcr.json, cdhit_cluster_sizes.tsv, dedup1.clstr
  - compute ReadCountingMode.COUNT_ALL but still emit COUNT_UNIQUE

- 3.20.1
  - Update s3parcp
  - Switch uploads to s3parcp
  - Support uploading directories with checksum metadata
  - Standardize gsnap and rapsearch index location
  - Update gsnap index generation to upload the raw index directory in addition to a tarball

- 3.20.0
  - Add a custom taxon whitelist mode. Fix taxon blacklist reference downloads.

- 3.19.6
  - Finished removal of optional_files_to_upload

- 3.19.5
  - Switch title of STAR description to be above first line of description

- 3.19.4
  - Copy change for STAR step
  - Don't break STAR if picard can't generate metrics

- 3.19.3
  - Handle case of null nucleotide type for collecting insert metrics

- 3.19.2
  - Use additional_output_files_visible

- 3.19.1
  - Upload additional cdhitdup output.

- 3.19.0
  - Compute insert size metrics for humans only

- 3.18.1
  - Update log statement for AMR bug for alerting purposes.

- 3.18.0
  - Version marker: Update NCBI index databases to those downloaded on 2020-02-10.

- 3.17.0
  - Version marker: Update NCBI index databases to those downloaded on 2020-02-03.

- 3.16.6
  - Isolate directories on alignment instances to chunks rather than whole samples
  - Clean up intermediate files from alignment instances after running alignment on a chunk
  - Ensure alignment instance is clean before running alignment on a chunk

- 3.16.5
  - Fix dag validation of Rapsearch index generation template.

- 3.16.4
  - Fail gracefully with INSUFFICIENT_READS error if all reads drop out during LZW filtering.

- 3.16.3
  - Use s3parcp with checksum for rapsearch index uploads

- 3.16.2
  - fix botocore import issue for util.s3
  - switch to subprocess command for `util.s3.list_s3_keys` for thread safety

- 3.16.1
  - implement LRU policy for reference downloads cache

- 3.16.0
  - Only compute insert size metrics for RNA reads if we have an organism-specific gtf file

- 3.15.1-4
  - change PySAM concurrency pattern to improve performance and eliminate deadlock
  - reduce logging from run_in_subprocess decorator
  - avoid using corrupt reference downloads
  - increase default Rapsearch timeout

- 3.15.0
  - Compute insert size metrics for all hosts for paired end DNA reads
  - Compute insert size metrics for hosts with gtf files for paired end RNA reads

- 3.14.1-4
  - aws credential caching and other stability improvements
  - fix bug that made reverse-strand alignments appear very short in the coverage viz
  - limit blastn BATCH_SIZE to avoid out-of-memory errors (results are unchanged)
  - reduce RAM footprint of blast rerank python to avoid out-of-memory errors (results are unchanged)

- 3.14
  - add average insert size computation

- 3.13.1 - 3.13.3
  - Make phylo tree and alignment viz steps more robust to missing accessions in index.
  - Ensure reference caching respects version.
  - Reduce frequency of s3 requests, other stability fixes.

- 3.13
  - Rerank NT blast results by taking into account all fragments for a given contig and reference sequence, not just the highest scoring one.

- 3.12.1
  - Fix bug where reads unassigned during alignment that were assembled into contigs were also being counted as loose reads.

- 3.12.0
  - Update NCBI databases to those downloaded on 2019-09-17.

- 3.11.0
  - Modify the LZW filter to apply a more stringent cutoff at higher read lengths.

- 3.10.2
  - Better logging for a rare AMR bug.

- 3.10.1
  - Increase GSNAP threads to 48 for better utilization of r5d.metal instances.

- 3.10.0
  - Apply a length filter, requiring all NT alignments (GSNAP and BLAST) be >= 36 nucleotides long.

- 3.9.4
  - Additional performance improvements in run_srst2 step, so that the step uses less RAM.

- 3.9.3
  - Fix typo in phylo tree generation step.

- 3.9.2
  - Fixed error in run_srst2 that failed to take into account different naming patterns from srst2 for the sorted bam file that it outputs.

- 3.9.1
   - Refactoring of command execution patterns and logs.
   - Removed some false error log messages related to lz4 file download support.

- 3.9.0
   - Add number of reads, reads per million, and depth per million to the output of PipelineStepRunSRST2.

- 3.8.0
   - Creates a [status name]_status.json file for each dag_json received, which each step updates with information
     about itself and its status.

- 3.7.6
   - Fail with an informative user error if the input contains broken read pairs.

- 3.7.0 .. 3.7.5
   - Validate whether input files to a pipeline step contain a sufficient number of reads.
     Output invalid_step_input.json file if validation fails.
   - Log output in JSON format. Change TraceLock log level to DEBUG.
   - Upgrade to python 3.7.3
   - Remove db_hack. Standardize db_open/db_assert_table/db_close log entries.
   - Fix division by zero error in coverage viz step.
   - Modify trimmomatic command to reduce MINLEN parameter to 35 and allow reads from fragments with small
     insert sizes (where R1 and R2 are reverse complements of each other) through the QC steps.

- 3.6.6
   - Another fix related to sqlite3 concurrency

- 3.6.0 .. 3.6.5
   - Fix an issue with the log event function when trying to log non json serializable fields.
   - A possible fix to some hanging issues in the pipeline that seem to be related to sqlite3 concurrency.
   - Address array index rounding error in coverage viz.
   - Extra logs to help detecting potential deadlocks in the pipeline
   - Add pipeline step to generate data for coverage visualization for IDseq report page. Data includes an index
     file that maps taxons to accessions with available coverage data, as well as data files for each accession
     that list various metrics including the coverage of the accession.

- 3.5.0 ... 3.5.4
   - New log methods to write log events. Added and replaced a few log entries.
   - Add ability to run STAR further downstream from input validation. This can be used to filter human reads
     after the host has been filtered out (if host is non-human).
   - Handle absence of m8 hits in PipelineStepBlastContigs.
   - Choose most-represented accessions of assembly/gsnap.hitsummary2.tab and assembly/rapsearch2.hitsummary2.tab
     as the NCBI references to include on phylogenetic trees, as opposed to making the choice based on pre-assembly
     align_viz files.
   - Improve the efficiency of S3 downloads and uploads in PipelineStepGenerateAlignmentViz.

- 3.4.0
   - switch from shelve to sqlite3 for all the lookup tables
   - add lineage generation step

- 3.3.2
   - Add input validation step.

- 3.3.1
   - Add step to generate nonhost fastq files by filtering the original fastq files for nonhost reads.

- 3.3.0
   - Upgrade GSNAP executable to version 2018-10-26.  Index remains unchanged at 2018-12-01.
     In comprehensive testing on a diverse set of samples, this has shown just a few minor
     effects on overall results, mostly for reads that align at the limit of detection.
     The benefit of the change is 3x-8x faster performance.  A/B test data is archived
     in slack channel #idseq-benchmarking.

- 3.2.5-3.2.1 only affect staging environment
   - 3.2.5 GSNAP Pre-release 2018-10-26, this time for real.
   - 3.2.4 Revert 3.2.3.
   - 3.2.3 GSNAP Pre-release 2018-10-20 (briefly thought to be 2018-10-20 by mistake).
   - 3.2.2 Revert 3.2.1.
   - 3.2.1 GSNAP Pre-release 2018-10-20.

- 3.2.0
   - Assembly with paired ends if available
   - Coverage Stats Step
- 3.1.0
   - Assembly based pipeline. Add assembly and blast the contigs to the aligned accessions

- 2.11.0
   - Add adapter trimming step.

- 2.10.0
   - Relax LZW filter for reads longer than 150 bp, linearly with read length.

- 2.9.0
   - Change how blacklist filter works so that if a read maps both to
     blacklisted and non-blacklisted entries, it isn't dropped, and
     only the non-blacklisted entries are used.  This improves recall
     for organisms whose DNA is used in cloning vectors, such
     as Chikunguya virus.

- 2.8.0
   - Add taxon blacklist filtering at hit calling

- 2.7.1 ... 2.7.4
   - Reduce LZW runtime from 2h 35m to 24 min on the largest samples
   - Increase GSNAP threads to 36 for i3.metal instances.
   - Addded Antimicrobial resistance step. Results for other steps won't change; only new results for AMR are expected.
   - Acquire lock before fork in run_in_subprocess decorator

- 2.7
   - ?

- 2.6
   - ?

- 2.5
   - ?

- 2.4.0
   - New directed acyclic graph-based execution model for the pipeline. Changes integration with the web app as well.

Below is copied from https://github.com/chanzuckerberg/idseq-pipeline :

- 1.8.7
   - Bug fix for count_reads and non-host read counts.

- 1.8.4 ... 1.8.6
   - Minor code quality, documentation, and logging improvements.

- 1.8.0 ... 1.8.3
   - Upload a status file that indicates when a job has completed.
   - Add a dedicated semaphore for S3 uploads.
   - Code quality and documentation improvements.
   - Restore capability to run non-host alignment from the development environment.
   - Try a more relaxed LZW fraction if the initial filter leaves 0 reads

- 1.7.2 ... 1.7.5
   - General code style changes and code cleanup.
   - Convert string exceptions and generic exceptions to RuntimeErrors.
   - Change some print statements for python3.
   - Add more documentation.

- 1.7.1
   - Truncate enormous inputs to 75 mil paired end / 150 mil unpaired reads.
   - Support input fasta with pre-filtered host, e.g. project NID.
   - Many operational improvements.

- 1.7.0
    - Add capability to  further filter out host reads by filtering all the hits
      from gsnapping host genomes. (i.e. gsnap hg38/patron5 for humans).

- 1.6.3 ... 1.6.1
    - Handle bogus 0-length alignments output by gsnap without crashing.
    - Fix crash for reruns which reuse compatible results from a previous run.
    - Fix crash for samples with unpaired reads.
    - Improve hit calling performance.

- 1.6.0
    - Fix fasta downloads broken by release 1.5.0, making sure only
      hits at the correct level are output in the deduped m8.
    - Fix fasta download for samples with unpaired reads by eliminating
      merged fasta for those samples.
    - Extend the partial fix in release 1.5.1 to repair more of the
      broken reports.  Full fix requires rerun with updated webapp.
    - Correctly aggregate counts for species with unclassified genera,
      such as e.g. genus-less species 1768803 from family 80864.
    - Fix total count in samples with unpaired reads (no longer doubled).
    - Fix crash when zero reads remain after host filtering.
    - Fix bug in enforcing command timeouts that could lead to hangs.
    - Fix performance regression in stage 2 (non-host alignment)
      introduced with 1.5.0.
    - Deduplicate and simplify much of stage 2, and improve performance
      by parallelizing uploads and downloads.

- 1.5.1
    - Fix bug introduced in 1.5.0 breaking samples with non-species-specific
      deuterostome hits.

- 1.5.0
    - Identify hits that match multiple species within the same genus as
      ""non species specific"" hits to the genus.

- 1.4.0
    - Version result folder.

- 1.3.0
    - Fix bug causing alignment to run before host subtraction in samples
      with unpaired reads.
    - Include ERCC gene counts from STAR.

- 1.2.0
    - Synchronize pair order after STAR to improve sensitivity in 10% of
      samples with paired-end reads.
",2023-07-07 15:53:58+00:00
czid-workflows,czid-workflows,chanzuckerberg/czid-workflows,Portable WDL workflows for CZ ID production pipelines,https://czid.org/,False,16,2023-05-31 16:13:51+00:00,2021-12-13 22:40:36+00:00,4,10,24,0,,,MIT License,779,vconsensus-genome-1.2.2,393,2020-08-19 00:10:53+00:00,2023-07-07 00:45:37+00:00,2023-07-05 19:02:22+00:00,"# czid-workflows - portable CZ ID production pipeline logic

#### Infectious Disease Sequencing Platform
CZ ID is a hypothesis-free global software platform that helps scientists identify pathogens in metagenomic sequencing
data.

- **Discover** - Identify the pathogen landscape
- **Detect** - Monitor and review potential outbreaks
- **Decipher** - Find potential infecting organisms in large datasets

CZ ID is a collaborative open project of [Chan Zuckerberg Initiative](https://www.chanzuckerberg.com/) and
[Chan Zuckerberg Biohub](https://czbiohub.org).

## Workflows
Currently we have 5 main workflows. The details of each pipeline are in a README in each of the workflow folders. 

* [short-read-mngs](workflows/short-read-mngs/README.md) 
* [consensus-genome](workflows/consensus-genome/README.md)
* [phylotree-ng](workflows/phylotree-ng/README.md)
* [long-read-mngs](workflows/long-read-mngs/README.md)
* [amr](workflows/amr/README.md)

## Running these workflows
This repository contains [WDL](https://openwdl.org/) workflows that the [CZ ID](https://czid.org) platform uses in
production. See [Running WDL workflows locally](https://github.com/chanzuckerberg/czid-workflows/wiki/Running-WDL-workflows-locally)
to get started with them.

### System Requirements 
The system requirements differs for each workflow and depending on the database being used. For example running the short-read-mngs workflow with the full NT and NR databases would require an instance with >1TB of disk space and >100GB of memory. Running other workflows (e.g. consensus-genome, amr) requires much less space. 

### Software requirements
* docker with buildx support (version >= 19.03)
* python3 
* virtualenv
* requirements-dev.txt - to automatically install this run `make python-dependencies`

### Quick Setup
To get setup, first set the workflow you want to run with 

```export WORKFLOW=<workflow-name>``` e.g.

```export WORKFLOW=amr```

You can see available workflows with `make ls`

Either `build` or `pull` the workflow docker image with 

```make pull  ## The faster option``` or 

```make build ## The slower option, but necessary if you're modifying the docker container```

### Running a workflow
Run a workflow with 

```make run```

Which simply runs the ```miniwdl run path_to_wdl.wdl``` command with some defaults

Each workflow has a number of required and optional inputs, and all require at least an input file (usually a fastq). Default inputs are set from the `workflows/<workflow-name>/test/local_test.yml` file. These may or may not be accurate for every analysis. You can override these defaults and add your own with:

```make run INPUT='-i your_file_here.yml'```

If you're happy with the defaults, you can add arguments to the `miniwdl` command using 

```make run EXTRA_INPUTS='input_fastq=/path/to/input.fastq' ```

### An example
Lets say I want to run a consensus-genome workflow. I would run the following:

```
export WORKFLOW=consensus-genome
make pull # pull the latest docker container from github packages
make python-dependencies # create a .venv and install the requirements-dev.txt dependencies 

make run EXTRA_INPUTS='fastqs_0=workflows/consensus-genome/test/sample_sars-cov-2_paired_r1.fastq.gz \
        fastqs_1=workflows/consensus-genome/test/sample_sars-cov-2_paired_r2.fastq.gz \
        technology=""Illumina"" \
        sample=""my_sample_name""'
```
## CI/CD

We use GitHub Actions for CI/CD. Lint and unit tests run on GitHub from jobs in `.github/workflows/wdl-ci.yml`
(triggered on every commit).

## Contributing

This project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to opensource@chanzuckerberg.com.

## Security

Please disclose security issues responsibly by contacting security@chanzuckerberg.com.
",2023-07-07 15:54:02+00:00
d6tflow,d6tflow,d6t/d6tflow,Python library for building highly effective data science workflows,https://d6tflow.readthedocs.io/en/latest/,False,949,2023-06-22 20:19:18+00:00,2019-02-03 01:51:22+00:00,74,24,11,0,,,MIT License,269,,0,,2023-06-11 14:02:55+00:00,2022-10-26 17:04:54+00:00,"# Databolt Flow

For data scientists and data engineers, `d6tflow` is a python library which makes building complex data science workflows easy, fast and intuitive. It is **primarily designed for data scientists to build better models faster**. For data engineers, it can also be a lightweight alternative and help productionize data science models faster. Unlike other data pipeline/workflow solutions, `d6tflow` focuses on managing data science research workflows instead of managing production data pipelines. 

## Why use d6tflow?

Data science workflows typically look like this.

![Sample Data Workflow](docs/d6tflow-docs-graph.png?raw=true ""Sample Data Workflow"")

The workflow involves chaining together parameterized tasks which pass multiple inputs and outputs between each other. The output data gets stored in multiple dataframes, files and databases but you have to manually keep track of where everything is. And often you want to rerun tasks with different parameters without inadvertently rerunning long-running tasks. The workflows get complex and your code gets messy, difficult to audit and doesn't scale well.

`d6tflow` to the rescue! **With d6tflow you can easily chain together complex data flows and execute them. You can quickly load input and output data for each task.** It makes your workflow very clear and intuitive.

#### Read more at:  
[4 Reasons Why Your Machine Learning Code is Probably Bad](https://github.com/d6t/d6t-python/blob/master/blogs/reasons-why-bad-ml-code.rst)  
[How d6tflow is different from airflow/luigi](https://github.com/d6t/d6t-python/blob/master/blogs/datasci-dags-airflow-meetup.md)

![Badge](https://www.kdnuggets.com/images/tkb-1904-p.png ""Badge"")
![Badge](https://www.kdnuggets.com/images/tkb-1902-g.png ""Badge"")

## When to use d6tflow?

* Data science: you want to build better models faster. Your workflow is EDA, feature engineering, model training and evaluation. d6tflow works with ANY ML library including sklearn, pytorch, keras
* Data engineering: you want to build robust data pipelines using a lightweight yet powerful library. You workflow is load, filter, transform, join data in pandas, dask, pyspark, sql, athena

## What can d6tflow do for you?

* Data science  
	* Experiment management: easily manage workflows that compare different models to find the best one
	* Scalable workflows: build an efficient data workflow that support rapid prototyping and iterations
	* Cache data: easily save/load intermediary calculations to reduce model training time
	* Model deployment: d6tflow workflows are easier to deploy to production
* Data engineering  
	* Build a data workflow made up of tasks with dependencies and parameters
	* Visualize task dependencies and their execution status
	* Execute tasks including dependencies
	* Intelligently continue workflows after failed tasks
	* Intelligently rerun workflow after changing parameters, code or data
	* Quickly share and hand off output data to others


## Installation

Install with `pip install d6tflow`. To update, run `pip install d6tflow -U`.

If you are behind an enterprise firewall, you can also clone/download the repo and run `pip install .`

**Python3 only** You might need to call `pip3 install d6tflow` if you have not set python 3 as default.

To install latest DEV `pip install git+git://github.com/d6t/d6tflow.git` or upgrade `pip install git+git://github.com/d6t/d6tflow.git -U --no-deps`

## Example: Model Comparison

Below is an introductory example that gets training data, trains two models and compares their performance.  

**[See the full ML workflow example here](http://tiny.cc/d6tflow-start-example)**  
**[Interactive mybinder jupyter notebook](http://tiny.cc/d6tflow-start-interactive)**

```python

import d6tflow
import sklearn.datasets, sklearn.ensemble, sklearn.linear_model
import pandas as pd


# get training data and save it
class GetData(d6tflow.tasks.TaskPqPandas):
    persist = ['x','y']

    def run(self):
        ds = sklearn.datasets.load_boston()
        df_trainX = pd.DataFrame(ds.data, columns=ds.feature_names)
        df_trainY = pd.DataFrame(ds.target, columns=['target'])
        self.save({'x': df_trainX, 'y': df_trainY}) # persist/cache training data


# train different models to compare
@d6tflow.requires(GetData)  # define dependency
class ModelTrain(d6tflow.tasks.TaskPickle):
    model = d6tflow.Parameter()  # parameter for model selection

    def run(self):
        df_trainX, df_trainY = self.inputLoad()  # quickly load input data

        if self.model=='ols':  # select model based on parameter
            model = sklearn.linear_model.LinearRegression()
        elif self.model=='gbm':
            model = sklearn.ensemble.GradientBoostingRegressor()

        # fit and save model with training score
        model.fit(df_trainX, df_trainY)
        self.save(model)  # persist/cache model
        self.saveMeta({'score': model.score(df_trainX, df_trainY)})  # save model score

# goal: compare performance of two models
# define workflow manager
flow = d6tflow.WorkflowMulti(ModelTrain, {'model1':{'model':'ols'}, 'model2':{'model':'gbm'}})
flow.reset_upstream(confirm=False) # DEMO ONLY: force re-run
flow.run()  # execute model training including all dependencies

'''
===== Execution Summary =====
Scheduled 2 tasks of which:
* 2 ran successfully:
    - 1 GetData()
    - 1 ModelTrain(model=ols)
This progress looks :) because there were no failed tasks or missing dependencies
'''

scores = flow.outputLoadMeta()  # load model scores
print(scores)
# {'model1': {'score': 0.7406426641094095}, 'gbm': {'model2': 0.9761405838418584}}


```


## Example Library

* [Minimal example](https://github.com/d6t/d6tflow/blob/master/docs/example-minimal.py)
* [Rapid Prototyping for Quantitative Investing with d6tflow](https://github.com/d6tdev/d6tflow-binder-interactive/blob/master/example-trading.ipynb) 
* d6tflow with functions only: get the power of d6tflow with little change in code. **[Jupyter notebook example](https://github.com/d6t/d6tflow/blob/master/docs/example-functional.ipynb)**

## Documentation

Library usage and reference https://d6tflow.readthedocs.io

## Getting started resources

[Transition to d6tflow from typical scripts](https://d6tflow.readthedocs.io/en/latest/transition.html)

[5 Step Guide to Scalable Deep Learning Pipelines with d6tflow](https://htmlpreview.github.io/?https://github.com/d6t/d6t-python/blob/master/blogs/blog-20190813-d6tflow-pytorch.html)

[Data science project starter templates](https://github.com/d6t/d6tflow-template)

## Pro version

Additional features:  
* Team sharing of workflows and data
* Integrations for datbase and cloud storage (SQL, S3)
* Integrations for distributed compute (dask, pyspark)
* Integrations for cloud execution (athena)
* Workflow deployment and scheduling

[Schedule demo](https://calendly.com/databolt/30min)

## Accelerate Data Science

Check out other d6t libraries, including  
* push/pull data: quickly get and share data files like code
* import data: quickly ingest messy raw CSV and XLS files to pandas, SQL and more
* join data: quickly combine multiple datasets using fuzzy joins

https://github.com/d6t/d6t-python

## Get notified

`d6tflow` is in active development. Join the [databolt blog](http://blog.databolt.tech) for the latest announcements and tips+tricks.

## Collecting Errors Messages and Usage statistics

We have put a lot of effort into making this library useful to you. To help us make this library even better, it collects ANONYMOUS error messages and usage statistics. See [d6tcollect](https://github.com/d6t/d6tcollect) for details including how to disable collection. Collection is asynchronous and doesn't impact your code in any way.

It may not catch all errors so if you run into any problems or have any questions, please raise an issue on github.

## How To Contribute

Thank you for considering to contribute to the project. First, fork the code repository and then pick an issue that is open. Afterwards follow these steps
* Create a branch called \[issue_no\]\_yyyymmdd\_\[feature\]
* Implement the feature
* Write unit tests for the desired behaviour
* Create a pull request to merge branch with master

A similar workflow applies to bug-fixes as well. In the case of a fix, just change the feature name with the bug-fix name. And make sure the code passes already written unit tests.
",2023-07-07 15:54:06+00:00
dagobah,dagobah,thieman/dagobah,Simple DAG-based job scheduler in Python,,False,744,2023-05-29 06:01:00+00:00,2013-05-10 14:48:40+00:00,161,51,7,0,,,Do What The F*ck You Want To Public License,312,v0.3.1,9,2014-09-26 20:18:34+00:00,2023-06-01 00:57:24+00:00,2018-03-05 15:59:41+00:00,"Dagobah
=======

<img src=""http://i.imgur.com/D5ZxGXA.png"" height=""400"">

[![Build Status](https://travis-ci.org/thieman/dagobah.png?branch=master)](https://travis-ci.org/thieman/dagobah?branch=master) [![PyPi version](https://img.shields.io/pypi/v/dagobah.svg)](https://pypi.python.org/pypi/dagobah/0.3.1)

Dagobah is a simple dependency-based job scheduler written in Python. Dagobah allows you to schedule periodic jobs using Cron syntax. Each job then kicks off a series of tasks (subprocesses) in an order defined by a dependency graph you can easily draw with click-and-drag in the web interface.

Dagobah lets you retry individual tasks from failure, sends you helpful email reports on job completion and failure, keeps track of your tasks' stdout and stderr, and persists its information in various backends so you don't have to worry about losing your data.

You can also [use Dagobah directly in Python.](../../wiki/Using Dagobah Directly in Python)

## Installation

Dagobah works with Python 2.6 or 2.7.

    pip install dagobah
    dagobahd  # start the web interface on localhost:9000

On first start, `dagobahd` will create a [config file](dagobah/daemon/dagobahd.yml) at `~/.dagobahd.yml`. You'll probably want to check that out to get your backend and email options set up before you start using Dagobah.

Dagobah does not require a backend, but unless you specify one, your jobs and tasks will be lost when the daemon exits. Each backend requires its own set of drivers. Once you've installed the drivers, you then need to specify any backend-specific options in the config. [See the config file for details.](dagobah/daemon/dagobahd.yml)

### Available Backends

To use a backend, you need to install the drivers using the commands below and then tell Dagobah to use the backend in the config file (default location `~/.dagobahd.yml`).

#### MongoDB

    pip install pymongo
    
#### Deprecated Backends

 * **SQLite**: Deprecated following version 0.3.1.

## Features

#### Single-user auth

<img src=""http://i.imgur.com/f843YXK.png"" height=""200"">

#### Manage multiple jobs scheduled with Cron syntax. Run times are shown in your local timezone.

<img src=""http://i.imgur.com/PjPQedn.png"" height=""400"">

#### Tasks can be anything you'd normally run at a shell prompt. Pipe and redirect your heart out.

<img src=""http://i.imgur.com/mWuQopx.png"" height=""400"">

#### Failed tasks don't break your entire job. Once you fix the task, the job picks up from where it left off.

<img src=""http://i.imgur.com/u2vDre2.png"" height=""400"">

#### On completion and failure, Dagobah sends you an email summary of the executed job (just set it up in the config file).

<img src=""http://i.imgur.com/yN6LUUZ.png"" height=""400"">

#### Tasks can even be [run on remote machines](https://github.com/thieman/dagobah/wiki/Adding-and-using-remote-hosts-in-Dagobah) (using your SSH config)
<img src=""http://i.imgur.com/3sNjJiz.png"" height=""200"">

#### Contributors

 * [Travis Thieman](https://twitter.com/thieman)
 * [Shon T. Urbas](https://github.com/surbas)
 * [Utkarsh Sengar](https://twitter.com/utsengar)
 * Stephanie Wei
 * [Ryan Clough](https://github.com/rclough)

#### Get Started Contributing

 * See the fledgling [wiki](../../wiki) or [create a new issue](../../issues) to get started
 * If you have any questions, go ahead and [email](mailto:travis.thieman@gmail.com) or [tweet at](https://twitter.com/thieman) me, or go ahead and create a new issue in this repository.
",2023-07-07 15:54:10+00:00
dagr,dagr,fulcrumgenomics/dagr,A scala based DSL and framework for writing and executing bioinformatics pipelines as Directed Acyclic GRaphs,,False,69,2022-12-04 13:18:57+00:00,2016-02-03 02:46:27+00:00,15,11,10,8,1.1.0,2022-03-12 12:58:30+00:00,MIT License,472,1.1.0,10,2022-03-12 12:58:30+00:00,,2022-03-12 13:12:21+00:00,"[![Build Status](https://github.com/fulcrumgenomics/dagr/workflows/unit%20tests/badge.svg)](https://github.com/fulcrumgenomics/dagr/actions?query=workflow%3A%22unit+tests%22)
[![Coverage Status](https://codecov.io/github/fulcrumgenomics/dagr/coverage.svg?branch=master)](https://codecov.io/github/fulcrumgenomics/dagr?branch=master)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.fulcrumgenomics/dagr_2.13/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.fulcrumgenomics/dagr_2.13)
[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/fulcrumgenomics/dagr/blob/master/LICENSE)
[![Language](http://img.shields.io/badge/language-scala-brightgreen.svg)](http://www.scala-lang.org/)

# Dagr

A task and pipeline execution system for directed acyclic graphs to support scientific, and more specifically, genomic analysis workflows.
We are currently in alpha development; please see the [Roadmap](#roadmap).
The latest API documentation can be found [here](http://javadoc.io/doc/com.fulcrumgenomics/dagr_2.13).

<!---toc start-->
  * [Goals](#goals)
  * [Building](#building)
  * [Command line](#command-line)
  * [Authors](#authors)
  * [License](#license)
  * [Include Dagr in your project](#include-dagr-in-your-project)
  * [Using DAGR](#using-dagr) - documentation on features and concepts in DAGR

<!---toc end-->

## Goals

There are many toolkits available for creating and executing pipelines of dependent jobs; dagr does not aim to be all things to all people but to make certain types of pipelines easier and more pleasurable to write.  It is specifically focused on:

* Writing pipelines that are concise, legible, and type-safe
* Easy composition of pipelines into bigger pipelines
* Providing safe and coherent ways to dynamically change the graph during execution
* Making the full power and expressiveness of [scala](http://www.scala-lang.org/) available to pipeline authors
* Efficiently executing tasks concurrently within the constraints of a single machine/instance

It is a tool for working data scientists, programmers and bioinformaticians.

## Building

DAGR uses [sbt](https://www.scala-sbt.org/).  Installation instructions are available [here](https://www.scala-sbt.org/download.html).

To build an executable jar run: `sbt assembly`.
Tests may be run with: `sbt test`.

## Command line

DAGR is run with: `java -jar target/scala-2.13/dagr-1.0.0-SNAPSHOT.jar`
Running the above with no options will produce the usage documentation.

## Include dagr in your project

You can include the three sub-projects that make up dagr using:

```
libraryDependencies += ""com.fulcrumgenomics"" %%  ""dagr-core"" % ""1.0.0""
libraryDependencies += ""com.fulcrumgenomics"" %%  ""dagr-tasks"" % ""1.0.0""
libraryDependencies += ""com.fulcrumgenomics"" %%  ""dagr-pipelines"" % ""1.0.0""
```

Or you can depend on the following which will pull in the three dependencies above:

```
libraryDependencies += ""com.fulcrumgenomics"" %% ""dagr"" % ""1.0.0"",
```

## Authors

* [Tim Fennell](https://github.com/tfenne) (maintainer)
* [Nils Homer](https://github.com/nh13) (maintainer)

## License

`dagr` is open source software released under the [MIT License](https://github.com/fulcrumgenomics/dagr/blob/master/LICENSE).


# Using DAGR

The following sections contain an overview of key features and principles behind DAGR that will be useful for anyone working with DAGR pipelines.

## Understanding DAGR's pipeline model

The atom in DAGR's model is called a [Task](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Task.scala).  There are many kinds of tasks, from ones that run a specific command line or execute a small piece of code, up to `Pipeline`s that are tasks which manage the construction of chains of other tasks.  The two main sub-types of Task are:

1. [Pipelines](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Pipeline.scala) which chain together one or more other tasks (including other pipelines)
2. [UnitTasks](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/UnitTask.scala) which perform some task when executed.  These can be tasks which run a command line inside a shell, or tasks that run some scala code within the JVM.

Pipelines in DAGR are written in scala, with the aid of an internal DSL for specifying and managing dependencies among other things.

DAGR pipelines are Directed Acyclic GRaphs (hence the name), meaning that a graph is contructed where the tasks are the nodes and the dependencies are the edges between the nodes.  While a programming language might control flow with with conditional constructs (e.g. if statements), these don't fit into a graph model.  In DAGR we allow some conditionality by defering construction of as much of the graph as possible until the last moment possible.

It's helpful to understand when code in Task and Pipeline classes is run:

* Code in the constructors of tasks and pipelines is run immediately just like with any other class.  Therefore code that relies on the existence of input files etc. may not work as expected if placed in the constructor, because input files may not exist yet.
* Code in the `build()` method of Pipelines, and code in the `args()` method of `ProcessTask`  are executed when all dependencies of the pipeline or task have been met.  As a result code run in these methods can generally rely on inputs existing.

## Example Pipeline

The following is an example of a [simple Example pipeline in dagr](pipelines/src/main/scala/dagr/pipelines/ExamplePipeline.scala), minus import and package statements:

```scala
@clp(description=""Example FASTQ to BAM pipeline."", group = classOf[Pipelines])
class ExamplePipeline
( @arg(flag=""i"", doc=""Input FASTQ."")        val fastq: PathToFastq,
  @arg(flag=""r"", doc=""Reference FASTA."")    val ref: PathToFasta,
  @arg(flag=""t"", doc=""Target regions."")     val targets: Option[PathToIntervals] = None,
  @arg(flag=""o"", doc=""Output directory."")   val out: DirPath,
  @arg(flag=""p"", doc=""Output file prefix."") val prefix: String
) extends Pipeline(Some(out)) {

  override def build(): Unit = {
    val bam    = out.resolve(prefix + "".bam"")
    val tmpBam = out.resolve(prefix + "".tmp.bam"")
    val metricsPrefix: Some[DirPath] = Some(out.resolve(prefix))
    Files.createDirectories(out)

    val bwa   = new BwaMem(fastq=fastq, ref=ref)
    val sort  = new SortSam(in=Io.StdIn, out=tmpBam, sortOrder=SortOrder.coordinate)
    val mark  = new MarkDuplicates(in=tmpBam, out=Some(bam), prefix=metricsPrefix)
    val rmtmp = new DeleteBam(tmpBam)

    root ==> (bwa | sort) ==> mark ==> rmtmp
    targets.foreach(path => root ==> new CollectHsMetrics(in=bam, prefix=metricsPrefix, targets=path, ref=ref))
  }
}
```

The `@clp` and `@arg` annotations are required to expose this pipeline for execution via the command line interface. These annotations come from the [sopt project](https://github.com/fulcrumgenomics/sopt) which DAGR uses for command line parsing.  For pipelines that do not need to be run via the command line (for example if they are only used as building blocks in other pipelines) they can be omitted.

## Dependency Language

As can be seen in the example, DAGR uses a number of operators to wire together tasks in a pipeline.  The following are all part of the dependency DSL:

* `task1 ==> task2` creates a dependency that requires that `task1` completes successfully before `task2` is started.  This operator can be chained, e.g. `a ==> b ==> c`
* `root ==> task` adds `task` to the set of root tasks (i.e. those without dependencies) for a pipeline.  `root` is a keyword in the DSL that can be though of as equivalent to `this`.
* `task1 :: task2` creates an object that can be used as a shorthand to mean ""task1 and task2"".  The result can be used anywhere a Task can be used, including chaining the operator: `a :: b :: c`

It is worth noting that no harm is done by adding a dependency more than once.  E.g. it is perfectly ok to write:

```scala
root ==> (a :: b :: c) ==> (d :: e)
root ==> (a :: c) ==> (e :: f) ==> g
```

### Using `Option`

In the dependency DSL, anywhere you can use a `Task` you can also use an `Option[Task]`.  For example:

```scala
...
val trim: Option[Int] = ...
val trimTask = trim.map(len => new TrimBam(in=bam, out=trimmedBam, length=len))
root ==> trimTask ==> nextTask
...
```

In the above example if `trim` is a `Some(int)` then we end up with a chain of `root => TrimBam ==> nextTask`.  On the other hand if `trim` is None, then the dependency is automatically reduced to `root ==> nextTask`.

### Examples

#### Example 1

```scala
root ==> a ==> (b :: c) ==> d
```

This example sets up the dependencies so that `a` will run immediately when the pipeline starts running, with `b` and `c` depending on `a` and finally with `d` depending on both `b` and `c`.

#### Example 2

```scala
val clipTasks = inputBams.map(b => new MarkIlluminaAdapters(in=b, out=Paths.get(""trimmed."" + b))
val merge = new MergeSamFiles(in=clipTasks.map(_.in), out=Paths.get(""merged.bam""))
root ==> clippedTasks.reduce(_ :: _) ==> merge
```

* The first line creates a `MarkIlluminaAdapters` task for each input BAM.
* The second line creates a `MergeSamFiles` task taking the inputs of all the marking tasks and creating a merged BAM
* The third line uses the `reduce()` collection function along with the `::` operator to combine all the marking tasks into a single dependency group, and then wires them in before the merge task

## Special Tasks & Classes

DAGR has a number of ""special"" tasks that are useful in their own right and also illustrate how to take advantage of the dynamic nature of DAGR's graph building.

### `Linker`

The [Linker](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Linker.scala) object creates a mechanism to safely copy state from one task to another when the first task finishes running.  This can be useful when one task computes a value that is not written to a file, that is needed by another task.  The following is a toy example that demonstrates usage:

```scala
val input: PathToBam = ...
val counter = new SimpleInJvmTask {
  var count: Option[Int] = None
  def run(): Unit = { count = Some(SamSource(input).iterator.size) }
}
val downsample = new DownsampleSam(in=input, out=Paths.get(""downsampled.bam""), target=1e6.toInt)
Linker(counter, downsample, (c, d) => d.inputReads = c.count)
```

The example shows one task that counts the reads in a BAM file and exposes the result in a `var` on the task when it's done running.  The count is then needed by the `downsample` task in order to determine how best to reach the target number of reads.  The `Linker` takes in the two tasks and a function that is to be run when the first task completes successfully, which transfers the count.

### EitherTask

The [EitherTask](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/EitherTask.scala) is similar to an `Either` in scala and other functional languages.  It can be thought of as a decision node in the pipeline, which goes left or right depending on some value that isn't yet known.  For example if a different duplicate marking algorithm should be used depending on whether the data has UMIs or not:

```scala
val umiCheck = new SimpleInJvmTask {
  var hasUmis: Boolean = false
  def run(): Unit = { hasUmis = SamSource(input).headOption.exists(_.get(""RX"").isDefined }
}
val deduper = Either.of(new MarkDuplicates(in=input, out=deduped), new SamBlaster(in=input, out=deduped), umiCheck.hasUmis)
root ==> umiCheck ==> deduper
```

## Shell Piping

DAGR also has a built in DSL and facilities for wiring together tasks that should stream information using shell pipes and redirects.  The piping is even typesafe to prevent, for example, accidentally piping SAM data into a process that expects FASTQ.  To support piping tasks must implement one of:

* [Pipe[A,B]](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Pipe.scala#L51) a trait that marks the task as supporting piping in type `A` and piping out type `B`
* [PipeIn[A]](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Pipe.scala#L98) a trait that states the program can accept piped input of type `A`
* [PipeOut[B]](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Pipe.scala#L101) a trait that states the task can produce piped output of type `B`

For example the `FastqToSam` task is defined as follows:

```scala
class FastqToSam(fastq1: PathToFastq,
                 fastq2: Option[PathToFastq] = None,
                 out: PathToBam,
                 ...)
  extends PicardTask with Pipe[Fastq,SamOrBam]
```

To use piping you might write something like:

```scala
val unmappedBam  = output.resolve(""unmapped.bam"")
val fqToSam      = new FastqToSam(fastq1=fastq, out=Io.StdOut)
val markAdapters = new MarkIlluminaAdapters(in=Io.StdIn, out=Io.StdOut)
val makeUnmapped = fqToSam | markAdapters > unmappedBam
root ==> makeUnmapped
```

The following operators are supported for tasks that support piping:

* `a | b` pipes stdout from a to b's stdin
* `a > path` redirects stdout from `a` into a file at `path`
* `a >> path` redirects stdout from `a` and appends it to the file at `path`
* `a >! path` redirects stderr from `a` into a file at `path`
* `a >>! path` redirects stderr from `a` and appends it to the file at `path`

Any types can be used when extending/implementing `Pipe`.  A number of types common to bioinformatics are defined in the [tasks package](https://github.com/fulcrumgenomics/dagr/blob/master/tasks/src/main/scala/dagr/tasks/package.scala#L43).

Note that because scala can mix-in traits at runtime, if you find a task you want to use piping with (and the tool supports reading from stdin or writing to stdout) but doesn't currently implement `Pipe`, you can add it on the fly.  For exsample if you want to downsample a BAM only to generate a single set of metrics from it:

```scala
Seq(0.25, 0.5, 0.75).foreach { frac =>
  val ds = new DownsampleSam(in=input, out=Io.StdOut) with PipeOut[SamOrBam]
  val isize = new InsertSizeMetrics(in=Io.StdIn, out=Paths.get(s""isize.$frac"")) with PipeIn[SamOrBam]
  root ==> (ds | isize)
}
```

## Resource Management

DAGR schedules tasks for execution when all their dependencies have been met and when there are sufficient resources.  Tasks in DAGR must therefore define the resources they need to execute.  The resources managed by DAGR are cpu cores and memory.

Pipelines themselves do not consume resources, but the concrete tasks ([UnitTasks](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/UnitTask.scala)) within pipelines do.  Tasks manage their resources needs through a pair of functions:

```scala
def pickResources(availableResources: ResourceSet): Option[ResourceSet]
def applyResources(resources : ResourceSet): Unit
```

The first function is called by DAGR to inform the task of the available resources, and ask the task a) whether it can run with the available resources and b) what subset of those resources it would like.  The second function is called by DAGR to inform the task of it's allocated resources immediately prior to execution.  While these functions can be implemented for each task, they are usually implemented by mixing in one of the following traits:

* [FixedResources](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Schedulable.scala#L169) allows tasks to define a fixed amount of CPU cores and memory through a number of `requires()` functions
* [VariableResources](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Schedulable.scala#L207) if the task can take advantage of more resources when they are available.

For example a tool which can be multi-threaded but requires a fixed amount of memory per thread might extend `VariableResources` and define the following function to pick it's resources:

```scala
override def pickResources(resources: ResourceSet): Option[ResourceSet] = {
  val memPerThread = Memory(""2G"")
  val maxThreads = 48
  val cpus = Range.inclusive(start = 48, end = 2, step= -1)
  cpus.flatMap(c => resources.subset(Cores(c), Memory(c * memPerThread.value).headOption
}
```

What this does is attempt to allocate 48 cpu cores, then 47 etc. down to 2, with 2GB per core.  The first combination that fits within the available resources (`ResourceSet.subset()` returns `Option[ResourceSet]`, yielding the requested resources if they fit and `None` if they don't) is then returned.  If no combination fits, `None` is returned and the task isn't scheduled yet.

## Retry

DAGR has the ability to retry tasks that fail.  The default behaviour is to try a task once and if it fails, it stays failed.  This can be changed by mixing in the [Retry](https://github.com/fulcrumgenomics/dagr/blob/master/core/src/main/scala/dagr/core/tasksystem/Retry.scala) trait to a task either when the task class is defined, or when the task is instantiated.  The trait defines one function:

```scala
def retry(resources: SystemResources, taskInfo: TaskExecutionInfo): Boolean = false
```

If, when invoked, the function returns `true` then the task will be retried.  Note that the task may modify it's internal state (arguments, resource requirements, etc.) when `retry()` is invoked to increase the chance of success.

A number of helper traits are available:

* `LinearMemoryRetry` will increase the memory used by a fixed increment each retry up to some maximum memory limit
* `MemoryDoublingRetry` will double the allocated memory on each retry up to some maximum memory limit
* `JvmRanOutOfMemory` is a trait that can be used with either of the above traits when the task is executing a Java program.  It examines the log to determine if the failure was caused by running out of memory, and prevents retry if the failure was not memory-related.
* `MultipleRetry` will retry a task a fixed number of times without altering anything. This can be useful if a task is prone to failure due to e.g. intermittent network connectivity problems.


## Scatter / Gather

DAGR includes a concise framework for performing scatter/gather operations, i.e. breaking an input into multiple partitions based on some criteria, processing each partition in parallel and then merging the results.  A common example of this in variant calling where the genome is broken down into non-overlapping sets of regions which are called independently and then the resulting VCFs are merged to create a final VCF.  DAGR's implementation supports the ability to chain together multiple tasks per partition before gathering.

Building a scatter/gather workflow is relatively simple and has few requirements beyond a normal workflow:

1. A task that implements the `Partitioner` trait
2. A task that can merge the results of the processing each partition

An example pipeline exists in DAGR that uses the GATK to genotype a sample using a chain of `HaplotypeCaller`, `GenotypeGVCFs` and `FilterVcf` on each partition.  The full example can be found in the [ParallelHaplotypeCaller](https://github.com/fulcrumgenomics/dagr/blob/master/pipelines/src/main/scala/dagr/pipelines/ParallelHaplotypeCaller.scala) workflow.

A simplified version follows:

```scala
import dagr.tasks.ScatterGather.Scatter
val scatterer = new SplitIntervalsForCallingRegions(ref=ref, intervals=intervals, output=Some(dir), maxBasesPerScatter=5e6.toInt)
val scatter = Scatter(scatterer)
val hc      = scatter.map(il => new HaplotypeCaller(ref=ref, intervals=Some(il), bam=input, vcf=PathUtil.replaceExtension(il, "".g.vcf.gz""))
val gt      = hc.map(h => GenotypeGvcfs(ref=ref, intervals=h.intervals, gvcf=h.vcf, vcf=replace(h.vcf, "".g.vcf.gz"", "".vcf.gz"")))
val filter  = gt.map(g => new FilterVcf(in=g.vcf, out=replace(g.vcf, "".vcf.gz"", "".filtered.vcf.gz"")))
val gather  = filter.gather(fs => new GatherVcfs(in=fs.map(_.out), out=output))

root ==> scatter
gather ==> new DeleteFiles(dir)
```

There's a lot going on in this short code snippet!

1. `SplitIntervalsForCallingRegions` is a task that breaks apart an interval list into many smaller interval lists. It also implements `Partitioner[PathToIntervals]` which allows the scatter/gather framework to use it to initiate a scatter operation.
2. On line 3 we instantiate a `Scatter` object which is used to wire together and manage all the tasks in the scatter/gather pipeline
3. On line 4 we use the `.map()` method on `Scatter` to create a `HaplotypeCaller` task per partition.  The `.map()` method takes as a parameter a function that is invoked once per input partition to create the jobs we need.
4. On lines 5 and 6 we continue the scattered processing by extending the chain with further calls to `.map()`.  In this case the function we provide to `.map()` receives as input the prior task in the chain.
5. On line 7 we use the `.gather()` function to generate a task that gathers the results of all the scatters.  It receives as input the last task from each branch of the scatter, and uses their outputs as the input to `GatherVcfs`.
6. Lastly on lines 9-10 we wire in the dependencies.  The framework has already hooked up all the dependencies between parts of the scatter/gather workflow so all that's left is to wire `scatter` to it's dependencies, and add any tasks that come afterwards

It should be noted that all of the objects generated during a scatter/gather workflow (e.g. `scatter`, `hc`, `gt`, `filter`) can be `.map()'d` and `.gather()'d` more than once.  I.e. you can build a scatter/gather pipeline with multiple branches and multiple endpoints without repeating any work.

",2023-07-07 15:54:14+00:00
dagster,dagster,dagster-io/dagster,"An orchestration platform for the development, production, and observation of data assets.",https://dagster.io,False,7840,2023-07-07 15:36:31+00:00,2018-04-30 16:30:04+00:00,978,97,293,251,1.3.13,2023-06-29 16:32:19+00:00,Apache License 2.0,14901,v0.2.8,679,2018-11-01 17:13:44+00:00,2023-07-07 15:50:31+00:00,2023-07-07 15:43:27+00:00,"<div align=""center"">
  <!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI -->
  <a target=""_blank"" href=""https://dagster.io"" style=""background:none"">
    <img alt=""dagster logo"" src="".github/dagster-readme-header.svg"" width=""auto"" height=""100%"">
  </a>
<p style=""text-align: center;"">Remember to <a target=""_blank"" href=""https://github.com/dagster-io/dagster"">star the Dagster GitHub repo</a> for future reference.</p>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster"" style=""background:none"">
    <img src=""https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&color=163B36&logo=github"">
  </a>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster/blob/master/LICENSE"" style=""background:none"">
    <img src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&labelColor=4F43DD&color=163B36"">
  </a>
  <a target=""_blank"" href=""https://pypi.org/project/dagster/"" style=""background:none"">
    <img src=""https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&color=163B36"">
  </a>
  <a target=""_blank"" href=""https://pypi.org/project/dagster/"" style=""background:none"">
    <img src=""https://img.shields.io/pypi/pyversions/dagster?labelColor=4F43DD&color=163B36"">
  </a>
  <a target=""_blank"" href=""https://twitter.com/dagster"" style=""background:none"">
    <img src=""https://img.shields.io/badge/twitter-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=twitter"" />
  </a>
  <a target=""_blank"" href=""https://dagster.io/slack"" style=""background:none"">
    <img src=""https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=slack"" />
  </a>
  <a target=""_blank"" href=""https://linkedin.com/showcase/dagster"" style=""background:none"">
    <img src=""https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&color=163B36&logo=linkedin"" />
  </a>
</div>

__Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.__

It is designed for **developing and maintaining data assets**, such as tables, data sets, machine learning models, and reports.

With Dagster, you declare—as Python functions—the data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.

Here is an example of a graph of three assets defined in Python:

```python
from dagster import asset
from pandas import DataFrame, read_html, get_dummies
from sklearn.linear_model import LinearRegression

@asset
def country_populations() -> DataFrame:
    df = read_html(""https://tinyurl.com/mry64ebh"")[0]
    df.columns = [""country"", ""continent"", ""rg"", ""pop2018"", ""pop2019"", ""change""]
    df[""change""] = df[""change""].str.rstrip(""%"").str.replace(""−"", ""-"").astype(""float"")
    return df

@asset
def continent_change_model(country_populations: DataFrame) -> LinearRegression:
    data = country_populations.dropna(subset=[""change""])
    return LinearRegression().fit(get_dummies(data[[""continent""]]), data[""change""])

@asset
def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:
    result = country_populations.groupby(""continent"").sum()
    result[""pop_change_factor""] = continent_change_model.coef_
    return result
```
The graph loaded into Dagster's web UI:

<p align=""center"">
  <img width=""400px"" alt=""An example asset graph as rendered in the Dagster UI"" src=""https://user-images.githubusercontent.com/654855/183537484-48dde394-91f2-4de0-9b17-a70b3e9a3823.png"">
</p>

Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.

## Quick Start:

If you're new to Dagster, we recommend reading about its [core concepts](https://docs.dagster.io/concepts) or learning with the hands-on [tutorial](https://docs.dagster.io/tutorial).

Dagster is available on PyPI and officially supports Python 3.7+.

```bash
pip install dagster dagit
```

This installs two modules:

- **Dagster**: The core programming model.
- **Dagit**: The web interface for developing and operating Dagster jobs and assets.

Running on Using a Mac with an M1 or M2 chip? Check the [install details here](https://docs.dagster.io/getting-started/install#installing-dagster-into-an-existing-python-environment).

## Documentation

You can find the full Dagster documentation [here](https://docs.dagster.io), including the ['getting started' guide](https://docs.dagster.io/getting-started).

<hr/>

## Key Features:

  <p align=""center"">
    <img width=""100%"" alt=""image"" src="".github/key-features-cards.svg"">
  </p>

### Dagster as a productivity platform
Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.

### Dagster as a robust orchestration engine
Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.

### Dagster as a unified control plane
Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.

<hr />

## Master the Modern Data Stack with integrations

Dagster provides a growing library of integrations for today’s most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.

<br/>
<p align=""center"">
    <a target=""_blank"" href=""https://dagster.io/integrations"" style=""background:none"">
        <img width=""100%"" alt=""image"" src="".github/integrations-bar-for-readme.png"">
    </a>
</p>

## Community

Connect with thousands of other data practitioners building with Dagster. Share knowledge, get help,
and contribute to the open-source project. To see featured material and upcoming events, check out
our [Dagster Community](https://dagster.io/community) page.

Join our community here:

- 🌟 [Star us on Github](https://github.com/dagster-io/dagster)
- 📥 [Subscribe to our Newsletter](https://dagster.io/newsletter-signup)
- 🐦 [Follow us on Twitter](https://twitter.com/dagster)
- 🕴️ [Follow us on LinkedIn](https://linkedin.com/showcase/dagster)
- 📺 [Subscribe to our YouTube channel](https://www.youtube.com/channel/UCfLnv9X8jyHTe6gJ4hVBo9Q)
- 📚 [Read our blog posts](https://dagster.io/blog)
- 👋 [Join us on Slack](https://dagster.io/slack)
- 🗃 [Browse Slack archives](https://discuss.dagster.io)
- ✏️ [Start a Github Discussion](https://github.com/dagster-io/dagster/discussions)

## Contributing

For details on contributing or running the project for development, check out our [contributing
guide](https://docs.dagster.io/community/contributing/).

## License

Dagster is [Apache 2.0 licensed](https://github.com/dagster-io/dagster/blob/master/LICENSE).
",2023-07-07 15:54:18+00:00
daliuge,daliuge,ICRAR/daliuge,The DALiuGE Execution Engine,,False,23,2023-07-07 08:24:44+00:00,2017-06-05 01:17:16+00:00,6,8,18,0,,,GNU Lesser General Public License v2.1,4630,v3.0.0,24,2023-05-18 12:47:48+00:00,2023-07-07 08:24:44+00:00,2023-07-05 11:03:34+00:00,"Data Activated 流 Graph Engine
==============================

.. image:: https://travis-ci.com/ICRAR/daliuge.svg?branch=master
    :target: https://travis-ci.com/github/ICRAR/daliuge

.. image:: https://coveralls.io/repos/github/ICRAR/daliuge/badge.svg?branch=master
    :target: https://coveralls.io/github/ICRAR/daliuge?branch=master

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/psf/black

.. image:: https://readthedocs.org/projects/daliuge/badge/?version=latest
    :target: https://daliuge.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/badge/License-LGPL_v2-blue.svg
    :target: https://www.gnu.org/licenses/lgpl-2.1
    :alt: License: LGPL v2.1

|daliuge|
is a workflow graph development, management and execution framework, specifically designed to support very large
scale processing graphs for the reduction of interferometric radio astronomy data sets.
|daliuge| has already been used for `processing large astronomical datasets 
<https://arxiv.org/abs/1702.07617>`_ in existing radio astronomy projects.
It originated from a prototyping activity as part of the `SDP Consortium
<https://www.skatelescope.org/sdp/>`_ called Data Flow Management System (DFMS). DFMS aimed to 
prototype the execution framework of the proposed SDP architecture.

For more information about the installation and usage of the system please refer to the `documentation <https://daliuge.readthedocs.io>`_


Development and maintenance of |daliuge| is currently hosted at ICRAR_
and is performed by the `DIA team <http://www.icrar.org/our-research/data-intensive-astronomy/>`_.

See the ``docs/`` directory for more information, or visit `our online
documentation <https://daliuge.readthedocs.io/>`_





.. |daliuge| replace:: DALiuGE
.. _ICRAR: http://www.icrar.org
.. [#f1] 流 (pronounced Liu) is the Chinese character for ""flow"".
",2023-07-07 15:54:22+00:00
dask,dask,dask/dask,Parallel computing with task scheduling,https://dask.org,False,11173,2023-07-07 08:23:53+00:00,2015-01-04 18:50:00+00:00,1633,214,421,0,,,"BSD 3-Clause ""New"" or ""Revised"" License",7811,list,270,2015-08-09 22:12:10+00:00,2023-07-07 11:54:00+00:00,2023-07-05 17:30:39+00:00,"Dask
====

|Build Status| |Coverage| |Doc Status| |Discourse| |Version Status| |NumFOCUS|

Dask is a flexible parallel computing library for analytics.  See
documentation_ for more information.


LICENSE
-------

New BSD. See `License File <https://github.com/dask/dask/blob/main/LICENSE.txt>`__.

.. _documentation: https://dask.org
.. |Build Status| image:: https://github.com/dask/dask/actions/workflows/tests.yml/badge.svg
   :target: https://github.com/dask/dask/actions/workflows/tests.yml
.. |Coverage| image:: https://codecov.io/gh/dask/dask/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/dask/dask/branch/main
   :alt: Coverage status
.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest
   :target: https://dask.org
   :alt: Documentation Status
.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group
   :alt: Discuss Dask-related things and ask for help
   :target: https://dask.discourse.group
.. |Version Status| image:: https://img.shields.io/pypi/v/dask.svg
   :target: https://pypi.python.org/pypi/dask/
.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
   :target: https://www.numfocus.org/
",2023-07-07 15:54:26+00:00
dataflows,dataflows,datahq/dataflows,"DataFlows is a simple, intuitive lightweight framework for building data processing flows in python.",https://dataflows.org,False,179,2023-06-22 04:59:39+00:00,2018-05-30 09:01:58+00:00,36,12,14,0,,,MIT License,255,v0.3.15,100,2022-07-31 15:40:14+00:00,2023-06-22 04:59:40+00:00,2023-04-17 06:24:28+00:00,"# ![logo](logo-s.png) DataFlows

[![Travis](https://img.shields.io/travis/datahq/dataflows/master.svg)](https://travis-ci.org/datahq/dataflows)
[![Coveralls](http://img.shields.io/coveralls/datahq/dataflows.svg?branch=master)](https://coveralls.io/r/datahq/dataflows?branch=master)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dataflows.svg)
[![Gitter chat](https://badges.gitter.im/dataflows-chat/Lobby.png)](https://gitter.im/dataflows-chat/Lobby)

DataFlows is a simple and intuitive way of building data processing flows.

- It's built for small-to-medium-data processing - data that fits on your hard drive, but is too big to load in Excel or as-is into Python, and not big enough to require spinning up a Hadoop cluster...
- It's built upon the foundation of the Frictionless Data project - which means that all data produced by these flows is easily reusable by others.
- It's a pattern not a heavy-weight framework: if you already have a bunch of download and extract scripts this will be a natural fit

Read more in the [Features section below](#features).

## QuickStart 

Install `dataflows` via `pip install.`

(If you are using minimal UNIX OS, run first `sudo apt install build-essential`)

Then use the command-line interface to bootstrap a basic processing script for any remote data file:

```bash

# Install from PyPi
$ pip install dataflows

# Inspect a remote CSV file
$ dataflows init https://raw.githubusercontent.com/datahq/dataflows/master/data/academy.csv
Writing processing code into academy_csv.py
Running academy_csv.py
academy:
#     Year           Ceremony  Award                                 Winner  Name                            Film
      (string)      (integer)  (string)                            (string)  (string)                        (string)
----  ----------  -----------  --------------------------------  ----------  ------------------------------  -------------------
1     1927/1928             1  Actor                                         Richard Barthelmess             The Noose
2     1927/1928             1  Actor                                      1  Emil Jannings                   The Last Command
3     1927/1928             1  Actress                                       Louise Dresser                  A Ship Comes In
4     1927/1928             1  Actress                                    1  Janet Gaynor                    7th Heaven
5     1927/1928             1  Actress                                       Gloria Swanson                  Sadie Thompson
6     1927/1928             1  Art Direction                                 Rochus Gliese                   Sunrise
7     1927/1928             1  Art Direction                              1  William Cameron Menzies         The Dove; Tempest
...

# dataflows create a local package of the data and a reusable processing script which you can tinker with
$ tree
.
├── academy_csv
│   ├── academy.csv
│   └── datapackage.json
└── academy_csv.py

1 directory, 3 files

# Resulting 'Data Package' is super easy to use in Python
[adam] ~/code/budgetkey-apps/budgetkey-app-main-page/tmp (master=) $ python
Python 3.6.1 (default, Mar 27 2017, 00:25:54)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from datapackage import Package
>>> pkg = Package('academy_csv/datapackage.json')
>>> it = pkg.resources[0].iter(keyed=True)
>>> next(it)
{'Year': '1927/1928', 'Ceremony': 1, 'Award': 'Actor', 'Winner': None, 'Name': 'Richard Barthelmess', 'Film': 'The Noose'}
>>> next(it)
{'Year': '1927/1928', 'Ceremony': 1, 'Award': 'Actor', 'Winner': '1', 'Name': 'Emil Jannings', 'Film': 'The Last Command'}

# You now run `academy_csv.py` to repeat the process
# And obviously modify it to add data modification steps
```

## Features

* Trivial to get started and easy to scale up
* Set up and run from command line in seconds ...
    * `dataflows init` => `flow.py`
    * `python flow.py`
* Validate input (and esp source) quickly (non-zero length, right structure, etc.)
* Supports caching data from source and even between steps
    * so that we can run and test quickly (retrieving is slow)
* Immediate test is run: and look at output ...
    * Log, debug, rerun
* Degrades to simple python
* Conventions over configuration
* Log exceptions and / or terminate
* The input to each stage is a Data Package or Data Resource (not a previous task)
	* Data package based and compatible
* Processors can be a function (or a class) processing row-by-row, resource-by-resource or a full package
* A pre-existing decent contrib library of Readers (Collectors) and Processors and Writers

## Learn more

Dive into the [Tutorial](TUTORIAL.md) to get a deeper glimpse into everything that `dataflows` can do.
Also review this list of [Built-in Processors](PROCESSORS.md), which also includes an API reference for each one of them.
",2023-07-07 15:54:30+00:00
dataform,dataform,dataform-co/dataform,"Dataform is a framework for managing SQL based data operations in BigQuery, Snowflake, and Redshift",https://docs.dataform.co,False,685,2023-07-06 19:42:11+00:00,2018-09-03 14:36:28+00:00,118,17,32,89,2.6.0,2023-06-09 08:36:39+00:00,MIT License,1489,v1.0.0-alpha.5,207,2019-06-26 13:29:14+00:00,2023-07-06 19:42:12+00:00,2023-06-28 12:32:17+00:00,"<p align=""center"">
  <img src=""https://github.com/dataform-co/dataform/blob/main/static/images/github_bg.png"">
</p>

# Intro

Dataform core is an open source meta-language to create SQL tables and workflows. Dataform core extends SQL by providing a dependency management system, automated data quality testing, and data documentation.

Using Dataform core, data teams can build scalable SQL data transformation pipelines following software engineering best practices, like version control and testing.

<br/>
<br/>

<p align=""center"">
  <img src=""https://assets.dataform.co/github-readme/single-source-of-truth%20(1).png"">
</p>

### Supported warehouses

- BigQuery
- Snowflake
- Redshift
- Postgres
- Azure SQL data warehouse

# Data modeling with Dataform

- [Quickstart](https://cloud.google.com/dataform/docs/quickstart)
- [Create tables and views](https://cloud.google.com/dataform/docs/tables)
- [Configure dependencies](https://cloud.google.com/dataform/docs/define-table#define_table_structure_and_dependencies)
- Write [data quality checks](https://cloud.google.com/dataform/docs/assertions)
- Enable [scripting](https://cloud.google.com/dataform/docs/develop-workflows-js) and code re-use with a JavaScript API

<div align=""center"">
  <img src=""https://assets.dataform.co/docs/introduction/simple_dag.png"" alt=""Dependency tree in a Dataform project"">
<i>Dependency tree in a Dataform project</i>
</div>

_Note: we have recently undergone a documentation transition from [docs.dataform.co](https://docs.dataform.co/) to [cloud.google.com/dataform/docs](https://cloud.google.com/dataform/docs). Content hosted on the old document site is published from the [`main_v1` branch](https://github.com/dataform-co/dataform/tree/main_v1)._

# Get started

## With the CLI

You can install the Dataform CLI tool using the following command line. Follow the [docs](https://cloud.google.com/dataform/docs/use-dataform-cli) to get started.

```
npm i -g @dataform/cli
```

<br/>

## In Google Cloud Platform

Dataform in Google Cloud Platform provides a fully managed experience to build scalable data transformations pipelines in **BigQuery** using SQL. It includes:

- a cloud development environment to develop data assets with SQL and Dataform core and version control code with GitHub, GitLab, and other Git providers.
- a fully managed, serverless orchestration environment for data pipelines, fully integrated in Google Cloud Platform.

You can learn more on [cloud.google.com/dataform](https://cloud.google.com/dataform)

## How it works

- Read the [docs here](https://cloud.google.com/dataform/docs/overview).

# Want to report a bug or request a feature?

- For Dataform core / open source requests, you can open an [issue](https://github.com/dataform-co/dataform/issues) in GitHub.
- For Dataform in Google Cloud Platform, you can file a bug [here](https://issuetracker.google.com/issues/new?component=1193995&template=1698201), and file feature requests [here](https://issuetracker.google.com/issues/new?component=1193995&template=1713836).

# Want to contribute?

Check out our [contributors guide](https://github.com/dataform-co/dataform/blob/main/contributing.md) to get started with setting up the repo.
",2023-07-07 15:54:34+00:00
datahub,datahub,datahub-project/datahub,The Metadata Platform for the Modern Data Stack,https://datahubproject.io,False,7979,2023-07-07 15:47:21+00:00,2015-11-18 05:47:40+00:00,2327,249,366,76,v0.10.4,2023-06-09 17:35:54+00:00,Apache License 2.0,8174,v0.10.4,77,2023-06-09 17:35:54+00:00,2023-07-07 15:47:21+00:00,2023-07-06 20:53:03+00:00,"<!--HOSTED_DOCS_ONLY
import useBaseUrl from '@docusaurus/useBaseUrl';

export const Logo = (props) => {
  return (
    <div style={{ display: ""flex"", justifyContent: ""center"", padding: ""20px"", height: ""190px"" }}>
      <img
        alt=""DataHub Logo""
        src={useBaseUrl(""/img/datahub-logo-color-mark.svg"")}
        {...props}
      />
    </div>
  );
};

<Logo />

<!--
HOSTED_DOCS_ONLY-->
<p align=""center"">
<img alt=""DataHub"" src=""docs/imgs/datahub-logo-color-mark.svg"" height=""150"" />
</p>
<!-- -->

# DataHub: The Metadata Platform for the Modern Data Stack
## Built with ❤️ by <img src=""https://datahubproject.io/img/acryl-logo-light-mark.png"" width=""25""/> [Acryl Data](https://acryldata.io) and <img src=""https://datahubproject.io/img/LI-In-Bug.png"" width=""25""/> [LinkedIn](https://engineering.linkedin.com)
[![Version](https://img.shields.io/github/v/release/datahub-project/datahub?include_prereleases)](https://github.com/datahub-project/datahub/releases/latest)
[![PyPI version](https://badge.fury.io/py/acryl-datahub.svg)](https://badge.fury.io/py/acryl-datahub)
[![build & test](https://github.com/datahub-project/datahub/workflows/build%20&%20test/badge.svg?branch=master&event=push)](https://github.com/datahub-project/datahub/actions?query=workflow%3A%22build+%26+test%22+branch%3Amaster+event%3Apush)
[![Docker Pulls](https://img.shields.io/docker/pulls/linkedin/datahub-gms.svg)](https://hub.docker.com/r/linkedin/datahub-gms)
[![Slack](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://slack.datahubproject.io)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/datahub-project/datahub/blob/master/docs/CONTRIBUTING.md)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/m/datahub-project/datahub)](https://github.com/datahub-project/datahub/pulls?q=is%3Apr)
[![License](https://img.shields.io/github/license/datahub-project/datahub)](https://github.com/datahub-project/datahub/blob/master/LICENSE)
[![YouTube](https://img.shields.io/youtube/channel/subscribers/UC3qFQC5IiwR5fvWEqi_tJ5w?style=social)](https://www.youtube.com/channel/UC3qFQC5IiwR5fvWEqi_tJ5w)
[![Medium](https://img.shields.io/badge/Medium-12100E?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/datahub-project)
[![Follow](https://img.shields.io/twitter/follow/datahubproject?label=Follow&style=social)](https://twitter.com/datahubproject)
### 🏠 Hosted DataHub Docs (Courtesy of Acryl Data): [datahubproject.io](https://datahubproject.io/docs)

---

[Quickstart](https://datahubproject.io/docs/quickstart) |
[Features](https://datahubproject.io/docs/features) |
[Roadmap](https://feature-requests.datahubproject.io/roadmap) |
[Adoption](#adoption) |
[Demo](https://demo.datahubproject.io/) |
[Town Hall](https://datahubproject.io/docs/townhalls)

---
> 📣 DataHub Town Hall is the 4th Thursday at 9am US PT of every month - [add it to your calendar!](https://rsvp.datahubproject.io/)
>
> - Town-hall Zoom link: [zoom.datahubproject.io](https://zoom.datahubproject.io)
> - [Meeting details](docs/townhalls.md) & [past recordings](docs/townhall-history.md)

> ✨ DataHub Community Highlights:
>
> - Read our Monthly Project Updates [here](https://blog.datahubproject.io/tagged/project-updates).
> - Bringing The Power Of The DataHub Real-Time Metadata Graph To Everyone At Acryl Data: [Data Engineering Podcast](https://www.dataengineeringpodcast.com/acryl-data-datahub-metadata-graph-episode-230/)
> - Check out our most-read blog post, [DataHub: Popular Metadata Architectures Explained](https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained) @ LinkedIn Engineering Blog.
> - Join us on [Slack](docs/slack.md)! Ask questions and keep up with the latest announcements.

## Introduction

DataHub is an open-source metadata platform for the modern data stack. Read about the architectures of different metadata systems and why DataHub excels [here](https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained). Also read our
[LinkedIn Engineering blog post](https://engineering.linkedin.com/blog/2019/data-hub), check out our [Strata presentation](https://speakerdeck.com/shirshanka/the-evolution-of-metadata-linkedins-journey-strata-nyc-2019) and watch our [Crunch Conference Talk](https://www.youtube.com/watch?v=OB-O0Y6OYDE). You should also visit [DataHub Architecture](docs/architecture/architecture.md) to get a better understanding of how DataHub is implemented.

## Features & Roadmap

Check out DataHub's [Features](docs/features.md) & [Roadmap](https://feature-requests.datahubproject.io/roadmap).

## Demo and Screenshots

There's a [hosted demo environment](https://demo.datahubproject.io/) courtesy of [Acryl Data](https://acryldata.io) where you can explore DataHub without installing it locally

## Quickstart

Please follow the [DataHub Quickstart Guide](https://datahubproject.io/docs/quickstart) to get a copy of DataHub up & running locally using [Docker](https://docker.com). As the guide assumes some basic knowledge of Docker, we'd recommend you to go through the ""Hello World"" example of [A Docker Tutorial for Beginners](https://docker-curriculum.com) if Docker is completely foreign to you.

## Development

If you're looking to build & modify datahub please take a look at our [Development Guide](https://datahubproject.io/docs/developers).

[![DataHub Demo GIF](docs/imgs/entity.png)](https://demo.datahubproject.io/)

## Source Code and Repositories

- [datahub-project/datahub](https://github.com/datahub-project/datahub): This repository contains the complete source code for DataHub's metadata model, metadata services, integration connectors and the web application.
- [acryldata/datahub-actions](https://github.com/acryldata/datahub-actions): DataHub Actions is a framework for responding to changes to your DataHub Metadata Graph in real time.
- [acryldata/datahub-helm](https://github.com/acryldata/datahub-helm): Repository of helm charts for deploying DataHub on a Kubernetes cluster
- [acryldata/meta-world](https://github.com/acryldata/meta-world): A repository to store recipes, custom sources, transformations and other things to make your DataHub experience magical

## Releases

See [Releases](https://github.com/datahub-project/datahub/releases) page for more details. We follow the [SemVer Specification](https://semver.org) when versioning the releases and adopt the [Keep a Changelog convention](https://keepachangelog.com/) for the changelog format.

## Contributing

We welcome contributions from the community. Please refer to our [Contributing Guidelines](docs/CONTRIBUTING.md) for more details. We also have a [contrib](contrib) directory for incubating experimental features.

## Community

Join our [Slack workspace](https://slack.datahubproject.io) for discussions and important announcements. You can also find out more about our upcoming [town hall meetings](docs/townhalls.md) and view past recordings.

## Adoption

Here are the companies that have officially adopted DataHub. Please feel free to add yours to the list if we missed it.

- [ABLY](https://ably.team/)
- [Adevinta](https://www.adevinta.com/)
- [Banksalad](https://www.banksalad.com)
- [Cabify](https://cabify.tech/)
- [ClassDojo](https://www.classdojo.com/)
- [Coursera](https://www.coursera.org/)
- [DefinedCrowd](http://www.definedcrowd.com)
- [DFDS](https://www.dfds.com/)
- [Digital Turbine](https://www.digitalturbine.com/)
- [Expedia Group](http://expedia.com)
- [Experius](https://www.experius.nl)
- [Geotab](https://www.geotab.com)
- [Grofers](https://grofers.com)
- [Haibo Technology](https://www.botech.com.cn)
- [hipages](https://hipages.com.au/)
- [inovex](https://www.inovex.de/)
- [IOMED](https://iomed.health)
- [Klarna](https://www.klarna.com)
- [LinkedIn](http://linkedin.com)
- [Moloco](https://www.moloco.com/en)
- [N26](https://n26brasil.com/)
- [Optum](https://www.optum.com/)
- [Peloton](https://www.onepeloton.com)
- [PITS Global Data Recovery Services](https://www.pitsdatarecovery.net/)
- [Razer](https://www.razer.com)
- [Saxo Bank](https://www.home.saxo)
- [Showroomprive](https://www.showroomprive.com/)
- [SpotHero](https://spothero.com)
- [Stash](https://www.stash.com)
- [Shanghai HuaRui Bank](https://www.shrbank.com)
- [ThoughtWorks](https://www.thoughtworks.com)
- [TypeForm](http://typeform.com)
- [Udemy](https://www.udemy.com/)
- [Uphold](https://uphold.com)
- [Viasat](https://viasat.com)
- [Wikimedia](https://www.wikimedia.org)
- [Wolt](https://wolt.com)
- [Zynga](https://www.zynga.com)


## Select Articles & Talks

- [DataHub Blog](https://blog.datahubproject.io/)
- [DataHub YouTube Channel](https://www.youtube.com/channel/UC3qFQC5IiwR5fvWEqi_tJ5w)
- [Optum: Data Mesh via DataHub](https://optum.github.io/blog/2022/03/23/data-mesh-via-datahub/)
- [Saxo Bank: Enabling Data Discovery in Data Mesh](https://medium.com/datahub-project/enabling-data-discovery-in-a-data-mesh-the-saxo-journey-451b06969c8f)
- [Bringing The Power Of The DataHub Real-Time Metadata Graph To Everyone At Acryl Data](https://www.dataengineeringpodcast.com/acryl-data-datahub-metadata-graph-episode-230/)
- [DataHub: Popular Metadata Architectures Explained](https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained)
- [Driving DataOps Culture with LinkedIn DataHub](https://www.youtube.com/watch?v=ccsIKK9nVxk) @ [DataOps Unleashed 2021](https://dataopsunleashed.com/#shirshanka-session)
- [The evolution of metadata: LinkedIn’s story](https://speakerdeck.com/shirshanka/the-evolution-of-metadata-linkedins-journey-strata-nyc-2019) @ [Strata Data Conference 2019](https://conferences.oreilly.com/strata/strata-ny-2019.html)
- [Journey of metadata at LinkedIn](https://www.youtube.com/watch?v=OB-O0Y6OYDE) @ [Crunch Data Conference 2019](https://crunchconf.com/2019)
- [DataHub Journey with Expedia Group](https://www.youtube.com/watch?v=ajcRdB22s5o)
- [Data Discoverability at SpotHero](https://www.slideshare.net/MaggieHays/data-discoverability-at-spothero)
- [Data Catalogue — Knowing your data](https://medium.com/albert-franzi/data-catalogue-knowing-your-data-15f7d0724900)
- [DataHub: A Generalized Metadata Search & Discovery Tool](https://engineering.linkedin.com/blog/2019/data-hub)
- [Open sourcing DataHub: LinkedIn’s metadata search and discovery platform](https://engineering.linkedin.com/blog/2020/open-sourcing-datahub--linkedins-metadata-search-and-discovery-p)
- [Emerging Architectures for Modern Data Infrastructure](https://future.com/emerging-architectures-for-modern-data-infrastructure-2020/)

See the full list [here](docs/links.md).

## License

[Apache License 2.0](./LICENSE).
",2023-07-07 15:54:39+00:00
datajoint,datajoint-python,datajoint/datajoint-python,Relational data pipelines for the science lab ,https://datajoint.com/docs,False,149,2023-06-30 08:34:24+00:00,2012-09-19 03:50:15+00:00,77,18,33,36,0.14.1,2023-06-07 21:35:40+00:00,GNU Lesser General Public License v2.1,3786,v0.13.7,73,2022-07-12 21:54:45+00:00,2023-07-06 22:39:58+00:00,2023-06-27 22:21:11+00:00,"[![DOI](https://zenodo.org/badge/16774/datajoint/datajoint-python.svg)](https://zenodo.org/badge/latestdoi/16774/datajoint/datajoint-python)
[![Coverage Status](https://coveralls.io/repos/datajoint/datajoint-python/badge.svg?branch=master&service=github)](https://coveralls.io/github/datajoint/datajoint-python?branch=master)
[![PyPI version](https://badge.fury.io/py/datajoint.svg)](http://badge.fury.io/py/datajoint)
[![Slack](https://img.shields.io/badge/slack-chat-green.svg)](https://datajoint.slack.com/)

# Welcome to DataJoint for Python!

DataJoint for Python is a framework for scientific workflow management based on relational principles. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, computing, and querying data.

DataJoint was initially developed in 2009 by Dimitri Yatsenko in Andreas Tolias' Lab at Baylor College of Medicine for the distributed processing and management of large volumes of data streaming from regular experiments. Starting in 2011, DataJoint has been available as an open-source project adopted by other labs and improved through contributions from several developers.
Presently, the primary developer of DataJoint open-source software is the company DataJoint (https://datajoint.com).

## Data Pipeline Example

![pipeline](https://raw.githubusercontent.com/datajoint/datajoint-python/master/images/pipeline.png)

[Yatsenko et al., bioRxiv 2021](https://doi.org/10.1101/2021.03.30.437358)

## Getting Started

- Install from PyPI

     ```bash
     pip install datajoint
     ```

- [Documentation & Tutorials](https://datajoint.com/docs/core/datajoint-python/)

- [Interactive Tutorials](https://github.com/datajoint/datajoint-tutorials) on GitHub Codespaces

- [DataJoint Elements](https://datajoint.com/docs/elements/) - Catalog of example pipelines for neuroscience experiments

- Contribute
  - [Development Environment](https://datajoint.com/docs/core/datajoint-python/latest/develop/)

  - [Guidelines](https://datajoint.com/docs/community/contribute/)

- Legacy Resources (To be replaced by above)
  - [Documentation](https://docs.datajoint.org)

  - [Tutorials](https://tutorials.datajoint.org)
",2023-07-07 15:54:44+00:00
datalad,datalad,datalad/datalad,"Keep code, data, containers under control with git  and git-annex",http://datalad.org,False,432,2023-07-05 08:48:58+00:00,2013-11-01 19:40:08+00:00,108,26,47,97,0.19.2,2023-07-03 01:45:07+00:00,Other,16962,_wrong_fix_2301,217,2018-04-04 14:35:55+00:00,2023-07-06 19:21:04+00:00,2023-07-03 01:45:07+00:00,"     ____            _             _                   _ 
    |  _ \    __ _  | |_    __ _  | |       __ _    __| |
    | | | |  / _` | | __|  / _` | | |      / _` |  / _` |
    | |_| | | (_| | | |_  | (_| | | |___  | (_| | | (_| |
    |____/   \__,_|  \__|  \__,_| |_____|  \__,_|  \__,_|
                                                  Read me

[![DOI](https://joss.theoj.org/papers/10.21105/joss.03262/status.svg)](https://doi.org/10.21105/joss.03262)
[![Travis tests status](https://app.travis-ci.com/datalad/datalad.svg?branch=master)](https://app.travis-ci.com/datalad/datalad)
[![Build status](https://ci.appveyor.com/api/projects/status/github/datalad/datalad?branch=master&svg=true)](https://ci.appveyor.com/project/mih/datalad/branch/master)
[![Extensions](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml)
[![Linters](https://github.com/datalad/datalad/actions/workflows/lint.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/lint.yml)
[![codecov.io](https://codecov.io/github/datalad/datalad/coverage.svg?branch=master)](https://codecov.io/github/datalad/datalad?branch=master)
[![Documentation](https://readthedocs.org/projects/datalad/badge/?version=latest)](http://datalad.rtfd.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub release](https://img.shields.io/github/release/datalad/datalad.svg)](https://GitHub.com/datalad/datalad/releases/)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/datalad)](https://pypi.org/project/datalad/)
[![Testimonials 4](https://img.shields.io/badge/testimonials-4-brightgreen.svg)](https://github.com/datalad/datalad/wiki/Testimonials)
[![https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg](https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg)](https://singularity-hub.org/collections/667)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://github.com/datalad/datalad/blob/master/CODE_OF_CONDUCT.md)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.808846.svg)](https://doi.org/10.5281/zenodo.808846)
<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![All Contributors](https://img.shields.io/badge/all_contributors-49-orange.svg?style=flat-square)](#contributors-)
<!-- ALL-CONTRIBUTORS-BADGE:END -->

## Distribution

[![Anaconda](https://anaconda.org/conda-forge/datalad/badges/version.svg)](https://anaconda.org/conda-forge/datalad)
[![Arch (AUR)](https://repology.org/badge/version-for-repo/aur/datalad.svg?header=Arch%20%28%41%55%52%29)](https://repology.org/project/datalad/versions)
[![Debian Stable](https://badges.debian.net/badges/debian/stable/datalad/version.svg)](https://packages.debian.org/stable/datalad)
[![Debian Unstable](https://badges.debian.net/badges/debian/unstable/datalad/version.svg)](https://packages.debian.org/unstable/datalad)
[![Fedora Rawhide package](https://repology.org/badge/version-for-repo/fedora_rawhide/datalad.svg?header=Fedora%20%28rawhide%29)](https://repology.org/project/datalad/versions)
[![Gentoo (::science)](https://repology.org/badge/version-for-repo/gentoo_ovl_science/datalad.svg?header=Gentoo%20%28%3A%3Ascience%29)](https://repology.org/project/datalad/versions)
[![PyPI package](https://repology.org/badge/version-for-repo/pypi/datalad.svg?header=PyPI)](https://repology.org/project/datalad/versions)

# 10000-ft. overview

DataLad makes data management and data distribution more accessible.
To do that, it stands on the shoulders of [Git] and [Git-annex] to deliver a
decentralized system for data exchange. This includes automated ingestion of
data from online portals and exposing it in readily usable form as Git(-annex)
repositories, so-called datasets. The actual data storage and permission
management, however, remains with the original data providers.

The full documentation is available at http://docs.datalad.org and
http://handbook.datalad.org provides a hands-on crash-course on DataLad.

# Extensions

A number of extensions are available that provide additional functionality for
DataLad. Extensions are separate packages that are to be installed in addition
to DataLad. In order to install DataLad customized for a particular domain, one
can simply install an extension directly, and DataLad itself will be
automatically installed with it. An [annotated list of
extensions](http://handbook.datalad.org/extension_pkgs.html) is available in
the [DataLad handbook](http://handbook.datalad.org).


# Support

The documentation for this project is found here:
http://docs.datalad.org

All bugs, concerns, and enhancement requests for this software can be submitted here:
https://github.com/datalad/datalad/issues

If you have a problem or would like to ask a question about how to use DataLad,
please [submit a question to
NeuroStars.org](https://neurostars.org/new-topic?body=-%20Please%20describe%20the%20problem.%0A-%20What%20steps%20will%20reproduce%20the%20problem%3F%0A-%20What%20version%20of%20DataLad%20are%20you%20using%20%28run%20%60datalad%20--version%60%29%3F%20On%20what%20operating%20system%20%28consider%20running%20%60datalad%20plugin%20wtf%60%29%3F%0A-%20Please%20provide%20any%20additional%20information%20below.%0A-%20Have%20you%20had%20any%20luck%20using%20DataLad%20before%3F%20%28Sometimes%20we%20get%20tired%20of%20reading%20bug%20reports%20all%20day%20and%20a%20lil'%20positive%20end%20note%20does%20wonders%29&tags=datalad)
with a `datalad` tag.  NeuroStars.org is a platform similar to StackOverflow
but dedicated to neuroinformatics.

All previous DataLad questions are available here:
http://neurostars.org/tags/datalad/


# Installation

## Debian-based systems

On Debian-based systems, we recommend enabling [NeuroDebian], via which we
provide recent releases of DataLad. Once enabled, just do:

    apt-get install datalad

## Gentoo-based systems

On Gentoo-based systems (i.e. all systems whose package manager can parse ebuilds as per the [Package Manager Specification]), we recommend [enabling the ::science overlay], via which we
provide recent releases of DataLad. Once enabled, just run:

    emerge datalad

## Other Linux'es via conda

    conda install -c conda-forge datalad

will install the most recently released version, and release candidates are
available via

    conda install -c conda-forge/label/rc datalad

## Other Linux'es, macOS via pip

Before you install this package, please make sure that you [install a recent
version of git-annex](https://git-annex.branchable.com/install).  Afterwards,
install the latest version of `datalad` from
[PyPI](https://pypi.org/project/datalad). It is recommended to use
a dedicated [virtualenv](https://virtualenv.pypa.io):

    # Create and enter a new virtual environment (optional)
    virtualenv --python=python3 ~/env/datalad
    . ~/env/datalad/bin/activate

    # Install from PyPI
    pip install datalad

By default, installation via pip installs the core functionality of DataLad,
allowing for managing datasets etc.  Additional installation schemes
are available, so you can request enhanced installation via
`pip install datalad[SCHEME]`, where `SCHEME` could be:

- `tests`
     to also install dependencies used by DataLad's battery of unit tests
- `full`
     to install all dependencies.

More details on installation and initial configuration can be found in the
[DataLad Handbook: Installation].

# License

MIT/Expat


# Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) if you are interested in internals or
contributing to the project. 

## Acknowledgements

DataLad development is supported by a US-German collaboration in computational
neuroscience (CRCNS) project ""DataGit: converging catalogues, warehouses, and
deployment logistics into a federated 'data distribution'"" (Halchenko/Hanke),
co-funded by the US National Science Foundation (NSF 1429999) and the German
Federal Ministry of Education and Research (BMBF 01GQ1411). Additional support
is provided by the German federal state of Saxony-Anhalt and the European
Regional Development Fund (ERDF), Project: Center for Behavioral Brain
Sciences, Imaging Platform.  This work is further facilitated by the ReproNim
project (NIH 1P41EB019936-01A1). Mac mini instance for development is provided
by [MacStadium](https://www.macstadium.com/).

### Contributors ✨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/glalteva""><img src=""https://avatars2.githubusercontent.com/u/14296143?v=4?s=100"" width=""100px;"" alt=""glalteva""/><br /><sub><b>glalteva</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=glalteva"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/adswa""><img src=""https://avatars1.githubusercontent.com/u/29738718?v=4?s=100"" width=""100px;"" alt=""adswa""/><br /><sub><b>adswa</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=adswa"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/chrhaeusler""><img src=""https://avatars0.githubusercontent.com/u/8115807?v=4?s=100"" width=""100px;"" alt=""chrhaeusler""/><br /><sub><b>chrhaeusler</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=chrhaeusler"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/soichih""><img src=""https://avatars3.githubusercontent.com/u/923896?v=4?s=100"" width=""100px;"" alt=""soichih""/><br /><sub><b>soichih</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=soichih"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mvdoc""><img src=""https://avatars1.githubusercontent.com/u/6150554?v=4?s=100"" width=""100px;"" alt=""mvdoc""/><br /><sub><b>mvdoc</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=mvdoc"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mih""><img src=""https://avatars1.githubusercontent.com/u/136479?v=4?s=100"" width=""100px;"" alt=""mih""/><br /><sub><b>mih</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=mih"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/yarikoptic""><img src=""https://avatars3.githubusercontent.com/u/39889?v=4?s=100"" width=""100px;"" alt=""yarikoptic""/><br /><sub><b>yarikoptic</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=yarikoptic"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/loj""><img src=""https://avatars2.githubusercontent.com/u/15157717?v=4?s=100"" width=""100px;"" alt=""loj""/><br /><sub><b>loj</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=loj"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/feilong""><img src=""https://avatars2.githubusercontent.com/u/2242261?v=4?s=100"" width=""100px;"" alt=""feilong""/><br /><sub><b>feilong</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=feilong"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jhpoelen""><img src=""https://avatars2.githubusercontent.com/u/1084872?v=4?s=100"" width=""100px;"" alt=""jhpoelen""/><br /><sub><b>jhpoelen</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=jhpoelen"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/andycon""><img src=""https://avatars1.githubusercontent.com/u/3965889?v=4?s=100"" width=""100px;"" alt=""andycon""/><br /><sub><b>andycon</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=andycon"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/nicholsn""><img src=""https://avatars3.githubusercontent.com/u/463344?v=4?s=100"" width=""100px;"" alt=""nicholsn""/><br /><sub><b>nicholsn</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=nicholsn"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/adelavega""><img src=""https://avatars0.githubusercontent.com/u/2774448?v=4?s=100"" width=""100px;"" alt=""adelavega""/><br /><sub><b>adelavega</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=adelavega"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/kskyten""><img src=""https://avatars0.githubusercontent.com/u/4163878?v=4?s=100"" width=""100px;"" alt=""kskyten""/><br /><sub><b>kskyten</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=kskyten"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/TheChymera""><img src=""https://avatars2.githubusercontent.com/u/950524?v=4?s=100"" width=""100px;"" alt=""TheChymera""/><br /><sub><b>TheChymera</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=TheChymera"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/effigies""><img src=""https://avatars0.githubusercontent.com/u/83442?v=4?s=100"" width=""100px;"" alt=""effigies""/><br /><sub><b>effigies</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=effigies"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jgors""><img src=""https://avatars1.githubusercontent.com/u/386585?v=4?s=100"" width=""100px;"" alt=""jgors""/><br /><sub><b>jgors</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=jgors"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/debanjum""><img src=""https://avatars1.githubusercontent.com/u/6413477?v=4?s=100"" width=""100px;"" alt=""debanjum""/><br /><sub><b>debanjum</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=debanjum"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/nellh""><img src=""https://avatars3.githubusercontent.com/u/11369795?v=4?s=100"" width=""100px;"" alt=""nellh""/><br /><sub><b>nellh</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=nellh"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/emdupre""><img src=""https://avatars3.githubusercontent.com/u/15017191?v=4?s=100"" width=""100px;"" alt=""emdupre""/><br /><sub><b>emdupre</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=emdupre"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/aqw""><img src=""https://avatars0.githubusercontent.com/u/765557?v=4?s=100"" width=""100px;"" alt=""aqw""/><br /><sub><b>aqw</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=aqw"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/vsoch""><img src=""https://avatars0.githubusercontent.com/u/814322?v=4?s=100"" width=""100px;"" alt=""vsoch""/><br /><sub><b>vsoch</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=vsoch"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/kyleam""><img src=""https://avatars2.githubusercontent.com/u/1297788?v=4?s=100"" width=""100px;"" alt=""kyleam""/><br /><sub><b>kyleam</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=kyleam"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/driusan""><img src=""https://avatars0.githubusercontent.com/u/498329?v=4?s=100"" width=""100px;"" alt=""driusan""/><br /><sub><b>driusan</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=driusan"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/overlake333""><img src=""https://avatars1.githubusercontent.com/u/28018084?v=4?s=100"" width=""100px;"" alt=""overlake333""/><br /><sub><b>overlake333</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=overlake333"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/akeshavan""><img src=""https://avatars0.githubusercontent.com/u/972008?v=4?s=100"" width=""100px;"" alt=""akeshavan""/><br /><sub><b>akeshavan</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=akeshavan"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jwodder""><img src=""https://avatars1.githubusercontent.com/u/98207?v=4?s=100"" width=""100px;"" alt=""jwodder""/><br /><sub><b>jwodder</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=jwodder"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/bpoldrack""><img src=""https://avatars2.githubusercontent.com/u/10498301?v=4?s=100"" width=""100px;"" alt=""bpoldrack""/><br /><sub><b>bpoldrack</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=bpoldrack"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/yetanothertestuser""><img src=""https://avatars0.githubusercontent.com/u/19335420?v=4?s=100"" width=""100px;"" alt=""yetanothertestuser""/><br /><sub><b>yetanothertestuser</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=yetanothertestuser"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/christian-monch""><img src=""https://avatars.githubusercontent.com/u/17925232?v=4?s=100"" width=""100px;"" alt=""Christian Mönch""/><br /><sub><b>Christian Mönch</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=christian-monch"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mattcieslak""><img src=""https://avatars.githubusercontent.com/u/170026?v=4?s=100"" width=""100px;"" alt=""Matt Cieslak""/><br /><sub><b>Matt Cieslak</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=mattcieslak"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mikapfl""><img src=""https://avatars.githubusercontent.com/u/7226087?v=4?s=100"" width=""100px;"" alt=""Mika Pflüger""/><br /><sub><b>Mika Pflüger</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=mikapfl"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://me.ypid.de/""><img src=""https://avatars.githubusercontent.com/u/1301158?v=4?s=100"" width=""100px;"" alt=""Robin Schneider""/><br /><sub><b>Robin Schneider</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=ypid"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://orcid.org/0000-0003-4652-3758""><img src=""https://avatars.githubusercontent.com/u/7570456?v=4?s=100"" width=""100px;"" alt=""Sin Kim""/><br /><sub><b>Sin Kim</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=kimsin98"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/DisasterMo""><img src=""https://avatars.githubusercontent.com/u/49207524?v=4?s=100"" width=""100px;"" alt=""Michael Burgardt""/><br /><sub><b>Michael Burgardt</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=DisasterMo"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://remi-gau.github.io/""><img src=""https://avatars.githubusercontent.com/u/6961185?v=4?s=100"" width=""100px;"" alt=""Remi Gau""/><br /><sub><b>Remi Gau</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=Remi-Gau"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mslw""><img src=""https://avatars.githubusercontent.com/u/11985212?v=4?s=100"" width=""100px;"" alt=""Michał Szczepanik""/><br /><sub><b>Michał Szczepanik</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=mslw"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/bpinsard""><img src=""https://avatars.githubusercontent.com/u/1155388?v=4?s=100"" width=""100px;"" alt=""Basile""/><br /><sub><b>Basile</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=bpinsard"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/taylols""><img src=""https://avatars.githubusercontent.com/u/28018084?v=4?s=100"" width=""100px;"" alt=""Taylor Olson""/><br /><sub><b>Taylor Olson</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=taylols"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://jdkent.github.io/""><img src=""https://avatars.githubusercontent.com/u/12564882?v=4?s=100"" width=""100px;"" alt=""James Kent""/><br /><sub><b>James Kent</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=jdkent"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/xgui3783""><img src=""https://avatars.githubusercontent.com/u/19381783?v=4?s=100"" width=""100px;"" alt=""xgui3783""/><br /><sub><b>xgui3783</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=xgui3783"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/tstoeter""><img src=""https://avatars.githubusercontent.com/u/4901704?v=4?s=100"" width=""100px;"" alt=""tstoeter""/><br /><sub><b>tstoeter</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=tstoeter"" title=""Code"">💻</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://jsheunis.github.io/""><img src=""https://avatars.githubusercontent.com/u/10141237?v=4?s=100"" width=""100px;"" alt=""Stephan Heunis""/><br /><sub><b>Stephan Heunis</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=jsheunis"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.mmmccormick.com""><img src=""https://avatars.githubusercontent.com/u/25432?v=4?s=100"" width=""100px;"" alt=""Matt McCormick""/><br /><sub><b>Matt McCormick</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=thewtex"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/vickychenglau""><img src=""https://avatars.githubusercontent.com/u/22065437?v=4?s=100"" width=""100px;"" alt=""Vicky C Lau""/><br /><sub><b>Vicky C Lau</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=vickychenglau"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://chris-lamb.co.uk""><img src=""https://avatars.githubusercontent.com/u/133209?v=4?s=100"" width=""100px;"" alt=""Chris Lamb""/><br /><sub><b>Chris Lamb</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=lamby"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/asmacdo""><img src=""https://avatars.githubusercontent.com/u/1028657?v=4?s=100"" width=""100px;"" alt=""Austin Macdonald""/><br /><sub><b>Austin Macdonald</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=asmacdo"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://nobodyinperson.de""><img src=""https://avatars.githubusercontent.com/u/19148271?v=4?s=100"" width=""100px;"" alt=""Yann Büchau""/><br /><sub><b>Yann Büchau</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=nobodyinperson"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/matrss""><img src=""https://avatars.githubusercontent.com/u/9308656?v=4?s=100"" width=""100px;"" alt=""Matthias Riße""/><br /><sub><b>Matthias Riße</b></sub></a><br /><a href=""https://github.com/datalad/datalad/commits?author=matrss"" title=""Code"">💻</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

[![macstadium](https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png)](https://www.macstadium.com/)

[Git]: https://git-scm.com
[Git-annex]: http://git-annex.branchable.com
[setup.py]: https://github.com/datalad/datalad/blob/master/setup.py
[NeuroDebian]: http://neuro.debian.net
[Package Manager Specification]: https://projects.gentoo.org/pms/latest/pms.html
[enabling the ::science overlay]: https://github.com/gentoo/sci#manual-install-

[DataLad Handbook: Installation]: http://handbook.datalad.org/en/latest/intro/installation.html
",2023-07-07 15:54:49+00:00
datashim,datashim,datashim-io/datashim,A kubernetes based framework for hassle free handling of datasets,http://datashim-io.github.io/datashim,False,360,2023-07-05 09:17:58+00:00,2019-09-26 18:06:14+00:00,55,8,21,3,v0.3.0,2022-07-18 14:48:46+00:00,Apache License 2.0,368,v0.3.0,3,2022-07-18 14:48:46+00:00,2023-07-07 08:57:02+00:00,2023-07-04 22:07:47+00:00,"[![Go Report Card](https://goreportcard.com/report/github.com/datashim-io/datashim)](https://goreportcard.com/report/github.com/datashim-io/datashim)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4821/badge)](https://bestpractices.coreinfrastructure.org/projects/4821)
# Datashim
<img src=""./docs/pictures/lfaidata-project-badge-incubation-color.png"" alt=""drawing"" width=""200""/>

>Our Framework introduces the **Dataset** CRD which is a pointer to existing S3 and NFS data sources. It includes the
>necessary logic to map these Datasets into Persistent Volume Claims and ConfigMaps which users can reference in their
>pods, letting them focus on the workload development and not on configuring/mounting/tuning the data access. Thanks to
>[Container Storage Interface](https://kubernetes-csi.github.io/docs/) it is extensible to support additional data sources in the future.

![DLF](./docs/pictures/dlf.png)

A Kubernetes Framework to provide easy access to S3 and NFS **Datasets** within pods. Orchestrates the provisioning of
**Persistent Volume Claims** and **ConfigMaps** needed for each **Dataset**. Find more details in our [FAQ](https://datashim-io.github.io/datashim/FAQ/)

## Quickstart

In order to quickly deploy DLF, based on your environment execute **one** of the following commands:

- **Kubernetes/Minikube**
```bash
kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf.yaml
```
- **Kubernetes on IBM Cloud**
```bash
kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-ibm-k8s.yaml
```
- **Openshift**
```bash
kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-oc.yaml
```
- **Openshift on IBM Cloud**
```bash
kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-ibm-oc.yaml
```

Wait for all the pods to be ready :)
```bash
kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=dlf -n dlf
```

As an **optional** step, label the namespace(or namespaces) you want in order have the pods labelling functionality (see below).
```bash
kubectl label namespace default monitor-pods-datasets=enabled
```

_In case don't have an existing S3 Bucket follow our wiki to [deploy an Object Store](https://github.com/datashim-io/datashim/wiki/Deployment-and-Usage-of-S3-Object-Stores)
and populate it with data._

We will create now a Dataset named `example-dataset` pointing to your S3 bucket.
```yaml
cat <<EOF | kubectl apply -f -
apiVersion: com.ie.ibm.hpsys/v1alpha1
kind: Dataset
metadata:
  name: example-dataset
spec:
  local:
    type: ""COS""
    accessKeyID: ""{AWS_ACCESS_KEY_ID}""
    secretAccessKey: ""{AWS_SECRET_ACCESS_KEY}""
    endpoint: ""{S3_SERVICE_URL}""
    bucket: ""{BUCKET_NAME}""
    readonly: ""true"" #OPTIONAL, default is false  
    region: """" #OPTIONAL
EOF
```

If everything worked okay, you should see a PVC and a ConfigMap named `example-dataset` which you can mount in your pods.
As an easier way to use the Dataset in your pod, you can instead label the pod as follows:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    dataset.0.id: ""example-dataset""
    dataset.0.useas: ""mount""
spec:
  containers:
    - name: nginx
      image: nginx
```

As a convention the Dataset will be mounted in `/mnt/datasets/example-dataset`. If instead you wish to pass the connection
details as environment variables, change the `useas` line to `dataset.0.useas: ""configmap""`

**Note:** We recommend using secrets to pass your S3/Object Storage Service credentials to Datashim, as shown in [this example](./examples/templates/example-dataset-s3-provision.yaml).

Feel free to explore our [other examples](./examples)

## Questions

The wiki and [Frequently Asked Questions](https://datashim-io.github.io/datashim/FAQ) documents are a bit out of date. We recommend browsing [the issues](https://github.com/datashim-io/datashim/issues?q=is%3Aissue+label%3Aquestion) for previously answered questions. Please open an issue if you are not able to find the answers to your questions, or if you have discovered a bug. 

## Contributing

We welcome all contributions to Datashim. Please read [this document](./docs/GitWorkflow.md) for setting up a Git workflow for contributing to Datashim. This project uses [DCO (Developer Certificate of Origin)](https://github.com/apps/dco) to certify code ownership and contribution rights. 

If you use VSCode, then we have [recommendations for setting it up for development](./docs/GolangVSCodeGit.md). 

If you have an idea for a feature request, please open an issue. Let us know in the issue description the problem or the pain point, and how the proposed feature would help solve it. If you are looking to contribute but you don't know where to start, we recommend looking at the open issues first. Thanks!

",2023-07-07 15:54:54+00:00
dataview,DATAVIEW,shiyonglu/DATAVIEW,DATAVIEW is a big data workflow management system. It uses Dropbox as the data cloud and Amazon EC2 as the compute cloud. Current research focuses on the security and privacy aspects of DATAVIEW as well as performance and cost optimization for running workflows in clouds.,,False,11,2022-02-12 01:48:57+00:00,2019-03-21 17:24:54+00:00,5,3,3,7,3.1,2022-06-11 14:16:28+00:00,,157,3.1,7,2022-06-11 14:16:28+00:00,,2022-06-11 14:16:28+00:00,"# DATAVIEW
DATAVIEW (www.dataview.org) is a big data workflow management system. It uses Dropbox as the data cloud and Amazon EC2 as the compute cloud. It also provides a workflow_LocalExecutor for users to run their local machine off the cloud. Current research focuses on the 1) performance and cost optimization for running workflows in clouds and 2) infrastructual-level support on GPU-enabled deep learning workflows. For deep learning workflows, it currently supports GPU infrastructures including 1) the Local NVIDIA GPU of a PC, 2) GPU Xavier and Nano SoMs (System-on-Module) and 3) the Heterogeneous GPU Cluster.



DATAVIEW supports two programing interfaces to develop and run workflows:

1. <b>JAVA API:</b> A programmer can develop various workflow tasks and workflows based on the DATAVIEW libraries. /DATAVIEW/src/<b>test.java</b> shows the six steps to create a customized workflow and execute it in Amazon EC2 or Local PC environment.
* The external dependecies libraries must be added to the Eclipse project from /DATAVIEW/WebContent/WEB-INF/lib
* To utilize the Amazon EC2, the accessKey and secretKey should be updated in config.properties under /DATAVIEW/WebContent/workflowLibDir/
* After finishing the workflow, please terminate all the EC2 instances from your AWS account manually (in the case of running worklfow in Amazon EC2).

2. <b>Visual Programming</b>: DATAVIEW is deployed as a Web site in Tomcat and a user can drag and drop tasks and link them into a workflow in a visual workflow design and execution environment called <b>Webbench</b>. 

* A dropbox accout is necessary to store all the input data,  workflow tasks, the final output files produced by the workflow execution. The user needs to create <b>Three</b> default folders 
Dropbox/DATAVIEW/Tasks,  which stores the task file (class file or jar file); Dropbox/DATAVIEW/Workflows, which stores the mxgraph file for the generated workflow; Dropbox/DATAVIEW-INPUT, which stores the input files for a workflow. 
Four relational algebra tasks (jar files) and input files are already stored under the DATAVIEW/WebContent/workflowTaskDir folder. 
* A local account needs to be registered to show a visualized workflow.
* A dropbox token should be provided in the main interface when you login in, which can be generated based on this tutorial:https://blogs.dropbox.com/developers/2014/05/generate-an-access-token-for-your-own-account/

<h2>Download and configure DATAVIEW as JAVA API</h2>
Check out tutorial: https://youtu.be/xJikeWptYSw or follow the instructions below: 

<OL>
    <li>Download the DATAVIEW package from https://github.com/shiyonglu/DATAVIEW by clicking the ""Clone or Download"" button.
    </li> 
     <li> Unzip the DATAVIEW-master.zip file and import the DATAVIEW project into Eclipse as an ""Existing Projects into Workspace"" by selecting ""Projects from Folder or Archive"".
    </li> 
    <li> The external dependecies libraries must be added to the Eclipse project from /DATAVIEW/WebContent/WEB-INF/lib </li>
    <li>/DATAVIEW/src/test.java shows the six steps to create a new workflow and execute it with local executor.</li>
   <!--
   <li> To use the EC2-Cloud, create an Access key ID and a Secret access key in Amazon EC2 following the tutotial: https://youtu.be/9741e4CubMQ </li>
    <li>Replace the accessKey(Access key ID) and the secretKey(Secret access key) in config.properties by the Access key ID and Secret access key created in the previous step. File config.properties is under /DATAVIEW/WebContent/workflowLibDir/. </li>
    <li>/DATAVIEW/src/test.java shows the six steps to create a new workflow and execute it in Amazon EC2.</li>
    <li>After the execution of a workflow completes, please terminate all the EC2 instances from your AWS account manually.</li>
    -->
</OL>


<h2>Download, configure, and deploy DATAVIEW as a Website</h2>
Check out tutorial: https://youtu.be/7Sz4PSD_6Cs or follow the instructions below: 
<OL>
    <li> Follow the first three steps from <b>Download and configure DATAVIEW as JAVA API</b> </li>
    <li>  Create three default folders Dropbox/DATAVIEW/Tasks, which stores the task file (class file or jar file); Dropbox/DATAVIEW/Workflows, which stores the mxgraph file for the generated workflow; Dropbox/DATAVIEW-INPUT, which stores the input files for a workflow in your dropbox. </li>
    <li> Get a dropbox token. </li>
</OL>

<h2>Run Deep Learning workflow (NNWorkflow) in DATAVIEW on Local NVIDIA GPU</h2>
Check out The introduction of DlaaW (Deep-learning-as-a-workflow) in DATAVIEW : https://www.youtube.com/watch?v=3KDq5CTcrGE.

Below are some extra tips aside from instructions in <b>Download, configure, and deploy DATAVIEW as a Website</b> and <b>Download, configure, and deploy DATAVIEW as a Website</b>:
1. <b>JAVA API:</b> A programmer can utilize various workflow NNasks and NNWorkflows based on the DATAVIEW libraries. /DATAVIEW/src/<b>NNTest.java</b> shows the 4 steps to create a customized NNWorkflow and execute it in one of NNTrainers (each corresponding to one specific execution plan and GPU infrastructure).
* There is no need to install extra libraries or driver (e.g. CUDA toolkit) as long as you have a local NVIDIA GPU on your PC. 
* In order to run NNWorkflow Java API version, need tomcat version lower than or equal to tomcat 9 (Our recommendation is tomcat 9). 

2. <b>Visual Programming</b>: DATAVIEW is deployed as a Web site in Tomcat and a user can drag and drop tasks and link them into a NNWorkflow in a visual workflow design and execution environment called <b>Webbench</b>. 
* In order to run NNWorkflow Website version on your Local PC, need java jdk version less than or equal to 15 (Our recommendation is JAVA JDK 15). 
* To run NNWorkflow in web GUIs, you should copy following files from your local DATAVIEW TrainerDLLs and ExecutorDLLs folders from /DATAVIEW/WebContent/workflowTaskDir repository to the DATAVIEW-INPUT folder in your dropbox, files including jsoncpp.dll, maintest.dll, nnExecutor.dll

<h2>DATAVIEW Tutorials</h2>
<OL>
    <li> Chapter 1: A gentle introduction to DATAVIEW https://youtu.be/7S4iGKXpaAc) </li>
    <li> How to download, import DATAVIEW into Eclipse as Java API and run a workflow with local executor (https://youtu.be/xJikeWptYSw)</li>
    <li> How to create a relational algebra workflow in DATAVIEW through the interface (https://youtu.be/AQw0S_QO8zg) </li>
    <!--
    <li> How to download and import DATAVIEW into Eclipse as Java API (https://youtu.be/R6A6jreySFc)</li>
    <li> How to create an Access Key ID and a Secret access key in Amazon EC2 (https://youtu.be/9741e4CubMQ)</li>
    <li> How to create a workflow task for DATAVIEW (the linear regression example) (https://youtu.be/BPaoR_zogPA)</li>
    <li> How to create a workflow task in Python (https://youtu.be/3vSx-g9FnZU)</li>
    <li> How to create a workflow task for DATAVIEW (the K-means example) (https://youtu.be/N4jIYbYSFd4) </li>
    <li> How to create a workflow in DATAVIEW (the word count example) (https://youtu.be/x1f8UgyShtI) </li>
    <li> How to create a workflow in DATAVIEW (the distributed K-means workflow example) (https://youtu.be/aQJPzdQQ3Uc)</li>
    <li> How to create a workflow in DATAVIEW (the word count example revisited) (https://youtu.be/U8mhL9vVXlM)</li>
    <li> How to create a workflow in DATAVIEW (the distributed K-means workflow example revisited) (https://youtu.be/QLN8q9Hg1eE)</li>
    <li> How to generate a random workflow and then visualize it (https://youtu.be/aQPIhe2ZnzU)</li>
    <li> How to debug the functionality of a task (https://youtu.be/N4jIYbYSFd4)</li>
    <li> How to use Dataview.debugger to debug your DATAVIEW applications (https://youtu.be/1d1vJRGPBYs) </li>
    <li> How to develop a new workflow planner (https://youtu.be/R0i2s-LkGV8) </li>
    <li> An introduction to WowkrlfowExecutor_Beta (<a href=""https://www.youtube.com/watch?v=kBIcxWyJgQA&t=2726s"">part 1</a>
        | <a href=""https://www.youtube.com/watch?v=Km24otM3rEM&t=582s"">part 2</a>)
        -->
</OL>
",2023-07-07 15:54:58+00:00
dbt,dbt-core,dbt-labs/dbt-core,dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.,https://getdbt.com,False,7230,2023-07-07 15:28:05+00:00,2016-03-10 02:38:00+00:00,1281,134,256,180,v1.5.2,2023-06-22 15:57:55+00:00,Apache License 2.0,6198,v1.6.0b8,186,2023-06-30 18:36:40+00:00,2023-07-07 15:52:25+00:00,2023-07-06 15:54:12+00:00,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg"" alt=""dbt logo"" width=""750""/>
</p>
<p align=""center"">
  <a href=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml"">
    <img src=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push"" alt=""CI Badge""/>
  </a>
</p>

**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.

![architecture](https://github.com/dbt-labs/dbt-core/blob/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png)

## Understanding dbt

Analysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.

These select statements, or ""models"", form a dbt project. Models frequently build on top of one another – dbt makes it easy to [manage relationships](https://docs.getdbt.com/docs/ref) between models, and [visualize these relationships](https://docs.getdbt.com/docs/documentation), as well as assure the quality of your transformations through [testing](https://docs.getdbt.com/docs/testing).

![dbt dag](https://raw.githubusercontent.com/dbt-labs/dbt-core/6c6649f9129d5d108aa3b0526f634cd8f3a9d1ed/etc/dbt-dag.png)

## Getting started

- [Install dbt](https://docs.getdbt.com/docs/get-started/installation)
- Read the [introduction](https://docs.getdbt.com/docs/introduction/) and [viewpoint](https://docs.getdbt.com/docs/about/viewpoint/)

## Join the dbt Community

- Be part of the conversation in the [dbt Community Slack](http://community.getdbt.com/)
- Read more on the [dbt Community Discourse](https://discourse.getdbt.com)

## Reporting bugs and contributing code

- Want to report a bug or request a feature? Let us know on [Slack](http://community.getdbt.com/), or open [an issue](https://github.com/dbt-labs/dbt-core/issues/new)
- Want to help us build dbt? Check out the [Contributing Guide](https://github.com/dbt-labs/dbt-core/blob/HEAD/CONTRIBUTING.md)

## Code of Conduct

Everyone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the [dbt Code of Conduct](https://community.getdbt.com/code-of-conduct).
",2023-07-07 15:55:02+00:00
deepspark,deep-spark,Stratio/deep-spark,Connecting Apache Spark with different data stores [DEPRECATED],http://stratio.github.io/deep-spark,True,196,2023-02-12 18:50:25+00:00,2014-02-18 08:34:20+00:00,42,116,15,0,,,Apache License 2.0,1084,firstP-rev1,28,2015-03-20 12:37:39+00:00,,2016-06-21 08:56:20+00:00,"*Disclaimer: As of 01/06/2015 this project has been deprecated. Thank you for your understanding and continued help throughout the project's life.


What is Deep?
=====================

Deep is a thin integration layer between Apache Spark and several NoSQL datastores.
We actually support Apache Cassandra, MongoDB, Elastic Search, Aerospike, HDFS, S3 and any database accessible through JDBC, but in the near future we will add support for sever other datastores.

- JIRA: https://deep-spark.atlassian.net

Install ojdbc driver
=====================

In order to compile the deep-jdbc module is necessary to add the Oracle ojdbc driver into your local repository. You can download it from the URL: http://www.oracle.com/technetwork/database/features/jdbc/default-2280470.html. When you are on the web you must click in ""Accept License Agreement"" and later downlad ojdbc7.jar library. You need a free oracle account to download the official driver.

To install the ojdbc driver in your local repository you must execute the command below:

> mvn install:install-file -Dfile=<path-to-file> -DgroupId=com.oracle -DartifactId=ojdbc7  -Dversion=12.1.0.2 -Dpackaging=jar


Compiling Deep
=====================

After that you can compile Deep executing the following steps:

> cd deep-parent

> mvn clean install

Creating Deep Dristribution 
=====================

If you want to create a Deep distribution you must execute the following steps:

> cd deep-scripts

> make-distribution-deep.sh

During the creation you'll see the following question:

> What tag want to use for Aerospike native repository?

You must type 0.7.0 and press enter.


Apache Cassandra integration
============================

The integration is _not_ based on the Cassandra's Hadoop interface.

Deep comes with an user friendly API that lets developers create Spark RDDs mapped to Cassandra column families.
We provide two different interfaces:

  * The first one will let developers map Cassandra tables to plain old java objects (POJOs), just like if you were using any other ORM. We call this API the 'entity objects' API.
    This abstraction is quite handy, it will let you work on RDD<YourEntityHere> (under the hood Deep will transparently map Cassandra's columns to entity properties).
    Your domain entities must be correctly annotated using Deep annotations (take a look at deep-core example entities in package com.stratio.deep.core.entity).

  * The second one is a more generic 'cell' API, that will let developerss work on RDD<com.stratio.deep.entity.Cells> where a 'Cells' object is a collection of com.stratio.deep.entity.Cell objects.
    Column metadata is automatically fetched from the data store. This interface is a little bit more cumbersome to work with (see the example below),
    but has the advantage that it doesn't require the definition of additional entity classes.
    Example: you have a table called 'users' and you decide to use the 'Cells' interface. Once you get an instance 'c' of the Cells object,
    to get the value of column 'address' you can issue a c.getCellByName(""address"").getCellValue().
    Please, refer to the Deep API documentation to know more about the Cells and Cell objects.

We encourage you to read the more comprehensive documentation hosted on the [Openstratio website](http://www.openstratio.org/).

Deep comes with an example sub project called 'deep-examples' containing a set of working examples, both in Java and Scala.
Please, refer to the deep-example project README for further information on how to setup a working environment.

MongoDB integration
===================

Spark-MongoDB connector is based on Hadoop-mongoDB.

Support for MongoDB has been added in version 0.3.0.

We provide two different interfaces:

  * ORM API, you just have to annotate your POJOs with Deep annotations and magic will begin, you will be able to connect MongoDB with Spark using your own model entities.

  * Generic cell API, you do not need to specify the collection's schema or add anything to your POJOs, each document will be transform to an object ""Cells"".

We added a few working examples for MongoDB in deep-examples subproject, take a look at:

Entities:

  * com.stratio.deep.examples.java.ReadingEntityFromMongoDB
  * com.stratio.deep.examples.java.WritingEntityToMongoDB
  * com.stratio.deep.examples.java.GroupingEntityWithMongoDB

Cells:

  * com.stratio.deep.examples.java.ReadingCellFromMongoDB
  * com.stratio.deep.examples.java.WritingCellToMongoDB
  * com.stratio.deep.examples.java.GroupingCellWithMongoDB


You can check out our first steps guide here:

[First steps with Deep-MongoDB](https://github.com/Stratio/deep-spark/blob/master/doc/src/site/sphinx/t20-first-steps-deep-mongodb.rst)


We are working on further improvements!

ElasticSearch integration
=========================

Support for ElasticSearch has been added in version 0.5.0.


Aerospike integration
=========================

Support for Aerospike has been added in version 0.6.0.

Examples:

Entities:

  * com.stratio.deep.examples.java.ReadingEntityFromAerospike
  * com.stratio.deep.examples.java.WritingEntityToAerospike
  * com.stratio.deep.examples.java.GroupingEntityWithAerospike

Cells:

  * com.stratio.deep.examples.java.ReadingCellFromAerospike
  * com.stratio.deep.examples.java.WritingCellToAerospike
  * com.stratio.deep.examples.java.GroupingCellWithAerospike

JDBC integration
================

Support for JDBC has been added in version 0.7.0.

Examples:

Entities:

  * package com.stratio.deep.examples.java.ReadingEntityWithJdbc
  * package com.stratio.deep.examples.java.WritingEntityWithJdbc

Cells:

  * package com.stratio.deep.examples.java.ReadingCellWithJdbc
  * package com.stratio.deep.examples.java.WritingCellWithJdbc  


Requirements
============

  * Cassandra, we tested versions from 1.2.8 up to 2.0.11 (for Spark <=> Cassandra integration).
  * MongoDB, we tested the integration with MongoDB versions 2.2, 2.4 y 2.6 using Standalone, Replica Set and Sharded Cluster (for Spark <=> MongoDB integration).
  * ElasticSearch, 1.3.0+
  * Aerospike, 3.3.0+
  * Spark 1.1.1
  * Apache Maven >= 3.0.4
  * Java 1.7
  * Scala 2.10.3

Configure the development and test environment
==============================================
* Clone the project
* To configure a development environment in Eclipse: import as Maven project. In IntelliJ: open the project by selecting the deep-parent POM file
* Install the project in you local maven repository. Enter deep-parent subproject and perform: mvn clean install (add -DskipTests to skip tests)
* Put Deep to work on a working cassandra + spark cluster. You have several options:
    * Download a pre-configured Stratio platform VM [Stratio's BigData platform (SDS)](http://www.stratio.com/).
      This VM will work on both Virtualbox and VMWare, and comes with a fully configured distribution that also includes Stratio Deep. We also distribute the VM with several preloaded datasets in Cassandra. This distribution will include Stratio's customized Cassandra distribution containing our powerful [open-source lucene-based secondary indexes](https://github.com/Stratio/stratio-cassandra), see Stratio documentation for further information.
      Once your VM is up and running you can test Deep using the shell. Enter /opt/sds and run bin/stratio-deep-shell.
    * Install a new cluster using the Stratio installer. Please refer to Stratio's website to download the installer and its documentation.
    * You already have a working Cassandra server on your development machine: you need a spark+deep bundle, we suggest to create one by running:
    
	    ``cd deep-scripts``
	    
	    ``./make-distribution-deep.sh``
	    
    this will build a Spark distribution package with StratioDeep and Cassandra's jars included (depending on your machine this script could take a while, since it will compile Spark from sources).
The package will be called ``spark-deep-distribution-X.Y.Z.tgz``, untar it to a folder of your choice, enter that folder and issue a ``./stratio-deep-shell``, this will start an interactive shell where you can test StratioDeep (you may have noticed this is will start a development cluster started with MASTER=""local"").

    * You already have a working installation os Cassandra and Spark on your development machine: this is the most difficult way to start testing Deep, but you know what you're doing you will have to
        1. copy the Stratio Deep jars to Spark's 'jars' folder (``$SPARK_HOME/jars``).
        2. copy Cassandra's jars to Spark's 'jar' folder.
        3. copy Datastax Java Driver jar (v 2.0.x) to Spark's 'jar' folder.
        4. start spark shell and import the following:
        

      ``import com.stratio.deep.commons.annotations._``

      ``import com.stratio.deep.commons.config._``
      
      ``import com.stratio.deep.commons.entity._``
      
      ``import com.stratio.deep.core.context._``
      
      ``import com.stratio.deep.cassandra.config._``
      
      ``import com.stratio.deep.cassandra.extractor._``
      
      ``import com.stratio.deep.mongodb.config._``
      
      ``import com.stratio.deep.mongodb.extractor._``
      
      ``import com.stratio.deep.es.config._``
      
      ``import com.stratio.deep.es.extractor._``
      
      ``import com.stratio.deep.aerospike.config._``
      
      ``import com.stratio.deep.aerospike.extractor._``
      
      ``import org.apache.spark.rdd._``
      
      ``import org.apache.spark.SparkContext._``
      
      ``import org.apache.spark.sql.api.java.JavaSQLContext``
      
      ``import org.apache.spark.sql.api.java.JavaSchemaRDD``
      
      ``import org.apache.spark.sql.api.java.Row``
      
      ``import scala.collection.JavaConversions._``
			
      

Once you have a working development environment you can finally start testing Deep. This are the basic steps you will always have to perform in order to use Deep:

First steps with Spark and Cassandra
====================================

* __Build an instance of a configuration object__: this will let you tell Deep the Cassandra endpoint, the keyspace, the table you want to access and much more.
  It will also let you specify which interface to use (the domain entity or the generic interface).
  We have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.
  Please take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive documentation at [OpenStratio website](https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t10-first-steps-deep-cassandra.md).
* __Create an RDD__: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.
* __Perform some computation over this RDD(s)__: this is up to you, we only help you fetching the data efficiently from Cassandra, you can use the powerful [Spark API](https://spark.apache.org/docs/1.1.1/api/java/index.html).
* __(optional) write the computation results out to Cassandra__: we provide a way to efficiently save the result of your computation to Cassandra.
  In order to do that you must have another configuration object where you specify the output keyspace/column family. We can create the output column family for you if needed.
  Please, refer to the comprehensive Stratio Deep documentation at [Stratio website](https://github.com/Stratio/deep-spark/blob/release/0.6/doc/about.md).

First steps with Spark and MongoDB
==================================

* __Build an instance of a configuration object__: this will let you tell Stratio Deep the MongoDB endpoint, the MongoDB database and collection you want to access and much more.
  It will also let you specify which interface to use (the domain entity).
  We have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.
  Please take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive Deep documentation at [OpenStratio website](https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t20-first-steps-deep-mongodb.md).
* __Create an RDD__: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.
* __Perform some computation over this RDD(s)__: this is up to you, we only help you fetching the data efficiently from MongoDB, you can use the powerful [Spark API](https://spark.apache.org/docs/1.1.1/api/java/index.html).
* __(optional) write the computation results out to MongoDB__: we provide a way to efficiently save the result of your computation to MongoDB.

Migrating from version 0.2.9
============================
From version 0.4.x, Deep supports multiple datastores, in order to correctly implement this new feature Deep has undergone an huge refactor between versions 0.2.9 and 0.4.x. To port your code to the new version you should take into account a few changes we made.

New Project Structure
---------------------
From version 0.4.x, Deep supports multiple datastores, in your project you should import only the maven dependency you will use: deep-cassandra, deep-mongodb, deep-elasticsearch or deep-aerospike.

Changes to 'com.stratio.deep.entity.Cells'
------------------------------------------

* Until version 0.4.x the 'Cells' was implicitly associated to a record coming from a specific table. When performing a join in Spark, 'Cell' objects coming from different tables are mixed into an single 'Cells' object.
  Deep now keeps track of the original table a Cell object comes from, changing the internal structure of 'Cells', where each 'Cell' is associated to its 'table'.
    1. If you are a user of 'Cells' objects returned from Deep, nothing changes for you. The 'Cells' API keeps working as usual.
    2. If you manually create 'Cells' objects you can keep using the original API, in this case each Cell you add to your Cells object is automatically associated to a default table name.
    3. You can specify the default table name, or let Deep chose an internal default table name for you.
    4. We added a new constructor to 'Cells' accepting the default table name. This way the 'old' API will always manipulate 'Cell' objects associated to the specified default table.
    5. For each method manipulating the content of a 'Cells' object, we added a new method that also accepts the table name: if you call the method	 whose signature does _not_ have the table name, the table action is performed over the Cell associated to the default table, otherwise the action is performed over the 'Cell'(s) associated to the specified table. 
    6. size() y isEmpty() will compute their results taking into account all the 'Cell' objects contained. 
    7. size(String tableName) and isEmpty(tableName) compute their result taking into account only the 'Cell' objects associated to the specified table.  
    8. Obviously, when dealing with Cells objects, Deep always associates a Cell to the correct table name.
    
Examples:
<pre>
Cells cells1 = new Cells(); // instantiate a Cells object whose default table name is generated internally.
Cells cells2 = new Cells(""my_default_table""); // creates a new Cells object whose default table name is specified by the user
cells2.add(new Cell(...)); // adds to the 'cells2' object a new Cell object associated to the default table
cells2.add(""my_other_table"", new Cell(...)); // adds to the 'cells2' object a new Cell associated to ""my_other_table""  
</pre>

Changes to objects hierarchy
-----------------------------------------------------------
* IDeepJobConfig interface has been splitted into ICassandraDeepJobConfig and IMongoDeepJobConfig sub-interfaces. Each sub-interface exposes only the configuration properties that make sense for each data base.
com.stratio.deep.config.DeepJobConfigFactory's factory methods now return the proper subinterface.
* __DeepSparkContext__ has been splitted into __CassandraDeepSparkContext__ and __MongoDeepSparkContext__.
* __DeepJobConfigFactory__ has been renamed to __ConfigFactory__ (to reduce verbosity).

RDD creation
----------------
Methods used to create Cell and Entity RDD has been merged into one single method:

* __DeepSparkContext__: createRDD(...)

",2023-07-07 15:55:07+00:00
dgsh,dgsh,dspinellis/dgsh,Shell supporting pipelines to and from multiple processes,http://www.spinellis.gr/sw/dgsh/,False,295,2023-06-02 05:05:07+00:00,2013-01-13 11:23:10+00:00,24,15,6,1,v1.0.0,2019-09-11 12:24:44+00:00,Other,1567,v1.0.0,1,2019-09-11 12:24:44+00:00,2023-06-02 05:05:04+00:00,2021-07-04 20:05:40+00:00,"## The Directed Graph Shell (dgsh)

[![Build Status](https://travis-ci.org/dspinellis/dgsh.svg?branch=master)](https://travis-ci.org/dspinellis/dgsh)

The directed graph shell, *dgsh*, allows the expressive expression of efficient big data set and streams processing pipelines using existing Unix tools as well as custom-built components. It is a Unix-style shell allowing the specification of pipelines with non-linear scatter-gather operations. These form a directed acyclic process graph, which is typically executed by multiple processor cores, thus increasing the operation's processing throughput.

You can find a complete introduction, reference documentation,
and illustrated examples in the suite's
[web site](http://www.spinellis.gr/sw/dgsh/).

See also,
a [quick video overview](https://youtu.be/crqzO4YanwA) and
the associated (open access) paper,
[Extending Unix pipelines to DAGs](http://dx.doi.org/10.1109/TC.2017.2695447),
published in the *IEEE Transactions on Computers*, 66(9):1547–1561, 2017.
",2023-07-07 15:55:10+00:00
digdag,digdag,treasure-data/digdag,Workload Automation System,https://www.digdag.io/,False,1245,2023-06-28 07:33:41+00:00,2015-09-18 23:18:08+00:00,222,131,97,106,v0.10.5,2023-02-14 10:00:03+00:00,Apache License 2.0,4392,v0.10.5,110,2023-02-14 10:00:03+00:00,2023-07-03 23:15:34+00:00,2023-03-28 09:16:33+00:00,"# Digdag

[![CircleCI](https://dl.circleci.com/status-badge/img/gh/treasure-data/digdag/tree/master.svg?style=svg&circle-token=5a93079551888e4dc43ad75fe6e2bd312153995c)](https://dl.circleci.com/status-badge/redirect/gh/treasure-data/digdag/tree/master)

[![CI](https://github.com/treasure-data/digdag/workflows/CI/badge.svg)](https://github.com/treasure-data/digdag/actions)


## [Documentation](https://docs.digdag.io)

Please check [digdag.io](https://digdag.io) and [docs.digdag.io](https://docs.digdag.io) for installation & user manual.

REST API document is available at [docs.digdag.io/api](http://docs.digdag.io/api/).

## Release Notes

The list of release note is [here](https://github.com/treasure-data/digdag/tree/master/digdag-docs/src/releases).


## Development

### Prerequirements

* JDK 8
* Node.js 12.x

Installing Node.js using nodebrew:

```
$ curl -L git.io/nodebrew | perl - setup
$ echo 'export PATH=$HOME/.nodebrew/current/bin:$PATH' >> ~/.bashrc
$ source ~/.bashrc
$ nodebrew install-binary v12.x
$ nodebrew use v12.x
```

Installing Node.js using Homebrew on Mac OS X:

```
$ brew install node
```

* Python 3
  * sphinx
  * sphinx_rtd_theme
  * recommonmark

### Running tests

```
$ ./gradlew check
```

Test coverage report is generated at `didgag-*/build/reports/jacoco/test/html/index.html`.
Findbugs report is generated at `digdag-*/build/reports/findbugs/main.html`.

```
$ CI_ACCEPTANCE_TEST=true ./gradlew digdag-tests:test --info --tests acceptance.BuiltInVariablesIT
```

To execute tests in digdag-tests subproject locally, `tests` option that is provided by Gradle is useful.
Environment variable `CI_ACCEPTANCE_TEST=true` is needed to execute digdag-tests.

### Testing with PostgreSQL

Test uses in-memory H2 database by default. To use PostgreSQL, set following environment variables:

```
$ export DIGDAG_TEST_POSTGRESQL=""$(cat config/test_postgresql.properties)""
```

### Building CLI executables

```
$ ./gradlew cli
$ ./gradlew cli -PwithoutUi  # build without integrated UI
```

(If the command fails during building UI due to errors from `node` command, you can try to add `-PwithoutUi` argument to exclude the UI from the package).

It makes an executable in `pkg/`, e.g. `pkg/digdag-$VERSION.jar`.

### Develop digdag-ui

Node.js development server is useful because it reloads changes of digdag-ui source code automatically.

First, put following lines to ~/.config/digdag/config and start digdag server:

```
server.http.headers.access-control-allow-origin = http://localhost:9000
server.http.headers.access-control-allow-headers = origin, content-type, accept, authorization, x-td-account-override, x-xsrf-token, cookie
server.http.headers.access-control-allow-credentials = true
server.http.headers.access-control-allow-methods = GET, POST, PUT, DELETE, OPTIONS, HEAD
server.http.headers.access-control-max-age = 1209600
```

Then, start digdag-ui development server:

```
$ cd digdag-ui/
$ npm install
$ npm run dev    # starts dev server on http://localhost:9000/
```

### Updating REST API document

Run this command to update REST API document file at digdag-docs/src/api/swagger.yaml.

```
./gradlew swaggerYaml  # dump swagger.yaml file
```

Use `--enable-swagger` option to check the current Digdag REST API.

```
$ ./gradlew cli
$ ./pkg/digdag-<current version>.jar server --memory --enable-swagger # Run server with --enable-swagger option

$ docker run -dp 8080:8080 swaggerapi/swagger-ui # Run Swagger-UI on different console
$ open http://localhost:8080/?url=http://localhost:65432/api/swagger.json # Open api/swagger.json on Swagger-UI
```

### Updating documents

Documents are in digdag-docs/src directory. They're built using Sphinx.

Website is hosted on [www.digdag.io](http://www.digdag.io) using Github Pages. Pages are built using deployment step of circle.yml and automatically pushed to [gh-pages branch of digdag-docs repository](https://github.com/treasure-data/digdag-docs/tree/gh-pages).

To build the pages and check them locally, follow this instruction.

Create a virtual environment of Python and install dependent Python libraries including Sphinx.

```
$ python3 -m venv .venv
$ source .venv/bin/activate
(.venv)$ pip install -r digdag-docs/requirements.txt -c digdag-docs/constraints.txt
```

After installation of Python libraries, You can build with running the following command:

```
(.venv)$ ./gradlew site
```

This might not always update all necessary files (Sphinx doesn't manage update dependencies well). In this case, run `./gradlew clean` first.

It builds index.html at digdag-docs/build/html/index.html.

### Development on IDEs

#### IntelliJ IDEA

Digdag is using a Java annotation processor `org.immutables:value.` The combination of Java annotation processing and Gradle on IntelliJ IDEA sometimes introduces some troubles. In Digdag's case, you may run into some compile errors like `cannot find symbol: class ImmutableRestWorkflowDefinitionCollection.`
So we'd recommend the followings to avoid those compile errors if you want to develop Digdag one the IDE.

1. There's an important configuration option to be enabled to fully have IntelliJ be fully integrated with an existing gradle build configuration: `Delegate IDE build/run actions to gradle` needs to be enabled.

![](https://user-images.githubusercontent.com/17990895/48221255-9706be80-e35f-11e8-8283-1ca6d713e31c.png)

## Releasing a new version
This is for committers only.
### Prerequisite: Sonatype OSSRH
You need an account in Sonatype OSSRH, and configure it in your `~/.gradle/gradle.properties`.

ossrhUsername=(your Sonatype OSSRH username)
ossrhPassword=(your Sonatype OSSRH password)

### Prerequisite: PGP signatures
You need your PGP signatures to release artifacts into Maven Central, and configure Gradle to use your key to sign.
Configure it in your `~/.gradle/gradle.properties`.

```
signing.gnupg.executable=gpg
signing.gnupg.useLegacyGpg=false
signing.gnupg.keyName=(the last 8 symbols of your keyId)
signing.gnupg.passphrase=(the passphrase used to protect your private key)
```

### Release procedure

As mentioned in the prerequirements, we need to build with JDK 8 in this procedure.

1. run `git pull upstream master --tags`.
1. run `./gradlew setVersion -Pto=<version>` command.
1. write release notes to `releases/release-<version>.rst` file. It must include at least version (the first line) and release date (the last line).
1. run `./gradlew clean cli site check releaseCheck`.
1. make a release branch. `git checkout -b release_v<version>` and commit.
1. push the release branch to origin and create a PR.
1. after the PR is merged to master, checkout master and pull latest upstream/master.
1. run `./gradlew clean cli site check releaseCheck` again.
1. if it succeeded, run `./gradlew release`.
1. create a tag `git tag -a v<version>` and push `git push upstream v<version>`
1. create a release in [GitHub releases](https://github.com/treasure-data/digdag/releases).
1. upload `pkg/digdag-<version>.jar` to the release
1. a few minutes later, run `digdag selfupdate` and confirm the version.

If major version is incremented, also update `version =` and `release =` at [digdag-docs/src/conf.py](digdag-docs/src/conf.py).

If you are expert, skip 5. to 7. and directly update master branch.

### Post-process of new release

You also need following steps after new version has been released.

1. create next snapshot version, run `./gradlew setVersion -Pto=<next-version>-SNAPSHOT`.
1. push to master.

### Releasing a SNAPSHOT version

```
./gradlew releaseSnapshot
```
**Note**
Snapshot release is not supported currently.
",2023-07-07 15:55:14+00:00
dispel4py,dispel4py,dispel4py/dispel4py,A Python library to describe abstract workflows for distributed data-intensive applications,,False,22,2023-05-09 21:25:31+00:00,2015-02-18 11:19:57+00:00,18,9,6,2,v1.2,2015-06-18 15:14:23+00:00,Apache License 2.0,389,v1.2,3,2015-06-18 15:14:23+00:00,2023-05-09 21:25:31+00:00,2018-10-02 18:59:34+00:00,"dispel4py
=========

dispel4py is a free and open-source Python library for describing abstract stream-based workflows for distributed data-intensive applications. It enables users to focus on their scientific methods, avoiding distracting details and retaining flexibility over the computing infrastructure they use.  It delivers mappings to diverse computing infrastructures, including cloud technologies, HPC architectures and  specialised data-intensive machines, to move seamlessly into production with large-scale data loads. The dispel4py system maps workflows dynamically onto multiple enactment systems, such as MPI, STORM and Multiprocessing, without users having to modify their workflows.

Dependencies 
------------

dispel4py has been tested with Python *2.7.6*, *2.7.5*, *2.7.2*, *2.6.6* and Python *3.4.3*.

The following Python packages are required to run dispel4py:

- networkx (https://networkx.github.io/)

If using the MPI mapping:

- mpi4py (http://mpi4py.scipy.org/)

If using the Storm mapping:

- Python Storm module, available here: https://github.com/apache/storm/tree/master/storm-multilang/python/src/main/resources/resources, to be placed in directory `resources`.
- Python Storm thrift generated code, available here: https://github.com/apache/storm/tree/master/storm-core/src/py

Installation
------------

The easiest way to install dispel4py is via pip (https://pypi.python.org/pypi/pip):

`pip install dispel4py`

Or, if you have git installed, you can install the latest development version directly from github:

`pip install git+git://github.com/dispel4py/dispel4py.git@master`

Alternatively, download the ZIP or clone this repository to your desktop. You can then install from the local copy to your python environment by calling:

`python setup.py install`

from the dispel4py root directory.

Docker image
------------

A Docker image with the latest dispel4py development version, based on Ubuntu 14.04 with OpenMPI, is available from the Docker Hub. For more details see: https://registry.hub.docker.com/u/dispel4py/dispel4py/

The dispel4py image is deployed as follows:

`docker pull dispel4py/dispel4py`

Documentation
-------------

The wiki documentation explains how to install and test dispel4py: https://github.com/dispel4py/dispel4py/wiki

[![Build Status](https://travis-ci.org/dispel4py/dispel4py.svg)](https://travis-ci.org/dispel4py/dispel4py)
[![PyPI version](https://badge.fury.io/py/dispel4py.svg)](http://badge.fury.io/py/dispel4py)
[![Coverage Status](https://coveralls.io/repos/dispel4py/dispel4py/badge.svg?branch=master)](https://coveralls.io/r/dispel4py/dispel4py?branch=master)


",2023-07-07 15:55:18+00:00
dockerflow,dockerflow,googlegenomics/dockerflow,Dockerflow is a workflow runner that uses Dataflow to run a series of tasks in Docker with the Pipelines API,,True,97,2023-06-09 10:10:27+00:00,2016-09-13 16:18:43+00:00,17,23,4,0,,,Apache License 2.0,75,,0,,2023-06-09 10:10:26+00:00,2017-11-21 00:49:29+00:00,"### Disclaimer

This is not an official Google product.

# Update

As of 11 Nov 2017, Dockerflow is no longer actively maintained and will not be enhanced with new
features.

For multi-step batch workflows consisting of Docker tasks, we now recommend running in the cloud
using:

*   [dsub](https://github.com/googlegenomics/dsub), a command-line batch submission tool

To run multi-step workflows with dsub, you can create a bash or python script with multiple dsub
calls. Execution graphs can be constructed using dsub's
[job control functionality](https://github.com/googlegenomics/dsub/blob/master/docs/job_control.md).

For any Dockerflow functionality that is not satisfied by dsub, please
[file an issue](https://github.com/googlegenomics/dsub/issues) in the dsub repository.

# Dockerflow

Dockerflow makes it easy to run a multi-step workflow of Docker tasks using
[Google Cloud Dataflow](https://cloud.google.com/dataflow) for orchestration.
Docker steps are run using the [Pipelines API](https://cloud.google.com/genomics/v1alpha2/pipelines).

You can run Dockerflow from a shell on your laptop, and the job will run in 
Google Cloud Platform using Dataflow's fully managed service and web UI.

Dockerflow workflows can be defined in [YAML](http://yaml.org) files, or by writing
Java code. Examples of workflows defined in YAML can be found in

*   [examples](examples)
*   [src/test/resources](src/test/resources)

Examples of workflows defined in Java can be found in

*   [examples](examples)
*   [src/test/java/com/google/cloud/genomics/dockerflow/examples](src/test/java/com/google/cloud/genomics/dockerflow/examples)

You can run a batch of workflows at once by providing a CSV file with one row per
workflow to define the parameters.

## Why Dockerflow?

This project was created as a proof-of-concept that Dataflow can be used
for monitoring and management of directed acyclic graphs of command-line tools.

Dataflow and Docker complement each other nicely:

*   Dataflow provides graph optimization, a nice monitoring interface, retries,
    and other niceties.
*   Docker provides portability of the tools themselves, and there's a large
    library of packaged tools already available as Docker images.

While Dockerflow supports a simple YAML workflow definition, a similar approach
could be taken to implement a runner for one of the open standards like [Common
Workflow Language]
(https://github.com/common-workflow-language/common-workflow-language) or
[Workflow Definition Language](github.com/broadinstitute/wdl).

## Table of contents

*   [Prerequisites](#prerequisites)
*   [Getting started](#getting-started)
*   [Docker + Dataflow vs custom scripts](#docker-and-dataflow-vs-custom-scripts)
*   [Creating your own workflows](#creating-your-own-workflows)
    *   [Sequential workflows](#sequential-workflows)
    *   [Parallel workflows](#parallel-workflows)
    *   [Branching workflows](#branching-workflows)
*   [Testing](#testing)
*   [What next](#what-next)

## Prerequisites

1.  Sign up for a Google Cloud Platform account and [create a project]
    (https://console.cloud.google.com/project?).
2.  [Enable the APIs]
    (https://console.cloud.google.com/flows/enableapi?apiid=genomics,dataflow,storage_component,compute_component&redirect=https://console.cloud.google.com)
    for Cloud Dataflow, Google Genomics, Compute Engine and Cloud Storage.\
3.  [Install the Google Cloud SDK](https://cloud.google.com/sdk/) and run

        gcloud init
        gcloud auth login
        gcloud auth application-default login

## Getting started

Run the following steps on your laptop or local workstation:

1.  git clone this repository.

        git clone https://github.com/googlegenomics/dockerflow

2.  Build it with Maven.

        cd dockerflow
        mvn package -DskipTests

3. Set up the DOCKERFLOW_HOME environment.

        export DOCKERFLOW_HOME=""$(pwd)""
        export PATH=""${PATH}"":""${DOCKERFLOW_HOME}/bin""
        chmod +x bin/*

4.  Run a sample workflow:

        dockerflow --project=MY-PROJECT \
            --workflow-file=src/test/resources/linear-graph.yaml \
            --workspace=gs://MY-BUCKET/MY-PATH \
            --input BASE_DIR=gs://MY-BUCKET/MY-PATH/MY-INPUT-FILE.txt
            --runner=DirectPipelineRunner

Set `MY-PROJECT` to your cloud project name, and set `MY-BUCKET` and `MY-PATH`
to your cloud bucket and folder. You'll need to have a text file in Cloud Storage
as well, here called `MY-INPUT-FILE.txt`. You can copy one from
[src/test/resources/input-one.txt](src/test/resources/input-one.txt):

        gsutil cp src/test/resources/input-one.txt gs://MY-BUCKET/MY-PATH/input-one.txt

The example will run Dataflow locally with the `DirectPipelineRunner`, for
orchestration. It will spin up VMs remotely in Google Cloud to run the
individual tasks in Docker. Execution of the local Dataflow runner will block
until the workflow completes. The DirectPipelineRunner is useful for
debugging, because you'll see all of the log messages output to your shell.

To run in your cloud project, and see the pretty Dataflow UI in Google Cloud
Console, you can remove the `--runner` option to use the default Dataflow runner.

## Docker and Dataflow vs custom scripts

How is Dataflow better than a shell script?

Dataflow provides:

*   **Complex workflow orchestration**: Dataflow supports arbitrary directed
acyclic graphs. The logic of branching, merging, parallelizing, and monitoring is
all handled automatically.
*   **Monitoring**:
[Dataflow's monitoring UI](https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf)
shows you what jobs you've run and shows an execution graph with nice details.
*   **Debugging**: Dataflow keeps logs at each step, and you can view them directly
in the UI.
*   **Task retries**: Dataflow automatically retries failed steps.
Dockerflow adds support for preemptible VMs, rerunning failures on standard VM
instances.
*   **Parallelization**: Dataflow can run 100 tasks on 100 files and
keep track of them all for you, retrying any steps that failed.
*   **Optimization**: Dataflow optimizes the execution graph for your workflow.

Docker provides:

*   **Portability**: Tools packaged in Docker images can be run
anywhere Docker is supported.
*   **A library of pre-packaged tools**: The community has contributed a growing
library of popular tools.

## Creating your own workflows

The Dockerflow command-line expects a static workflow graph definition in YAML,
or the Java class name of a Java definition.

If you'd rather define workflows in code, you'll use the Java SDK. See

*   [src/test/java/com/google/cloud/genomics/dockerflow/examples](src/test/java/com/google/cloud/genomics/dockerflow/examples)

Everything that can be done with YAML can also be done (and more compactly) in
Java code. Java provides greater flexibility too.

The documentation below provides details for defining workflows in YAML. To
create a workflow, you define the tasks and execution graph. You can
define the tasks and execution graph in a single file, or your graph can
reference tasks that are defined in separate YAML files.

A workflow is a recursive format, meaning that a workflow can contain multiple
steps, and each of the steps can be a workflow.

## Hello, world

Dockerflow has lots of features for creating complex, real-world workflows.
The best way to get started with your own workflows is to look at the
[examples](examples). 

The [hello, world](examples/hello) example shows the most basic workflow
in both YAML and Java.

All of the advanced features can be seen in the more complex
[GATK](examples/gatk) example. Again, it offers both YAML and Java
versions. You'll see pretty much the full range of functionality.

## Testing

Workflows can be tricky to test and debug. Dataflow has a local runner that
makes it easy to fix the obvious bugs before running in your Google Cloud
Platform project.

To test locally, set `--runner=DirectPipelineRunner`. Now Dataflow will run on
your local computer rather than in the cloud. You'll be able to see all of the
log messages.

Two other flags are really useful for testing: `--test=true` and
`--resume=true`.

When you set `test` to true, you'll get a dry run of the pipeline. No calls to
the Pipelines API will be made. Instead, the code will print a log message and
continue. That lets you do a first sanity check before submitting and running on
the cloud. You can catch many errors, mismatched parameters, etc.

When you use the `resume` flag, Dockerflow will try to resume a failed pipeline
run. For example, suppose you're trying to get your 10-step pipeline to work. It
fails on step 6. You go into your YAML definition file, or edit your Java code.
Now you want to re-run the pipeline. However, it takes 1 hour to run steps 1-5.
That's a long time to wait. With `--resume=true`, Dockerflow will look to see if
the outputs of each step exist already, and if they do, it will print a log
message and proceed to the next step. That means it takes only seconds to skip
ahead to the failed step and try to rerun it.

## What next?

*   See the YAML examples in the [src/test/resources](src/test/resources) directory.
*   See the Java code examples in
    *    [examples](examples)
    *    [src/test/java/com/google/cloud/genomics/dockerflow/examples](src/test/java/com/google/cloud/genomics/dockerflow/examples)
*   Learn about the [Pipelines API]
    (https://cloud.google.com/genomics/v1alpha2/pipelines).
*   Read about [Dataflow](https://cloud.google.com/dataflow).
*   Write your own workflows!

## FAQ and Troubleshooting

### What if I want to run large batch jobs?

Google Cloud Platform has various quotas that affect how many VMs and IP addresses, and how much disk space you can get. Some tips:

* [Check and potentially increase quotas](https://console.cloud.google.com/compute/quotas)
* Consider listing all zones in the region or geography where your Cloud Storage bucket is (e.g., for standard buckets in the EU, use ""eu-""; for regional buckets in US central, use ""us-central-"")
* The pipeline system will queue jobs until resources are available if quotas are exceeded
* Dockerflow will abort if any job fails. Use the '--abort=false' flag for different behavior.
",2023-07-07 15:55:23+00:00
dog,dog,dogtools/dog,Dog wants to be a very good task runner,,False,83,2023-06-30 02:51:55+00:00,2016-06-10 10:29:29+00:00,12,11,7,5,v0.5.0,2018-05-18 13:51:30+00:00,Apache License 2.0,124,v0.5.0,5,2018-05-18 13:51:30+00:00,2023-06-30 02:51:56+00:00,2018-08-25 19:46:45+00:00,"<p align=""center""><a href=""https://github.com/dogtools/dog"" target=""_blank""><img width=""300""src=""https://raw.githubusercontent.com/dogtools/dog/master/img/dog_logo.png""></a></p>

<p align=""center"">
  <a href=""https://github.com/dogtools/dog/releases/latest""><img src=""https://img.shields.io/github/release/dogtools/dog.svg?style=flat-square""/></a>
  <a href=""https://godoc.org/github.com/dogtools/dog""><img src=""http://img.shields.io/badge/godoc-reference-5272B4.svg?style=flat-square""/></a>
  <a href=""https://travis-ci.org/dogtools/dog""><img src=""https://img.shields.io/travis/dogtools/dog.svg?style=flat-square""/></a>
  <a href=""https://goreportcard.com/report/github.com/dogtools/dog""><img src=""https://goreportcard.com/badge/github.com/dogtools/dog?style=flat-square&x=1""/></a>
  <a href=""https://github.com/dogtools/dog/blob/master/LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202.0-blue.svg?style=flat-square""/></a>
<p>

# Dog

Dog is a command line application that executes automated tasks.

## Using Dog

List all tasks in current project

    dog

Execute a task

    dog taskname

Execute a task, printing elapsed time and exit status

    dog -i taskname

## What is a Dogfile?

Dogfile is a specification that uses YAML to describe the tasks related to a project. We think that the specification will be finished (no further breaking changes) by the v1.0.0 version of Dog.

- Read Dog's own [dog.yml][1]
- Read the [Dogfile Spec][2]

## Installing Dog

If you are using macOS you can install Dog using brew:

    brew tap dogtools/dog
    brew install dog

If you have your golang environment set up, you can use:

    go get -u github.com/dogtools/dog

## Other tools

Tools that use the Dogfile Specification are called *dogtools*. Dog is the first dogtool but there are other things that can be implemented in the future: web and desktop UIs, chat bot interfaces, plugins for text editors and IDEs, tools to export Dogfiles to other formats, HTTP API interfaces, even implementations of the cli in other languages!

The root directory of this repository contains the dog package that can be used to create dogtools in Go.

    import ""github.com/dogtools/dog""

Check the `examples/` directory to see how it works.

## Contributing

If you want to help, take a look at the open [bugs][3], the list of all [issues][4] and our [Code of Conduct][5].

[1]: https://github.com/dogtools/dog/blob/master/dog.yml
[2]: https://github.com/dogtools/dog/blob/master/DOGFILE_SPEC.md
[3]: https://github.com/dogtools/dog/issues?q=is%3Aissue+is%3Aopen+label%3Abug
[4]: https://github.com/dogtools/dog/issues
[5]: https://github.com/dogtools/dog/blob/master/CODE_OF_CONDUCT.md
",2023-07-07 15:55:26+00:00
doit,doit,pydoit/doit,task management & automation tool,http://pydoit.org,False,1630,2023-07-06 13:50:56+00:00,2014-02-14 22:21:23+00:00,170,52,59,0,,,MIT License,1483,git/master,47,2013-01-05 21:40:38+00:00,2023-07-06 13:50:56+00:00,2023-01-16 03:40:03+00:00,"================
README
================

.. display some badges

.. image:: https://img.shields.io/pypi/v/doit.svg
    :target: https://pypi.python.org/pypi/doit

.. image:: https://github.com/pydoit/doit/actions/workflows/ci.yml/badge.svg?branch=master
    :target: https://github.com/pydoit/doit/actions/workflows/ci.yml?query=branch%3Amaster

.. image:: https://codecov.io/gh/pydoit/doit/branch/master/graph/badge.svg?token=wxKa1h11zn
    :target: https://codecov.io/gh/pydoit/doit

.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4892136.svg
   :target: https://doi.org/10.5281/zenodo.4892136


Financial contributions on `Open Collective <https://opencollective.com/doit/tiers>`_


doit - automation tool
======================

*doit* comes from the idea of bringing the power of build-tools to execute any
kind of task

*doit* can be uses as a simple **Task Runner** allowing you to easily define ad hoc
tasks, helping you to organize all your project related tasks in an unified
easy-to-use & discoverable way.

*doit* scales-up with an efficient execution model like a **build-tool**.
*doit* creates a DAG (direct acyclic graph) and is able to cache task results.
It ensures that only required tasks will be executed and in the correct order
(aka incremental-builds).

The *up-to-date* check to cache task results is not restricted to looking for
file modification on dependencies.  Nor it requires ""target"" files.
So it is also suitable to handle **workflows** not handled by traditional build-tools.

Tasks' dependencies and creation can be done dynamically during it is execution
making it suitable to drive complex workflows and **pipelines**.

*doit* is build with a plugin architecture allowing extensible commands, custom
output, storage backend and ""task loader"". It also provides an API allowing
users to create new applications/tools leveraging *doit* functionality like a framework.

*doit* is a mature project being actively developed for more than 10 years.
It includes several extras like: parallel execution, auto execution (watch for file
changes), shell tab-completion, DAG visualisation, IPython integration, and more.



Sample Code
===========

Define functions returning python dict with task's meta-data.

Snippet from `tutorial <http://pydoit.org/tutorial-1.html>`_:

.. code:: python

  def task_imports():
      """"""find imports from a python module""""""
      for name, module in PKG_MODULES.by_name.items():
          yield {
              'name': name,
              'file_dep': [module.path],
              'actions': [(get_imports, (PKG_MODULES, module.path))],
          }

  def task_dot():
      """"""generate a graphviz's dot graph from module imports""""""
      return {
          'targets': ['requests.dot'],
          'actions': [module_to_dot],
          'getargs': {'imports': ('imports', 'modules')},
          'clean': True,
      }

  def task_draw():
      """"""generate image from a dot file""""""
      return {
          'file_dep': ['requests.dot'],
          'targets': ['requests.png'],
          'actions': ['dot -Tpng %(dependencies)s -o %(targets)s'],
          'clean': True,
      }


Run from terminal::

  $ doit list
  dot       generate a graphviz's dot graph from module imports
  draw      generate image from a dot file
  imports   find imports from a python module
  $ doit
  .  imports:requests.models
  .  imports:requests.__init__
  .  imports:requests.help
  (...)
  .  dot
  .  draw


Project Details
===============

 - Website & docs - http://pydoit.org
 - Project management on github - https://github.com/pydoit/doit
 - Discussion group - https://groups.google.com/forum/#!forum/python-doit
 - News/twitter - https://twitter.com/pydoit
 - Plugins, extensions and projects based on doit - https://github.com/pydoit/doit/wiki/powered-by-doit

license
=======

The MIT License
Copyright (c) 2008-2021 Eduardo Naufel Schettino

see LICENSE file


developers / contributors
==========================

see AUTHORS file


install
=======

*doit* is tested on python 3.6 to 3.10.

The last version supporting python 2 is version 0.29.

.. code:: bash

 $ pip install doit


dependencies
=============

- cloudpickle
- pyinotify (linux)
- macfsevents (mac)

Tools required for development:

- git * VCS
- py.test * unit-tests
- coverage * code coverage
- sphinx * doc tool
- pyflakes * syntax checker
- doit-py * helper to run dev tasks


development setup
==================

The best way to setup an environment to develop *doit* itself is to
create a virtualenv...

.. code:: bash

  doit$ virtualenv dev
  doit$ source dev/bin/activate

install ``doit`` as ""editable"", and add development dependencies
from `dev_requirements.txt`:

.. code:: bash

  (dev) doit$ pip install --editable .
  (dev) doit$ pip install --requirement dev_requirements.txt



tests
=======

Use py.test - http://pytest.org

.. code:: bash

  $ py.test



documentation
=============

``doc`` folder contains ReST documentation based on Sphinx.

.. code:: bash

 doc$ make html

They are the base for creating the website. The only difference is
that the website includes analytics tracking.
To create it (after installing *doit*):

.. code:: bash

 $ doit website



spell checking
--------------

All documentation is spell checked using the task `spell`:

.. code:: bash

  $ doit spell

It is a bit annoying that code snippets and names always fails the check,
these words must be added into the file `doc/dictionary.txt`.

The spell checker currently uses `hunspell`, to install it on debian based
systems install the hunspell package: `apt-get install hunspell`.


profiling
---------

.. code:: bash

  python -m cProfile -o output.pstats `which doit` list

  gprof2dot -f pstats output.pstats | dot -Tpng -o output.png


releases
========

Update version number at:

- doit/version.py
- setup.py
- doc/conf.py
- doc/index.html

.. code:: bash

   python setup.py sdist
   python setup.py bdist_wheel
   twine upload dist/doit-X.Y.Z.tar.gz
   twine upload dist/doit-X.Y.Z-py3-none-any.whl

Remember to push GIT tags::

  git push --tags



contributing
==============

On github create pull requests using a named feature branch.

Financial contribution to support maintenance welcome.

.. image:: https://opencollective.com/doit/tiers/backers.svg?avatarHeight=50
    :target: https://opencollective.com/doit/tiers
",2023-07-07 15:55:31+00:00
dotmesh,dotmesh,dotmesh-io/dotmesh,"dotmesh (dm) is like git for your data volumes (databases, files etc) in Docker and Kubernetes",https://dotmesh.com,False,538,2023-07-05 16:17:49+00:00,2017-06-18 17:49:52+00:00,28,19,10,22,release-0.8.1,2020-04-28 14:09:01+00:00,Apache License 2.0,2816,version-test,24,2018-02-20 10:35:40+00:00,,2020-04-28 14:09:01+00:00,"# dotmesh: git for data

[![pipeline status](https://gitlab.dotmesh.com/dotmesh/dotmesh/badges/master/pipeline.svg)](https://gitlab.dotmesh.com/dotmesh/dotmesh/commits/master)

Dotmesh is a **git-like CLI for capturing, organizing and sharing application states**.

In other words, it's a **snapshotting tool for databases** and other filesystem states.

The application states that dotmesh captures are stored in **datadots**.

It can capture the state of multiple databases, each one in a [subdot](https://docs.dotmesh.com/concepts/what-is-a-datadot/#subdots), in a single atomic commit.

## installing on docker (Mac or Ubuntu 16.04+)

Install the dotmesh client `dm`:

```plain
sudo curl -sSL -o /usr/local/bin/dm \
    https://get.dotmesh.io/$(uname -s)/dm
```

Make the client binary executable.
```plain
sudo chmod +x /usr/local/bin/dm
```

Then use the client to install `dotmesh-server`, assuming you have Docker installed and your user account has access to the Docker daemon.

```plain
dm cluster init
```

```plain
Checking suitable Docker is installed... yes, got 17.12.0-ce.
Checking dotmesh isn't running... done.
Pulling dotmesh-server docker image...
[...]
```

This will set up a single-instance cluster on your local machine.

Verify that the `dm` client can talk to the `dotmesh-server`:
```
dm version
```

If the installation fails, please [report an issue](https://github.com/dotmesh-io/dotmesh).
You can also experiment in our [online learning environment](https://dotmesh.com/try-dotmesh/).
Thanks!

See [the installation docs](https://docs.dotmesh.com/install-setup/) for more details, including installing dotmesh on Kubernetes.




## getting started guide

Try our [hosted tutorial](https://dotmesh.com/try-dotmesh/)!

Alternatively, try the [hello Dotmesh on Docker](https://docs.dotmesh.com/tutorials/hello-dotmesh-docker/) guided tutorial.

## what is a datadot?

A **datadot** allows you to capture your application's state and treat it like a `git` repo.

A simple example is to start a PostgreSQL container using a datadot called `myapp`:

```bash
docker run -d --volume-driver dm \
    -v myapp:/var/lib/postgresql/data --name postgres postgres:9.6.6
```

This creates a datadot called `myapp`, creates the writeable filesystem for the default `master` branch in that datadot, mounts the writeable filesystem for the `master` branch into `/var/lib/postgresql/data` in the `postgres` container, and starts the `postgres` container, like this:

![myapp dot with master branch and postgres container's /data volume attached](datadot.png ""Diagram of a datadot"")

First, switch to it, which, like `cd`'ing into a git repo, makes it the ""current"" dot -- the dot which later `dm` commands will operate on by default:

```bash
dm switch myapp
```

You can then see the `dm list` output:

```bash
dm list
```

```plain
  DOT      BRANCH  SERVER   CONTAINERS  SIZE       COMMITS  DIRTY
* myapp    master  a1b2c3d  /postgres   40.82 MiB  0        40.82 MiB
```
The current branch is shown in the `BRANCH` column and the current dot is marked with a `*` in the `dm list` output.

## what's next?

* Learn more in the [concept docs](https://docs.dotmesh.com/concepts/what-is-a-datadot/).
* Try another [tutorial](https://docs.dotmesh.com/tutorials/).

",2023-07-07 15:55:35+00:00
drake,drake,Factual/drake,"Data workflow tool, like a ""Make for data""",,False,1482,2023-06-19 12:46:12+00:00,2013-01-24 15:38:50+00:00,110,152,14,6,1.0.3,2016-04-15 22:26:45+00:00,Other,486,v1.0.3,8,2016-04-15 22:12:39+00:00,2023-06-19 12:46:11+00:00,2016-05-18 22:58:04+00:00,"# Drake

<img src=""http://gdurl.com/2jhD"" align=""right""/>

Drake is a simple-to-use, extensible, text-based data workflow tool that organizes command execution around data and its dependencies. Data processing steps are defined along with their inputs and outputs and Drake automatically resolves their dependencies and calculates:

 * which commands to execute (based on file timestamps)
 * in what order to execute the commands (based on dependencies)

Drake is similar to _GNU Make_, but designed especially for data workflow management. It has HDFS support, allows multiple inputs and outputs, and includes a host of features designed to help you bring sanity to your otherwise chaotic data processing workflows.

## Drake walk-through

If you like screencasts, check out this [Drake walk-through video](http://www.youtube.com/watch?v=BUgxmvpuKAs) recorded by Artem Boytsov, Drake's primary designer:

<a href=""http://www.youtube.com/watch?v=BUgxmvpuKAs"">
  <img src=""https://lh6.googleusercontent.com/-wOmqvTkHHk0/UQBnQaVcXJI/AAAAAAAAAC4/apFtmcPXCPQ/s800/Screen%2520Shot%25202013-01-23%2520at%25202.41.43%2520PM.png"" width=""320"" height=""195""/>
</a>

## Installation

Drake has been tested under Linux, Mac OS X and Windows 8. We've not tested it on other operating systems.

Drake installs itself on the first run of the `drake` shell script; there is no
separate install script.  Follow these instructions to install drake manually:

1. Make sure you have [Java](https://www.java.com) version 6 or later.
2. [Download the `drake` script from the `master` branch](https://raw.githubusercontent.com/Factual/drake/master/bin/drake)
 of this project.
3. Place the `drake` script on your `$PATH`. (`~/bin` is a good choice if it is on your path.)
4. Set it to be executable. (`chmod 755 ~/bin/drake`)
5. Run it (`drake`) 

### Homebrew

If you're on a Mac you can alternatively use [Homebrew](http://brew.sh/) to install Drake:
```
brew install drake
```

### Upgrade Drake

Starting with Drake version 1.0.0, once you have Drake installed you can easily upgrade your version of Drake by running `drake --upgrade`. The latest version of Drake will be downloaded and installed for you.

### Download or build the uberjar

You can build Drake from source or run from a prebuilt jar. [Detailed instructions](https://github.com/Factual/drake/wiki/Download-or-build-the-uberjar)


### Use Drake as a Clojure library

You can programmatically use Drake from your Clojure project by using [Drake's Clojure front end](https://github.com/Factual/drake/wiki/A-Clojure-Frontend-to-Drake). Your project.clj dependencies should include the latest Drake library, e.g.:

```clojure
[factual/drake ""1.0.3""]
```

### Faster startup time

The JVM startup time can be a nuisance. To reduce startup time, we recommend using the way cool [Drip](https://github.com/flatland/drip). Please see [the Drake with Drip](https://github.com/Factual/drake/wiki/Faster-startup:-Drake-with-Drip) wiki page.

## Basic Usage

The [wiki](https://github.com/Factual/drake/wiki) is the home for Drake's documentation, but here are simple notes on usage:

To build a specific target (and any out-of-date dependencies, if necessary):

```bash
$ drake mytarget
```

To build a target and everything that depends on it (a.k.a. ""down-tree"" mode):

```bash
$ drake ^mytarget
```

To build a specific target only, without any dependencies, up or down the tree:

```bash
$ drake =mytarget
```

To force build a target:

```bash
$ drake +mytarget
```

To force build a target and all its downtree dependencies:

```bash
$ drake +^mytarget
```

To force build the entire workflow:

```bash
$ drake +...
```

To exclude targets:

```bash
$ drake ... -sometarget -anothertarget
```

By default, Drake will look for `./Drakefile`. The simplest way to run your workflow is to name your workflow file `Drakefile`, and make sure you're in the same directory. Then, simply:

```bash
$ drake
```

To specify the workflow file explicitly, use `-w` or `--workflow`. E.g.:

```bash
$ drake -w /myworkflow/my-workflow.drake
```

Use `drake --help` for the full list of options.

## Documentation, etc.

The [wiki](https://github.com/Factual/drake/wiki) is the home for Drake's documentation.

A lot of work went into designing and specifying Drake. To prove it, here's [the 60 page specification and user manual](https://docs.google.com/document/d/1bF-OKNLIG10v_lMes_m4yyaJtAaJKtdK0Jizvi_MNsg/edit). It's stored in Google Docs, and we encourage everyone to use its superb commenting feature to provide feedback. Just select the text you want to comment on, and click Insert -> Comment (Ctrl + Alt + M on Windows, Cmd + Option + M on Mac). It can also be downloaded as a PDF.

There are annotated workflow examples in the demos directory.

There's a [Google Group for Drake](https://groups.google.com/forum/?fromgroups#!forum/drake-workflow) where you can ask questions. And if you found a bug or want to submit a feature request, go to [Drake's GitHub issues page](https://github.com/Factual/drake/issues?sort=created&state=open).

## Visualize your workflow
See more [detail](https://github.com/Factual/drake/wiki/Visualize-your-workflow)

<img src=""https://cloud.githubusercontent.com/assets/855457/7533038/509e37f8-f5a0-11e4-8c2e-8951272811af.png""/>

## Asynchronous Execution of Steps

Please see [the wiki page on async](https://github.com/Factual/drake/wiki/Async-Execution-of-Steps).

## Plugins

Drake has a plugin mechanism, allowing developers to publish and use custom plugins that extend Drake. See the [Plugin wiki page](https://github.com/Factual/drake/wiki/Plugins) for details.

## HDFS Compatibility

Drake provides HDFS support by allowing you to specify inputs and outputs like `hdfs:/my/big_file.txt`.

If you plan to use Drake with HDFS, please see [the wiki page on HDFS Compatibility](https://github.com/Factual/drake/wiki/HDFS-Compatibility).

## Amazon S3 Compatibility

Thanks to [Chris Howe](https://github.com/howech), Drake now has basic compatibility with Amazon S3 by allowing you to specify
inputs and outputs like `s3://bucket/path/to/object`.

If you plan to use Drake with S3, please see [the wiki doc on S3 Compatibility](https://github.com/Factual/drake/wiki/S3-Compatibility).

## Drake on the REPL

You can use Drake from your Clojure REPL, via `drake.core/run-workflow`. Please see [the Drake on the REPL wiki page](https://github.com/Factual/drake/wiki/Drake-on-the-REPL) for more details.

## Stuff outside this repo

Thanks to [Lars Yencken](https://github.com/larsyencken), we now have [Vim syntax support](https://bitbucket.org/larsyencken/vim-drake-syntax) for Drake:

<img src=""https://lh3.googleusercontent.com/-mqNpFqf7P0k/UQoXkpAqr1I/AAAAAAAAADU/U5zrvozVmzE/s400/image.png""/>

Also thanks to [Lars Yencken](https://github.com/larsyencken), [utilities for making life easier in Python with Drake workflows](https://pypi.python.org/pypi/drakeutil).

Courtesy of [@daguar](https://gist.github.com/daguar), an [alternative approach to installing Drake on Mac OS X](https://gist.github.com/daguar/5368778).

[Original blog post](http://blog.factual.com/introducing-drake-a-kind-of-make-for-data) announcing Drake's open source release

[An epic knock-down-drag-out set of threads on Hacker News](https://news.ycombinator.com/item?id=5110921) discussing the design merits of Drake



## License

Source Copyright © 2012-2015 Factual, Inc.

Distributed under the Eclipse Public License, the same as Clojure uses. See the file COPYING.
",2023-07-07 15:55:39+00:00
drakerpackage,drake,ropensci/drake,An R-focused pipeline toolkit for reproducibility and high-performance computing,https://docs.ropensci.org/drake,False,1328,2023-06-16 16:30:59+00:00,2017-02-20 22:28:40+00:00,132,35,37,57,7.13.5,2023-03-24 12:44:37+00:00,GNU General Public License v3.0,6730,v7.12.7,57,2020-10-27 17:58:10+00:00,2023-06-30 21:42:36+00:00,2023-05-21 15:47:54+00:00,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

<center>
<img src=""https://docs.ropensci.org/drake/reference/figures/infographic.svg"" alt=""infographic"" align=""center"" style = ""border: none; float: center;"">
</center>
<table class=""table"">
<thead>
<tr class=""header"">
<th align=""left"">
Usage
</th>
<th align=""left"">
Release
</th>
<th align=""left"">
Development
</th>
</tr>
</thead>
<tbody>
<tr class=""odd"">
<td align=""left"">
<a href=""https://www.gnu.org/licenses/gpl-3.0.en.html""><img src=""https://img.shields.io/badge/licence-GPL--3-blue.svg"" alt=""Licence""></a>
</td>
<td align=""left"">
<a href=""https://cran.r-project.org/package=drake""><img src=""https://www.r-pkg.org/badges/version/drake"" alt=""CRAN""></a>
</td>
<td align=""left"">
<a href=""https://github.com/ropensci/drake/actions?query=workflow%3Acheck""><img src=""https://github.com/ropensci/drake/workflows/check/badge.svg"" alt=""check""></a>
</td>
</tr>
<tr class=""even"">
<td align=""left"">
<a href=""https://cran.r-project.org/""><img src=""https://img.shields.io/badge/R%3E%3D-3.3.0-blue.svg"" alt=""minimal R version""></a>
</td>
<td align=""left"">
<a href=""https://cran.r-project.org/web/checks/check_results_drake.html""><img src=""https://cranchecks.info/badges/summary/drake"" alt=""cran-checks""></a>
</td>
<td align=""left"">
<a href=""https://github.com/ropensci/drake/actions?query=workflow%3Alint""><img src=""https://github.com/ropensci/drake/workflows/lint/badge.svg"" alt=""lint""></a>
</td>
</tr>
<tr class=""odd"">
<td align=""left"">
<a href=""https://CRAN.R-project.org/package=drake""><img src=""https://tinyverse.netlify.com/badge/drake""></a>
</td>
<td align=""left"">
<a href=""https://github.com/ropensci/software-review/issues/156""><img src=""https://badges.ropensci.org/156_status.svg"" alt=""rOpenSci""></a>
</td>
<td align=""left"">
<a href=""https://codecov.io/github/ropensci/drake?branch=main""><img src=""https://codecov.io/github/ropensci/drake/coverage.svg?branch=main"" alt=""Codecov""></a>
</td>
</tr>
<tr class=""even"">
<td align=""left"">
<a href=""https://CRAN.R-project.org/package=drake""><img src=""https://cranlogs.r-pkg.org/badges/drake"" alt=""downloads""></a>
</td>
<td align=""left"">
<a href=""https://doi.org/10.21105/joss.00550""><img src=""https://joss.theoj.org/papers/10.21105/joss.00550/status.svg"" alt=""JOSS""></a>
</td>
<td align=""left"">
<a href=""https://bestpractices.coreinfrastructure.org/projects/2135""><img src=""https://bestpractices.coreinfrastructure.org/projects/2135/badge""></a>
</td>
</tr>
<tr class=""odd"">
<td align=""left"">
</td>
<td align=""left"">
<a href=""https://zenodo.org/badge/latestdoi/82609103""><img src=""https://zenodo.org/badge/82609103.svg"" alt=""Zenodo""></a>
</td>
<td align=""left"">
<a href=""https://lifecycle.r-lib.org/articles/stages.html""><img src=""https://img.shields.io/badge/lifecycle-superseded-blue.svg"" alt='superseded lifecycle'></a>
</td>
</tr>
</tbody>
</table>
<br>

# drake is superseded. Consider targets instead.

As of 2021-01-21, `drake` is [superseded](https://lifecycle.r-lib.org/articles/stages.html). The [`targets`](https://docs.ropensci.org/targets/) R package is the long-term successor of `drake`, and it is more robust and easier to use. Please visit <https://books.ropensci.org/targets/drake.html> for full context and advice on transitioning.

# The drake R package <img src=""https://docs.ropensci.org/drake/reference/figures/logo.svg"" align=""right"" alt=""logo"" width=""120"" height = ""139"" style = ""border: none; float: right;"">

Data analysis can be slow. A round of scientific computation can take
several minutes, hours, or even days to complete. After it finishes, if
you update your code or data, your hard-earned results may no longer be
valid. How much of that valuable output can you keep, and how much do
you need to update? How much runtime must you endure all over again?

For projects in R, the `drake` package can help. It [analyzes your
workflow](https://books.ropensci.org/drake/plans.html), skips steps with
up-to-date results, and orchestrates the rest with [optional distributed
computing](https://books.ropensci.org/drake/hpc.html). At the end,
`drake` provides evidence that your results match the underlying code
and data, which increases your ability to trust your research.

# Video

## That Feeling of Workflowing (Miles McBain)

<center>

<a href=""https://www.youtube.com/embed/jU1Zv21GvT4"">
<img src=""https://docs.ropensci.org/drake/reference/figures/workflowing.png"" alt=""workflowing"" align=""center"" style = ""border: none; float: center;"">
</a>

</center>

(By [Miles McBain](https://github.com/MilesMcBain);
[venue](https://nyhackr.org/index.html),
[resources](https://github.com/MilesMcBain/nycr_meetup_talk))

## rOpenSci Community Call

<center>

<a href=""https://ropensci.org/commcalls/2019-09-24/"">
<img src=""https://docs.ropensci.org/drake/reference/figures/commcall.png"" alt=""commcall"" align=""center"" style = ""border: none; float: center;"">
</a>

</center>

([resources](https://ropensci.org/commcalls/2019-09-24/))

# What gets done stays done.

Too many data science projects follow a [Sisyphean
loop](https://en.wikipedia.org/wiki/Sisyphus):

1.  Launch the code.
2.  Wait while it runs.
3.  Discover an issue.
4.  Rerun from scratch.

For projects with long runtimes, this process gets tedious. But with
`drake`, you can automatically

1.  Launch the parts that changed since last time.
2.  Skip the rest.

# How it works

To set up a project, load your packages,

``` r
library(drake)
library(dplyr)
library(ggplot2)
library(tidyr)
#> 
#> Attaching package: 'tidyr'
#> The following objects are masked from 'package:drake':
#> 
#>     expand, gather
```

load your custom functions,

``` r
create_plot <- function(data) {
  ggplot(data) +
    geom_histogram(aes(x = Ozone)) +
    theme_gray(24)
}
```

check any supporting files (optional),

``` r
# Get the files with drake_example(""main"").
file.exists(""raw_data.xlsx"")
#> [1] TRUE
file.exists(""report.Rmd"")
#> [1] TRUE
```

and plan what you are going to do.

``` r
plan <- drake_plan(
  raw_data = readxl::read_excel(file_in(""raw_data.xlsx"")),
  data = raw_data %>%
    mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))),
  hist = create_plot(data),
  fit = lm(Ozone ~ Wind + Temp, data),
  report = rmarkdown::render(
    knitr_in(""report.Rmd""),
    output_file = file_out(""report.html""),
    quiet = TRUE
  )
)

plan
#> # A tibble: 5 x 2
#>   target   command                                                              
#>   <chr>    <expr_lst>                                                           
#> 1 raw_data readxl::read_excel(file_in(""raw_data.xlsx""))                        …
#> 2 data     raw_data %>% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TR…
#> 3 hist     create_plot(data)                                                   …
#> 4 fit      lm(Ozone ~ Wind + Temp, data)                                       …
#> 5 report   rmarkdown::render(knitr_in(""report.Rmd""), output_file = file_out(""re…
```

So far, we have just been setting the stage. Use `make()` or
[`r_make()`](https://books.ropensci.org/drake/projects.html#safer-interactivity)
to do the real work. Targets are built in the correct order regardless
of the row order of `plan`.

``` r
make(plan) # See also r_make().
#> ▶ target raw_data
#> ▶ target data
#> ▶ target fit
#> ▶ target hist
#> ▶ target report
```

Except for files like `report.html`, your output is stored in a hidden
`.drake/` folder. Reading it back is easy.

``` r
readd(data) # See also loadd().
#> # A tibble: 153 x 6
#>    Ozone Solar.R  Wind  Temp Month   Day
#>    <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>
#>  1  41       190   7.4    67     5     1
#>  2  36       118   8      72     5     2
#>  3  12       149  12.6    74     5     3
#>  4  18       313  11.5    62     5     4
#>  5  42.1      NA  14.3    56     5     5
#>  6  28        NA  14.9    66     5     6
#>  7  23       299   8.6    65     5     7
#>  8  19        99  13.8    59     5     8
#>  9   8        19  20.1    61     5     9
#> 10  42.1     194   8.6    69     5    10
#> # … with 143 more rows
```

You may look back on your work and see room for improvement, but it’s
all good\! The whole point of `drake` is to help you go back and change
things quickly and painlessly. For example, we forgot to give our
histogram a bin width.

``` r
readd(hist)
#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
```

![](man/figures/unnamed-chunk-9-1.png)<!-- -->

So let’s fix the plotting function.

``` r
create_plot <- function(data) {
  ggplot(data) +
    geom_histogram(aes(x = Ozone), binwidth = 10) +
    theme_gray(24)
}
```

`drake` knows which results are affected.

``` r
vis_drake_graph(plan) # See also r_vis_drake_graph().
```

<img src=""https://docs.ropensci.org/drake/reference/figures/graph.png"" alt=""hist1"" align=""center"" style = ""border: none; float: center;"" width = ""600px"">

The next `make()` just builds `hist` and `report.html`. No point in
wasting time on the data or model.

``` r
make(plan) # See also r_make().
#> ▶ target hist
#> ▶ target report
```

``` r
loadd(hist)
hist
```

![](man/figures/unnamed-chunk-13-1.png)<!-- -->

# Reproducibility with confidence

The R community emphasizes reproducibility. Traditional themes include
[scientific
replicability](https://en.wikipedia.org/wiki/Replication_crisis),
literate programming with [knitr](https://yihui.org/knitr/), and
version control with
[git](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control).
But internal consistency is important too. Reproducibility carries the
promise that your output matches the code and data you say you used.
With the exception of [non-default
triggers](https://books.ropensci.org/drake/triggers.html) and [hasty
mode](https://books.ropensci.org/drake/hpc.html#hasty-mode), `drake`
strives to keep this promise.

## Evidence

Suppose you are reviewing someone else’s data analysis project for
reproducibility. You scrutinize it carefully, checking that the datasets
are available and the documentation is thorough. But could you re-create
the results without the help of the original author? With `drake`, it is
quick and easy to find out.

``` r
make(plan) # See also r_make().
#> ℹ unloading 1 targets from environment
#> ✓ All targets are already up to date.

outdated(plan) # See also r_outdated().
#> character(0)
```

With everything already up to date, you have **tangible evidence** of
reproducibility. Even though you did not re-create the results, you know
the results are recreatable. They **faithfully show** what the code is
producing. Given the right [package
environment](https://rstudio.github.io/packrat/) and [system
configuration](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/sessionInfo.html),
you have everything you need to reproduce all the output by yourself.

## Ease

When it comes time to actually rerun the entire project, you have much
more confidence. Starting over from scratch is trivially easy.

``` r
clean()    # Remove the original author's results.
make(plan) # Independently re-create the results from the code and input data.
#> ▶ target raw_data
#> ▶ target data
#> ▶ target fit
#> ▶ target hist
#> ▶ target report
```

## Big data efficiency

Select specialized data formats to increase speed and reduce memory
consumption. In version 7.5.2.9000 and above, the available formats are
[“fst”](https://github.com/fstpackage/fst) for data frames (example
below) and “keras” for [Keras](https://tensorflow.rstudio.com/) models
([example here](https://books.ropensci.org/drake/churn.html#plan)).

``` r
library(drake)
n <- 1e8 # Each target is 1.6 GB in memory.
plan <- drake_plan(
  data_fst = target(
    data.frame(x = runif(n), y = runif(n)),
    format = ""fst""
  ),
  data_old = data.frame(x = runif(n), y = runif(n))
)
make(plan)
#> target data_fst
#> target data_old
build_times(type = ""build"")
#> # A tibble: 2 x 4
#>   target   elapsed              user                 system    
#>   <chr>    <Duration>           <Duration>           <Duration>
#> 1 data_fst 13.93s               37.562s              7.954s    
#> 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s
```

## History and provenance

As of version 7.5.2, `drake` tracks the history and provenance of your
targets: what you built, when you built it, how you built it, the
arguments you used in your function calls, and how to get the data back.
(Disable with `make(history = FALSE)`)

``` r
history <- drake_history(analyze = TRUE)
history
#> # A tibble: 12 x 11
#>    target current built exists hash  command   seed runtime na.rm quiet
#>    <chr>  <lgl>   <chr> <lgl>  <chr> <chr>    <int>   <dbl> <lgl> <lgl>
#>  1 data   TRUE    2020… TRUE   11e2… ""raw_d… 1.29e9 0.011   TRUE  NA   
#>  2 data   TRUE    2020… TRUE   11e2… ""raw_d… 1.29e9 0.00400 TRUE  NA   
#>  3 fit    TRUE    2020… TRUE   3c87… ""lm(Oz… 1.11e9 0.006   NA    NA   
#>  4 fit    TRUE    2020… TRUE   3c87… ""lm(Oz… 1.11e9 0.002   NA    NA   
#>  5 hist   FALSE   2020… TRUE   88ae… ""creat… 2.10e8 0.011   NA    NA   
#>  6 hist   TRUE    2020… TRUE   0304… ""creat… 2.10e8 0.003   NA    NA   
#>  7 hist   TRUE    2020… TRUE   0304… ""creat… 2.10e8 0.009   NA    NA   
#>  8 raw_d… TRUE    2020… TRUE   855d… ""readx… 1.20e9 0.02    NA    NA   
#>  9 raw_d… TRUE    2020… TRUE   855d… ""readx… 1.20e9 0.0330  NA    NA   
#> 10 report TRUE    2020… TRUE   5504… ""rmark… 1.30e9 1.31    NA    TRUE 
#> 11 report TRUE    2020… TRUE   5504… ""rmark… 1.30e9 0.413   NA    TRUE 
#> 12 report TRUE    2020… TRUE   5504… ""rmark… 1.30e9 0.475   NA    TRUE 
#> # … with 1 more variable: output_file <chr>
```

Remarks:

  - The `quiet` column appears above because one of the `drake_plan()`
    commands has `knit(quiet = TRUE)`.
  - The `hash` column identifies all the previous versions of your
    targets. As long as `exists` is `TRUE`, you can recover old data.
  - Advanced: if you use `make(cache_log_file = TRUE)` and put the cache
    log file under version control, you can match the hashes from
    `drake_history()` with the `git` commit history of your code.

Let’s use the history to recover the oldest histogram.

``` r
hash <- history %>%
  filter(target == ""hist"") %>%
  pull(hash) %>%
  head(n = 1)
cache <- drake_cache()
cache$get_value(hash)
#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
```

![](man/figures/unnamed-chunk-18-1.png)<!-- -->

## Independent replication

With even more evidence and confidence, you can invest the time to
independently replicate the original code base if necessary. Up until
this point, you relied on basic `drake` functions such as `make()`, so
you may not have needed to peek at any substantive author-defined code
in advance. In that case, you can stay usefully ignorant as you
reimplement the original author’s methodology. In other words, `drake`
could potentially improve the integrity of independent replication.

## Readability and transparency

Ideally, independent observers should be able to read your code and
understand it. `drake` helps in several ways.

  - The [drake
    plan](https://docs.ropensci.org/drake/reference/drake_plan.html)
    explicitly outlines the steps of the analysis, and
    [`vis_drake_graph()`](https://docs.ropensci.org/drake/reference/vis_drake_graph.html)
    visualizes how those steps depend on each other.
  - `drake` takes care of the parallel scheduling and high-performance
    computing (HPC) for you. That means the HPC code is no longer
    tangled up with the code that actually expresses your ideas.
  - You can [generate large collections of
    targets](https://books.ropensci.org/drake/gsp.html) without
    necessarily changing your code base of imported functions, another
    nice separation between the concepts and the execution of your
    workflow

# Scale up and out.

Not every project can complete in a single R session on your laptop.
Some projects need more speed or computing power. Some require a few
local processor cores, and some need large high-performance computing
systems. But parallel computing is hard. Your tables and figures depend
on your analysis results, and your analyses depend on your datasets, so
some tasks must finish before others even begin. `drake` knows what to
do. Parallelism is implicit and automatic. See the [high-performance
computing guide](https://books.ropensci.org/drake/hpc.html) for all the
details.

``` r
# Use the spare cores on your local machine.
make(plan, jobs = 4)

# Or scale up to a supercomputer.
drake_hpc_template_file(""slurm_clustermq.tmpl"") # https://slurm.schedmd.com/
options(
  clustermq.scheduler = ""clustermq"",
  clustermq.template = ""slurm_clustermq.tmpl""
)
make(plan, parallelism = ""clustermq"", jobs = 4)
```

# With Docker

`drake` and Docker are compatible and complementary. Here are some
examples that run `drake` inside a Docker image.

  - [`drake-gitlab-docker-example`](https://gitlab.com/ecohealthalliance/drake-gitlab-docker-example):
    A small pedagogical example workflow that leverages `drake`, Docker,
    GitLab, and continuous integration in a reproducible analysis
    pipeline. Created by [Noam Ross](https://www.noamross.net/).
  - [`pleurosoriopsis`](https://github.com/joelnitta/pleurosoriopsis):
    The workflow that supports [Ebihara *et al.* 2019. “Growth Dynamics
    of the Independent Gametophytes of *Pleurorosiopsis makinoi*
    (Polypodiaceae)” *Bulletin of the National Science Museum Series B
    (Botany)*
    45:77-86.](https://www.kahaku.go.jp/research/publication/botany.html).
    Created by [Joel Nitta](https://github.com/joelnitta).

Alternatively, it is possible to run `drake` outside Docker and use the
[`future`](https://github.com/HenrikBengtsson/future) package to send
targets to a Docker image. `drake`’s
[`Docker-psock`](https://github.com/wlandau/drake-examples/tree/main/Docker-psock)
example demonstrates how. Download the code with
`drake_example(""Docker-psock"")`.

# Installation

You can choose among different versions of `drake`. The CRAN release
often lags behind the [online manual](https://books.ropensci.org/drake/)
but may have fewer bugs.

``` r
# Install the latest stable release from CRAN.
install.packages(""drake"")

# Alternatively, install the development version from GitHub.
install.packages(""devtools"")
library(devtools)
install_github(""ropensci/drake"")
```

# Function reference

The [reference
section](https://docs.ropensci.org/drake/reference/index.html) lists all
the available functions. Here are the most important ones.

  - `drake_plan()`: create a workflow data frame (like `my_plan`).
  - `make()`: build your project.
  - `drake_history()`: show what you built, when you built it, and the
    function arguments you used.
  - `r_make()`: launch a fresh
    [`callr::r()`](https://github.com/r-lib/callr) process to build your
    project. Called from an interactive R session, `r_make()` is more
    reproducible than `make()`.
  - `loadd()`: load one or more built targets into your R session.
  - `readd()`: read and return a built target.
  - `vis_drake_graph()`: show an interactive visual network
    representation of your workflow.
  - `recoverable()`: Which targets can we salvage using `make(recover =
    TRUE)` (experimental).
  - `outdated()`: see which targets will be built in the next `make()`.
  - `deps_code()`: check the dependencies of a command or function.
  - `drake_failed()`: list the targets that failed to build in the last
    `make()`.
  - `diagnose()`: return the full context of a build, including errors,
    warnings, and messages.

# Documentation

## Core concepts

The following resources explain what `drake` can do and how it works.
The workshop at `https://github.com/wlandau/learndrake`
devotes particular attention to `drake`’s mental model.

  - The [user manual](https://books.ropensci.org/drake/)
  - [`drakeplanner`](https://github.com/wlandau/drakeplanner), an
    R/Shiny app to help learn `drake` and create new projects. Run
    locally with `drakeplanner::drakeplanner()` or access it at
    <https://wlandau.shinyapps.io/drakeplanner>.
  - ``https://github.com/wlandau/learndrake`, an R package
    for teaching an extended `drake` workshop. It contains notebooks,
    slides, Shiny apps, the latter two of which are publicly deployed.
    See `https://github.com/wlandau/learndrake/blob/main/README.md`
    for instructions and links.

## In practice

  - [Miles McBain](https://github.com/MilesMcBain)’s [excellent blog
    post](https://milesmcbain.xyz/the-drake-post/) explains the
    motivating factors and practical issues {drake} solves for most
    projects, how to set up a project as quickly and painlessly as
    possible, and how to overcome common obstacles.
  - Miles’ [`dflow`](https://github.com/MilesMcBain/dflow) package
    generates the file structure for a boilerplate `drake` project. It
    is a more thorough alternative to `drake::use_drake()`.
  - `drake` is heavily function-oriented by design, and Miles’
    [`fnmate`](https://github.com/MilesMcBain/fnmate) package
    automatically generates boilerplate code and docstrings for
    functions you mention in `drake` plans.

## Reference

  - The [reference website](https://docs.ropensci.org/drake/).
  - The [official repository of example
    code](https://github.com/wlandau/drake-examples). Download an
    example workflow from here with `drake_example()`.
  - Presentations and workshops by [Will
    Landau](https://github.com/wlandau), [Kirill
    Müller](https://github.com/krlmlr), [Amanda
    Dobbyn](https://github.com/aedobbyn), [Karthik
    Ram](https://github.com/karthik), [Sina
    Rüeger](https://github.com/sinarueeger), [Christine
    Stawitz](https://github.com/cstawitz), and others. See specific
    links at <https://books.ropensci.org/drake/index.html#presentations>
  - The [FAQ page](https://books.ropensci.org/drake/faq.html), which
    links to [appropriately-labeled issues on
    GitHub](https://github.com/ropensci/drake/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22frequently+asked+question%22+).

## Use cases

The official [rOpenSci use
cases](https://discuss.ropensci.org/c/usecases/10) and [associated
discussion threads](https://discuss.ropensci.org/c/usecases/10) describe
applications of `drake` in the real world. Many of these use cases are
linked from the [`drake` tag on the rOpenSci discussion
forum](https://discuss.ropensci.org/tag/drake).

Here are some additional applications of `drake` in real-world projects.

  - [efcaguab/demografia-del-voto](https://github.com/efcaguab/demografia-del-voto)
  - [efcaguab/great-white-shark-nsw](https://github.com/efcaguab/great-white-shark-nsw)
  - [IndianaCHE/Detailed-SSP-Reports](https://github.com/IndianaCHE/Detailed-SSP-Reports)
  - [joelnitta/pleurosoriopsis](https://github.com/joelnitta/pleurosoriopsis)
  - [pat-s/pathogen-modeling](https://github.com/pat-s/pathogen-modeling)
  - [sol-eng/tensorflow-w-r](https://github.com/sol-eng/tensorflow-w-r)
  - [tiernanmartin/home-and-hope](https://github.com/tiernanmartin/home-and-hope)

## `drake` projects as R packages

Some folks like to structure their `drake` workflows as R packages.
Examples are below. In your own analysis packages, be sure to call
`drake::expose_imports(yourPackage)` so `drake` can watch you package’s
functions for changes and rebuild downstream targets accordingly.

  - [b-rodrigues/coolmlproject](https://github.com/b-rodrigues/coolmlproject)
  - [tiernanmartin/drakepkg](https://github.com/tiernanmartin/drakepkg)

# Help and troubleshooting

The following resources document many known issues and challenges.

  - [Frequently-asked
    questions](https://github.com/ropensci/drake/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22frequently+asked+question%22+).
  - [Debugging and testing drake
    projects](https://books.ropensci.org/drake/debugging.html)
  - [Other known issues](https://github.com/ropensci/drake/issues)
    (please search both open and closed ones).

If you are still having trouble, please submit a [new
issue](https://github.com/ropensci/drake/issues/new) with a bug report
or feature request, along with a minimal reproducible example where
appropriate.

The GitHub issue tracker is mainly intended for bug reports and feature
requests. While questions about usage etc. are also highly encouraged,
you may alternatively wish to post to [Stack
Overflow](https://stackoverflow.com) and use the [`drake-r-package`
tag](https://stackoverflow.com/tags/drake-r-package).

# Contributing

Development is a community effort, and we encourage participation.
Please read
[CONTRIBUTING.md](https://github.com/ropensci/drake/blob/main/CONTRIBUTING.md)
for details.

# Similar work

`drake` enhances reproducibility and high-performance computing, but not
in all respects. [Literate programming](https://rmarkdown.rstudio.com/),
[local library managers](https://rstudio.github.io/packrat/),
[containerization](https://www.docker.com/), and [strict session
managers](https://github.com/tidyverse/reprex) offer more robust
solutions in their respective domains. And for the problems `drake`
*does* solve, it stands on the shoulders of the giants that came before.

## Pipeline tools

### GNU Make

The original idea of a time-saving reproducible build system extends
back at least as far as [GNU Make](https://www.gnu.org/software/make/),
which still aids the work of [data
scientists](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/)
as well as the original user base of complied language programmers. In
fact, the name “drake” stands for “Data Frames in R for Make”.
[Make](https://kbroman.org/minimal_make/) is used widely in reproducible
research. Below are some examples from [Karl Broman’s
website](https://kbroman.org/minimal_make/).

  - Bostock, Mike (2013). “A map of flowlines from NHDPlus.”
    <https://github.com/mbostock/us-rivers>. Powered by the Makefile at
    <https://github.com/mbostock/us-rivers/blob/master/Makefile>.
  - Broman, Karl W (2012). “Halotype Probabilities in Advanced
    Intercross Populations.” *G3* 2(2), 199-202.Powered by the
    `Makefile` at
    <https://github.com/kbroman/ailProbPaper/blob/master/Makefile>.
  - Broman, Karl W (2012). “Genotype Probabilities at Intermediate
    Generations in the Construction of Recombinant Inbred Lines.”
    \*Genetics 190(2), 403-412. Powered by the Makefile at
    <https://github.com/kbroman/preCCProbPaper/blob/master/Makefile>.
  - Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and
    Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a
    Phylogenetic Tree.” *Genetics* 192(2), 267-279. Powered by the
    `Makefile` at
    <https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile>.

Whereas [GNU Make](https://www.gnu.org/software/make/) is
language-agnostic, `drake` is fundamentally designed for R.

  - Instead of a
    [Makefile](https://github.com/kbroman/preCCProbPaper/blob/master/Makefile),
    `drake` supports an R-friendly [domain-specific
    language](https://books.ropensci.org/drake/plans.html#large-plans)
    for declaring targets.
  - Targets in [GNU Make](https://www.gnu.org/software/make/) are files,
    whereas targets in `drake` are arbitrary variables in memory.
    (`drake` does have opt-in support for files via `file_out()`,
    `file_in()`, and `knitr_in()`.) `drake` caches these objects in its
    own [storage system](https://github.com/richfitz/storr) so R users
    rarely have to think about output files.

### Remake

[remake](https://github.com/richfitz/remake) itself is no longer
maintained, but its founding design goals and principles live on through
[drake](https://github.com/ropensci/drake). In fact,
[drake](https://github.com/ropensci/drake) is a direct re-imagining of
[remake](https://github.com/richfitz/remake) with enhanced scalability,
reproducibility, high-performance computing, visualization, and
documentation.

### Factual’s Drake

[Factual’s Drake](https://github.com/Factual/drake) is similar in
concept, but the development effort is completely unrelated to the
[drake R package](https://github.com/ropensci/drake).

### Other pipeline tools

There are [countless other successful pipeline
toolkits](https://github.com/pditommaso/awesome-pipeline). The `drake`
package distinguishes itself with its R-focused approach,
Tidyverse-friendly interface, and a [thorough selection of parallel
computing technologies and scheduling
algorithms](https://books.ropensci.org/drake/hpc.html).

## Memoization

Memoization is the strategic caching of the return values of functions.
It is a lightweight approach to the core problem that `drake` and other
pipeline tools are trying to solve. Every time a memoized function is
called with a new set of arguments, the return value is saved for future
use. Later, whenever the same function is called with the same
arguments, the previous return value is salvaged, and the function call
is skipped to save time. The
[`memoise`](https://github.com/r-lib/memoise) package is the primary
implementation of memoization in R.

Memoization saves time for small projects, but it arguably does not go
far enough for large reproducible pipelines. In reality, the return
value of a function depends not only on the function body and the
arguments, but also on any nested functions and global variables, the
dependencies of those dependencies, and so on upstream. `drake` tracks
this deeper context, while [memoise](https://github.com/r-lib/memoise)
does not.

## Literate programming

[Literate programming](https://rmarkdown.rstudio.com/) is the practice
of narrating code in plain vernacular. The goal is to communicate the
research process clearly, transparently, and reproducibly. Whereas
commented code is still mostly code, literate
[knitr](https://yihui.org/knitr/) / [R
Markdown](https://rmarkdown.rstudio.com/) reports can become websites,
presentation slides, lecture notes, serious scientific manuscripts, and
even books.

### knitr and R Markdown

`drake` and [knitr](https://yihui.org/knitr/) are symbiotic. `drake`’s
job is to manage large computation and orchestrate the demanding tasks
of a complex data analysis pipeline.
[knitr](https://yihui.org/knitr/)’s job is to communicate those
expensive results after `drake` computes them.
[knitr](https://yihui.org/knitr/) / [R
Markdown](https://rmarkdown.rstudio.com/) reports are small pieces of an
overarching `drake` pipeline. They should focus on communication, and
they should do as little computation as possible.

To insert a [knitr](https://yihui.org/knitr/) report in a `drake`
pipeline, use the `knitr_in()` function inside your [`drake`
plan](https://books.ropensci.org/drake/plans.html), and use `loadd()`
and `readd()` to refer to targets in the report itself. See an [example
here](https://github.com/wlandau/drake-examples/tree/main/main).

### Version control

`drake` is not a version control tool. However, it is fully compatible
with [`git`](https://git-scm.com/),
[`svn`](https://en.wikipedia.org/wiki/Apache_Subversion), and similar
software. In fact, it is good practice to use
[`git`](https://git-scm.com/) alongside `drake` for reproducible
workflows.

However, data poses a challenge. The datasets created by `make()` can
get large and numerous, and it is not recommended to put the `.drake/`
cache or the `.drake_history/` logs under version control. Instead, it
is recommended to use a data storage solution such as
DropBox or [OSF](https://osf.io/ka7jv/wiki/home/).

### Containerization and R package environments

`drake` does not track R packages or system dependencies for changes.
Instead, it defers to tools like [Docker](https://www.docker.com),
[Singularity](https://sylabs.io/singularity/),
[`renv`](https://github.com/rstudio/renv), and
[`packrat`](https://github.com/rstudio/packrat), which create
self-contained portable environments to reproducibly isolate and ship
data analysis projects. `drake` is fully compatible with these tools.

### workflowr

The `workflowr` package is a
project manager that focuses on literate programming, sharing over the
web, file organization, and version control. Its brand of
reproducibility is all about transparency, communication, and
discoverability. For an example of
`workflowr` and `drake`
working together, see [this machine learning
project](https://github.com/pat-s/2019-feature-selection) by [Patrick
Schratz](https://github.com/pat-s).

# Citation

``` r
citation(""drake"")
#> 
#> To cite drake in publications use:
#> 
#>   William Michael Landau, (2018). The drake R package: a pipeline
#>   toolkit for reproducibility and high-performance computing. Journal
#>   of Open Source Software, 3(21), 550,
#>   https://doi.org/10.21105/joss.00550
#> 
#> A BibTeX entry for LaTeX users is
#> 
#>   @Article{,
#>     title = {The drake R package: a pipeline toolkit for reproducibility and high-performance computing},
#>     author = {William Michael Landau},
#>     journal = {Journal of Open Source Software},
#>     year = {2018},
#>     volume = {3},
#>     number = {21},
#>     url = {https://doi.org/10.21105/joss.00550},
#>   }
```

# Acknowledgements

Special thanks to [Jarad Niemi](https://www.jarad.me/), my advisor from
[graduate school](https://www.stat.iastate.edu/), for first introducing me
to the idea of [Makefiles](https://www.gnu.org/software/make/) for
research. He originally set me down the path that led to `drake`.

Many thanks to [Julia Lowndes](https://github.com/jules32), [Ben
Marwick](https://github.com/benmarwick), and [Peter
Slaughter](https://github.com/gothub) for [reviewing drake for
rOpenSci](https://github.com/ropensci/software-review/issues/156), and to
[Maëlle Salmon](https://github.com/maelle) for such active involvement
as the editor. Thanks also to the following people for contributing
early in development.

  - [Alex Axthelm](https://github.com/AlexAxthelm)
  - [Chan-Yub Park](https://github.com/mrchypark)
  - [Daniel Falster](https://github.com/dfalster)
  - [Eric Nantz](https://github.com/rpodcast)
  - [Henrik Bengtsson](https://github.com/HenrikBengtsson)
  - [Ian Watson](https://github.com/IanAWatson)
  - [Jasper Clarkberg](https://github.com/dapperjapper)
  - [Kendon Bell](https://github.com/kendonB)
  - [Kirill Müller](https://github.com/krlmlr)
  - [Michael Schubert](https://github.com/mschubert)

Credit for images is [attributed
here](https://github.com/ropensci/drake/blob/main/man/figures/image-credit.md).

[![ropensci\_footer](https://ropensci.org/public_images/github_footer.png)](https://ropensci.org)
",2023-07-07 15:55:43+00:00
dray,dray,CenturyLinkLabs/dray,An engine for managing the execution of container-based workflows.,http://Dray.it,False,381,2023-05-25 04:01:39+00:00,2014-12-23 22:38:32+00:00,39,34,4,0,,,Apache License 2.0,55,v0.10.0,4,2015-03-19 15:37:33+00:00,2023-05-25 04:01:39+00:00,2020-01-24 17:34:57+00:00,"# dray

![Dray Logo](http://www.centurylinklabs.com/wp-content/uploads/2015/03/dray-600x360.jpg)

[![Circle CI](https://circleci.com/gh/CenturyLinkLabs/dray.svg?style=svg)](https://circleci.com/gh/CenturyLinkLabs/dray)
[![GoDoc](http://godoc.org/github.com/CenturyLinkLabs/dray?status.png)](http://godoc.org/github.com/CenturyLinkLabs/dray)
[![Docker Hub](https://img.shields.io/badge/docker-ready-blue.svg)](https://registry.hub.docker.com/u/centurylink/dray/)
[![](https://badge.imagelayers.io/centurylink/dray.svg)](https://imagelayers.io/?images=centurylink/dray:latest 'Get your own badge on imagelayers.io')
[![Analytics](https://ga-beacon.appspot.com/UA-49491413-7/dray/README?pixel)](https://github.com/CenturyLinkLabs/dray)

An engine for managing the execution of container-based workflows.

Most common Docker use cases involve using containers for hosting long-running
services. These are things like a web application, database or message queue -- services
that are running continuously, waiting to service requests.

Another interesting use case for Docker is to wrap short-lived, single-purpose tasks.
Perhaps it's a Ruby app that needs to be execute periodically or a set of bash scripts 
that need to be executed in sequence. Much like the services described above, these things
can be wrapped in a Docker container to provide an isolated execution environment. The only
real difference is that the task containers exit when they've finished their work while the
service containers run until they are explicitly stopped.

Once you start using task containers, you may find it useful to execute a set of these containers
together in sequence. Maybe you want to string together a set of tasks and have
the output of one container feed the input of the next container. Something like unix pipes:

    cat customers.txt | sort | uniq | wc -l

This is the service that Dray provides. Dray allows you to define a serial workflow, or job, as a 
list of Docker containers with each container encapsulating a step in the workflow. Dray 
will ensure that each step of the workflow (each container) is started in the correct 
order and handles the work of marshaling data between the different steps.

## NOTE

This repo is no longer being maintained. Users are welcome to fork it, but we make no warranty of its functionality.

## Overview
Dray is a Go application that provides a RESTful API for managing jobs. A job is simply a list of Docker containers to be executed in sequence that is posted to Dray as a JSON document:

	{  
	  ""name"":""Word Job"",
	  ""steps"":[  
	    {  
	      ""source"":""centurylink/randword""
	    },
	    {  
	      ""source"":""centurylink/upper""
	    },
	    {  
	      ""source"":""centurylink/reverse""
	    }
	  ]
	}

The JSON above describes a job named ""Word Job"" which consists of three steps. Each step references the name of a Docker image to be executed.

When receiving this job description, Dray will immediately return a response containing an ID for the job and then execute the ""centurylink/randword"" image . As the container is executing Dray will capture any data written to the container's *stdout* stream so that it can be passed along to the next step in the list (there are other output channels you can use, but *stdout* is the default).

Once the ""randword"" container exits, Dray will inspect the exit code for the container. If, and only if, the exit code is zero, Dray will start the ""centurylink/upper"" container and pass any data captured in the previous step to that container's *stdin* stream.

Dray will continue executing each of the steps in this manner, marshalling the *stdout* of one step to the *stdin* of the next step, until all of the steps have been completed (or until one of the steps exits with a non-zero exit code).

That status of a running job can be queried at any point by hitting Dray's `/jobs/(id)` endpoint. Additionally, any output generated by the job can be viewed by querying the `/jobs/(id)/log` endpoint.

Note that the example above is a working job description that you can execute on your own Dray installation -- each of the referenced images can be found on the Docker Hub.

## Running

Dray is packaged as a small Docker image and can easily be executed with the Docker *run* command.

Dray relies on [Redis](http://redis.io/) for persisting information about jobs so you'll first need to start one of the [numerous](https://registry.hub.docker.com/search?q=redis&searchfield=) Redis Docker images. In the example below we're simply using the [official Redis image](https://registry.hub.docker.com/_/redis/):

    docker run -d --name redis redis
    
Once Redis is running, you can start the Dray container with the following:

    docker run -d --name dray \
      --link redis:redis \
      -v /var/run/docker.sock:/var/run/docker.sock \
      -p 3000:3000 \
      centurylink/dray:latest

The Dray container must be linked to the Redis container using the `--link` flag so that Dray can find the correct Redis endpoint. The Redis container can be named anything you like, but the alias used in the `--link` flag must be ""redis"".

Since Dray interacts with the Docker API in order launch containers it needs access to the Docker API socket. When starting the container, the `-v` flag needs to be used to make the Docker socket available inside the container.

In the example above, the `-p` flag is used to map the Dray API endpoint
(listening on port 3000 in the container) to port 3000 on the host machine. In
situations where you don't need a mapped port (like when linking another
container to the Dray container) the `-p` flag can be omitted.

If you'd like to use [Docker Compose](https://docs.docker.com/compose/) to start Dray, the following `docker-compose.yml` is equivalent to the steps shown above

	dray:                                                                                                                   
	  image: centurylink/dray
	  links:
	   - redis
	  volumes:
	   - /var/run/docker.sock:/var/run/docker.sock
	  ports:
	   - ""3000:3000""
	redis:
	  image: redis
	  
With this `docker-compose.yml` file you can start Redis and Dray by simply issuing a `docker-compose up -d` command.

### Configuration
The Dray service can be configured by injecting environment variables into the container when it is started. At this time, Dray supports the following configuration variables:

* `LOG_LEVEL` - Valid values are ""panic"", ""fatal"", ""error"", ""warn"", ""info"" and ""debug"". By default, Dray writes messages at and above the ""info"" level. To increase the amount of logging, set the log level to ""debug"".

Environment variables can be passed to the Dray container by using the `-e` flag as part of the Docker *run* command:

    docker run -d --name dray \
      --link redis:redis \
      -e LOG_LEVEL=debug \
      -v /var/run/docker.sock:/var/run/docker.sock \
      -p 3000:3000 \
      centurylink/dray:latest
      
## Example
Below is an actual Dray job description that is being used as part of the [Panamax](http://panamax.io/) project. The goal of this job is to provision a cluster of servers on AWS and then install some software on those servers.

	{  
	  ""name"":""aws=fleet"",
	  ""environment"":[  
	    { ""variable"":""AWS_ACCESS_KEY_ID"", ""value"":""xxxxxx"" },
	    { ""variable"":""AWS_SECRET_ACCESS_KEY"", ""value"":""xxxxxxx"" },
	    { ""variable"":""REGION"", ""value"":""us-west-2a"" },
	    { ""variable"":""NODE_COUNT"", ""value"":""2"" },
	    { ""variable"":""VM_SIZE"", ""value"":""t2.small"" },
	    { ""variable"":""REMOTE_TARGET_NAME"", ""value"":""AWS - Fleet-CoreOS"" }
	  ],
	  ""steps"":[  
	    {  
	      ""name"":""Step 1"",
	      ""source"":""centurylink/cluster-deploy:aws.fleet""
	    },
	    {  
	      ""name"":""Step 2"",
	      ""source"":""centurylink/cluster-deploy:agent""
	    },
	    {  
	      ""name"":""Step 3"",
	      ""source"":""centurylink/remote-agent-install:latest""
	    }
	  ]
	}

This job uses environment variables to pass a bunch of configuration data into the different steps. Things like the AWS credentials and node count can be passed-in at run-time instead of being hard-coded into the images themselves.

This job uses Dray's data marshalling to pass information between the different steps. Step 1 provisions a cluster of virtual serves and the IP addresses of those servers are needed in step 2. The first step simply writes those IP addresses to the *stdout* stream where they are captured by Dray and passed to the *stdin* stream of the second step.

The way this job is structured, job templates can be created for different cloud providers by simply swapping-out the provider-specific steps and changing some environment variables.

## API
Dray jobs are created and monitored using the API endpoints described below.

### Create Job

    POST /jobs
    
Submits a new job for execution. The execution of the job happens asynchronous to the API call -- the API will respond immediately while execution happens in the background. 

The response body will echo back the submitted job description including the ID assigned to the job. The returned job ID can be used to retrieve information about the job using either the `/jobs/(id)` or `/jobs/(id)/log` endpoints.

**Input:**

*job*

* `name` (`string`) - **Optional.** Name of job.
* `environment` (`array` of `envVar`) - **Optional.** List of environment variables. Environment variables specified at the job level will be injected into **all** job steps.
* `steps` (`array` of `step`) - **Required.** List of job steps.

*envVar*

* `variable` (`string`) - **Required.** Name of the environment variable.
* `value` (`string`) - **Required.** Value of the environment variable.

*step*

* `name` (`string`) - **Optional.** Name of step.
* `environment` (`array` of `envVar`) - **Optional.** List of environment variables to be injected into this step's container.
* `source` (`string`) - **Required.** Name of the Docker image to be executed for this step. If the tag is omitted from the image name, will default to ""latest"".
* `output` (`string`) - **Optional.** Output channel to be captured and passed to the next step in the job. Valid values are ""stdout"", ""stderr"" or any absolute file path. Defaults to ""stdout"" if not specified. See the ""Output Channels"" section below for more details.
* `refresh` (`boolean`) - **Optional.** Flag indicating whether or not the image identified by the *source* attribute should be refreshed before it is executed. A *true* value will force Dray to do a `docker pull` before the job step is started. A *false* value (the default) indicates that a `docker pull` should be done only if the image doesn't already exist in the local image cache.

**Example Request:**

    POST /jobs HTTP/1.1
    Content-Type: application/json
    
	{  
	  ""name"":""Demo Job"",
	  ""steps"":[  
	    {  
	      ""name"":""random-word"",
	      ""source"":""centurylink/randword"",
	      ""environment"":[  
	        { ""variable"":""WORD_COUNT"", ""value"":""10"" }
	      ]
	    },
	    {  
	      ""name"":""uppercase"",
	      ""source"":""centurylink/upper""
	    },
	    {  
	      ""name"":""reverse"",
	      ""source"":""centurylink/reverse""
	    },	    
	  ]
	}
    
**Example Response:**

	HTTP/1.1 201 Created
	Content-Type: application/json
	
	{  
	  ""id"":""51E0E756-A6B4-9CC7-67BD-364970C2268C"",
	  ""name"":""Demo Job"",
	  ""steps"":[  
	    {  
	      ""name"":""random-word"",
	      ""source"":""centurylink/randword"",
	      ""environment"":[  
	        { ""variable"":""WORD_COUNT"", ""value"":""10"" }
	      ]
	    },
	    {  
	      ""name"":""uppercase"",
	      ""source"":""centurylink/upper""
	    },
	    {  
	      ""name"":""reverse"",
	      ""source"":""centurylink/reverse""
	    },	    
	  ]
	}
	
**Status Codes:**

* **201** - no error
* **500** - server error
	  
### List Jobs

    GET /jobs
    
Returns a list of all the job IDs. Every time that a job is started, it is assigned a unique ID and some basic information is persisted. This call will return the IDs of all the persisted jobs.

**Example Request:**

    GET /jobs HTTP/1.1
    
**Example Response:**

	HTTP/1.1 200 OK
	Content-Type: application/json
	
	[  
	  {  
	    ""id"":""E2C7017E-449D-B4AA-1BEB-F85224DFC0E1""
	  },
	  {  
	    ""id"":""26C4A46D-C615-E978-521F-A0D8FDD80801""
	  },
	  {  
	    ""id"":""51E0E756-A6B4-9CC7-67BD-364970C2268C""
	  }
	]
	
**Status Codes:**

* **200** - no error
* **500** - server error

### Get Job

    GET /jobs/(id)
    
Returns the state of the specified job. The response will include the number of steps which have been completed and an overall status for the job. 

The status will be one of ""running"", ""complete"", or ""error"". The ""error"" status indicates that one of the steps exited with a non-zero exit code.

**Exampel Request:**

    GET /jobs/51E0E756-A6B4-9CC7-67BD-364970C2268C HTTP/1.1
    
**Example Response:**

    HTTP/1.1 200 OK
    Content-Type: application/json
    
	{
  	  ""id"": ""51E0E756-A6B4-9CC7-67BD-364970C2268C"",
	  ""stepsCompleted"": 2,
	  ""status"": ""complete""
	}
	
**Status Codes:**

* **200** - no error
* **404** - no such job
* **500** - server error

### Get Job Log

    GET /jobs/(id)/log
    
Retrieves the log output of the specified job. While a job is executing any data written to the *stdout* or *stderr* streams (by any of the steps) is persisted and made available via this API endpoint.

**Querystring Params:**

* `index` (`number`) - **Optional.** The starting index for the log output. The response will contain all the log lines starting with the specified index. This can be useful if you are trying to monitor a job while it is still executing. If your first call responds with 10 lines of log output, you can pass `index=10` on your next request and you'll only receive log entries which have been added since your first call. Defaults to 0 if the index is not specified.

**Example Request:**

    GET /jobs/51E0E756-A6B4-9CC7-67BD-364970C2268C/log?index=0 HTTP/1.1
    
**Example Response:**

    HTTP/1.1 200 OK
    Content-Type: application/json
    
    {
      ""lines"": [
        ""Standard output line 1"",
		""Standard output line 2"",
		""Standard output line 3"",
		""Standard error line 1"",				
      ]
    }
      
**Status Codes:**

* **200** - no error
* **404** - no such job
* **500** - server error
      
### Delete Job

    DELETE /jobs/(id)
   
Deletes all the information persisted for a given job ID. Note that this will **not** stop a running job, it merely removes all the information persisted for the job in Redis.

**Example Request:**

    DELETE /jobs/51E0E756-A6B4-9CC7-67BD-364970C2268C HTTP/1.1
    
**Example Response:**

    HTTP/1.1 204 No Content
    
**Status Codes:**

* **204** - no error
* **404** - no such job
* **500** - server error

## Output Channels
One of the key features that Dray provides is the ability to marshal data between the different steps (containers) in a job. By default, Dray will capture anything written to the container's *stdout* stream and automatically feed that into the next container's *stdin* stream. However, different output channels can be configured on a step-by-step basis.

### stderr
It is common for tasks/services running in Docker containers to use the *stdout* stream for log output. If you're already using *stdout* for log output and want to use a different channel for data that should be passed to the next job step you can opt to use the *stderr* stream instead. 

To configure Dray to monitor *stderr* for a particular job step you simply use the `output` field for that step in the job description:

	{  
	  ""steps"":[  
	    {  
	      ""source"":""jdoe/foo"",
	      ""output"":""stderr""
	    },
	    {  
	      ""source"":""jdoe/bar""
	    }
	  ]
	}

When creating the ""foo"" image for use in this job you just need to make sure that any data you want passed to the next step in the job is written to the *stderr* stream.

Here are some examples of writing to *stderr* in different languages:

* Bash - `echo ""hello world"" >&2`
* Ruby - `STDERR.puts 'hello world'`
* Python - `print(""hello world"", file=sys.stderr)`

### Custom File
If you don't want to use either the *stdout* or *stderr* streams for passing data, you also have the option of using a regular file. When configured in this way, Dray will volume mount a file into the Docker container at start-up time and then read the contents of the file when the container stops.

To configure Dray to monitor a custom file you need to specify the fully-qualified path of the file (relative to the root of the container) in the `output` field for that step in the job description:

	{  
	  ""steps"":[  
	    {  
	      ""source"":""jdoe/foo"",
	      ""output"":""/output.txt""
	    },
	    {  
	      ""source"":""jdoe/bar""
	    }
	  ]
	}

The `output` value specified **must** begin with a `/` character. The specified file doesn't necessarily need to exist in the image already, Dray will create a temporary file and than volume mount it into the container at the specified location at start-up time.

From within your container, you'll simply need to open the specified file and write to it any data that you would like to have passed to the next step in the job.

There is one other bit of configuration that is also required when using a custom file as an output channel. Since Dray, Docker and your job container all need access to this file, we need a common directory to which all three have access. To enable this you'll need to specify an additional volume mount flag when starting Dray that exposes the host's `/tmp` directory to the Dray container.

    docker run -d --name dray \
      --link redis:redis \
      -v /tmp:/tmp \
      -v /var/run/docker.sock:/var/run/docker.sock \
      -p 3000:3000 \
      centurylink/dray:latest
      
Note the addition of the `-v /tmp:/tmp` flag in the Docker `run` command above. This setting is required **only** if you intend to use custom files as a data-passing mechanism and can be omitted otherwise.

## Building

To facilitate the creation of small Docker image, Dray is compiled into a statically linked binary that can be run with no external dependencies.

The `build.sh` script included in the Dray repository will compile the executable and create the Docker image by leveraging the [centurylink\golang-builder](https://registry.hub.docker.com/u/centurylink/golang-builder/) image. The resulting image is tagged as `centurylink/dray:latest`.
",2023-07-07 15:55:47+00:00
drill,drill,apache/drill,Apache Drill is a distributed MPP query layer for self describing data,https://drill.apache.org/,False,1825,2023-07-05 19:45:29+00:00,2012-09-05 07:00:26+00:00,944,159,160,28,drill-1.20.3,2022-12-27 04:16:15+00:00,Apache License 2.0,4441,oscon_workshop,34,2013-07-22 00:35:33+00:00,2023-07-05 19:45:29+00:00,2023-07-05 13:48:39+00:00,"# Apache Drill

[![Build Status](https://github.com/apache/drill/workflows/Github%20CI/badge.svg)](https://github.com/apache/drill/actions)
[![Artifact](https://img.shields.io/maven-central/v/org.apache.drill/distribution.svg)](https://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.drill%22%20AND%20a%3A%22distribution%22)
[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)
[![Stack Overflow](https://img.shields.io/:stack%20overflow-apache--drill-brightgreen.svg)](http://stackoverflow.com/questions/tagged/apache-drill)
[![Join Drill Slack](https://img.shields.io/badge/slack-open-e01563.svg)](http://apache-drill.slack.com ""Join our Slack community"")


Apache Drill is a distributed MPP query layer that supports SQL and alternative query languages against NoSQL and Hadoop data storage systems.  It was inspired in part by [Google's Dremel](http://research.google.com/pubs/pub36632.html).  

## Developers

Please read [Environment.md](docs/dev/Environment.md) for setting up and running Apache Drill. For complete developer documentation see [DevDocs.md](docs/dev/DevDocs.md).

## More Information
Please see the [Apache Drill Website](http://drill.apache.org/) or the [Apache Drill Documentation](http://drill.apache.org/docs/) for more information including:

 * Remote Execution Installation Instructions
 * [Running Drill on Docker instructions](https://drill.apache.org/docs/running-drill-on-docker/)
 * Information about how to submit logical and distributed physical plans
 * More example queries and sample data
 * Find out ways to be involved or discuss Drill


## Join the community!
Apache Drill is an Apache Foundation project and is seeking all types of users and contributions.
Please say hello on the [Apache Drill mailing list](http://drill.apache.org/mailinglists/).You can also join our [Google Hangouts](http://drill.apache.org/community-resources/)
or [join](https://bit.ly/2VM0XS8) our [Slack Channel](https://join.slack.com/t/apache-drill/shared_invite/enQtNTQ4MjM1MDA3MzQ2LTJlYmUxMTRkMmUwYmQ2NTllYmFmMjU4MDk0NjYwZjBmYjg0MDZmOTE2ZDg0ZjBlYmI3Yjc4Y2I2NTQyNGVlZTc) if you need help with using or developing Apache Drill (more information can be found on [Apache Drill website](http://drill.apache.org/)).

## Export Control
This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See <http://www.wassenaar.org/> for more information.  
The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.
The following provides more details on the included cryptographic software: 
 Java SE Security packages are used to provide support for authentication, authorization and secure sockets communication. The Jetty Web Server is used to provide communication via HTTPS. The Cyrus SASL libraries, Kerberos Libraries and OpenSSL Libraries are used to provide SASL based authentication and SSL communication.
",2023-07-07 15:55:51+00:00
drmr,drmr,ParkerLab/drmr,A tool for submitting pipeline scripts to distributed resource managers.,https://drmr.readthedocs.io/,False,4,2018-07-10 16:24:15+00:00,2016-06-15 15:51:44+00:00,2,3,0,0,,,GNU General Public License v3.0,4,1.0.2,6,2016-12-15 18:50:04+00:00,,2017-05-09 12:32:56+00:00,"====
drmr
====

A tool for submitting pipeline scripts to distributed resource
managers.

Introduction
============

Drmr (pronounced 'drummer') lets you write computational pipelines in
simple shell scripts. It's designed to work with common distributed
resource management (DRM) systems (Slurm and PBS so far).

Why another pipeline tool?
--------------------------

Because of a quirk in our local environment, really. Most of the tools
I evaluated required a persistent supervisor process to manage
pipelines. Our local cluster kills any process that accumulates 15
minutes of CPU time on the login nodes. That's not usually a problem,
but for big important jobs, I didn't want to worry about it. With
drmr, it's fire and forget: the pipeline management is delegated to
the DRM via job dependencies.

Most of the more capable tools also have a bigger learning curve. With
drmr, you start with a regular sequential shell script, and add DRM
directives -- in shell comment lines -- as you need to.

By default every line in the script will be submitted as a concurrent
job.

If some steps in your pipeline depend on the completion of earlier
tasks, you can insert wait directives, which act as checkpoints.

If there are no dependencies between jobs, you can use drmrarray to
submit the entire script as one job array, which is less work for the
DRM, therefore faster to submit, and easier to manage.

You can also use directives to specify limits or required resources
for subsequent tasks.

Requirements
============

* Python. We've run it successfully under versions 2.7.10 and 3.5.
* Jinja2 (If you install drmr with pip, Jinja2 should be installed automatically.)
* lxml (Again, pip should install it for you.)

Installation
============

At the command line::

  git clone https://github.com/ParkerLab/drmr
  pip install ./drmr

Or in one step::

  pip install git+https://github.com/ParkerLab/drmr

Documentation
=============

https://drmr.readthedocs.io/

License
=======

GPL version 3 (or any later version).
",2023-07-07 15:55:56+00:00
durabletask,durabletask,Azure/durabletask,Durable Task Framework allows users to write long running persistent workflows in C# using the async/await capabilities.,,False,1315,2023-07-06 20:53:49+00:00,2014-10-07 15:58:45+00:00,254,80,64,79,durabletask.azurestorage-v1.13.8,2023-06-19 20:51:42+00:00,Apache License 2.0,646,urabletask.servicebus-v2.5.2,81,2021-03-17 19:53:46+00:00,2023-07-07 01:22:11+00:00,2023-06-22 15:58:23+00:00,"# Durable Task Framework

The Durable Task Framework (DTFx) is a library that allows users to write long running persistent workflows (referred to as _orchestrations_) in C# using simple async/await coding constructs. It is used heavily within various teams at Microsoft to reliably orchestrate long running provisioning, monitoring, and management operations. The orchestrations scale out linearly by simply adding more worker machines. This framework is also used to power the serverless [Durable Functions](https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview) extension of [Azure Functions](https://azure.microsoft.com/services/functions/).

By open sourcing this project we hope to give the community a very cost-effective alternative to heavy duty workflow systems. We also hope to build an ecosystem of providers and activities around this simple yet incredibly powerful framework.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Supported persistance stores

Starting in v2.x, the Durable Task Framework supports an extensible set of backend persistence stores. Each store can be enabled using a different NuGet package. The latest version of all packages are signed and available for download at nuget.org.

| Package | Latest Version | Details | Development Status |
| ------- | -------------- | ------- | ------------------ |
| DurableTask.ServiceBus | [![NuGet](https://img.shields.io/nuget/v/Microsoft.Azure.DurableTask.ServiceBus.svg?style=flat)](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.ServiceBus/) | Orchestration message and runtime state is stored in Service Bus queues while tracking state is stored in Azure Storage. The strength of this provider is its maturity and transactional consistency. However, it is no longer in active development at Microsoft. | Production ready but not actively maintained |
| DurableTask.AzureStorage | [![NuGet](https://img.shields.io/nuget/v/Microsoft.Azure.DurableTask.AzureStorage.svg?style=flat)](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.AzureStorage/) | All orchestration state is stored in Azure Storage queues, tables, and blobs. The strength of this provider is the minimal service dependencies, high efficiency, and rich feature-set. This is the only backend available for [Durable Functions](https://docs.microsoft.com/azure/azure-functions/durable/). | Production ready and actively maintained |
| DurableTask.AzureServiceFabric | [![NuGet](https://img.shields.io/nuget/v/Microsoft.Azure.DurableTask.AzureServiceFabric.svg?style=flat)](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.AzureServiceFabric/) | All orchestration state is stored in [Azure Service Fabric Reliable Collections](https://docs.microsoft.com/azure/service-fabric/service-fabric-reliable-services-reliable-collections). This is an ideal choice if you are hosting your application in [Azure Service Fabric](https://azure.microsoft.com/services/service-fabric/) and don't want to take on external dependencies for storing state. | Production ready and actively maintained |
| DurableTask.Netherite | [![NuGet](https://img.shields.io/nuget/v/Microsoft.Azure.DurableTask.Netherite.svg?style=flat)](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.Netherite/) | An ultra-high performance backend developed by Microsoft Research where state is stored in [Azure Event Hubs](https://azure.microsoft.com/en-us/services/event-hubs/) and Azure Page Blobs using [FASTER](https://www.microsoft.com/research/project/faster/) database technology from Microsoft Research. [👉 GitHub Repo](https://github.com/microsoft/durabletask-netherite) | Production ready and actively maintained |
| DurableTask.SqlServer | [![NuGet](https://img.shields.io/nuget/v/Microsoft.DurableTask.SqlServer.svg?style=flat)](https://www.nuget.org/packages/Microsoft.DurableTask.SqlServer/) | All orchestration state is stored in a [Microsoft SQL Server](https://www.microsoft.com/sql-server/sql-server-2019) or [Azure SQL](https://azure.microsoft.com/products/azure-sql/database/) database with indexed tables and stored procedures for direct interaction. [👉 GitHub Repo](https://github.com/microsoft/durabletask-mssql) | Production ready and actively maintained |
| DurableTask.Emulator | [![NuGet](https://img.shields.io/nuget/v/Microsoft.Azure.DurableTask.Emulator.svg?style=flat)](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.Emulator/) | This is an in-memory store intended for testing purposes only. It is not designed or recommended for any production workloads. | Not actively maintained |

The core programming model for the Durable Task Framework is contained in the [DurableTask.Core](https://www.nuget.org/packages/Microsoft.Azure.DurableTask.Core/) package, which is also under active development.

## Learning more

The associated wiki contains more details about the framework and how it can be used: https://github.com/Azure/durabletask/wiki. You can also find great information in [this blog series](https://abhikmitra.github.io/blog/durable-task/). In some cases, the [Durable Functions documentation](https://docs.microsoft.com/en-us/azure/azure-functions/durable/) can actually be useful in learning things about the underlying framework, although not everything will apply. Lastly, you can watch a video with some of the original maintainers in [this Channel 9 video](https://channel9.msdn.com/Shows/On-NET/Building-workflows-with-the-Durable-Task-Framework).

## Development Notes

To run unit tests, you must specify your Service Bus connection string for the tests to use. You can do this via the **ServiceBusConnectionString** app.config value in the test project, or by defining a **DurableTaskTestServiceBusConnectionString** environment variable. The benefit of the environment variable is that no temporary source changes are required.

Unit tests also require [Azure Storage Emulator](https://docs.microsoft.com/azure/storage/common/storage-use-emulator), so make sure it's installed and running.

> Note: While it's possible to use in tests a real Azure Storage account it is not recommended to do so because many tests will fail with a 409 Conflict error. This is because tests delete and quickly recreate the same storage tables, and Azure Storage doesn't do well in these conditions. If you really want to change Azure Storage connection string you can do so via the **StorageConnectionString** app.config value in the test project, or by defining a **DurableTaskTestStorageConnectionString** environment variable. 

There is a gitter for this repo, but it's not currently being monitored. We're leaving the link for it up for now and will update this message if anything changes.

[![Join the chat at https://gitter.im/azure/durabletask](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/azure/durabletask?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) 
",2023-07-07 15:55:59+00:00
dvc,dvc,iterative/dvc,🦉 Data Version Control | Git for Data & Models | ML Experiments Management,https://dvc.org,False,11722,2023-07-07 06:09:35+00:00,2017-03-04 08:16:33+00:00,1078,135,260,452,3.4.0,2023-07-03 23:17:53+00:00,Apache License 2.0,8859,3.4.0,466,2023-07-03 23:17:53+00:00,2023-07-07 13:40:54+00:00,2023-07-05 16:42:07+00:00,"|Banner|

`Website <https://dvc.org>`_
• `Docs <https://dvc.org/doc>`_
• `Blog <http://blog.dataversioncontrol.com>`_
• `Tutorial <https://dvc.org/doc/get-started>`_
• `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
• `How DVC works`_
• `VS Code Extension`_
• `Installation`_
• `Contributing`_
• `Community and Support`_

|CI| |Python Version| |Coverage| |VS Code| |DOI|

|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|

|

**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:

#. **Version** your data and models.
   Store them in your cloud storage but keep their version info in your Git repo.

#. **Iterate** fast with lightweight pipelines.
   When you make changes, only run the steps impacted by those changes.

#. **Track** experiments in your local Git repo (no servers needed).

#. **Compare** any data, code, parameters, model, or performance plots.

#. **Share** experiments and automatically reproduce anyone's experiment.

Quick start
===========

    Please read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.

A common CLI workflow includes:


+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Task                              | Terminal                                                                                           |
+===================================+====================================================================================================+
| Track data                        | | ``$ git add train.py params.yaml``                                                               |
|                                   | | ``$ dvc add images/``                                                                            |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Connect code and data             | | ``$ dvc stage add -n featurize -d images/ -o features/ python featurize.py``                     |
|                                   | | ``$ dvc stage add -n train -d features/ -d train.py -o model.p -M metrics.json python train.py`` |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Make changes and experiment       | | ``$ dvc exp run -n exp-baseline``                                                                |
|                                   | | ``$ vi train.py``                                                                                |
|                                   | | ``$ dvc exp run -n exp-code-change``                                                             |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Compare and select experiments    | | ``$ dvc exp show``                                                                               |
|                                   | | ``$ dvc exp apply exp-baseline``                                                                 |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Share code                        | | ``$ git add .``                                                                                  |
|                                   | | ``$ git commit -m 'The baseline model'``                                                         |
|                                   | | ``$ git push``                                                                                   |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                                         |
|                                   | | ``$ dvc push``                                                                                   |
+-----------------------------------+----------------------------------------------------------------------------------------------------+

How DVC works
=============

    We encourage you to read our `Get Started
    <https://dvc.org/doc/get-started>`_ docs to better understand what DVC
    does and how it can fit your scenarios.

The closest *analogies* to describe the main DVC features are these:

#. **Git for data**: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!
#. **Makefiles** for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.
#. Local **experiment tracking**: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).

Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).
DVC `stores data and model files <https://dvc.org/doc/start/data-management>`_ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.
To share and back up the *data cache*, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).

|Flowchart|

`DVC pipelines <https://dvc.org/doc/start/data-management/data-pipelines>`_ (computational graphs) connect code and data together.
They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.

Last but not least, `DVC Experiment Versioning <https://dvc.org/doc/start/experiments>`_ lets you prepare and run a large number of experiments.
Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots.

.. _`VS Code Extension`:

Visual Studio Code Extension
============================

|VS Code|

To use DVC as a GUI right from your VS Code IDE, install the `DVC Extension <https://marketplace.visualstudio.com/items?itemName=Iterative.dvc>`_ from the Marketplace.
It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!

|VS Code Extension Overview|

    Note: You'll have to install core DVC on your system separately (as detailed
    below). The Extension will guide you if needed.

Installation
============

There are several ways to install DVC: in VS Code; using ``snap``, ``choco``, ``brew``, ``conda``, ``pip``; or with an OS-specific package.
Full instructions are `available here <https://dvc.org/doc/get-started/install>`_.

Snapcraft (Linux)
-----------------

|Snap|

.. code-block:: bash

   snap install dvc --classic

This corresponds to the latest tagged release.
Add ``--beta`` for the latest tagged release candidate, or ``--edge`` for the latest ``main`` version.

Chocolatey (Windows)
--------------------

|Choco|

.. code-block:: bash

   choco install dvc

Brew (mac OS)
-------------

|Brew|

.. code-block:: bash

   brew install dvc

Anaconda (Any platform)
-----------------------

|Conda|

.. code-block:: bash

   conda install -c conda-forge mamba # installs much faster than conda
   mamba install -c conda-forge dvc

Depending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.

PyPI (Python)
-------------

|PyPI|

.. code-block:: bash

   pip install dvc

Depending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
The command should look like this: ``pip install 'dvc[s3]'`` (in this case AWS S3 dependencies such as ``boto3`` will be installed automatically).

To install the development version, run:

.. code-block:: bash

   pip install git+git://github.com/iterative/dvc

Package (Platform-specific)
---------------------------

|Packages|

Self-contained packages for Linux, Windows, and Mac are available.
The latest version of the packages can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.

Ubuntu / Debian (deb)
^^^^^^^^^^^^^^^^^^^^^
.. code-block:: bash

   sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list
   wget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -
   sudo apt update
   sudo apt install dvc

Fedora / CentOS (rpm)
^^^^^^^^^^^^^^^^^^^^^
.. code-block:: bash

   sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo
   sudo rpm --import https://dvc.org/rpm/iterative.asc
   sudo yum update
   sudo yum install dvc

Contributing
============

|Maintainability|

Contributions are welcome!
Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more details.
Thanks to all our contributors!

|Contribs|

Community and Support
=====================

* `Twitter <https://twitter.com/DVCorg>`_
* `Forum <https://discuss.dvc.org/>`_
* `Discord Chat <https://dvc.org/chat>`_
* `Email <mailto:support@dvc.org>`_
* `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_

Copyright
=========

This project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).

By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.

Citation
========

|DOI|

Iterative, *DVC: Data Version Control - Git for Data & Models* (2020)
`DOI:10.5281/zenodo.012345 <https://doi.org/10.5281/zenodo.3677553>`_.

Barrak, A., Eghan, E.E. and Adams, B. `On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects <https://mcis.cs.queensu.ca/publications/2021/saner.pdf>`_ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA.


.. |Banner| image:: https://dvc.org/img/logo-github-readme.png
   :target: https://dvc.org
   :alt: DVC logo

.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif
   :alt: DVC Extension for VS Code

.. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main
   :target: https://github.com/iterative/dvc/actions
   :alt: GHA Tests

.. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg
   :target: https://codeclimate.com/github/iterative/dvc
   :alt: Code Climate

.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc
   :target: https://pypi.org/project/dvc
   :alt: Python Version

.. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/iterative/dvc
   :alt: Codecov

.. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
   :target: https://snapcraft.io/dvc
   :alt: Snapcraft

.. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco
   :target: https://chocolatey.org/packages/dvc
   :alt: Chocolatey

.. |Brew| image:: https://img.shields.io/homebrew/v/dvc?label=brew
   :target: https://formulae.brew.sh/formula/dvc
   :alt: Homebrew

.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/dvc.svg?label=conda&logo=conda-forge
   :target: https://anaconda.org/conda-forge/dvc
   :alt: Conda-forge

.. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white
   :target: https://pypi.org/project/dvc
   :alt: PyPI

.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold
   :target: https://pypi.org/project/dvc
   :alt: PyPI Downloads

.. |Packages| image:: https://img.shields.io/badge/deb|pkg|rpm|exe-blue
   :target: https://dvc.org/doc/install
   :alt: deb|pkg|rpm|exe

.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg
   :target: https://doi.org/10.5281/zenodo.3677553
   :alt: DOI

.. |Flowchart| image:: https://dvc.org/img/flow.gif
   :target: https://dvc.org/img/flow.gif
   :alt: how_dvc_works

.. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc
   :target: https://github.com/iterative/dvc/graphs/contributors
   :alt: Contributors

.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue
   :target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc
   :alt: VS Code Extension
",2023-07-07 15:56:08+00:00
dwork,dwork,frobnitzem/dwork,Distributed Work Scheduling System,,False,0,2021-08-31 17:21:18+00:00,2020-12-15 20:02:24+00:00,0,2,1,0,,,GNU General Public License v3.0,23,,0,,,2021-08-31 17:21:12+00:00,"# dwork

Task graph scheduler with a minimalistic API.

# installing

dwork builds using cmake.  An example build.sh script is included.
It requires the following dependencies:

* [TKRZW](https://dbmx.net/tkrzw)
* [Protobuf](https://developers.google.com/protocol-buffers/docs/cpptutorial)
* [cppzmq](https://github.com/zeromq/cppzmq)
   - more information on ZeroMQ at https://zeromq.org/

# Basic Usage

Most people will be interested in the following workflow:

  1) run the dwork hub `dhub &`
  2) load up a list of tasks `dquery addfile <file listing 1 task per line>`
  3) start up a pool of workers `see example in tests/kworker` (can also use `dquery steal/complete` from shell)
    - Note: Processor elements that steal a task are responsible for calling `complete` when it's finished,
      or else the task's successors will not run.
  4) monitor task progress `bin/dquery status`

Steps 1, 2 and 3 can be run in any order as long as
2 comes after 1.

What's happening in the background is that `dhub` is listening for client connections on tcp port 6125.
The `dquery` tool sends and receives messages from the server to accomplish adding/dequeuing tasks.
All valid messages are enumerated documentation.
The underlying format is in [the protobuf file](https://github.com/frobnitzem/dwork/blob/master/proto/TaskMsg.proto).

# Writing a client in python

Since dwork is a server that speaks protobuf over zmq, you can easily interact
with the task server from python.

Python Package Setup:

* pip3 install protobuf pyzmq


Example Script:

```
import taskMsg
import zmq

# Create a request message
msg = taskMsg.TaskMsg()
msg.type = taskMsg.REQUEST
print(""Sending: "")
print(msg)

# Open a connection to the dhub
context = zmq.Context()
dhub = context.socket(zmq.REQ)
dhub.connect(""tcp://localhost:5555"")

# Send the request. Receive and process the reply.
dhub.send(msg.SerializeToString())
msg.ParseFromString( dhub.recv() )

print(""Received: "")
print(msg)
```

# Advanced Usage

The behavior of the above programs can be altered by command-line arguments.  Notably, `dhub` accepts a ""-f""
argument to store/recover its task database to/from file.  `dquery` also accepts several options that
are documented by running `dquery -h`.

It is our intent to build variants of `dquery` that work as (library-level) function calls from various
languages (python being a major one).  Basically, this just requires
creating machinery to send/receive protobuf over zmq in those languages.
If you are interested in this project, contact the developers directly!

A C++ variant is essentially done, since it just requires adapting `test/kworker` and linking to `libdwork`.
The bash variant is `dquery` itself.

# Building the Documentation

The following packages are required to build the docs:

* cmake
* doxygen
* python3-sphinx

In addition, you'll need some python packages:
```
pip3 install sphinx_rtd_theme
pip3 install breathe
```
",2023-07-07 15:56:12+00:00
dynamic-pipeline,dynamic-pipeline,mairarodrigues/dynamic-pipeline,Automatically exported from code.google.com/p/dynamic-pipeline,,False,0,2015-03-15 13:53:58+00:00,2015-03-15 13:53:58+00:00,0,1,0,0,,,,84,,0,,,2011-07-07 20:38:17+00:00,,2023-07-07 15:56:15+00:00
ecflow,ecflow,ecmwf/ecflow,Workflow manager ,,False,33,2023-06-02 13:40:04+00:00,2020-08-06 08:21:49+00:00,10,20,16,0,,,Apache License 2.0,6537,5.11.1,49,2023-07-03 09:37:09+00:00,2023-07-07 10:17:54+00:00,2023-07-07 10:17:01+00:00,"**ecFlow**
==========

*ecFlow* is a client/server workflow package that enables users to run a large number of programs (with dependencies on each other and on time) in a controlled environment. It provides tolerance for hardware and software failures, combined with restart capabilities. It is used at ECMWF to run all our operational suites across a range of platforms.

The documentation can be found at https://ecflow.readthedocs.io/.


COPYRIGHT AND LICENCE
----------------------

Copyright 2005- European Centre for Medium-Range Weather Forecasts (ECMWF).

This software is licensed under the terms of the Apache Licence Version 2.0
which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.

In applying this licence, ECMWF does not waive the privileges and immunities granted to it by
virtue of its status as an intergovernmental organisation nor does it submit to any jurisdiction.
",2023-07-07 15:56:18+00:00
edge,EDGE,LANL-Bioinformatics/EDGE,EDGE is a highly adaptable bioinformatics platform that allows laboratories to quickly analyze and interpret genomic sequence data. ,https://lanl-bioinformatics.github.io/EDGE/,False,72,2023-04-18 14:43:16+00:00,2014-01-28 17:50:53+00:00,32,23,6,7,v2.4.0,2020-12-03 00:26:54+00:00,GNU General Public License v3.0,1072,v2.4.0,10,2020-12-02 23:27:14+00:00,2023-06-06 17:02:37+00:00,2022-04-06 21:58:21+00:00,"# EDGE Bioinformatics

This is version 2.x.x-dev of EDGE Bioinformatics, a product of Los Alamos National Laboratory, Naval Medical Research Center, and the Defense Threat Reduction Agency.

EDGE is a highly adaptable bioinformatics platform that allows laboratories to quickly analyze and interpret genomic sequence data. The bioinformatics platform allows users to address a wide range of use cases including assay validation and the characterization of novel biological threats, clinical samples, and complex environmental samples.

To demo EDGE, go to https://bioedge.lanl.gov/ or https://edgebioinformatics.org/ and follow the ""GUI"" instructions from our documentation, found at https://edge.readthedocs.org/en/latest/introduction.html. It is also linked at the bottom of the EDGE homepage.

## Documentation
	
[ReadTheDocs](http://edge.readthedocs.org)

[PDF](https://readthedocs.org/projects/edge/downloads/pdf/develop/)

## Contact Info
Support: The EDGE user's group is a Google group for users and developers: [edge-users@googlegroups.com](mailto:edge-users@googlegroups.com)

## Citation

Po-E Li, Chien-Chi Lo, Joseph J. Anderson, Karen W. Davenport, Kimberly A. Bishop-Lilly, Yan Xu, Sanaa Ahmed, Shihai Feng, Vishwesh P. Mokashi, Patrick S.G. Chain; Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform, Nucleic Acids Research, Volume 45, Issue 1, 9 January 2017, Pages 67–80, https://doi.org/10.1093/nar/gkw1027

## Copyright

Copyright (2019).  Triad National Security, LLC. All rights reserved.
 
This program was produced under U.S. Government contract 89233218CNA000001 for Los Alamos National 
Laboratory (LANL), which is operated by Triad National Security, LLC for the U.S. Department of Energy/National 
Nuclear Security Administration.
 
All rights in the program are reserved by Triad National Security, LLC, and the U.S. Department of Energy/National 
Nuclear Security Administration. The Government is granted for itself and others acting on its behalf a nonexclusive, 
paid-up, irrevocable worldwide license in this material to reproduce, prepare derivative works, distribute copies to 
the public, perform publicly and display publicly, and to permit others to do so.

This is open source software; you can redistribute it and/or modify it under the terms of the GPLv3 License. If software 
is modified to produce derivative works, such modified software should be clearly marked, so as not to confuse it with 
the version available from LANL. Full text of the [GPLv3 License](https://github.com/LANL-Bioinformatics/edge/blob/master/LICENSE) can be found in the License file in the main development 
branch of the repository.
",2023-07-07 15:56:22+00:00
egeria,egeria,odpi/egeria,Egeria core,https://egeria-project.org,False,690,2023-07-07 10:12:02+00:00,2018-05-31 12:18:34+00:00,251,36,62,43,V4.1,2023-05-29 07:50:03+00:00,Apache License 2.0,19945,V4.1,43,2023-05-29 07:50:03+00:00,2023-07-07 00:27:44+00:00,2023-06-29 08:43:39+00:00,"<!-- SPDX-License-Identifier: CC-BY-4.0 -->
<!-- Copyright Contributors to the Egeria project. -->

![Egeria Logo](assets/img/ODPi_Egeria_Logo_color.png)

[![GitHub](https://img.shields.io/github/license/odpi/egeria)](LICENSE)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3044/badge)](https://bestpractices.coreinfrastructure.org/projects/3044)
[![Maven Central](https://img.shields.io/maven-central/v/org.odpi.egeria/egeria)](https://mvnrepository.com/artifact/org.odpi.egeria)

<!-- [![Azure](https://dev.azure.com/odpi/egeria/_apis/build/status/odpi.egeria)](https://dev.azure.com/odpi/Egeria/_build) -->
<!-- [![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=odpi_egeria&metric=alert_status)](https://sonarcloud.io/dashboard?id=odpi_egeria) -->


# Egeria - Open Metadata and Governance
  
Egeria provides the Apache-2.0 licensed [open metadata and governance](https://egeria-project.org)
type system, frameworks, APIs, event payloads and interchange protocols to enable tools,
engines and platforms to exchange metadata in order to get the best
value from data, whilst ensuring it is properly governed.

## Egeria governance

This project aims to operate in a transparent, accessible way for the benefit of the Egeria community.
All participation in this project is open and not bound to any corporate affiliation.

To understand how to join and contribute, see the 
[Community Guide](https://egeria-project.org/guides/community/).  This includes the call schedule.

All participants are bound to the Egeria's [Code of Conduct](CODE_OF_CONDUCT.md).
The governance of the project is described in more detail in the
[Egeria Operations Guide](https://egeria-project.org/guides/project-operations/).

## Acknowledgements

![YourKit](https://www.yourkit.com/images/yklogo.png)

We are grateful to [YourKit, LLC](https://www.yourkit.com) for supporting open source projects with its full-feature
Java Profiler.

----
License: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/),
Copyright Contributors to the Egeria project.
",2023-07-07 15:56:26+00:00
ehive,ensembl-hive,Ensembl/ensembl-hive,EnsEMBL Hive - a system for creating and running pipelines on a distributed compute resource,,False,49,2023-01-31 22:28:17+00:00,2013-12-05 10:20:06+00:00,29,17,25,0,,,Apache License 2.0,4771,sql_schema_97_start,96,2019-11-20 13:47:36+00:00,2023-05-03 14:46:40+00:00,2023-01-10 09:33:42+00:00,"eHive
=====

[![Travis Build Status](https://travis-ci.org/Ensembl/ensembl-hive.svg?branch=version/2.6)](https://travis-ci.org/Ensembl/ensembl-hive)
[![Coverage Status](https://coveralls.io/repos/Ensembl/ensembl-hive/badge.svg?branch=version/2.6&service=github)](https://coveralls.io/github/Ensembl/ensembl-hive?branch=version/2.6)
[![Documentation Status](https://readthedocs.org/projects/ensembl-hive/badge/?version=version-2.6)](http://ensembl-hive.readthedocs.io/en/version-2.6)
[![codecov](https://codecov.io/gh/Ensembl/ensembl-hive/branch/version%2F2.6/graph/badge.svg)](https://codecov.io/gh/Ensembl/ensembl-hive/branch/version%2F2.6)
[![Code Climate](https://codeclimate.com/github/Ensembl/ensembl-hive/badges/gpa.svg)](https://codeclimate.com/github/Ensembl/ensembl-hive)
[![Docker Build Status](https://img.shields.io/docker/build/ensemblorg/ensembl-hive.svg)](https://hub.docker.com/r/ensemblorg/ensembl-hive)

eHive is a system for running computation pipelines on distributed computing resources - clusters, farms or grids.

The name comes from the way pipelines are processed by a swarm of autonomous agents.

Available documentation
-----------------------

The main entry point is available online in the [user
manual](https://ensembl-hive.readthedocs.io/en/version-2.6/), from where it can
be downloaded for offline access.


eHive in a nutshell
-------------------

### Blackboard, Jobs and Workers

In the centre of each running pipeline is a database that acts as a blackboard with individual tasks to be run.
These tasks (we call them Jobs) are claimed and processed by ""Worker bees"" or just Workers - autonomous processes
that are continuously running on the compute farm and connect to the pipeline database to report about the progress of Jobs
or claim some more. When a Worker discovers that its predefined time is up or that there are no more Jobs to do,
it claims no more Jobs and exits the compute farm freeing the resources.

### Beekeeper

A separate Beekeeper process makes sure there are always enough Workers on the farm.
It regularly checks the states of both the blackboard and the farm and submits more Workers when needed.
There is no direct communication between Beekeeper and Workers, which makes the system rather fault-tolerant,
as crashing of any of the agents for whatever reason doesn't stop the rest of the system from running.

### Analyses

Jobs that share same code, common parameters and resource requirements are typically grouped into Analyses,
and generally an Analysis can be viewed as a ""base class"" for the Jobs that belong to it.
However in some sense an Analysis also acts as a ""container"" for them.

An analysis is implemented as a Runnable file which is a Perl, Python or
Java module conforming to a special interface. eHive provides some basic
Runnables, especially one that allows running arbitrary commands (programs
and scripts written in other languages).

### PipeConfig file defines Analyses and dependency rules of the pipeline

eHive pipeline databases are molded according to PipeConfig files which are Perl modules conforming to a special interface.
A PipeConfig file defines the stucture of the pipeline, which is a graph whose nodes are Analyses
(with their code, parameters and resource requirements) and edges are various dependency rules:

* Dataflow rules define how data that flows out of an Analysis can be used to trigger creation of Jobs in other Analyses
* Control rules define dependencies between Analyses as Jobs' containers (""Jobs of Analysis Y can only start when all Jobs of Analysis X are done"")
* Semaphore rules define dependencies between individual Jobs on a more fine-grained level

There are also other parameters of Analyses that control, for example:

* how many Workers can simultaneously work on a given Analysis,
* how many times a Job should be tried until it is considered failed,
* what should be automatically done with a Job if it needs more memory/time,
  etc.

Grid scheduler and Meadows
--------------------------

eHive has a generic interface named _Meadow_ that describes how to interact with an underlying grid scheduler (submit jobs, query job's status, etc). eHive is compatible with
[IBM Platform LSF](http://www-03.ibm.com/systems/spectrum-computing/products/lsf/),
Sun Grid Engine (now known as Oracle Grid Engine),
[HTCondor](https://research.cs.wisc.edu/htcondor/),
[PBS Pro](http://www.pbspro.org),
[Docker Swarm](https://docs.docker.com/engine/swarm/) and maybe others. Read more about this on [the user manual](http://ensembl-hive.readthedocs.io/en/version-2.6/contrib/alternative_meadows.html).

Docker image
------------

We have a Docker image available on the [Docker
Hub](https://hub.docker.com/r/ensemblorg/ensembl-hive/). It can be used to
showcase eHive scripts (`init_pipeline.pl`, `beekeeper.pl`, `runWorker.pl`) in a
container

### Open a session in a new container (will run bash)

```bash
docker run -it ensemblorg/ensembl-hive
```

### Initialize and run a pipeline

```bash
docker run -it ensemblorg/ensembl-hive init_pipeline.pl Bio::EnsEMBL::Hive::Examples::LongMult::PipeConfig::LongMult_conf -pipeline_url $URL
docker run -it ensemblorg/ensembl-hive beekeeper.pl -url $URL -loop -sleep 0.2
docker run -it ensemblorg/ensembl-hive runWorker.pl -url $URL
```

Docker Swarm
------------

Once packaged into Docker images, a pipeline can actually be run under the
Docker Swarm orchestrator, and thus on any cloud infrastructure that supports
it (e.g. [Amazon Web Services](https://docs.docker.com/docker-cloud/cloud-swarm/create-cloud-swarm-aws/),
[Microsoft Azure](https://docs.docker.com/docker-cloud/cloud-swarm/create-cloud-swarm-azure/)).

Read more about this on [the user manual](http://ensembl-hive.readthedocs.io/en/version-2.6/contrib/docker-swarm.html).

Contact us (mailing list)
-------------------------

eHive was originally conceived and used within EnsEMBL Compara group
for running Comparative Genomics pipelines, but since then it has been separated
into a separate software tool and is used in many projects both in Genome Campus, Cambridge and outside.
There is eHive users' mailing list for questions, suggestions, discussions and announcements.

To subscribe to it please visit <http://listserver.ebi.ac.uk/mailman/listinfo/ehive-users>
",2023-07-07 15:56:31+00:00
endofday,endofday,joestubbs/endofday,Execute pipelines and other workflows of docker containers.,,False,10,2018-03-08 23:06:15+00:00,2014-12-02 21:25:55+00:00,4,4,2,0,,,Other,270,v0.2.0,2,2016-02-29 03:26:54+00:00,,2016-04-02 22:00:23+00:00,"========
endofday
========

Execute workflows of docker containers described in yaml files.


Quickstart
==========

The only dependency is ``docker`` and an image that is available from
the docker hub, so you do not need to clone this repository or
download anything. To get started:

1. Change into a directory where you will work and execute:

   .. code-block:: bash

      $ docker run -v $(pwd):/staging jstubbs/endofday --setup

   This command installs a single script ``endofday.sh`` in the
   current working directory.

2. Create a yaml file defining your workflow: specify which containers
   to run and define the inputs, outputs, and command. Use
   outputs from one container as inputs to another to create a
   dependency. You can also specify files on your local system to use
   as inputs. Use any image available locally or on the docker hub.
   Examples can be found in the examples directory of this
   repository. See the ``endofday.yml`` reference for more details.

3. Execute the workflow using the endofday.sh script:

   .. code-block:: bash

      $ ./endofday.sh my_workflow.yml


More Details
============

Suppose we found several Docker images that do part of the work in
which we are interested.  Each of the images would have its own way to
obtain inputs and to generate outputs.  The following diagram shows a
(fictitious) example and the associated ``endofday`` yaml file that
represents it.  Each block shows an individual image and the files
(relative to each container) which are expected as inputs and outputs.

.. sidebar:: Example

   .. image:: endofday.png
      :align: center
      :width: 100%


.. code-block:: yaml

   inputs:
     - input1 <- /home/user/input.txt

   outputs:
     - /data/output.txt

   processes:
     P:
       image: user/image_p
       inputs:
         - inputs.input1 -> /data/input_p.txt
       outputs:
         - /data/output_p_1.txt -> output_p_1
         - /data/output_p_2.txt -> output_p_2
       command: run_p

     N1:
       image: user/image_n1
       inputs:
         - P.output_p_1 -> /tmp/input_n1.txt
       outputs:
         - /tmp/output_n1.txt -> output_n1
       command: run_n1

     N2:
       image: user/image_n2
       inputs:
         - P.output_p_2 -> /target/input_n2.txt
       outputs:
         - /target/output_n2.txt -> output_n2
       command: run_n2

     S:
       image: user/image_s
       inputs:
         - N1.output_n1 -> /data/a.txt
         - N2.output_n2 -> /data/b.txt
       outputs:
         - /data/output.txt
       command: run_s


Agave Integration
=================
We are building support for running endofday tasks on the Agave Platform's compute cloud. Initially, two use cases will be
supported: 1) executing entire workflows on the cloud and 2) farming out individual task computations to Agave as part
of a workflow running on your local machine.

After configuring endofday to use your Agave account for submitting jobs, you can execute an entire workflow on the
Agave cloud simply by executing:

   .. code-block:: bash

      $ ./endofday --agave my_workflow.yml

All docker containers will be executed on the Agave cloud and their outputs archived to your default storage system
or another storage system you configure. You can configure an email address to get a notification when the results
are ready.

Alternatively, you can instruct endofday to execute specific tasks on the Agave cloud as part of a larger workflow
executing on your local machine. The endofday engine will send instructions to Agave to run the specific container
and command in cloud after uploading all necessary dependencies to the storage system defined. Once the job completes,
endofday will download the results and continue executing the workflow.

To execute a specific task on the Agave cloud, specify 'agave' as the value for execution directly in the yaml
workflow definition file. For example, to run N2 on Agave we would update the above yaml file with this stanza:

.. code-block:: yaml

     .   .   .

     N2:
       image: user/image_n2
       execution: agave
       inputs:
         - P.output_p_2 -> /target/input_n2.txt
       outputs:
         - /target/output_n2.txt -> output_n2
       command: run_n2


To use either approach, you first need an Agave account and an API client. If you don't have those already you can
get those here: http://preview.agaveapi.co/documentation/beginners-guides/

Configuration
-------------

Configure endofday to use your Agave credentials by adding the following fields to your endofday.conf file under
the Agave section.

.. code-block:: yaml

       [agave]
       # these configurations are only needed when running on the Agave platform
   
       # the base URL for the Agave tenant to use
       api_server: https://agave.iplantc.org
   
       # Agave username
       username: testuser
       password: abcd123
   
       # client credentials
       client_name: demo
       client_key: MY_CLIENT_KEY
       client_secret: MY_CLIENT_SECRET
   
       # storage system for persisting results
       storage_system: data.iplantcollaborative.org
   
       # home directory for endofday. Each work flow execution will automatically get a directory within this directory.
       # Default is to use the Agave username.
       home_dir: testuser

",2023-07-07 15:56:35+00:00
ensembletoolkit,radical.entk,radical-cybertools/radical.entk,The RADICAL Ensemble Toolkit,https://radical-cybertools.github.io/entk/index.html,False,29,2023-05-22 22:20:44+00:00,2014-07-24 12:07:35+00:00,17,17,17,9,v1.6.7,2021-07-15 08:39:04+00:00,Other,2895,workshop/bioexcel,81,2018-12-12 13:45:17+00:00,2023-07-07 05:58:06+00:00,2023-06-22 14:33:36+00:00,"# RADICAL-EnsembleToolkit (EnTK)

[![Build Status](https://github.com/radical-cybertools/radical.entk/actions/workflows/python-app.yml/badge.svg)](https://github.com/radical-cybertools/radical.entk/actions/workflows/python-app.yml)
[![codecov](https://codecov.io/gh/radical-cybertools/radical.entk/branch/devel/graph/badge.svg?token=dHn74ChzmX)](https://codecov.io/gh/radical-cybertools/radical.entk)
[![conda](https://anaconda.org/conda-forge/radical.entk/badges/version.svg)](https://anaconda.org/conda-forge/radical.entk)

The documentation for Ensemble Toolkit is available at
[http://radicalentk.readthedocs.io/en/latest/](http://radicalentk.readthedocs.io/en/latest/)
",2023-07-07 15:56:39+00:00
enso,enso,enso-org/enso,Hybrid visual and textual functional programming.,https://enso.org,False,6529,2023-07-07 11:01:59+00:00,2016-12-16 17:23:37+00:00,243,86,43,426,2023.2.1-nightly.2023.6.20,2023-06-19 19:03:36+00:00,Apache License 2.0,3184,ide-v2.0.0-alpha.18,447,2021-10-17 20:56:49+00:00,2023-07-07 15:56:32+00:00,2023-07-07 12:47:10+00:00,"<p align=""center"">
  <a href=""https://enso.org"">
<img src=""https://user-images.githubusercontent.com/1623053/114557275-cbd27a80-9c69-11eb-9e4d-a60187cdb7a4.gif"" width=""640"" height=""640""/>
  </a>
</p>

<p align=""center"">
  <a href=""https://discord.gg/enso"">
    <img src=""https://img.shields.io/discord/401396655599124480.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2""
         alt=""Chat"">
  </a>
  <a href=""https://github.com/enso-org/enso/actions"">
    <img src=""https://github.com/enso-org/enso/workflows/Engine%20CI/badge.svg""
         alt=""Actions Status"">
  </a>
  <a href=""https://github.com/enso-org/enso/actions"">
    <img src=""https://github.com/enso-org/enso/workflows/GUI%20CI/badge.svg""
         alt=""Actions Status"">
  </a>
  <a href=""https://github.com/enso-org/enso/blob/develop/LICENSE"">
    <img src=""https://img.shields.io/static/v1?label=Compiler%20License&message=Apache%20v2&color=2ec352&labelColor=2c3239""
         alt=""License"">
  </a>
  <a href=""https://github.com/enso-org/enso/blob/develop/app/gui/LICENSE"">
    <img src=""https://img.shields.io/static/v1?label=GUI%20License&message=AGPL%20v3&color=2ec352&labelColor=2c3239""
         alt=""License"">
  </a>
</p>

<br/>

# [Enso.org](https://enso.org). Get insights you can rely on. In real time.

Enso is an award-winning interactive programming language with dual visual and
textual representations. It is a tool that spans the entire stack, going from
high-level visualization and communication to the nitty-gritty of backend
services, all in a single language. Watch the following introduction video to
learn what Enso is, and how it helps companies build data workflows in minutes
instead of weeks.

<br/>

<a href=""https://www.youtube.com/watch?v=fQvWMoOjmQk"" rel=""nofollow"">
<img width=""692"" alt=""Screenshot 2021-04-15 at 12 16 32"" src=""https://user-images.githubusercontent.com/1623053/114854125-c8173300-9de4-11eb-9b10-99a331eb2251.png"">
</a>

<br/>

<br/>

# Enso's Features

Turning your data into knowledge is slow and error-prone. You can’t trust tools
that don’t embrace best practices and provide quality assurance. Enso redefines
the way you can work with your data: it is interactive, provides intelligent
assistance, and was designed on a strong mathematical foundation, so you can
always trust the results you get.

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/web-apps-development/001-algorithm.svg"">
<ul><ul>
    <b>Intelligent suggestions of possible next steps. Build workflows in minutes instead of weeks.</b><br/>
    Enso analyses the data, suggests possible next steps, and displays related
    help and examples. It lets you build dashboards, RPA workflows, and apps,
    with no coding required. Enso ships with a robust set of libraries, allowing
    you to work with local files, databases, HTTP services, and other
    applications in a seamless fashion.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/badges/018-military.svg"">
<ul><ul>
    <b>Reproducible, trustworthy results.</b><br/>
    Versioning and visual data quality management allow you to trust the results
    that you get.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/basic-ui/041-graph.svg"">
<ul><ul>
    <b>A powerful, purely functional language. Both visual and textual.</b><br/>
    Enso incorporates many recent innovations in data processing and programming
    language design to allow you to work interactively and trust the results
    that you get. It is a purely functional programming language with
    higher-order functions, user-defined algebraic datatypes, pattern-matching,
    and two equivalent representations that you can switch between on-demand.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/business/036-puzzle.svg"">
<ul><ul>
    <b>Mix languages with close-to-zero interop overhead.</b><br/>
    Import any library from Enso, Java, JavaScript, R, or Python, and use
    functions, callbacks, and data types without any wrappers. Enso uses
    <a href=""https://www.graalvm.org"">GraalVM</a> to compile them to the same
    instruction set with a unified memory model.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/startup-and-new-business/051-rocket.svg"">
<ul><ul>
    <b>Fast. Up to 80x faster than Python.</b><br/>
    It can even run other languages faster than their official runtimes.
    <a href=""https://github.com/oracle/fastr"">Enso-R (using FastR on the GraalVM)</a>
    is 36x faster than GNU-R.
  <br/><a href=""https://github.com/enso-org/benchmarks"">See benchmarks →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/school/063-palette.svg"">
<ul><ul>
    <b>A cutting-edge visualization engine.</b><br/>
    Enso is equipped with a highly-tailored WebGL visualization engine capable
    of displaying many millions of data points at 60 frames per second in a web
    browser. Currently, Enso includes a set of core data visualizations out of
    the box, and you can easily extend it with libraries such as D3.js,
    Three.js, Babylon.js, deck.gl, VTK.js, Potree, and many more.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<img align=""left"" width=""44px"" src=""https://raw.githubusercontent.com/enso-org/icons/master/blue/with-bg/it/shipping/004-cargo-ship.svg"">
<ul><ul>
    <b>Runs everywhere.</b><br/>
    Enso is available on macOS, Windows, and GNU/Linux, and the Enso IDE runs on
    web-native technologies. In time, you'll be able to run it in the
    web-browser, giving even your tablet and phone access to your data.
  <br/><a href=""https://enso.org"">Learn more →<a/>
</ul></ul>

<br/>

# Getting Started

<img align=""right"" alt=""An example Enso graph"" src=""https://user-images.githubusercontent.com/1623053/105841783-7c1ed400-5fd5-11eb-8493-7c6a629a84b7.png"" width=""380"">

<img align=""left"" width=""36px"" src=""https://github.com/google/material-design-icons/blob/master/src/action/get_app/materialiconsround/24px.svg"">
<ul><ul>
    <b>Download Enso</b><br/>
    <ul>
        <li><a href=""https://github.com/enso-org/enso/releases"">Enso Interactive Environment</a></li>
        <li><a href=""https://github.com/enso-org/enso/releases"">Enso Compiler (CLI, optional)</a></li>
    </ul>
</ul></ul>

<img align=""left"" width=""36px"" src=""https://github.com/google/material-design-icons/blob/master/src/social/school/materialiconsround/24px.svg"">
<ul><ul>
    <b>Watch Tutorials</b><br/>
    <ul>
        <li><a href=""https://github.com/enso-org/enso/blob/develop/app/gui/docs/product/shortcuts.md"">Enso keyboard shortcuts</a></li>
        <li><a href=""https://youtu.be/_Twh45PI_vU&list=PLk8NuufOVK01GhaObYr1_gqeASlkj2um0"">Enso 101</a></li>
        <li><a href=""https://youtu.be/hFxugfGbvGI?list=PLk8NuufOVK01GhaObYr1_gqeASlkj2um0"">Analyze trams data</a></li>
        <li><a href=""https://youtu.be/gXnojGR6wOI?list=PLk8NuufOVK01GhaObYr1_gqeASlkj2um0"">Analyze GitHub Stargazers data</a></li>
        <li><a href=""https://www.youtube.com/playlist?list=PLk8NuufOVK01GhaObYr1_gqeASlkj2um0"">... other tutorials</a></li>
    </ul>
</ul></ul>

<img align=""left"" width=""36px"" src=""https://github.com/google/material-design-icons/blob/master/src/hardware/cast_for_education/materialiconsround/24px.svg"">
<ul><ul>
    <b>Watch Video Podcasts</b><br/>
    <ul>
        <li><a href=""https://www.youtube.com/watch?v=U3pb7HiZIBg&t=2996s&ab_channel=Enso"">Enso Textual Language Basics</a></li>
      <li><a href=""https://youtu.be/bcpOEX1x06I"">Using Java libraries in Enso</a></li>
      <li><a href=""https://youtu.be/wFkh5LgAZTs"">Custom data visualizations</a></li>
      <li><a href=""https://youtu.be/BibjcUjdkO4"">Enso vision. What is in the future?</a></li>
        <li><a href=""https://www.youtube.com/c/Enso_org/videos?view=2&sort=dd&live_view=503&shelf_id=3"">... other video podcasts</a></li>
    </ul>
</ul></ul>

<img align=""left"" width=""36px"" src=""https://github.com/google/material-design-icons/blob/master/src/communication/forum/materialiconsround/24px.svg"">
<ul><ul>
    <b>Join Our Community</b><br/>
    <ul>
        <li><a href=""https://discord.gg/enso"">Discord chat. Get help, share your use cases, meet the team behind Enso and other Enso users!</a></li>
    </ul>
</ul></ul>

<img align=""left"" width=""36px"" src=""https://github.com/google/material-design-icons/blob/master/src/av/new_releases/materialiconsround/24px.svg"">
<ul><ul>
    <b>Keep Up With the Latest Updates</b><br/>
    <ul>
        <li><a href=""https://medium.com/@enso_org"">Enso Development Blog</a></li>
        <li><a href=""http://eepurl.com/bRru9j"">Enso Mailing List</a></li>
    </ul>
</ul></ul>

<br/><br/>

# Enso Source Code

If you want to start _using_ Enso, please see the download links in the
[getting started](#getting-started) section above. Alternatively, you can get
the IDE [here](https://github.com/enso-org/enso/releases). This section is
intended for people interested in contributing to the development of Enso.

Enso is a community-driven open source project which is, and will always be,
open and free to use. Join us, help us to build it, and spread the word!

<br/>

### Project Components

Enso consists of several sub projects:

- **Enso Engine:** The Enso Engine is the set of tools that implement the Enso
  language and its associated services. These include the Enso interpreter, a
  just-in-time compiler and runtime (both powered by
  [GraalVM](https://www.graalvm.org)), and a language server that lets you
  inspect Enso code as it runs. These components can be used on their own as
  command line tools.

- **Enso IDE:** The
  [Enso IDE](https://github.com/enso-org/enso/tree/develop/app/gui) is a desktop
  application that allows working with the visual form of Enso. It consists of
  an Electron application, a high performance WebGL UI framework, and the
  searcher which provides contextual search, hints, and documentation for all of
  Enso's functionality.

<br/>

### License

The Enso Engine is licensed under the
[Apache 2.0](https://opensource.org/licenses/apache-2.0), as specified in the
[LICENSE](https://github.com/enso-org/enso/blob/develop/LICENSE) file. The Enso
IDE is licensed under the [AGPL 3.0](https://opensource.org/licenses/AGPL-3.0),
as specified in the
[LICENSE](https://github.com/enso-org/enso/blob/develop/app/gui/LICENSE) file.

This license set was chosen to provide you with complete freedom to use Enso,
create libraries, and release them under any license of your choice, while also
allowing us to release commercial products on top of the platform, including
Enso Cloud and Enso Enterprise server managers.

<br/>

### Contributing to Enso

Enso is a community-driven open source project which is and will always be open
and free to use. We are committed to a fully transparent development process and
highly appreciate every contribution. If you love the vision behind Enso and you
want to redefine the data processing world, join us and help us track down bugs,
implement new features, improve the documentation or spread the word!

If you'd like to help us make this vision a reality, please feel free to join
our [chat](https://discord.gg/enso), and take a look at our
[development and contribution guidelines](./docs/CONTRIBUTING.md). The latter
describes all the ways in which you can help out with the project, as well as
provides detailed instructions for building and hacking on Enso.

If you believe that you have found a security vulnerability in Enso, or that you
have a bug report that poses a security risk to Enso's users, please take a look
at our [security guidelines](./docs/SECURITY.md) for a course of action.

<br/>

### Enso's Design

If you would like to gain a better understanding of the principles on which Enso
is based, or just delve into the why's and what's of Enso's design, please take
a look in the [`docs/` folder](./docs/). It is split up into subfolders for each
component of Enso. You can view this same documentation in a rendered form at
[the developer docs website](https://enso.org/docs/developer).

This folder also contains a document on Enso's
[design philosophy](./docs/enso-philosophy.md), that details the thought process
that we use when contemplating changes or additions to the language.

This documentation will evolve as Enso does, both to help newcomers to the
project understand the reasoning behind the code, and also to act as a record of
the decisions that have been made through Enso's evolution.
",2023-07-07 15:56:43+00:00
eoulsan,eoulsan,GenomiqueENS/eoulsan,  A pipeline and a framework for NGS analysis (RNA-Seq and soon Chip-Seq),http://www.outils.genomique.biologie.ens.fr/eoulsan2,False,50,2023-06-15 07:32:13+00:00,2014-06-24 08:09:37+00:00,11,16,10,21,v2.6.1,2022-12-13 14:12:18+00:00,Other,5379,v2.6.1,30,2022-12-13 14:12:18+00:00,2023-07-04 09:29:23+00:00,2023-07-04 09:29:03+00:00,"![Eoulsan Logo](/src/site/resources/images/eoulsan.png)

This is the Eoulsan git reference repository.

Eoulsan is a versatile framework based on the Hadoop implementation of the MapReduce algorithm, dedicated to high throughput sequencing data analysis on distributed computers. With Eoulsan, users can easily set up a cloud computing cluster and automate the analysis of several samples at once using various software solutions available. Working either on standalone workstations or cloud computing clusters, Eoulsan provides an integrated and flexible solution for RNA-Seq data analysis of differential expression.

Here is a quick access to the main pages:
- [Install Eoulsan](http://www.outils.genomique.biologie.ens.fr/eoulsan2/installing.html)
- New user's [tutorials](https://github.com/GenomicParisCentre/eoulsan/wiki):
  - [Bulk RNA-seq data](https://github.com/GenomicParisCentre/eoulsan/wiki/Bulk-RNA-Seq-tutorial)
  - [Bulk RNA-seq long read data (Nanopore)](https://github.com/GenomicParisCentre/eoulsan/wiki/Bulk-RNA-Seq-Nanopore-tutorial)
  - [Single cell RNA-seq (Smart-Seq2)](https://github.com/GenomicParisCentre/eoulsan/wiki/Smart-Seq2-scRNA-seq-tutorial)
  - [Single cell RNA-seq (10x Genomics)](https://github.com/GenomicParisCentre/eoulsan/wiki/10x-Genomics-scRNA-seq-tutorial)
- [Reference website](http://outils.genomique.biologie.ens.fr/eoulsan/) to get the complete documentation.
- [Developers wiki](https://github.com/GenomicParisCentre/eoulsan/wiki/HomeDeveloper) to get information on how to contribute to the Eoulsan project and develop plug-ins.

![Eoulsan single-cell workflows graphics](https://github.com/GenomicParisCentre/eoulsan/blob/master/src/main/java/files/Eoulsan_single_cell_workflow_v2.png)
",2023-07-07 15:56:48+00:00
ergatis,ergatis,jorvis/ergatis,"Ergatis is a web-based utility that is used to create, run, and monitor reusable computational analysis pipelines.",,False,16,2022-12-07 13:51:05+00:00,2013-10-21 03:11:24+00:00,15,9,6,4,v2r19b4,2015-07-29 13:04:29+00:00,Other,6908,v2r19b4,4,2015-07-29 13:04:29+00:00,,2023-01-04 15:16:46+00:00,,2023-07-07 15:56:52+00:00
eugene,eugene,tschiex/eugene,Eugene is an integrative genome annotation software,,False,0,2022-08-18 08:37:43+00:00,2022-08-18 08:30:46+00:00,1,2,5,1,v4.3a,2022-08-18 10:42:06+00:00,,1782,v4.3,6,2021-07-05 09:38:17+00:00,,2022-08-18 10:42:06+00:00,"# Welcome to eugene
## An integrative gene finder for eukaryotic and prokaryotic genomes

This software is OSI Certified  Open Source Software. OSI Certified is
a  certification  mark  of  the  Open Source  Initiative.   eugene  is
governed by the ARTISTIC  LICENSE (see www.opensource.org). Please see
the file COPYING for details.  For documentation, please see the files
in the  doc subdirectory.  For building  and installation instructions
please see the INSTALL file. For creating a new eugene release, please
see the RELEASE file.

## For more information

Visit eugene's web site at [INRAE](http://eugene.toulouse.inrae.fr).
",2023-07-07 15:56:56+00:00
fireworks,fireworks,materialsproject/fireworks,The Fireworks Workflow Management Repo.,https://materialsproject.github.io/fireworks,False,290,2023-07-07 02:27:04+00:00,2013-01-08 19:18:02+00:00,166,63,49,105,v2.0.2,2021-03-11 23:22:59+00:00,Other,3836,v2.0.2,190,2021-03-11 23:22:59+00:00,2023-07-07 02:27:06+00:00,2023-05-17 16:26:56+00:00,"# <img alt=""FireWorks"" src=""docs_rst/_static/FireWorks_logo.png"" width=""250"">

FireWorks stores, executes, and manages calculation workflows.

- **Website (including documentation):** https://materialsproject.github.io/fireworks/
- **Help/Support:** https://discuss.matsci.org/c/fireworks
- **Source:** https://github.com/materialsproject/fireworks/

If you like FireWorks, you might also like [rocketsled](https://github.com/hackingmaterials/rocketsled).

If you find FireWorks useful, please consider citing the paper:
```
Jain, A., Ong, S. P., Chen, W., Medasani, B., Qu, X., Kocher, M., Brafman, M.,
Petretto, G., Rignanese, G.-M., Hautier, G., Gunter, D., and Persson, K. A.
(2015) FireWorks: a dynamic workflow system designed for high-throughput
applications. Concurrency Computat.: Pract. Exper., 27: 5037–5059.
doi: 10.1002/cpe.3505.
```
",2023-07-07 15:57:04+00:00
fissionworkflows,fission-workflows,fission/fission-workflows,"Workflows for Fission: Fast, reliable and lightweight function composition for serverless functions",,False,355,2023-06-15 02:56:28+00:00,2017-07-13 07:06:47+00:00,39,30,9,6,0.6.0,2018-10-15 18:56:46+00:00,Apache License 2.0,589,0.6.0,6,2018-10-15 18:56:46+00:00,2023-06-15 02:56:27+00:00,2020-07-02 07:50:15+00:00,"# Fission Workflows
[![Build Status](https://travis-ci.org/fission/fission-workflows.svg?branch=master)](https://travis-ci.org/fission/fission-workflows)
[![Go Report Card](https://goreportcard.com/badge/github.com/fission/fission-workflows)](https://goreportcard.com/report/github.com/fission/fission-workflows)
[![Fission Slack](http://slack.fission.io/badge.svg)](http://slack.fission.io)

[![Fission Workflows](Docs/assets/fission-workflows-logo.png)](http://fission.io/workflows)

----
[fission.io](http://fission.io)  [@fissionio](http://twitter.com/fissionio)

Fission Workflows is a workflow-based serverless function composition framework built on top of the [Fission](https://github.com/fission/fission) Function-as-a-Service (FaaS) platform.

:warning: Fission Workflows is currently in maintenance mode due to a time constraints of the core Fission team. If you are interested in contributing or helping maintain this project, contact the Fission team on the Fission Slack. :warning:

### Highlights
- **Fault-Tolerant**: Fission Workflows engine keeps track of state, re-tries, handling of errors, etc. By internally utilizing event sourcing, it allows the engine to recover from failures and continue exactly were it left off.   
- **Scalable**: Other than a backing data store, the workflow system is stateless and can easily be scaled. The independent nature of workflows allows for relatively straightforward sharding of workloads over multiple workflow engines.
- **High Performance**: In contrast to existing workflow engines targeting other domains, Fission Workflows is designed from the ground up for low overhead, low latency workflow executions.
- **Extensible**: All main aspects of the engine are extensible. For example, you can even define your own control flow constructs.
- **Lightweight**: With just the need for a single data store (NATS Streaming) and a FaaS platform (Fission), the engine consumes minimal resources.

----

### Philosophy

The Fission Functions-as-a-Service framework provides simplicity and
quick time-to-value for functions on any infrastructure using
Kubernetes.  

Functions tend to do one logically separate task, and they're usually
short-lived.  For many relatively simple applications this is good
enough.  But a more complex application that uses serverless functions
must, in some way, _compose_ functions together.

There are several ways to compose functions.  A function could invoke
another function using the Fission API or HTTP.  But this requires the
calling function to handle serialization, networking, etc.

You could also set up functions to be invoked using message queue topics.
This requires less boilerplate within each function, but the structure
of the application is not explicit; dependencies are buried inside
mappings of message queue topics to functions.

In addition, both these approaches are operationally difficult, in
terms of error analysis, performance debugging, upgrades, etc.

Workflows have been popular in other domains, such as data processing
and scientific applications, and recently got introduced into the
serverless model by AWS Step Functions and Azure Logic Apps.

Fission Workflows is an open-source alternative to these workflow
systems.  It allows users to compose Fission functions in powerful
ways. Users can define their own control flow constructs, which in
most other workflow engines are part of the internal API.

### Concepts

**Workflows** can generally be represented in as a Directed Acyclic Graph (DAG).
Consider the following example, a common pattern, the diamond-shaped workflow:

<!--
```
(A) -> (B) -> (C) ----
          \            \     
            -> (D) ->  (E) -> (F)
```
-->

![Workflow Example](Docs/assets/workflow-example.png)

In this graph there is a single _starting task_ A, a _scatter task_ B
triggering parallel execution of two _branches_ with tasks C and D,
followed by a _synchronizing task_ E collecting the outputs of the
parallel tasks.

Finally the graph concludes once _final task_ F completes.

Although Fission Workflows has more functionality such as
conditional branches and advanced control flow options, it
fundamentally executes a dependency graph.

```yaml
apiVersion: 1
output: WhaleWithFortune
tasks:
  GenerateFortune:
    run: fortune
    inputs: ""{$.Invocation.Inputs.default}""

  WhaleWithFortune:
    run: whalesay
    inputs: ""{$.Tasks.GenerateFortune.Output}""
    requires:
    - GenerateFortune
```
**Task** (also called a function here) is an atomic task, the 'building block' of a workflows. 

Currently there are two options for executing tasks.  First, Fission is
used as the main function execution runtime, using _fission functions_
as tasks.  Second, for very small tasks, such as flow control
constructs, _internal functions_ execute within the workflow
engine to minimize the network overhead.

A workflow execution is called a **(Workflow) Invocation**. The Fission Workflows engine
assigns an UID invocation and stores it, persistently, in the data-store.  This allows
users to reference the invocation during and after the execution, such as to view the
progress so far.

Finally, **selectors** and **data transformations** are _inline
functions_ which you can use to manipulate data without having to
create a task for it.  These _inline functions_ consist of commonly
used transformations, such as getting the length of an array or
string.  Additionally, selectors allow users to only pass through
certain properties of data. In the example workflow, the JSONPath-like
selector selects the `default` input of the invocation:

See the [docs](./Docs) for a more extensive, in-depth overview of the system.

### Usage
```bash
#
# Add binary environment and create two test function on your Fission setup 
#
fission env create --name binary --image fission/binary-env
fission function create --name whalesay --env binary --deploy examples/whales/whalesay.sh
fission function create --name fortune --env binary --deploy examples/whales/fortune.sh

#
# Create a workflow that uses those two functions; a workflow
# is just a function that uses the special ""workflow"" environment.
#
fission function create --name fortunewhale --env workflow --src examples/whales/fortunewhale.wf.yaml

#
# Map a HTTP GET to your new workflow function
#
fission route create --method GET --url /fortunewhale --function fortunewhale

#
# Invoke the workflow with an HTTP request
#
curl $FISSION_ROUTER/fortunewhale
``` 
See [examples](./examples) for other workflow examples.

### Installation
See the [installation guide](./INSTALL.md).

### Compiling
See the [compilation guide](./compiling.md).

### Status and Roadmap

This is an early release for community participation and user
feedback. It is under active development; none of the interfaces are
stable yet. It should not yet be used in production!

Contributions are welcome in whatever form, including testing,
examples, use cases, or adding features. For an overview of current
issues, checkout the [roadmap](./Docs/roadmap.md) or browse through
the open issues on Github.

Finally, we're looking for early developer feedback -- if you do use
Fission or Fission Workflows, we'd love to hear how it's working for
you, what parts in particular you'd like to see improved, and so on.

Talk to us on [slack](http://slack.fission.io) or
[twitter](https://twitter.com/fissionio).
",2023-07-07 15:57:08+00:00
flex,xp,druths/xp,A framework (comand line tool + libraries) for creating flexible compute pipelines,,False,54,2021-12-27 22:25:08+00:00,2015-05-03 09:10:58+00:00,9,5,2,1,r1-0-0,2016-02-22 04:16:59+00:00,Other,158,r1-0-0,1,2016-02-22 04:16:59+00:00,,2019-04-16 04:46:28+00:00,"
# xp [![Build Status](https://travis-ci.org/druths/xp.svg?branch=master)](https://travis-ci.org/druths/xp) [![Doc Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](http://xpds.readthedocs.org/en/latest/) #

*Expressive pipelines for data science*

Data science projects get disorganized quickly. Every test involves a new
script, each script requires a panoply of arguments and produces one or more
data files.  Keeping track of all this implied structure is a pain - what is
this script for?  What does it depend on?  What created this data file? Which
parameters updated this table in the database?

Enter xp - a utility that allows you to express and run all the computational
tasks in a project. Crucially, it captures the specific parameters used for
each task, the data files produced, and any dependencies that task has on other
tasks. All this is captured in files called *pipelines* (which can even be
connected to one another).  Toss in some helpful comments, and you have
executable documentation for your project.

This may sound a lot like scientific notebook environments (e.g., Jupyter and
Mathematica), but there are some key differences. Notebooks only allow linear
dependencies between computational tasks - which is a tremendous simplification
of even moderate-sized projects.

To this end, it has three primary goals:

  1. Capture the task-level logic and structure of a data science project in a
     language-agnostic way

  1. Make it possible to trace data back to the specific tasks (and commands)
     that produced it

  1. Connect documentation of tasks to the task logic itself

It aims to achieve these three things without introducing any overhead.  You
won't have to write more code than you currently are doing, write or maintain
any extra documentation, or use fancy data management solutions. Whatever you
are already doing, xp is compatible with it.

xp is a command-line tool for building data-science pipelines, particularly
in the research context. By *pipeline*, we mean writing tasks that depend on
one another.  Imagine *make*, except with a lot of intelligence built into it
that is relevant to data science work.

Moreover, xp makes it easy to create and update pipelines while always
retaining a connection to the data that the pipeline produced and is
self-documenting at the same time.

*Detailed documentation is available on [readthedocs](http://xpds.readthedocs.org/en/latest/).*

# Installation #

Install xp off [pypi](https://pypi.python.org/pypi?name=xp&:action=display) using

	pip install xp

or install from source by downloading from github and running

	python setup.py install

or

	sudo python setup.py install

depending on permission settings for your python site-packages.

# Writing a pipeline #

A *pipeline* is a sequence of steps (called *tasks*) that manipulates data.  On
a very practical level, each task takes some data as input and produces
other data as output. In the example below, there are four tasks, each
involved in a different part of the workflow.

```
# Pipeline: cluster_data

DATA_URL=http://foobar:8080/data.tsv
NAME_COLUMN=1
COUNT_COLUMN=2
ALPHA=2

download_data:
	code.sh:
		curl $DATA_URL > data.tsv

extract_columns: download_data
	code.sh:
		cut -f $NAME_COLUMN,$COUNT_COLUMN data.tsv | tail +2 > xdata.tsv
	code.py:
		from csv import reader
		fout = open('xdata2.tsv','w')
		for data in reader(open('xdata.tsv','r'),delimiter='\t'):
			fout.write('%s\t%s\n' % (data[0],data[1][1:-1])
	
cluster_rows.sh: extract_columns
	./cluster.sh --alpha $ALPHA xdata2.tsv > clusters.tsv

plot_clusters: cluster_rows
	code.gnuplot:
		plot ""clusters.tsv"" using 1:2 title 'Cluster quality'

```

Tasks can depend on other tasks (e.g., `extract_columns` depends on
`download_data`) - either in the same pipeline or in other pipelines.  By
making tasks depend on other pipelines, it's possible to treat pipelines as
modular parts of much larger workflows.

Once a task completes without error, it is *marked* - which flags it as not
needing to be run again.  In order to re-run a task, one can simply unmark it
and run it again.

If you choose to run a task which has unmarked dependencies, these will be run
before the task itself is run - in this way, an entire workflow can be run
using a single command.

A task contains one or more *blocks* which describe the actual computational
steps being taken. As is seen in the example above, blocks can contain code for
various different languages - making it possible to stitch together workflows
that involve different languages. A single task can even contain multiple
blocks for the same or different languages.

Currently, xp supports four block types:

  - *export* (`export`) - this allows environment variables to be set and 
  	unset within the context of a specific task

  - *python* (`code.py`)

  - *shell* (`code.sh`)

  * *gnuplot* (`code.gnuplot`)

These, of course, require that the appropriate executables are present on the
system. To customize the executable used, environment variables can be set
(`PYTHON_EXEC` and `GNUPLOT_EXEC`, respectively).

Future releases will support additional languages natively and also provide a
plugin mechanism for adding new block types. 

Notice in the example above that the task `cluster_rows` places a language
suffix right after the task name.  This is called a simple task: it consists of
exactly one block, written in the language of the language suffix, which
follows the task definition line directly. This basically is a useful shortcut
for tasks which contain only one block.

Once a pipeline has been written, it can be run using the xp command-line tool.

  xp run pipeline_file

The command-line tool also allows easy marking (`mark`), unmarking (`unmark`),
and querying task info (`tasks`) for a pipeline.

## Pipeline-specific Data ##

A common activity that creates a lot of data management issues is running
effectively the same or similar pipelines using different parameter settings:
files can get overwritten and, more generally, the user typically loses track
of exactly which files came from what setting.

In xp, files produced by a pipeline can be easily bound to their pipeline,
eliminating this confusion.

```
DATA_URL=http://foobar:8080/data.tsv
ALPHA=2
...

download_data:
	code.sh:
		curl $DATA_URL > $PLN(data.tsv)

...
```

In the excerpt above, the file `data.tsv` is being bound to this pipeline using
the `$PLN(.)` function. In effect, the file is prefixed (either by name or
placed in a pipeline-specific directory).  Future references to this file via
`$PLN(data.tsv)` will access only this pipeline's version of the file - even if
many pipelines are downloading the files at various times.

(Note that `$` is treated as a special character by xp and so identifiers such
as `$PATH` or `$1` will be parsed as xp variables. To avoid this, escape the
`$` using `\`, e.g. `\$PATH` or `\$1`).

## Extending Pipelines ##

In some cases, one will want to run exactly the same pipeline over and over
with different parameter settings. To support this, xp allows *extending*
pipelines.  Much like subclassing, extending a pipeline brings all the content
of one pipeline into another one.  Assume we are clustering some data using the
process here (in pipeline `cluster_pln`).  The process is parameterized by the
alpha value.

```
# Pipeline: cluster_pln

CLUSTERS_FNAME=clusters.tsv

cluster_rows: extract_columns
	code.sh:
		./cluster.sh --alpha $ALPHA xdata2.tsv > $PLN($CLUSTERS_FNAME)

plot_clusters: cluster_rows
	code.gnuplot:
		plot ""$PLN($CLUSTERS_FNAME)"" using 1:2 title 'Cluster quality at alpha=$ALPHA'
```

We can extend this pipeline to retain the same workflow, but use different values:

```
# Pipeline: cluster_a2
extend cluster_pln

ALPHA=2
```

and again for a different value

```
# Pipeline: cluster_a3
extend cluster_pln

ALPHA=3
```

Note that in each case the cluster data will be stored to `$PLN(clusters.tsv)`,
so that each pipeline will have its own separate stored data.

## Connecting Pipelines Together ##

It's quite reasonable to expect that one pipeline could feed into another
pipeline. xp supports this - pipelines can depend on the tasks in other
pipelines - and in doing so, create even larger workflows that retain their
nice modular organization.

Consider that the earlier pipeline given above, `cluster_a2`, could actually be
assembling the data for a classifier. Let's break this classifier portion of
the project into its own workflow.

```
# Pipeline: lda_classifier

use cluster_a2 as cdata

NEWS_ARTICLES=articles/*.gz

build_lda: cdata.cluster_rows
	export:
		LDA_CLASSIFIER=lda_runner

	code.sh:
		${LDA_CLASSIFIER} -input $PLN(cdata, ${cdata.CLUSTERS_FNAME}) -output $PLN(lda_model.json)

label_articles: build_lda
	export:
		LDA_LABELER=/opt/bin/lda_labeler
	code.sh:	
		${LDA_LABELER} -model $PLN(lda_model.json) -data ""$NEWS_ARTICLES"" > $PLN(news.labels)
```

In the example above, notice how the task `build_lda` both depends on a task
from the `cluster_a2` pipeline and *also* uses data from that pipeline's
namespace, `$PLN(cdata, ${cdata.CLUSTERS_FNAME})`, where `${cdata.CLUSTERS_FNAME}`
references the `CLUSTERS_FNAME` variable inherited by `cluster_a2` from
`cluster_pln`.

Of course, we might want to try multiple classifiers on the same source data,
so we can create other pipelines that use `cluster_a2`, shown next.

```
# Pipeline: crf_classifier

use cluster_a2 as cdata

NEWS_ARTICLES=articles/*.gz

build_crf_model: cdata.cluster_rows
	code.sh:
		/opt/bin/build_crf -data $PLN(cdata, ${cdata.CLUSTERS_FNAME}) -output $PLN(crf_model.json)

label_articles.py: build_crf_model
		import crf_model

		model = crf_model.load_model('$PLN(crf_model.json)')
		model.label_documents(docs='$NEWS_ARTICLES',out_file='$PLN(news.labels)')
```

## Examples ##

See the `examples/` directory in the xp root directory to see some real
pipelines that demonstrate the core features of the tool.

# Command-line usage #

The `xp` command provides several core capabilities:

  - `xp tasks <pipeline>` will out info about one or more tasks in the pipeline including whether they are marked

  - `xp run <pipeline>` will run a pipeline (or a task within a pipeline)

  - `xp mark <pipeline>` will mark specific tasks or an entire pipeline

  - `xp unmark <pipeline>` will unmark specific tasks or an entire pipeline

All of these commands have help messages to help their correct use.

",2023-07-07 15:57:13+00:00
apacheflink,flink,apache/flink,Apache Flink,,False,21542,2023-07-07 13:46:05+00:00,2014-06-07 07:00:10+00:00,12117,943,284,0,,,Apache License 2.0,33661,v0.4-rc1,225,2013-12-20 12:45:17+00:00,2023-07-07 15:37:16+00:00,2023-07-07 13:25:26+00:00,"# Apache Flink

Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.

Learn more about Flink at [https://flink.apache.org/](https://flink.apache.org/)


### Features

* A streaming-first runtime that supports both batch processing and data streaming programs

* Elegant and fluent APIs in Java and Scala

* A runtime that supports very high throughput and low event latency at the same time

* Support for *event time* and *out-of-order* processing in the DataStream API, based on the *Dataflow Model*

* Flexible windowing (time, count, sessions, custom triggers) across different time semantics (event time, processing time)

* Fault-tolerance with *exactly-once* processing guarantees

* Natural back-pressure in streaming programs

* Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming)

* Built-in support for iterative programs (BSP) in the DataSet (batch) API

* Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms

* Compatibility layers for Apache Hadoop MapReduce

* Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem


### Streaming Example
```scala
case class WordWithCount(word: String, count: Long)

val text = env.socketTextStream(host, port, '\n')

val windowCounts = text.flatMap { w => w.split(""\\s"") }
  .map { w => WordWithCount(w, 1) }
  .keyBy(""word"")
  .window(TumblingProcessingTimeWindow.of(Time.seconds(5)))
  .sum(""count"")

windowCounts.print()
```

### Batch Example
```scala
case class WordWithCount(word: String, count: Long)

val text = env.readTextFile(path)

val counts = text.flatMap { w => w.split(""\\s"") }
  .map { w => WordWithCount(w, 1) }
  .groupBy(""word"")
  .sum(""count"")

counts.writeAsCsv(outputPath)
```



## Building Apache Flink from Source

Prerequisites for building Flink:

* Unix-like environment (we use Linux, Mac OS X, Cygwin, WSL)
* Git
* Maven (we recommend version 3.8.6 and require at least 3.1.1)
* Java 8 or 11 (Java 9 or 10 may work)

```
git clone https://github.com/apache/flink.git
cd flink
./mvnw clean package -DskipTests # this will take up to 10 minutes
```

Flink is now installed in `build-target`.

*NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies. Maven 3.1.1 creates the libraries properly.
To build unit tests with Java 8, use Java 8u51 or above to prevent failures in unit tests that use the PowerMock runner.*

## Developing Flink

The Flink committers use IntelliJ IDEA to develop the Flink codebase.
We recommend IntelliJ IDEA for developing projects that involve Scala code.

Minimal requirements for an IDE are:
* Support for Java and Scala (also mixed projects)
* Support for Maven with Java and Scala


### IntelliJ IDEA

The IntelliJ IDE supports Maven out of the box and offers a plugin for Scala development.

* IntelliJ download: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)
* IntelliJ Scala Plugin: [https://plugins.jetbrains.com/plugin/?id=1347](https://plugins.jetbrains.com/plugin/?id=1347)

Check out our [Setting up IntelliJ](https://nightlies.apache.org/flink/flink-docs-master/flinkDev/ide_setup.html#intellij-idea) guide for details.

### Eclipse Scala IDE

**NOTE:** From our experience, this setup does not work with Flink
due to deficiencies of the old Eclipse version bundled with Scala IDE 3.0.3 or
due to version incompatibilities with the bundled Scala version in Scala IDE 4.4.1.

**We recommend to use IntelliJ instead (see above)**

## Support

Don’t hesitate to ask!

Contact the developers and community on the [mailing lists](https://flink.apache.org/community.html#mailing-lists) if you need any help.

[Open an issue](https://issues.apache.org/jira/browse/FLINK) if you find a bug in Flink.


## Documentation

The documentation of Apache Flink is located on the website: [https://flink.apache.org](https://flink.apache.org)
or in the `docs/` directory of the source code.


## Fork and Contribute

This is an active open-source project. We are always open to people who want to use the system or contribute to it.
Contact us if you are looking for implementation tasks that fit your skills.
This article describes [how to contribute to Apache Flink](https://flink.apache.org/contributing/how-to-contribute.html).


## About

Apache Flink is an open source project of The Apache Software Foundation (ASF).
The Apache Flink project originated from the [Stratosphere](http://stratosphere.eu) research project.
",2023-07-07 15:57:17+00:00
flowing-clj,flowing-clj,stain/flowing-clj,Flowing - Define and execute workflows in Clojure,,False,2,2022-04-11 18:36:23+00:00,2015-06-09 14:45:53+00:00,0,2,1,0,,,Apache License 2.0,27,0.1.0,1,2015-06-09 23:31:15+00:00,,2015-11-16 14:08:25+00:00,"# flowing-clj

A [Clojure](http://clojure.org/) library for building data-driven workflows (dataflows).

[![Clojars Project](http://clojars.org/flowing/latest-version.svg)](http://clojars.org/flowing)

[![Build Status](https://travis-ci.org/stain/flowing-clj.svg)](https://travis-ci.org/stain/flowing-clj)



## Building

Build or test using [Leiningen](http://leiningen.org/):

```
lein test
lein install
lein repl
```

## Related projects

As flowing-clj is just a prototype, you might prefer trying:

* Prismatic's [Plumbing and Graph](https://github.com/Prismatic/plumbing#graph-the-functional-swiss-army-knife)
* Threading macros [->](http://clojuredocs.org/clojure.core/-%3E) and [->>](http://clojuredocs.org/clojure.core/-%3E%3E) and [as->](http://clojuredocs.org/clojure.core/as-%3E)
* [core.async channels](http://www.braveclojure.com/core-async/)

## Usage

```clojure
(use 'flowing.core)

(def example-wf
  (workflow
    ; Define a series of steps, which can take 0 or more
    ; arguments. They do not need to be defined in any
    ; particular order.
    ; (The below are dummy examples with Thread/sleep to pretend
    ; to be doing some processing)
    (defstep get-sequence [id]
      (println ""Retrieving sequence for id"" id)
      (Thread/sleep 1000)
      ""GATTAGCAT"")
    (defstep alignment [sequence database]
      (println ""Aligning"" sequence ""in"" database)
      (Thread/sleep 2000)
      (if (= database ""cat"")
        (str "">"" database "" "" sequence)))
    (defstep pathways [fasta]
      (println ""Finding pathways for"" fasta)
      (Thread/sleep 1000)
      { :dog :cat
        :cat :tree
        :fireman :tree})
    (defstep similar [fasta paths]
      (println ""Finding similarities in"" (count paths) ""paths"")
      (Thread/sleep 1000)
      [ ""tiger"" ""lion"" ])

    ; Now link them together. Links be provided
    ; in any order, but you can only link to an input
    ; parameter once.
    (link ""CATGENE15"" (:id get-sequence)) ; constant value
    ; Each input parameter linked separately
    (link get-sequence (:sequence alignment))
    (link ""cat"" (:database alignment))
    ; Same output do multiple destinations
    (link alignment (:fasta pathways))
    (link alignment (:fasta similar))
    ; Steps are executed as soon as all inputs are ready
    ; and in parallell threads, but this link would
    ; cause :similar to run after :alignment
    (link pathways (:paths similar))))
```

The steps in the workflow will start executing in parallel,
as soon as all inputs are received:

```
#'user/example-wf
Retrieving sequence for id CATGENE15
user=> Aligning GATTAGCAT in cat
Finding pathways for >cat GATTAGCAT
Finding similarities in 3 paths
user=> 
```


Outputs can be retrieved from individual steps, while the workflow 
is running (and after):

```clojure
(println (wait-for-output get-sequence))
GATTAGCAT
```

`(wait-for-output)` is so called as it will block until 
the step has received all its required inputs and finished
executing.

```clojure
(println (wait-for-output similar))
[""tiger"" ""lion""]
```

`(wait-for-workflow)` will ensure all steps are complete, and return
a map with all the results values.

```clojure
(println (wait-for-workflow example-wf))
{ :get-sequence GATTAGCAT, 
  :alignment >cat GATTAGCAT, 
  :pathways {:cat :tree, :dog :cat, :fireman :tree}, 
  :similar [tiger lion] }
```

## How does it work?

`(step)` create a 
[promise](http://clojuredocs.org/clojure.core/promise)
for each input parameter. The body of the step is executed in a 
[future](http://clojuredocs.org/clojure.core/future), where it 
call [deref](http://clojuredocs.org/clojure.core/deref) on
the inputs so that the step body use the parameters as if
passed in normally to a function.

`(defstep)` is a shortcut for 
`(def foo (step ""foo"" [x y] ...))`, the defined symbol means 
the step can be referred to during both definition time and
after execution.

`(link)` will [deliver](http://clojuredocs.org/clojure.core/deliver)
the source value to the input promise, looked up as `(:inputA foo-step)`. 
The source can be either one of the other steps (in which case their 
*future* object is passed as-is) or any other expression (which is 
sent through a [delay](http://clojuredocs.org/clojure.core/delay) object).

Both `(step)` and `(link)` will return a map. The keys of `(step)` will
correspond to the input parameters, but as keywords. Additional special keys 
include `::name`, `::inputs` and `::output`.  Access these as `:flowing.core/name` 
etc., or use the convenience accessor functions `(step-name)`, `(inputs)` and
`(output-ref)`.

The map from `(link)` contain 
`::from` and `::to` showing the quoted source and destination expressions of 
the link, e.g. `(:inputB foo-step)`.

`(workflow)` creates a map of the steps, with keys being the keyword version
of the defined step name, e.g. `:step2`. The links are available under the key
`::links`. While the `(workflow)` call is not strictly needed to group or
execute the steps, it might be required in a future version of this library 
to be combined with a method like 
`(run-workflow)` ([issue #1](https://github.com/stain/flowing-clj/issues/1)).




## License

Copyright © 2015 Stian Soiland-Reyes

Distributed under the
[Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
See the file `LICENSE` for details.


## Contribute

This Clojure library is currently an experimental prototype; the
API and data structures might change at any time. To influence
this project, feel free to **contribute** by
raising [pull requests](https://github.com/stain/flowing-clj/pulls) or 
adding [issues](https://github.com/stain/flowing-clj/issues).

",2023-07-07 15:57:22+00:00
flowr,flowr,flow-r/flowr,Robust and efficient workflows using a simple language agnostic approach,http://flow-r.github.io/flowr,False,82,2023-06-05 11:05:45+00:00,2014-05-01 19:20:29+00:00,10,10,3,9,v0.9.11,2021-03-02 22:49:34+00:00,Other,795,v0.95,12,2015-03-18 21:15:21+00:00,2023-06-05 11:05:44+00:00,2021-03-10 15:43:53+00:00,"

<!--brand: |-
  <a href=""http://flow-r.github.io/flowr"">
  <img src='https://raw.githubusercontent.com/sahilseth/flowr/devel/vignettes/files/logo.png' alt='flowr icon' width='40px' height='40px' style='margin-top: -10px;'>
  </a>
-->

  
<meta property=""og:description"" content=""Easy, scalable big data pipelines using hpcc (high performance computing cluster)"">
<meta property=""og:title"" content=""flowr — Easy, scalable big data pipelines using hpcc"">
<meta name=""twitter:description"" content=""flowr - Easy, scalable big data pipelines using hpcc (high performance computing cluster)"">
<meta name=""twitter:title"" content=""flowr — Easy, scalable big data pipelines using hpcc (high performance computing cluster)"">

[![Build Status](https://github.com/sahilseth/flowr/workflows/R-CMD-check/badge.svg)](https://github.com/sahilseth/flowr/actions)
[![cran](http://www.r-pkg.org/badges/version/flowr)](https://cran.r-project.org/package=flowr)
<!--[![codecov.io](http://codecov.io/github/sahilseth/flowr/coverage.svg?branch=devel)](http://codecov.io/github/sahilseth/flowr?branch=devel)-->
![downloads](http://cranlogs.r-pkg.org/badges/grand-total/flowr)

<!--
![license](https://img.shields.io/badge/license-MIT-blue.svg)
-->


## [![flow-r.github.io/flowr](https://raw.githubusercontent.com/sahilseth/flowr/devel/vignettes/files/logo.png) Streamlining Computing workflows](http://flow-r.github.io/flowr/)

**Latest documentation: [flow-r.github.io/flowr](http://flow-r.github.io/flowr/)**



Flowr framework allows you to design and implement complex pipelines, and
deploy them on your institution's computing cluster. This has been built
keeping in mind the needs of bioinformatics workflows. However, it is
easily extendable to any field where a series of steps (shell commands)
are to be executed in a (work)flow.

### Highlights

- No new **syntax or language**. Put all shell commands as a tsv file called [flow mat](http://flow-r.github.io/flowr/overview.html#flow_matrix).
- Define the [flow of steps](http://flow-r.github.io/flowr/overview.html#relationships) using a simple tsv file (serial, scatter, gather, burst...) called [flow def](http://flow-r.github.io/flowr/overview.html#flow_definition).
- Works on your laptop/server or cluster (/cloud).
- Supports **multiple cluster computing platforms** (torque, lsf, sge, slurm ...), cloud (star cluster) OR a local machine.
- One line installation (`install.packages(""flowr"")`)
- **Reproducible** and **transparent**, with cleanly structured execution logs
- **Track** and **re-run** flows
- **Lean** and **Portable**, with easy installation
- **Fine grain** control over resources (CPU, memory, walltime, etc.) of each step.

### Example
[![ex_fq_bam](http://flow-r.github.io/flowr/files/ex_fq_bam.png)](https://github.com/flow-r/fastq_bam)

<!---
- Example: 
	- A typical case in next-generation sequencing involves processing of tens of
   [fastqs](http://en.wikipedia.org/wiki/FASTQ_format) for a sample,
   [mapping](http://en.wikipedia.org/wiki/Sequence_alignment) them to a reference genome.
	- Each step requires a range resources in terms of CPU, RAM etc.
	- Consider step 1 uses 10 cores for each file; with 50 files it would use 500 cores in total.
	- Next step uses one core for each file, 50 cores in total.
	- Say step C merges them, and uses only 1 core.
	- Some pipelines may reserve the maximum, example say 500 cores throught steps 1 to 3, 
	flowr would handle the **surge**, reserving 500, 50 or 1; when needed.
	- Now consider the run has 10 samples, all of them would be procesed in
	 parallel, spawning **thousands of cores**.
--->


### A few lines, to get started


```r
## Official stable release from CRAN (updated every other month)
## visit flow-r.github.io/flowr/install for more details
install.packages(""flowr"",  repos = ""http://cran.rstudio.com"")

# or a latest version from DRAT, provide cran for dependencies
install.packages(""flowr"", repos = c(CRAN=""http://cran.rstudio.com"", DRAT=""http://sahilseth.github.io/drat""))

library(flowr) ## load the library
setup() ## copy flowr bash script; and create a folder flowr under home.

# Run an example pipeline

# style 1: sleep_pipe() function creates system cmds
flowr run x=sleep_pipe platform=local execute=TRUE

# style 2: we start with a tsv of system cmds
# get example files
wget --no-check-certificate http://raw.githubusercontent.com/sahilseth/flowr/master/inst/pipelines/sleep_pipe.tsv
wget --no-check-certificate http://raw.githubusercontent.com/sahilseth/flowr/master/inst/pipelines/sleep_pipe.def

# submit to local machine
flowr to_flow x=sleep_pipe.tsv def=sleep_pipe.def platform=local execute=TRUE
# submit to local LSF cluster
flowr to_flow x=sleep_pipe.tsv def=sleep_pipe.def platform=lsf execute=TRUE
```

**Example pipelines** [inst/pipelines](https://github.com/flow-r/flowr/tree/master/inst/pipelines)

### Resources
- For a quick overview, you may browse through,
 these [introductory slides](http://sahilseth.com/slides/flowrintro/).
- The [overview](http://flow-r.github.io/flowr/overview.html) provides additional details regarding
the ideas and concepts used in flowr
- We have a [tutorial](http://flow-r.github.io/flowr/tutorial.html) which can walk you through creating a
new pipeline
- Additionally, a subset of important functions are described in the [package reference](http://flow-r.github.io/flowr/rd.html)
page
- You may follow detailed instructions on [installing and configuring](http://flow-r.github.io/flowr/install.html)
- You can use flow creator: https://sseth.shinyapps.io/flow_creator), a shiny app to aid in
	designing a *shiny* new flow. This provides a good example of the concepts

### Updates
This package is under active-development, 
you may watch for changes using
the [watch link above](https://help.github.com/articles/watching-repositories/).

### Feedback
Please feel free to raise a [github issue](https://github.com/flow-r/flowr/issues) with questions and comments.

### Acknowledgements

-   Jianhua Zhang
-   Samir Amin
-   Roger Moye
-   Kadir Akdemir
-   Ethan Mao
-   Henry Song
-   An excellent resource for writing your own R packages:
    [r-pkgs.org](https://r-pkgs.org/)

<!--why this license http://kbroman.org/pkg_primer/pages/licenses.html -->
<script src = ""vignettes/files/googl.js""></script>
",2023-07-07 15:57:26+00:00
flyte,flyte,flyteorg/flyte,"Scalable and flexible workflow orchestration platform that seamlessly unifies data, ML and analytics stacks.",https://flyte.org,False,3561,2023-07-07 03:47:52+00:00,2019-10-21 17:40:04+00:00,362,263,121,87,v1.7.0,2023-06-13 20:32:04+00:00,Apache License 2.0,900,v1.7.0,96,2023-06-13 20:32:04+00:00,2023-07-07 15:35:12+00:00,2023-07-06 20:52:57+00:00,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/flyteorg/static-resources/main/flyte/readme/flyte_and_lf.png"" alt=""Flyte and LF AI & Data Logo"" width=""250"">
</p>

<h1 align=""center"">
  Flyte
</h1>

<p align=""center"">
  🏗️ 🚀 📈 
</p>

<p align=""center"">
  <strong>
    Build. Deploy. Scale.
  </strong>
</p>

Flyte is an **open-source orchestrator** that facilitates building production-grade data and ML pipelines. It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform. With Flyte, user teams can construct pipelines using the Python SDK, and seamlessly deploy them on both cloud and on-premises environments, enabling distributed processing and efficient resource utilization.

<p align=""center"">
  <a href=""https://github.com/flyteorg/flyte/releases/latest"">
    <img src=""https://img.shields.io/github/release/flyteorg/flyte.svg?style=for-the-badge"" alt=""Current Release"" /></a>
  <a href=""https://github.com/flyteorg/flyte/actions/workflows/sandbox.yml"">
    <img src=""https://img.shields.io/github/actions/workflow/status/flyteorg/flyte/sandbox.yml?label=Sandbox%20docker%20image&style=for-the-badge"" alt=""Sandbox Status"" /></a>
  <a href=""https://github.com/flyteorg/flyte/actions/workflows/tests.yml"">
    <img src=""https://img.shields.io/github/actions/workflow/status/flyteorg/flyte/tests.yml?label=tests&style=for-the-badge"" alt=""Test Status"" /></a>
  <a href=""http://www.apache.org/licenses/LICENSE-2.0.html"">
    <img src=""https://img.shields.io/badge/LICENSE-Apache2.0-ff69b4.svg?style=for-the-badge"" alt=""License"" /></a>
  <a href=""https://bestpractices.coreinfrastructure.org/projects/4670"">
    <img src=""https://img.shields.io/badge/openssf%20best%20practices-passing-green?style=for-the-badge"" alt=""OpenSSF Best Practices"" /></a>
  <a href=""https://artifacthub.io/packages/search?repo=flyte"">
    <img src=""https://img.shields.io/endpoint?style=for-the-badge&url=https://artifacthub.io/badge/repository/flyte"" alt=""Flyte Helm Chart"" /></a>
  <a href=""https://twitter.com/flyteorg"">
    <img src=""https://img.shields.io/badge/Twitter-Follow-blue?style=for-the-badge&logo=twitter"" alt=""Flyte Twitter"" /></a>
  <a href=""https://slack.flyte.org"">
    <img src=""https://img.shields.io/badge/Slack-Chat-pink?style=for-the-badge&logo=slack"" alt=""Flyte Slack"" /></a>
</p>

<h3 align=""center"">
  <a href=""#quick-start"">Get Started</a>
  <span> · </span>
  <a href=""https://docs.flyte.org/"">Documentation</a>
  <span> · </span>
  <a href=""#resources"">Resources</a>
</h3>

<br />

<img alt=""Getting started with Flyte"" src=""https://raw.githubusercontent.com/flyteorg/static-resources/main/flytesnacks/getting_started/getting_started_console.gif"" style=""width: 100%; height: auto;"" />

---

##  Features

We ship new features, bug fixes and performance improvements regularly. Read our [release notes](https://github.com/flyteorg/flyte/releases) to stay updated.

🚀 **Strongly typed interfaces**: Validate your data at every step of the workflow by defining data guardrails using Flyte types. <br />
🌐 **Any language**: Write code in any language using raw containers, or choose [Python](https://github.com/flyteorg/flytekit), [Java](https://github.com/flyteorg/flytekit-java), [Scala](https://github.com/flyteorg/flytekit-java) or [JavaScript](https://github.com/NotMatthewGriffin/pterodactyl) SDKs to develop your Flyte workflows. <br />
📊 **Map tasks**: Achieve parallel code execution with minimal configuration using [map tasks](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/control_flow/map_task.html). <br />
🌟 **Dynamic workflows**: [Build flexible and adaptable workflows](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/control_flow/dynamics.html) that can change and evolve as needed, making it easier to respond to changing requirements. <br />
🌳 **Branching**: [Selectively execute branches](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/control_flow/conditions.html) of your workflow based on static or dynamic data produced by other tasks or input data. <br />
📂 **FlyteFile & FlyteDirectory**: Transfer [files](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/basics/files.html) and [directories](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/basics/folders.html) between local and cloud storage. <br />
🗃️ **Structured dataset**: Convert dataframes between types and enforce column-level type checking using the abstract 2D representation provided by [Structured Dataset](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/type_system/structured_dataset.html). <br />
🛡️ **Recover from failures**: Recover only the failed tasks. <br />
🔁 **Rerun a single task**: Rerun workflows at the most granular level without modifying the previous state of a data/ML workflow. <br />
🚦 **Versioned workflows**: Reproduce results and roll back to a previous workflow version any time. <br />
🔍 **Cache outputs**: Cache task outputs by passing `cache=True` to the task decorator. <br />
🚩 **Intra-task checkpointing**: [Checkpoint progress](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/control_flow/checkpoint.html) within a task execution. <br />
🌎 **Multi-tenancy**: Multiple users can share the same platform while maintaining their own distinct data and configurations. <br />
⏰ **Timeout**: Define a timeout period, after which the task is marked as failure. <br />
🔒 **Immutability**: Immutable executions help ensure reproducibility by preventing any changes to the state of an execution. <br />
🧬 **Data lineage**: Track the movement and transformation of data throughout the lifecycle of your data and ML workflows. <br />
📈 **Data visualization**: Visualize data, monitor models and view training history through plots. <br />
🏭 **Dev to prod**: As simple as changing your [domain](https://docs.flyte.org/en/latest/concepts/domains.html) from development or staging to production. <br />
💸 **Spot or preemptible instances**: Schedule your workflows on spot instances by setting `interruptible` to `True` in the task decorator. <br />
☁️ **Cloud-native deployment**: Deploy Flyte on AWS, GCP, Azure and other cloud services. <br />
📅 **Scheduling**: [Schedule](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/basics/lp_schedules.html) your data and ML workflows to run at a specific time. <br />
📢 **Notifications**: Stay informed about changes to your workflow's state by configuring [notifications](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/deployment/lp_notifications.html) through Slack, PagerDuty or email. <br />
⌛️ **Timeline view**: Evaluate the duration of each of your Flyte tasks and identify potential bottlenecks. <br />
💨 **GPU acceleration**: Enable and control your tasks’ GPU demands by requesting resources in the task decorator. <br />
🐳 **Dependency isolation via containers**: Maintain separate sets of dependencies for your tasks so no dependency conflicts arise. <br />
🔀 **Parallelism**: Flyte tasks are inherently parallel to optimize resource consumption and improve performance. <br /> 
💾 **Allocate resources dynamically** at the task level. <br />
⏯️ [Wait](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/control_flow/waiting_for_external_inputs.html) for **external inputs** before proceeding with the execution. <br />

## Quick start

If you want to try out Flyte, the easiest way to get started is by using the Flyte Hosted Sandbox, available at https://sandbox.union.ai/. It allows you to experiment with Flyte's capabilities without installing anything on your local machine.

However, if you prefer to install Flyte locally and run a workflow, our [getting started guide](https://docs.flyte.org/projects/cookbook/en/latest/index.html) is the best place to start. It provides step-by-step instructions on how to install Flyte locally and run your first workflow.

> If you're unsure what a Flyte [task](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/basics/task.html) and [workflow](https://docs.flyte.org/projects/cookbook/en/latest/auto_examples/basics/basic_workflow.html) are, be sure to check out our guides on both!

### Deploy Flyte to production

The [deployment guide](https://docs.flyte.org/en/latest/deployment/index.html) provides useful information on how to self-host and manage Flyte. 

## Adopters

Join the likes of LinkedIn, Spotify, Freenome, Pachama, Gojek, and Woven Planet in adopting Flyte. For a full list of adopters and information on how to add your organization or project, please visit our [ADOPTERS](https://github.com/flyteorg/community/blob/main/ADOPTERS.md) page.

## Roadmap

Stay up to date with the latest project roadmap by checking out our live [roadmap](https://github.com/orgs/flyteorg/projects/3) on GitHub.

## Resources

📆 [Weekly office hours](https://calendly.com/flyte-office-hours-01/30min): Live informal sessions with the Flyte team held every week. Book a 30-minute slot and get your questions answered. <br>
👥 [Biweekly community sync](https://www.addevent.com/event/EA7823958): A biweekly event where the Flyte team provides updates on the project and community members can share their progress and ask questions. <br>
💬 [Slack](https://slack.flyte.org/): Join the Flyte community on Slack to chat with other users, ask questions, and get help. <br>
⚠️ [GitHub](https://github.com/flyteorg/flyte): Check out the Flyte project on GitHub to file issues, contribute code, and stay up to date on the latest development. <br>
📹 [Youtube](https://www.youtube.com/channel/UCNduEoLOToNo3nFVly-vUTQ): Tune into panel discussions, customer success stories, community updates and feature deep dives. <br>
📄 [Blog](https://flyte.org/blog): Here, you can find tutorials and feature deep dives to help you learn more about Flyte. <br>
💡 [RFCs](rfc/.): RFCs are used for proposing new ideas and features to improve Flyte. You can refer to them to stay updated on the latest developments and contribute to the growth of the platform.

## How to contribute

There are many ways to get involved in Flyte, including:

* Submitting [bugs](https://github.com/flyteorg/flyte/issues/new?assignees=&labels=bug%2Cuntriaged&template=bug_report.yaml&title=%5BBUG%5D+) and [feature requests](https://github.com/flyteorg/flyte/issues/new?assignees=&labels=enhancement%2Cuntriaged&template=feature_request.yaml&title=%5BCore+feature%5D+) for various components.
* Reviewing [the documentation](https://docs.flyte.org/en/latest/) and submitting [pull requests](https://github.com/flyteorg/flytesnacks) for anything from fixing typos to adding new content.
* Speaking or writing about Flyte or any other ecosystem integration and [letting us know](https://flyte-org.slack.com/archives/C02JMT8KTEE)!
* Taking on a [`help wanted`](https://github.com/flyteorg/flyte/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+) or [`good-first-issue`](https://github.com/flyteorg/flyte/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) and following the [CONTRIBUTING](https://docs.flyte.org/en/latest/community/contribute.html#contribute-flyte) guide to submit changes to the codebase.
* Upvoting [popular feature requests](https://github.com/flyteorg/flyte/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement+sort%3Areactions-%2B1-desc) to show your support.

### We ❤️ our contributors

<!-- CONTRIBUTORS START -->
[![953358](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/953358?v=4&w=50&h=50&mask=circle)](https://github.com/katrogan)[![37090125](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/37090125?v=4&w=50&h=50&mask=circle)](https://github.com/lyft-metaservice-3)[![7597118](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7597118?v=4&w=50&h=50&mask=circle)](https://github.com/matthewphsmith)[![27159](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/27159?v=4&w=50&h=50&mask=circle)](https://github.com/EngHabu)[![29843943](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/29843943?v=4&w=50&h=50&mask=circle)](https://github.com/goreleaserbot)[![10830562](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10830562?v=4&w=50&h=50&mask=circle)](https://github.com/evalsocket)[![8888115](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8888115?v=4&w=50&h=50&mask=circle)](https://github.com/hamersaw)[![78108056](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/78108056?v=4&w=50&h=50&mask=circle)](https://github.com/flyte-bot)[![158892](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/158892?v=4&w=50&h=50&mask=circle)](https://github.com/honnix)[![18408237](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/18408237?v=4&w=50&h=50&mask=circle)](https://github.com/anandswaminathan)[![2896568](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2896568?v=4&w=50&h=50&mask=circle)](https://github.com/wild-endeavor)[![1518524](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1518524?v=4&w=50&h=50&mask=circle)](https://github.com/bnsblue)[![37936015](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/37936015?v=4&w=50&h=50&mask=circle)](https://github.com/pingsutw)[![27724763](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/27724763?v=4&w=50&h=50&mask=circle)](https://github.com/iaroslav-ciupin)[![16888709](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16888709?v=4&w=50&h=50&mask=circle)](https://github.com/kumare3)[![27777173](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/27777173?v=4&w=50&h=50&mask=circle)](https://github.com/samhita-alla)[![452166](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/452166?v=4&w=50&h=50&mask=circle)](https://github.com/MorpheusXAUT)[![4748985](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4748985?v=4&w=50&h=50&mask=circle)](https://github.com/aliabbasjaffri)[![6562898](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6562898?v=4&w=50&h=50&mask=circle)](https://github.com/ckiosidis)[![6239450](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6239450?v=4&w=50&h=50&mask=circle)](https://github.com/mayitbeegh)[![8805803](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8805803?v=4&w=50&h=50&mask=circle)](https://github.com/alexlipa91)[![5032356](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5032356?v=4&w=50&h=50&mask=circle)](https://github.com/brucearctor)[![77798312](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/77798312?v=4&w=50&h=50&mask=circle)](https://github.com/pmahindrakar-oss)[![23062603](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/23062603?v=4&w=50&h=50&mask=circle)](https://github.com/Antaxify)[![653394](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/653394?v=4&w=50&h=50&mask=circle)](https://github.com/eapolinario)[![5725707](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5725707?v=4&w=50&h=50&mask=circle)](https://github.com/andrewwdye)[![8122852](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8122852?v=4&w=50&h=50&mask=circle)](https://github.com/ariefrahmansyah)[![10869815](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10869815?v=4&w=50&h=50&mask=circle)](https://github.com/jeevb)[![3880645](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3880645?v=4&w=50&h=50&mask=circle)](https://github.com/jonathanburns)[![3936213](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3936213?v=4&w=50&h=50&mask=circle)](https://github.com/lu4nm3)[![26174213](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26174213?v=4&w=50&h=50&mask=circle)](https://github.com/lyft-metaservice-2)[![9142716](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/9142716?v=4&w=50&h=50&mask=circle)](https://github.com/2uasimojo)[![5487021](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5487021?v=4&w=50&h=50&mask=circle)](https://github.com/veggiemonk)[![1815175](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1815175?v=4&w=50&h=50&mask=circle)](https://github.com/schottra)[![46989299](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/46989299?v=4&w=50&h=50&mask=circle)](https://github.com/supreeth7)[![2816689](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2816689?v=4&w=50&h=50&mask=circle)](https://github.com/cosmicBboy)[![19375241](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/19375241?v=4&w=50&h=50&mask=circle)](https://github.com/migueltol22)[![6065051](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6065051?v=4&w=50&h=50&mask=circle)](https://github.com/milton0825)[![70988](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/70988?v=4&w=50&h=50&mask=circle)](https://github.com/slai)[![94349093](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/94349093?v=4&w=50&h=50&mask=circle)](https://github.com/SmritiSatyanV)[![16090976](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16090976?v=4&w=50&h=50&mask=circle)](https://github.com/surindersinghp)[![43610471](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/43610471?v=4&w=50&h=50&mask=circle)](https://github.com/TheYk98)[![53313394](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/53313394?v=4&w=50&h=50&mask=circle)](https://github.com/kosigz-lyft)[![4967458](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4967458?v=4&w=50&h=50&mask=circle)](https://github.com/chanadian)[![467927](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/467927?v=4&w=50&h=50&mask=circle)](https://github.com/kanterov)[![248688](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/248688?v=4&w=50&h=50&mask=circle)](https://github.com/hanzo)[![1330233](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1330233?v=4&w=50&h=50&mask=circle)](https://github.com/igorvalko)[![31255434](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/31255434?v=4&w=50&h=50&mask=circle)](https://github.com/kennyworkman)[![1472826](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1472826?v=4&w=50&h=50&mask=circle)](https://github.com/maximsmol)[![5026554](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5026554?v=4&w=50&h=50&mask=circle)](https://github.com/vsbus)[![34587798](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/34587798?v=4&w=50&h=50&mask=circle)](https://github.com/akhurana001)[![11799671](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11799671?v=4&w=50&h=50&mask=circle)](https://github.com/bstadlbauer)[![95110820](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/95110820?v=4&w=50&h=50&mask=circle)](https://github.com/jerempy)[![38207208](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/38207208?v=4&w=50&h=50&mask=circle)](https://github.com/tnsetting)[![8200209](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8200209?v=4&w=50&h=50&mask=circle)](https://github.com/catalinii)[![24364830](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/24364830?v=4&w=50&h=50&mask=circle)](https://github.com/ByronHsu)[![43587819](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/43587819?v=4&w=50&h=50&mask=circle)](https://github.com/chetcode)[![163899](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/163899?v=4&w=50&h=50&mask=circle)](https://github.com/regadas)[![36511035](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/36511035?v=4&w=50&h=50&mask=circle)](https://github.com/fg91)[![22784654](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/22784654?v=4&w=50&h=50&mask=circle)](https://github.com/aybidi)[![1316881](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1316881?v=4&w=50&h=50&mask=circle)](https://github.com/akashkatipally)[![1777447](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1777447?v=4&w=50&h=50&mask=circle)](https://github.com/goyalankit)[![1360529](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1360529?v=4&w=50&h=50&mask=circle)](https://github.com/clairemcginty)[![104257](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/104257?v=4&w=50&h=50&mask=circle)](https://github.com/flixr)[![2538760](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2538760?v=4&w=50&h=50&mask=circle)](https://github.com/akumor)[![11970258](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11970258?v=4&w=50&h=50&mask=circle)](https://github.com/niliayu)[![19733683](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/19733683?v=4&w=50&h=50&mask=circle)](https://github.com/snyk-bot)[![155087](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/155087?v=4&w=50&h=50&mask=circle)](https://github.com/derwiki)[![1399455](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1399455?v=4&w=50&h=50&mask=circle)](https://github.com/th0114nd)[![21109744](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/21109744?v=4&w=50&h=50&mask=circle)](https://github.com/AlekhyaSasi)[![49699333](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/in/29110?v=4&w=50&h=50&mask=circle)](https://github.com/apps/dependabot)[![1810591](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1810591?v=4&w=50&h=50&mask=circle)](https://github.com/asottile)[![80421934](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/80421934?v=4&w=50&h=50&mask=circle)](https://github.com/SandraGH5)[![3939659](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3939659?v=4&w=50&h=50&mask=circle)](https://github.com/sbrunk)[![9609986](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/9609986?v=4&w=50&h=50&mask=circle)](https://github.com/sonjaer)[![12219405](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/12219405?v=4&w=50&h=50&mask=circle)](https://github.com/fediazgon)[![98349643](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/98349643?v=4&w=50&h=50&mask=circle)](https://github.com/rahul-theorem)[![16509490](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16509490?v=4&w=50&h=50&mask=circle)](https://github.com/ryankarlos)[![6774758](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6774758?v=4&w=50&h=50&mask=circle)](https://github.com/ddhirajkumar)[![18337807](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/18337807?v=4&w=50&h=50&mask=circle)](https://github.com/max-hoffman)[![322624](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/322624?v=4&w=50&h=50&mask=circle)](https://github.com/AdrianoKF)[![1168692](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1168692?v=4&w=50&h=50&mask=circle)](https://github.com/dennisobrien)[![91385411](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/91385411?v=4&w=50&h=50&mask=circle)](https://github.com/Ln11211)[![30621230](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/30621230?v=4&w=50&h=50&mask=circle)](https://github.com/aeioulisa)[![54334265](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/54334265?v=4&w=50&h=50&mask=circle)](https://github.com/michaels-lyft)[![48736656](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/48736656?v=4&w=50&h=50&mask=circle)](https://github.com/murilommen)[![17165004](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17165004?v=4&w=50&h=50&mask=circle)](https://github.com/RobertoRRW)[![30375389](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/30375389?v=4&w=50&h=50&mask=circle)](https://github.com/bimtauer)[![97543480](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/97543480?v=4&w=50&h=50&mask=circle)](https://github.com/esadler-hbo)[![69013027](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/69013027?v=4&w=50&h=50&mask=circle)](https://github.com/ggydush-fn)[![116700206](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/116700206?v=4&w=50&h=50&mask=circle)](https://github.com/kiliangojek)[![422486](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/422486?v=4&w=50&h=50&mask=circle)](https://github.com/bethebunny)[![54333860](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/54333860?v=4&w=50&h=50&mask=circle)](https://github.com/aalavian)[![7005765](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7005765?v=4&w=50&h=50&mask=circle)](https://github.com/convexquad)[![4025771](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4025771?v=4&w=50&h=50&mask=circle)](https://github.com/andresgomezfrr)[![48966647](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/48966647?v=4&w=50&h=50&mask=circle)](https://github.com/asahalyft)[![77167782](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/77167782?v=4&w=50&h=50&mask=circle)](https://github.com/apatel-fn)[![23013825](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/23013825?v=4&w=50&h=50&mask=circle)](https://github.com/arpitbhardwaj)[![31381038](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/31381038?v=4&w=50&h=50&mask=circle)](https://github.com/lordnodd)[![4396228](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4396228?v=4&w=50&h=50&mask=circle)](https://github.com/bryanwweber)[![6288302](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6288302?v=4&w=50&h=50&mask=circle)](https://github.com/CalvinLeather)[![23107192](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/23107192?v=4&w=50&h=50&mask=circle)](https://github.com/YmirKhang)[![121866694](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/121866694?v=4&w=50&h=50&mask=circle)](https://github.com/franco-bocci)[![7358951](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7358951?v=4&w=50&h=50&mask=circle)](https://github.com/frsann)[![33652917](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/33652917?v=4&w=50&h=50&mask=circle)](https://github.com/hfurkanvural)[![6984748](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6984748?v=4&w=50&h=50&mask=circle)](https://github.com/jbrambleDC)[![488594](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/488594?v=4&w=50&h=50&mask=circle)](https://github.com/jcugat)[![20173739](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/20173739?v=4&w=50&h=50&mask=circle)](https://github.com/madhur-tandon)[![34498039](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/34498039?v=4&w=50&h=50&mask=circle)](https://github.com/matheusMoreno)[![19853373](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/19853373?v=4&w=50&h=50&mask=circle)](https://github.com/NotMatthewGriffin)[![10376195](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10376195?v=4&w=50&h=50&mask=circle)](https://github.com/myz540)[![125105](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/125105?v=4&w=50&h=50&mask=circle)](https://github.com/tekumara)[![1153481](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1153481?v=4&w=50&h=50&mask=circle)](https://github.com/ppiegaze)[![37170063](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/37170063?v=4&w=50&h=50&mask=circle)](https://github.com/Qiwen-Yu)[![2614101](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2614101?v=4&w=50&h=50&mask=circle)](https://github.com/RobinKa)[![4308533](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4308533?v=4&w=50&h=50&mask=circle)](https://github.com/rubenbarragan)[![10201242](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10201242?v=4&w=50&h=50&mask=circle)](https://github.com/sugatoray)[![11269256](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11269256?v=4&w=50&h=50&mask=circle)](https://github.com/sushrut111)[![61228633](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/61228633?v=4&w=50&h=50&mask=circle)](https://github.com/Tat-V)[![13070236](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/13070236?v=4&w=50&h=50&mask=circle)](https://github.com/TeoZosa)[![8817639](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8817639?v=4&w=50&h=50&mask=circle)](https://github.com/ThomVett)[![17309187](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17309187?v=4&w=50&h=50&mask=circle)](https://github.com/datability-io)[![26834658](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26834658?v=4&w=50&h=50&mask=circle)](https://github.com/techytushar)[![5092599](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5092599?v=4&w=50&h=50&mask=circle)](https://github.com/vchowdhary)[![57967031](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/57967031?v=4&w=50&h=50&mask=circle)](https://github.com/varshaparthay)[![67166843](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/67166843?v=4&w=50&h=50&mask=circle)](https://github.com/vvasavada-fn)[![1778407](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1778407?v=4&w=50&h=50&mask=circle)](https://github.com/ybubnov)[![51814063](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/51814063?v=4&w=50&h=50&mask=circle)](https://github.com/Yicheng-Lu-llll)[![3741621](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3741621?v=4&w=50&h=50&mask=circle)](https://github.com/palchicz)[![12450632](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/12450632?v=4&w=50&h=50&mask=circle)](https://github.com/ajsalow)[![35151789](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/35151789?v=4&w=50&h=50&mask=circle)](https://github.com/ggydush)[![13331724](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/13331724?v=4&w=50&h=50&mask=circle)](https://github.com/martinlyra)[![119345186](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/119345186?v=4&w=50&h=50&mask=circle)](https://github.com/mcloney-ddm)[![1521126](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1521126?v=4&w=50&h=50&mask=circle)](https://github.com/pbrogan12)[![73247359](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/73247359?v=4&w=50&h=50&mask=circle)](https://github.com/stef-stripe)[![50860453](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/50860453?v=4&w=50&h=50&mask=circle)](https://github.com/charlie0220)[![6506810](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6506810?v=4&w=50&h=50&mask=circle)](https://github.com/stephen37)[![55718143](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/55718143?v=4&w=50&h=50&mask=circle)](https://github.com/anrusina)[![65977800](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/65977800?v=4&w=50&h=50&mask=circle)](https://github.com/service-github-lyft-semantic-release)[![6610300](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6610300?v=4&w=50&h=50&mask=circle)](https://github.com/ursucarina)[![84735036](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/84735036?v=4&w=50&h=50&mask=circle)](https://github.com/jsonporter)[![85753828](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/85753828?v=4&w=50&h=50&mask=circle)](https://github.com/csirius)[![101579322](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/101579322?v=4&w=50&h=50&mask=circle)](https://github.com/olga-union)[![26953709](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26953709?v=4&w=50&h=50&mask=circle)](https://github.com/Pianist038801)[![105876962](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/105876962?v=4&w=50&h=50&mask=circle)](https://github.com/james-union)[![25038146](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25038146?v=4&w=50&h=50&mask=circle)](https://github.com/eugenejahn)[![88684372](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/88684372?v=4&w=50&h=50&mask=circle)](https://github.com/4nalog)[![99441958](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/99441958?v=4&w=50&h=50&mask=circle)](https://github.com/apTalya)[![1388071](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1388071?v=4&w=50&h=50&mask=circle)](https://github.com/aviaviavi)[![58770001](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/58770001?v=4&w=50&h=50&mask=circle)](https://github.com/Professional0321)[![20668349](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/20668349?v=4&w=50&h=50&mask=circle)](https://github.com/HiromuHota)[![100569684](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/100569684?v=4&w=50&h=50&mask=circle)](https://github.com/rafaelraposospot)[![17351764](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17351764?v=4&w=50&h=50&mask=circle)](https://github.com/daniel-shuy)[![6399428](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6399428?v=4&w=50&h=50&mask=circle)](https://github.com/live-wire)[![25695302](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25695302?v=4&w=50&h=50&mask=circle)](https://github.com/sisco0)[![18363301](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/18363301?v=4&w=50&h=50&mask=circle)](https://github.com/jimbobby5)[![4023015](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4023015?v=4&w=50&h=50&mask=circle)](https://github.com/pradithya)[![3451399](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3451399?v=4&w=50&h=50&mask=circle)](https://github.com/skiptomyliu)[![25364490](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25364490?v=4&w=50&h=50&mask=circle)](https://github.com/haoyuez)[![50679871](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/50679871?v=4&w=50&h=50&mask=circle)](https://github.com/lupasarin)[![7548823](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7548823?v=4&w=50&h=50&mask=circle)](https://github.com/Dread1982)[![7515359](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7515359?v=4&w=50&h=50&mask=circle)](https://github.com/narape)[![31982395](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/31982395?v=4&w=50&h=50&mask=circle)](https://github.com/alexapdev)[![62209650](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/62209650?v=4&w=50&h=50&mask=circle)](https://github.com/3t8)[![1892175](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1892175?v=4&w=50&h=50&mask=circle)](https://github.com/zeryx)[![200401](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/200401?v=4&w=50&h=50&mask=circle)](https://github.com/arturdryomov)[![13770222](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/13770222?v=4&w=50&h=50&mask=circle)](https://github.com/ChickenTarm)[![2380665](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2380665?v=4&w=50&h=50&mask=circle)](https://github.com/DavidMertz)[![24739949](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/24739949?v=4&w=50&h=50&mask=circle)](https://github.com/felixwang9817)[![10430635](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10430635?v=4&w=50&h=50&mask=circle)](https://github.com/juandiegopalomino)[![31911175](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/31911175?v=4&w=50&h=50&mask=circle)](https://github.com/kanyesthaker)[![104152793](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/104152793?v=4&w=50&h=50&mask=circle)](https://github.com/marc-union)[![27818609](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/27818609?v=4&w=50&h=50&mask=circle)](https://github.com/michaeltinsley)[![6486584](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6486584?v=4&w=50&h=50&mask=circle)](https://github.com/mucahitkantepe)[![321459](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/321459?v=4&w=50&h=50&mask=circle)](https://github.com/oyevtushok)[![35962310](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/35962310?v=4&w=50&h=50&mask=circle)](https://github.com/trishitapingolia)[![91927689](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/91927689?v=4&w=50&h=50&mask=circle)](https://github.com/Smartmind12)[![726061](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/726061?v=4&w=50&h=50&mask=circle)](https://github.com/huxuan)[![47872044](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/47872044?v=4&w=50&h=50&mask=circle)](https://github.com/privatedumbo)[![105229971](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/105229971?v=4&w=50&h=50&mask=circle)](https://github.com/tjKairos)[![405480](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/405480?v=4&w=50&h=50&mask=circle)](https://github.com/georgesnelling)[![1004789](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1004789?v=4&w=50&h=50&mask=circle)](https://github.com/dschaller)[![82604841](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/82604841?v=4&w=50&h=50&mask=circle)](https://github.com/davidmirror-ops)[![480621](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/480621?v=4&w=50&h=50&mask=circle)](https://github.com/davidxia)[![1335881](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1335881?v=4&w=50&h=50&mask=circle)](https://github.com/hoyajigi)[![100597998](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/100597998?v=4&w=50&h=50&mask=circle)](https://github.com/MrKrishnaAgarwal)[![4830700](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4830700?v=4&w=50&h=50&mask=circle)](https://github.com/NitinAgg)[![69161722](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/69161722?v=4&w=50&h=50&mask=circle)](https://github.com/noobkid2411)[![43336767](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/43336767?v=4&w=50&h=50&mask=circle)](https://github.com/yongchand)[![25391173](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25391173?v=4&w=50&h=50&mask=circle)](https://github.com/nicklofaso)[![66388192](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/66388192?v=4&w=50&h=50&mask=circle)](https://github.com/mounesi)[![14992189](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/14992189?v=4&w=50&h=50&mask=circle)](https://github.com/eanakhl)[![1175392](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1175392?v=4&w=50&h=50&mask=circle)](https://github.com/adinin)[![7475946](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7475946?v=4&w=50&h=50&mask=circle)](https://github.com/anton-malakhov)[![11796986](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11796986?v=4&w=50&h=50&mask=circle)](https://github.com/avan-sh)[![304786](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/304786?v=4&w=50&h=50&mask=circle)](https://github.com/kinow)[![24402505](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/24402505?v=4&w=50&h=50&mask=circle)](https://github.com/Daeruin)[![1659415](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1659415?v=4&w=50&h=50&mask=circle)](https://github.com/dav009)[![86911142](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/86911142?v=4&w=50&h=50&mask=circle)](https://github.com/idivyanshbansal)[![11456773](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11456773?v=4&w=50&h=50&mask=circle)](https://github.com/fvde)[![7490199](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7490199?v=4&w=50&h=50&mask=circle)](https://github.com/Lundez)[![10345184](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10345184?v=4&w=50&h=50&mask=circle)](https://github.com/hasukmistry)[![29532638](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/29532638?v=4&w=50&h=50&mask=circle)](https://github.com/rokrokss)[![14008978](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/14008978?v=4&w=50&h=50&mask=circle)](https://github.com/jeremydonahue)[![9272376](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/9272376?v=4&w=50&h=50&mask=circle)](https://github.com/jonasdebeukelaer)[![1633460](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1633460?v=4&w=50&h=50&mask=circle)](https://github.com/jmcarp)[![3033592](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3033592?v=4&w=50&h=50&mask=circle)](https://github.com/kazesberger)[![19229049](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/19229049?v=4&w=50&h=50&mask=circle)](https://github.com/lsena)[![36594527](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/36594527?v=4&w=50&h=50&mask=circle)](https://github.com/mishmanners)[![8755869](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8755869?v=4&w=50&h=50&mask=circle)](https://github.com/paravatha)[![6528449](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6528449?v=4&w=50&h=50&mask=circle)](https://github.com/uschi2000)[![790725](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/790725?v=4&w=50&h=50&mask=circle)](https://github.com/rodrigobaron)[![576968](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/576968?v=4&w=50&h=50&mask=circle)](https://github.com/ronaldosaheki)[![36827492](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/36827492?v=4&w=50&h=50&mask=circle)](https://github.com/shahwar9)[![133936](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/133936?v=4&w=50&h=50&mask=circle)](https://github.com/shihgianlee)[![10438373](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10438373?v=4&w=50&h=50&mask=circle)](https://github.com/SKalt)[![33272587](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/33272587?v=4&w=50&h=50&mask=circle)](https://github.com/samuel-sujith)[![580328](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/580328?v=4&w=50&h=50&mask=circle)](https://github.com/ilikedata)[![1027207](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1027207?v=4&w=50&h=50&mask=circle)](https://github.com/orf)[![16526627](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16526627?v=4&w=50&h=50&mask=circle)](https://github.com/vijaysaravana)[![10526540](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10526540?v=4&w=50&h=50&mask=circle)](https://github.com/yubofredwang)[![5346764](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5346764?v=4&w=50&h=50&mask=circle)](https://github.com/fsz285)[![22917741](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/22917741?v=4&w=50&h=50&mask=circle)](https://github.com/gigi-at-zymergen)[![40143026](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/40143026?v=4&w=50&h=50&mask=circle)](https://github.com/hampusrosvall)[![77197126](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/77197126?v=4&w=50&h=50&mask=circle)](https://github.com/hitarth01)[![300315](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/300315?v=4&w=50&h=50&mask=circle)](https://github.com/jcourteau)[![106815366](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/106815366?v=4&w=50&h=50&mask=circle)](https://github.com/jw0515)[![1568889](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1568889?v=4&w=50&h=50&mask=circle)](https://github.com/leorleor)[![937967](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/937967?v=4&w=50&h=50&mask=circle)](https://github.com/moose007)[![114232404](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/114232404?v=4&w=50&h=50&mask=circle)](https://github.com/sanjaychouhan-adf)[![14996868](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/14996868?v=4&w=50&h=50&mask=circle)](https://github.com/v01dXYZ)[![93438190](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/93438190?v=4&w=50&h=50&mask=circle)](https://github.com/wanderer163)[![1043051](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1043051?v=4&w=50&h=50&mask=circle)](https://github.com/kylewaynebenson)[![21953442](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/21953442?v=4&w=50&h=50&mask=circle)](https://github.com/Gui11aum3)[![16461847](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16461847?v=4&w=50&h=50&mask=circle)](https://github.com/JakeNeyer)[![64676594](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/64676594?v=4&w=50&h=50&mask=circle)](https://github.com/abhijeet007rocks8)[![1174730](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1174730?v=4&w=50&h=50&mask=circle)](https://github.com/mouuff)[![20135478](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/20135478?v=4&w=50&h=50&mask=circle)](https://github.com/Juneezee)[![151841](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/151841?v=4&w=50&h=50&mask=circle)](https://github.com/goodgravy)[![44368997](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/44368997?v=4&w=50&h=50&mask=circle)](https://github.com/radiantly)[![36989112](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/36989112?v=4&w=50&h=50&mask=circle)](https://github.com/nishantwrp)[![7144772](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7144772?v=4&w=50&h=50&mask=circle)](https://github.com/sighingnow)[![697033](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/697033?v=4&w=50&h=50&mask=circle)](https://github.com/vglocus)[![2845540](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2845540?v=4&w=50&h=50&mask=circle)](https://github.com/RustedBones)[![4056828](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4056828?v=4&w=50&h=50&mask=circle)](https://github.com/pablocasares)[![1071153](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1071153?v=4&w=50&h=50&mask=circle)](https://github.com/evdokim)[![5732047](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5732047?v=4&w=50&h=50&mask=circle)](https://github.com/stormy-ua)[![471021](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/471021?v=4&w=50&h=50&mask=circle)](https://github.com/marschall)[![71284190](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/71284190?v=4&w=50&h=50&mask=circle)](https://github.com/gdungca-fn)[![26265392](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26265392?v=4&w=50&h=50&mask=circle)](https://github.com/ttanay)[![85021780](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/85021780?v=4&w=50&h=50&mask=circle)](https://github.com/Abdullahi-Ahmed)[![48512530](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/48512530?v=4&w=50&h=50&mask=circle)](https://github.com/amaleelhamri)[![3275593](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3275593?v=4&w=50&h=50&mask=circle)](https://github.com/pradyunsg)[![66853113](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/in/68672?v=4&w=50&h=50&mask=circle)](https://github.com/apps/pre-commit-ci)[![1834509](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1834509?v=4&w=50&h=50&mask=circle)](https://github.com/jdknight)[![107893](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/107893?v=4&w=50&h=50&mask=circle)](https://github.com/kmike)[![1324225](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1324225?v=4&w=50&h=50&mask=circle)](https://github.com/hugovk)[![1300022](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1300022?v=4&w=50&h=50&mask=circle)](https://github.com/sirosen)[![244656](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/244656?v=4&w=50&h=50&mask=circle)](https://github.com/humitos)[![467294](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/467294?v=4&w=50&h=50&mask=circle)](https://github.com/bastimeyer)[![71486](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/71486?v=4&w=50&h=50&mask=circle)](https://github.com/asmeurer)[![20280470](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/20280470?v=4&w=50&h=50&mask=circle)](https://github.com/drewyh)[![3533182](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3533182?v=4&w=50&h=50&mask=circle)](https://github.com/polyzen)[![199429](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/199429?v=4&w=50&h=50&mask=circle)](https://github.com/dvarrazzo)[![1032633](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1032633?v=4&w=50&h=50&mask=circle)](https://github.com/dbitouze)[![1313087](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1313087?v=4&w=50&h=50&mask=circle)](https://github.com/idryzhov)[![521097](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/521097?v=4&w=50&h=50&mask=circle)](https://github.com/pauloxnet)[![63936253](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/63936253?v=4&w=50&h=50&mask=circle)](https://github.com/ichard26)[![18519037](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/18519037?v=4&w=50&h=50&mask=circle)](https://github.com/sethmlarson)[![413772](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/413772?v=4&w=50&h=50&mask=circle)](https://github.com/graingert)[![11478411](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11478411?v=4&w=50&h=50&mask=circle)](https://github.com/stonecharioteer)[![6739793](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6739793?v=4&w=50&h=50&mask=circle)](https://github.com/yeraydiazdiaz)[![83365562](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/83365562?v=4&w=50&h=50&mask=circle)](https://github.com/eviau-sat)[![6670894](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6670894?v=4&w=50&h=50&mask=circle)](https://github.com/rozsasarpi)[![86675](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/86675?v=4&w=50&h=50&mask=circle)](https://github.com/estan)[![4748863](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4748863?v=4&w=50&h=50&mask=circle)](https://github.com/pseudomuto)[![181308](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/181308?v=4&w=50&h=50&mask=circle)](https://github.com/htdvisser)[![1390277](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1390277?v=4&w=50&h=50&mask=circle)](https://github.com/jacobtolar)[![1391982](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1391982?v=4&w=50&h=50&mask=circle)](https://github.com/ezimanyi)[![3880001](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3880001?v=4&w=50&h=50&mask=circle)](https://github.com/lpabon)[![770392](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/770392?v=4&w=50&h=50&mask=circle)](https://github.com/ArcEye)[![6178510](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6178510?v=4&w=50&h=50&mask=circle)](https://github.com/mingrammer)[![5111931](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5111931?v=4&w=50&h=50&mask=circle)](https://github.com/aschrijver)[![873434](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/873434?v=4&w=50&h=50&mask=circle)](https://github.com/panzerfahrer)[![16724](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16724?v=4&w=50&h=50&mask=circle)](https://github.com/glasser)[![17330872](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17330872?v=4&w=50&h=50&mask=circle)](https://github.com/murph0)[![419419](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/419419?v=4&w=50&h=50&mask=circle)](https://github.com/zetaron)[![1014](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1014?v=4&w=50&h=50&mask=circle)](https://github.com/sunfmin)[![504507](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/504507?v=4&w=50&h=50&mask=circle)](https://github.com/guozheng)[![8841470](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8841470?v=4&w=50&h=50&mask=circle)](https://github.com/suusan2go)[![901479](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/901479?v=4&w=50&h=50&mask=circle)](https://github.com/mhaberler)[![6400253](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6400253?v=4&w=50&h=50&mask=circle)](https://github.com/s4ichi)[![353644](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/353644?v=4&w=50&h=50&mask=circle)](https://github.com/dreampuf)[![12421077](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/12421077?v=4&w=50&h=50&mask=circle)](https://github.com/UnicodingUnicorn)[![809865](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/809865?v=4&w=50&h=50&mask=circle)](https://github.com/philiptzou)[![19378](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/19378?v=4&w=50&h=50&mask=circle)](https://github.com/timabell)[![1113245](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1113245?v=4&w=50&h=50&mask=circle)](https://github.com/jasonhancock)[![101659](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/101659?v=4&w=50&h=50&mask=circle)](https://github.com/matryer)[![4730508](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/4730508?v=4&w=50&h=50&mask=circle)](https://github.com/piotrrojek)[![33036160](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/33036160?v=4&w=50&h=50&mask=circle)](https://github.com/jasonsattler)[![470810](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/470810?v=4&w=50&h=50&mask=circle)](https://github.com/sbward)[![7592392](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7592392?v=4&w=50&h=50&mask=circle)](https://github.com/Pisush)[![94814](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/94814?v=4&w=50&h=50&mask=circle)](https://github.com/tamalsaha)[![8147854](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8147854?v=4&w=50&h=50&mask=circle)](https://github.com/marianina8)[![1005](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1005?v=4&w=50&h=50&mask=circle)](https://github.com/ernesto-jimenez)[![17263167](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17263167?v=4&w=50&h=50&mask=circle)](https://github.com/jsteenb2)[![2807589](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2807589?v=4&w=50&h=50&mask=circle)](https://github.com/darwayne)[![1683714](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1683714?v=4&w=50&h=50&mask=circle)](https://github.com/naysayer)[![6386887](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6386887?v=4&w=50&h=50&mask=circle)](https://github.com/AgrimPrasad)[![615811](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/615811?v=4&w=50&h=50&mask=circle)](https://github.com/dahernan)[![75184](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/75184?v=4&w=50&h=50&mask=circle)](https://github.com/jtarchie)[![469669](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/469669?v=4&w=50&h=50&mask=circle)](https://github.com/jdtobe)[![28523](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/28523?v=4&w=50&h=50&mask=circle)](https://github.com/alrs)[![10113228](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10113228?v=4&w=50&h=50&mask=circle)](https://github.com/urisimchoni)[![5751464](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5751464?v=4&w=50&h=50&mask=circle)](https://github.com/Xercoy)[![2405410](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2405410?v=4&w=50&h=50&mask=circle)](https://github.com/marbergq)[![5082160](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5082160?v=4&w=50&h=50&mask=circle)](https://github.com/anothrNick)[![11335612](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11335612?v=4&w=50&h=50&mask=circle)](https://github.com/fermoya)[![23391642](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/23391642?v=4&w=50&h=50&mask=circle)](https://github.com/sbe-arg)[![1024762](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/1024762?v=4&w=50&h=50&mask=circle)](https://github.com/PeerXu)[![7390781](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7390781?v=4&w=50&h=50&mask=circle)](https://github.com/reececomo)[![49680](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/49680?v=4&w=50&h=50&mask=circle)](https://github.com/dmerrick)[![87524](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/87524?v=4&w=50&h=50&mask=circle)](https://github.com/andrewcole)[![866505](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/866505?v=4&w=50&h=50&mask=circle)](https://github.com/phish108)[![2611549](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2611549?v=4&w=50&h=50&mask=circle)](https://github.com/endrjuskr)[![49961058](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/49961058?v=4&w=50&h=50&mask=circle)](https://github.com/bevans-HD)[![5655837](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5655837?v=4&w=50&h=50&mask=circle)](https://github.com/gukoff)[![8320753](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8320753?v=4&w=50&h=50&mask=circle)](https://github.com/lovromazgon)[![16513382](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16513382?v=4&w=50&h=50&mask=circle)](https://github.com/117)[![3807434](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/3807434?v=4&w=50&h=50&mask=circle)](https://github.com/tomsolem)[![118945041](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/118945041?v=4&w=50&h=50&mask=circle)](https://github.com/vq-ambiata)[![8232503](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8232503?v=4&w=50&h=50&mask=circle)](https://github.com/sjauld)[![69170839](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/69170839?v=4&w=50&h=50&mask=circle)](https://github.com/adam-berrio)[![6388483](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/6388483?v=4&w=50&h=50&mask=circle)](https://github.com/zsedem)[![8296645](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8296645?v=4&w=50&h=50&mask=circle)](https://github.com/imdanielsp)[![17337515](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/17337515?v=4&w=50&h=50&mask=circle)](https://github.com/fabricepipart)[![10090384](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/10090384?v=4&w=50&h=50&mask=circle)](https://github.com/ivanpk)[![2302957](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2302957?v=4&w=50&h=50&mask=circle)](https://github.com/JeremyLWright)[![995707](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/995707?v=4&w=50&h=50&mask=circle)](https://github.com/OskarStark)[![25486791](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25486791?v=4&w=50&h=50&mask=circle)](https://github.com/pavyarov)[![5067549](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/5067549?v=4&w=50&h=50&mask=circle)](https://github.com/pellared)[![53085803](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/53085803?v=4&w=50&h=50&mask=circle)](https://github.com/cuttingedge1109)[![62775347](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/62775347?v=4&w=50&h=50&mask=circle)](https://github.com/okozachenko1203)[![25625597](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/25625597?v=4&w=50&h=50&mask=circle)](https://github.com/zero-below)[![282792](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/282792?v=4&w=50&h=50&mask=circle)](https://github.com/asford)[![38894122](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/38894122?v=4&w=50&h=50&mask=circle)](https://github.com/bmcconeghy)[![16698198](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/16698198?v=4&w=50&h=50&mask=circle)](https://github.com/conda-forge-admin)[![36490558](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/36490558?v=4&w=50&h=50&mask=circle)](https://github.com/regro-cf-autotick-bot)[![79913779](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/in/102928?v=4&w=50&h=50&mask=circle)](https://github.com/apps/conda-forge-curator)[![41898282](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/in/15368?v=4&w=50&h=50&mask=circle)](https://github.com/apps/github-actions)[![18567580](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/18567580?v=4&w=50&h=50&mask=circle)](https://github.com/conda-forge-linter)[![26092524](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26092524?v=4&w=50&h=50&mask=circle)](https://github.com/fellhorn)
<!-- CONTRIBUTORS END -->


## License

Flyte is available under the Apache License 2.0. Use it wisely. 
",2023-07-07 15:57:30+00:00
forestflow,ForestFlow,ForestFlow/ForestFlow,ForestFlow is a policy-driven Machine Learning Model Server. It is an LF AI Foundation incubation project.,https://forestflow.ai/,False,65,2023-05-31 18:03:35+00:00,2019-10-09 19:03:58+00:00,16,7,5,0,,,Apache License 2.0,84,,0,,2023-05-31 18:03:35+00:00,2021-06-04 04:38:54+00:00,"<!--
    Copyright 2020 DreamWorks Animation L.L.C.
    Licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an ""AS IS"" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
[![ForestFlow](docs/static/forestflow_logo_text.png)](https://dreamworksanimation.github.io/ForestFlow/)


ForestFlow is a scalable policy-based cloud-native machine learning model server. ForestFlow strives to strike a balance between the flexibility it offers data scientists and the adoption of standards while reducing friction between Data Science, Engineering and Operations teams.

ForestFlow is policy-based because we believe automation for Machine Learning/Deep Learning operations is critical to scaling human resources. ForestFlow lends itself well to workflows based on automatic retraining, version control, A/B testing, Canary Model deployments, Shadow testing, automatic time or performance-based model deprecation and time or performance-based model routing in real-time.

Our aim with ForestFlow is to provide data scientists a simple means to deploy models to a production system with minimal friction accelerating the development to production value proposition.

To achieve these goals, ForestFlow looks to address the proliferation of model serving formats and standards for inference API specifications by adopting, what we believe, are currently, or are becoming widely adopted open source frameworks, formats, and API specifications. We do this in a pluggable format such that we can continue to evolve ForestFlow as the industry and space matures and we see a need for additional support.



# Contents

  - [Overview](./docs/overview)
      - [Why ForestFlow?](./docs/overview.md#why-forestflow)
      - [Model Deployment](./docs/overview.md#model-deployment)
      - [Inference](./docs/overview.md#inference)
      - [Currently Supported model formats](./docs/overview.md#currently-supported-model-formats)
  - [Quick Start Guide](./docs/quickstart.md)
  - [Building and Configuration](./docs/buildconfig.md)
      - [Building ForestFlow](./docs/buildconfig.md#building-forestflow)
         - [Create JAR without Kubernetes dependencies](./docs/buildconfig.md#create-jar-without-kubernetes-dependencies)
         - [Create a JAR with Kubernetes dependencies](./docs/buildconfig.md#create-a-jar-with-kubernetes-dependencies)
      - [ForestFlow Configuration](./docs/buildconfig.md#forestflow-configuration)
      - [Creating an OCI-compliant Image](./docs/buildconfig.md#creating-an-oci-compliant-image)
  - [Inference](./docs/inference.md)
      - [Servable implementation interface support matrix](./docs/inference.md#servable-implementation-interface-support-matrix)
      - [Inference - Using the BASIC REST API](./docs/inference.md#using-the-basic-rest-api)
      - [Inference - Using the GraphPipe API](./docs/inference.md#using-the-graphpipe-api)
  - [Concepts](./docs/concepts.md)
      - [A ForestFlow Cluster](./docs/concepts.md#a-forestflow-cluster)
          - [Servable](./docs/concepts.md#servable)
          - [Fully Qualified Release Version (FQRV)](./docs/concepts.md#fully-qualified-release-version-fqrv)
      - [Servable (Model) Deployment](./docs/concepts.md#servable-model-deployment)
          - [Creating a Contract](./docs/concepts.md#creating-a-contract)
          - [Creating a Servable](./docs/concepts.md#creating-a-servable)
  - [Contributing to ForestFlow](#contributing)

# Overview
## Why ForestFlow?
Continuous deployment and lifecycle management of Machine Learning/Deep Learning models is currently widely accepted as a primary bottleneck for gaining value out of ML projects.

We first set out to find a solution to deploy our own models. The model server implementations we found were either proprietary, closed-source solutions or had too many limitations in what we wanted to achieve.
The main concerns for creating ForestFlow can be summarized as:
   - We wanted to reduce friction between our data science, engineering and operations teams
   - We wanted to give data scientists the flexibility to use the tools they wanted (H2O, TensorFlow, Spark export to PFA etc..)
   - We wanted to automate certain lifecycle management aspects of model deployments like automatic performance or time-based routing and retirement of stale models
   - We wanted a model server that allows easy A/B testing, Shadow (listen only) deployments and and Canary deployments. This allows our Data Scientists to experiment with real production data without impacting production and using the same tooling they would when deployment to production. 
   - We wanted something that was easy to deploy and scale for different deployment scenarios (on-prem local data center single instance, cluster of instances, Kubernetes managed, Cloud native etc..)
   - We wanted the ability to treat inference requests as a stream and log predictions as a stream. This allows us to test new models against a stream of older infer requests.
   - We wanted to avoid the ""super-hero"" data scientist that knows how to dockerize an application, apply the science, build an API and deploy to production. This does not scale well and is difficult to support and maintain.
   - Most of all, we wanted repeatability. We didn't want to re-invent the wheel once we had support for a specific framework. 

## Model Deployment
For model deployment, ForestFlow supports models described via [MLfLow Model](https://mlflow.org/docs/latest/models.html) format which allows for different flavors i..e, frameworks & storage formats.

ForestFlow also supports a BASIC REST API for model deployment as well that mimics the MLflow Model format but does not require it.

## Inference
For inference, we’ve adopted a similar approach. ForestFlow provides 2 interfaces for maximum flexibility; 
a [BASIC REST API](./docs/inference.md#using-the-basic-rest-api) in addition to 
standardizing on the [GraphPipe](https://oracle.github.io/graphpipe) 
[API specification](https://oracle.github.io/graphpipe/#/guide/user-guide/spec).

Relying on standards, for example using GraphPipe’s specification means immediate availability of client libraries in a variety of languages that already support working with ForestFlow; see [GraphPipe clients](https://oracle.github.io/graphpipe/#/guide/clients/overview).

Please visit the [quickstart guide](./docs/quickstart.md) to get a quick overview of setting up ForestFlow and an example on inference.
Also please visit the [Inference](./docs/inference.md) documentation for a deeper dive. 

## Currently Supported model formats
 - H2O - Mojo Model
 - TensorFlow & Keras - Planned
 - PFA - Planned
 - Spark ML Models and Pipelines via [Aardpfark](https://github.com/CODAIT/aardpfark) and PFA - Planned

Go to the [Quick Start Guide](./docs/quickstart.md) to get started then dive a little deeper and 
learn about [ForestFlow Concepts](./docs/concepts.md) and how you can 
[tailor](./docs/concepts.md#servable-model-deployment) it to fit your own use-cases.


# Contributing
While ForestFlow has already delivered tremendous value for us in production, it's still in early phases of development
as there are plenty of features we have planned and this continues to evolve at a rapid pace. 
We appreciate and consistently, make use of and, contribute open source projects back to the community. 
We realize the problems we're facing aren't unique to us so we welcome feedback, ideas and contributions from the 
community to help develop our roadmap and implementation of ForestFlow.

Check out [Contribution Guide](./docs/contributing.md) for more details on contributing to ForestFlow.

",2023-07-07 15:57:34+00:00
fractalide,fractalide,fractalide/fractalide,Reusable Reproducible Composable Software,http://fractalide.com,False,819,2023-07-06 08:52:04+00:00,2015-12-06 14:04:07+00:00,57,48,10,0,,,Mozilla Public License 2.0,1680,v20170415,5,2017-04-13 07:34:54+00:00,2023-07-06 08:52:04+00:00,2019-04-19 04:09:32+00:00,"image::https://raw.githubusercontent.com/fractalide/fractalide/master/pkgs/hyperflow/imgs/fractalide.png[Fractalide,align=""center""]

**Reusable Reproducible Composable Software**

image:https://img.shields.io/badge/license-MPLv2-blue.svg[LICENSE,link=https://github.com/fractalide/fractalide/blob/master/LICENSE]
image:https://travis-ci.org/fractalide/fractalide.svg?branch=master[""Build Status"", link=""https://travis-ci.org/fractalide/fractalide""]

== Welcome

// tag::doc[]

== What is this?

Fractalide is a free and open source service programming platform using dataflow graphs. Graph nodes represent computations, while graph edges represent typed data (may also describe tensors) communicated between them. This flexible architecture can be applied to many different computation problems, initially the focus will be Microservices to be expanded out into the Internet of Things.

Fractalide is in the same vein as the https://en.wikipedia.org/wiki/Apache_NiFi[NSA's Niagrafiles] (now known as https://nifi.apache.org/[Apache-NiFi]) or https://en.wikipedia.org/wiki/TensorFlow[Google's TensorFlow] but stripped of all Java, Python and GUI bloat. Fractalide faces big corporate players like http://abinitio.com/[Ab Initio], a company that charges a lot of money for dataflow solutions.

Truly reusable and reproducible efficient nodes is what differentiates Fractalide from the others. It's this feature that allows open communities to mix and match nodes quickly and easily.

== Features

Fractalide stands on the shoulders of giants by combining the strengths of each language into one programming model.


[cols=""9*^""]
|===
|Op |Technology |Safe|Zero-cost Abstractions|Reuse|Reproducible|Distributed Type System| Concurrent| Service Config Man.


|   |NixOS      |     |                      |         |          |                        |          |✔
|+  |Nix Expr   |     |                      |         |✔         |                        |          |
|+  |Rust       |✔    |✔                    |         |          |                        |✔         |
|+  |Flow-based Programming |    |           |✔       |          |                        |✔         |
|+  |Cap'n Proto|     |                      |         |          |✔                      |          |
|=  |Fractalide Model |✔   |✔                |✔       |✔         |✔                       |✔        |✔
|===


== What's new from different perspectives

=== Nix Programmers

Fractalide brings __safe, fast, reusable, black-box__ dataflow functions and a means to compose them.

__Tagline: ""Nixpkgs is not enough! Here, have 'Nixfuncs' too!""__

=== Rust Programmers

Fractalide brings __reproducible, reusable, black-box__ dataflow functions, a means to compose them and a https://www.usenix.org/legacy/event/lisa02/tech/full_papers/traugott/traugott_html/[congruent] model of configuration management.

__Tagline: Safety extended beyond the application boundary into infrastructure.__

=== Flow-based Programmers

Fractalide brings __safe fast reproducible__ classical Flow-based programming components, and a https://www.usenix.org/legacy/event/lisa02/tech/full_papers/traugott/traugott_html/[congruent] model of configuration management.

__Tagline: Reproducible components!__

=== Programmers

Fractalide brings __safe, fast, reusable, reproducible, black-box__ dataflow functions, a means to compose them and a https://www.usenix.org/legacy/event/lisa02/tech/full_papers/traugott/traugott_html/[congruent] model of configuration management.

__Tagline: Here, have a beer!__

== Solved problems

=== Modules-code coupling

Language level modules become tightly coupled with the rest of the code, moving around these modules also poses a problem.

==== Solution

An unanticipated outcome occurred when combining FBP and Nix. It's become our peanut butter and jam combination, so to say, but requires a bit of explaining, so hang tight.

===== Reproducibility

Nix is a content addressable store, so is git, so is docker, except that docker's SHA resolution is at container level and git's SHA resolution is at changeset level. Nix on the other hand has a SHA resolution at package level, and it's known as a `derivation`. If you're trying to create reproducible systems this is the correct resolution. Too big and you're copying around large container sized images with multiple versions occupying gigabytes of space, too small and you run into problems of git not being able to scale to support thousands of binaries that build an operating system. Therefore Nix subsumes Docker.

Indeed it's these simple `derivations` that allow python 2.7 and 3.0 to exist side-by-side without conflicts. It's what allows the Nix community to compose an entire operating system, NixOS. These `derivations` are what makes NixOS a congruent configuration management system, and congruent systems are reproducible systems. They have to be.

===== Reusability

Flow-based programming in our books has delivered on its promise. In our system FBP components are known as a `nodes` and they are reusable, clean and composable. It's a very nice way to program computers. Though, we've found, the larger the network of `nodes`, the more overhead required to build, manage, version, package, connect, test and distribute all these moving pieces. This really doesn't weigh well against FBP's advantages. Still, there is this beautiful reusable side that is highly advantageous! If only we could take the good parts?

===== Reproducibility + Reusability

When nix is assigned the responsibility of declaratively building fbp `nodes`, a magic thing happens. All that manual overhead of having to build, manage and package etc gets done once and only once by the `node` author, and completely disappears for everyone thereafter. We're left with the reusable good parts that FBP has to offer. Indeed the greatest overhead a `node` user has, is typing the ``node``'s name. We've gone further and distilled the overhead to a few lines, no more intimidating than a typical config file such as `Cargo.toml`:

[source, nix, subs=""none""]
----
{ agent, edges, mods, pkgs }:

agent {
  src = ./.;
  edges = with edges; [ PrimText FsPath ];
  mods = with mods.rs; [ rustfbp rusqlite ];
  osdeps = with pkgs; [ sqlite pkgconfig ];
}
----

Now just to be absolutely clear of the implications; it's possible to call an extremely complex community developed hierarchy of potentially 1000+ nodes, where each node might have different https://crates.io dependencies, they might have OS level dependencies such as `openssl` etc and nix will ensure the entire hierarchy is correctly built and made available. All this is done by just typing the `node` name and issuing a build command.

It's this feature that sets us apart from Google TensorFlow and Apache-NiFi. It contains the DNA to build a massive sprawling community of open source programmers, this and the C4, that is. It's our hope anyway!

=== Complex configuration management model

The vast majority of system configuration management solutions use either the divergent or convergent model.

We're going to quote Steve Traugott's excellent work vebatim.

==== Divergent

image::https://raw.githubusercontent.com/fractalide/fractalide/master/doc/images/divergent.png[]

[quote, Steve Traugott]
____
""One quick way to tell if a shop is divergent is to ask how changes are made on production hosts, how those same changes are incorporated into the baseline build for new or replacement hosts, and how they are made on hosts that were down at the time the change was first deployed. If you get different answers, then the shop is likely divergent.

The symptoms of divergence include unpredictable host behavior, unscheduled downtime, unexpected package and patch installation failure, unclosed security vulnerabilities, significant time spent ""firefighting"", and high troubleshooting and maintenance costs.""
____

==== Convergent

image::https://raw.githubusercontent.com/fractalide/fractalide/master/doc/images/convergent.png[]

[quote, Steve Traugott]
____
""The baseline description in a converging infrastructure is characteristically an incomplete description of machine state. You can quickly detect convergence in a shop by asking how many files are currently under management control. If an approximate answer is readily available and is on the order of a few hundred files or less, then the shop is likely converging legacy machines on a file-by-file basis.

A convergence tool is an excellent means of bringing some semblance of order to a chaotic infrastructure. Convergent tools typically work by sampling a small subset of the disk - via a checksum of one or more files, for example - and taking some action in response to what they find. The samples and actions are often defined in a declarative or descriptive language that is optimized for this use. This emulates and preempts the firefighting behavior of a reactive human systems administrator - ""see a problem, fix it."" Automating this process provides great economies of scale and speed over doing the same thing manually.

Because convergence typically includes an intentional process of managing a specific subset of files, there will always be unmanaged files on each host. Whether current differences between unmanaged files will have an impact on future changes is undecidable, because at any point in time we do not know the entire set of future changes, or what files they will depend on.

It appears that a central problem with convergent administration of an initially divergent infrastructure is that there is no documentation or knowledge as to when convergence is complete. One must treat the whole infrastructure as if the convergence is incomplete, whether it is or not. So without more information, an attempt to converge formerly divergent hosts to an ideal configuration is a never-ending process. By contrast, an infrastructure based upon first loading a known baseline configuration on all hosts, and limited to purely orthogonal and non-interacting sets of changes, implements congruence. Unfortunately, this is not the way most shops use convergent tools...""
____

==== Solution

===== Congruent

image::https://raw.githubusercontent.com/fractalide/fractalide/master/doc/images/congruent.png[]

[quote, Steve Traugott]
____
""By definition, divergence from baseline disk state in a congruent environment is symptomatic of a failure of code, administrative procedures, or security. In any of these three cases, we may not be able to assume that we know exactly which disk content was damaged. It is usually safe to handle all three cases as a security breach: correct the root cause, then rebuild.

You can detect congruence in a shop by asking how the oldest, most complex machine in the infrastructure would be rebuilt if destroyed. If years of sysadmin work can be replayed in an hour, unattended, without resorting to backups, and only user data need be restored from tape, then host management is likely congruent.

Rebuilds in a congruent infrastructure are completely unattended and generally faster than in any other; anywhere from ten minutes for a simple workstation to two hours for a node in a complex high-availability server cluster (most of that two hours is spent in blocking sleeps while meeting barrier conditions with other nodes).

Symptoms of a congruent infrastructure include rapid, predictable, ""fire-and-forget"" deployments and changes. Disaster recovery and production sites can be easily maintained or rebuilt on demand in a bit-for-bit identical state. Changes are not tested for the first time in production, and there are no unforeseen differences between hosts. Unscheduled production downtime is reduced to that caused by hardware and application problems; firefighting activities drop considerably. Old and new hosts are equally predictable and maintainable, and there are fewer host classes to maintain. There are no ad-hoc or manual changes. We have found that congruence makes cost of ownership much lower, and reliability much higher, than any other method.""
____

Fractalide does not violate the congruent model of Nix, and it's why NixOS is a dependency. Appreciation for safety has extended beyond the application boundary into infrastructure as a whole.

=== Language choice

A language needed to be chosen to implement Fractalide. Now as Fractalide is primarily a Flow-based programming environment, it would be beneficial to choose a language that at least gets concurrency right.

==== Solution

Rust was a perfect fit. The concept of ownership is critical in Flow-based Programming. The Flow-based scheduler is typically responsible for tracking every Information Packet (IP) as it flows through the system. Fortunately Rust excels at getting the concept of ownership right. To the point of leveraging this concept that a garbage collector is not needed. Indeed, different forms of concurrency can be layered on Rust's ownership concept. One very neat advantage Rust gives us is that we can very elegantly implement Flow-based Programming's idea of concurrency. This makes our scheduler extremely lightweight as it doesn't need to track IPs at all. Once an IP isn't owned by any component, Rust makes it wink out of existance, no harm to anyone.

=== API contracts

It's easy to disrespect API contracts in a distributed services setup.

==== Solution

We wanted to ensure there was no ambiguity about the shape of the data a node receives. Also if the shape of data changes, the error must be caught at compile time. Cap'n Proto schema fits these requirements, and fits them *perfectly* when nix builds the `nodes` calling the Cap'n Proto schema. Because, if a schema changes, nix will register the change and will rebuild everything (`nodes` and `subgraphs`) that depends on that schema, thus catching the error. We've also made it such, during graph load time `agents` cannot connect their ports unless they use the same Cap'n Proto schema. This is a very nice safety property.

== The mandatory Hello-like World example.

From a fresh install of NixOS (using the `nixos-unstable` channel) we'll build the ``fractalide virtual machine (fvm)`` and execute the humble NAND logic gate on it.

[source, sh]
----
$ git clone https://github.com/fractalide/fractalide.git
$ cd fractalide
$ nix-build --argstr node test_nand
...
$ ./result
boolean : false
----

// end::doc[]

== Documentation

* https://github.com/fractalide/fractalide/tree/master/nodes/README.adoc[Nodes]
* https://github.com/fractalide/fractalide/tree/master/edges/README.adoc[Edges]
* https://github.com/fractalide/fractalide/tree/master/services/README.adoc[Services]
* https://github.com/fractalide/fractalide/tree/master/fractals/README.adoc[Fractals]
* https://github.com/fractalide/fractalide/tree/master/HOWTO.adoc[HOWTO]
* https://docs.rs/rustfbp[RustFBP]

== Contributing to Fractalide

* Contributors are listed in link:./AUTHORS[AUTHORS]. Copyright is distributed throughout the community to protect contributors from having their work used unethically.
* Our contribution policy is the link:./CONTRIBUTING.md[C4.2 (Collective Code Construction Contract)].

### Contributing FAQ
#### Q: I'm kind of new to Github, how do I get started?
* Read the link:./CONTRIBUTING.md[C4.2 (Collective Code Construction Contract)] and the https://github.com/Blockrazor/blockrazor/blob/master/DESCRIPTIVE_C4.MD[line by line explanation] of the protocol.
* Fork this github repository under your own github account.
* Clone _your_ fork locally on your development machine.
* Choose _one_ problem to solve. If you aren't solving a problem that's already in the issue tracker you should describe the problem there (and your idea of the solution) first to see if anyone else has something to say about it (maybe someone is already working on a solution, or maybe you're doing somthing wrong). **If the issue is in the issue tracker, you should comment on the issue to say you're working on the solution so that other people don't work on the same thing.**
* Add the Fractalide repository as an upstream source and pull any changes:
[source, sh]
----
$ git remote add upstream git://github.com/fractalide/fractalide //only needs to be done once
$ git checkout master //just to make sure you're on the correct branch
$ git pull upstream master //this grabs any code that has changed, you want to be working on the latest 'version'
$ git push //update your remote fork with the changes you just pulled from upstream master
----
* Create a local branch on your machine `git checkout -b branch_name`(it's usually a good idea to call the branch something that describes the problem you are solving).
* Solve the problem in the absolute most simple and fastest possible way with the smallest number of changes humanly possible. Tell other people what you're doing by putting _very clear and descriptive comments in your code every 2-3 lines_.
Add your name to the AUTHORS file so that you become a part owner of Fractalide.
* Commit your changes to your own fork:
Before you commit changes, you should check if you are working on the latest version (again). Go to the github website and open _your_ fork of Fractalide, it should say _This branch is even with Fractalide:master._
If **not**, you need to pull the latest changes from the upstream Fractalide repository and replay your changes on top of the latest version:
[source, sh]
----
$ git stash //save your work locally
$ git checkout master
$ git pull upstream master
$ git push
$ git checkout -b branch_name_stash
$ git stash pop //_replay_ your work on the new branch which is now fully up to date with the fractalide repository
----

Now you can add and commit your changes:
[source, sh]
----
$ git add changed_file.js //repeat for each file you changed
$ git commit -m 'problem: very short description of problem //do not close the &#x27;&#x27;, press ENTER two (2) times
>
>solution: short description of how you solved the problem.' //Now you can close the &#x27;&#x27;. Also mention the issue number if there is one (e.g. #6)
$ git push //this will send your changes to _your_ fork on Github
----
* Go to your fork on Github and select the branch you just worked on. Click ""pull request"" to send a pull request back to the Fractalide repository.
* Send the pull request.

#### Q: What happens after I send a pull request?
If your pull request contains a correct patch (read the C4) a maintainer should merge it.
If you want to work on another problem in the meantime simply repeat the above steps starting at:
[source, sh]
----
$ git checkout master
----

#### Q: Can I be paid to contribute to Fractalide?
Yes, this is sometimes possible. Your first step is to _very carefully read and understand everything above_, including the linked files, then start fixing problems and sending pull requests! If your code is amazing and brilliant but you don't understand the contribution process we cannot consider you for a paid position. Make sure you follow the project on Github so you get updates. Contact Fractalide's BDFL (Benevolent Dictator For Life): mailto:setori88@gmail.com[Stewart Mackenzie] if you've been contributing code to Fractalide and want to keep doing it but but you require financial assistance.

== Consulting and Support

[cols=3]
|===
|Name | Info | Language

|mailto:setori88@gmail.com[Stewart Mackenzie] | Founder and maintainer of Fractalide | English
|mailto:dmichiels@mailoo.org[Denis Michiels] | Founder and maintainer of Fractalide | French
|===

== License

The project license is specified in LICENSE.
Fractalide is free software; you can redistribute it and/or modify it under the terms of the Mozilla Public License Version 2 as approved by the Free Software Foundation.

== Social

Follow us on https://twitter.com/fractalide[twitter]

== Thanks

* Peter Van Roy
* Pieter Hintjens
* Joachim Schiele & Paul Seitz
* P Meunier
",2023-07-07 15:57:38+00:00
frictionlessdatapackagepipelines,datapackage-pipelines,frictionlessdata/datapackage-pipelines,Framework for processing data packages in pipelines of modular components.,https://frictionlessdata.io/,False,115,2023-06-23 19:49:29+00:00,2016-08-30 08:33:18+00:00,32,11,15,1,v2.1.9,2019-07-05 10:50:42+00:00,MIT License,480,v2.1.9,84,2019-07-05 10:50:42+00:00,2023-06-15 20:11:42+00:00,2023-06-12 11:17:41+00:00,"# Datapackage Pipelines

[![Travis](https://img.shields.io/travis/frictionlessdata/datapackage-pipelines/master.svg)](https://travis-ci.org/frictionlessdata/datapackage-pipelines)
[![Coveralls](http://img.shields.io/coveralls/frictionlessdata/datapackage-pipelines.svg?branch=master)](https://coveralls.io/r/frictionlessdata/datapackage-pipelines?branch=master)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/datapackage-pipelines.svg)

## The Basics

### What is it?

`datapackage-pipelines` is a framework for declarative stream-processing of tabular data. It is built upon the concepts and tooling of the Frictionless Data project.

### Pipelines

The basic concept in this framework is the pipeline.

A pipeline has a list of processing steps, and it generates a single *data package* as its output. Each step is executed in a _processor_ and consists of the following stages:

- **Modify the data package descriptor** - For example: add metadata, add or remove resources, change resources' data schema etc.
- **Process resources** - Each row of each resource is processed sequentially. The processor can drop rows, add new ones or modify their contents.
- **Return stats** - If necessary, the processor can report a dictionary of data which will be returned to the user when the pipeline execution terminates. This can be used, for example, for calculating quality measures for the processed data.

Not every processor needs to do all of these. In fact, you would often find each processing step doing only one of these.

### `pipeline-spec.yaml` file

Pipelines are defined in a declarative way, and not in code. One or more pipelines can be defined in a `pipeline-spec.yaml` file. This file specifies the list of processors (referenced by name) and the execution parameters for each of the processors.

Here's an example of a `pipeline-spec.yaml` file:

```yaml
worldbank-co2-emissions:
  title: CO2 emission data from the World Bank
  description: Data per year, provided in metric tons per capita.
  environment:
    DEBUG: true
  pipeline:
    -
      run: update_package
      parameters:
        name: 'co2-emissions'
        title: 'CO2 emissions (metric tons per capita)'
        homepage: 'http://worldbank.org/'
    -
      run: load
      parameters:
        from: ""http://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel""
        name: 'global-data'
        format: xls
        headers: 4
    -
      run: set_types
      parameters:
         resources: global-data
         types:
           ""[12][0-9]{3}"":
              type: number
    -
      run: dump_to_zip
      parameters:
          out-file: co2-emissions-wb.zip
```

In this example we see one pipeline called `worldbank-co2-emissions`. Its pipeline consists of 4 steps:

- `update_package`: This is a library processor  (see below), which modifies the data-package's descriptor (in our case: the initial, empty descriptor) - adding `name`, `title` and other properties to the datapackage.
- `load`: This is another library processor, which loads data into the data-package.
  This resource has a `name` and a `from` property, pointing to the remote location of the data.
- `set_types`: This processor assigns data types to fields in the data. In this example, field headers looking like years will be assigned the `number` type.
- `dump_to_zip`: Create a zipped and validated datapackage with the provided file name.

Also, we have provided some metadata:

- `title`: Title of a pipeline
- `description`: Description of a pipeline
- `environment`: Dictionary of environment variables to be set for all the pipeline's steps. For examples, it can be used to change the behaviour of the underlaying `requests` library - https://requests.readthedocs.io/en/master/user/advanced/#ssl-cert-verification

> Full JSONSchema of the `pipeline-spec.yaml` file can be found [here](https://github.com/frictionlessdata/datapackage-pipelines/blob/master/datapackage_pipelines/specs/schemas/pipeline-spec.schema.json)

### Mechanics

An important aspect of how the pipelines are run is the fact that data is passed in streams from one processor to another. If we get ""technical"" here, then each processor is run in its own dedicated process, where the datapackage is read from its `stdin` and output to its `stdout`. The important thing to note here is that no processor holds the entire data set at any point.

This limitation is by design - to keep the memory and disk requirements of each processor limited and independent of the dataset size.

### Quick Start

First off, create a `pipeline-spec.yaml` file in your current directory. You can take the above file if you just want to try it out.

Then, you can either install `datapackage-pipelines` locally - note that _Python 3.6_ or higher is required due to use of [Type Hinting](https://www.python.org/dev/peps/pep-0484/) and advanced `asyncio` use:

```shell
$ pip install datapackage-pipelines
```

You should now be able to use the `dpp` command:

```shell
$ dpp
Available Pipelines:
- ./worldbank-co2-emissions (*)

$ $ dpp run --verbose ./worldbank-co2-emissions
RUNNING ./worldbank-co2-emissions
Collecting dependencies
Running async task
Waiting for completion
Async task starting
Searching for existing caches
Building process chain:
- update_package
- load
- set_types
- dump_to_zip
- (sink)
DONE /Users/adam/code/dhq/specstore/dpp_repo/datapackage_pipelines/specs/../lib/update_package.py
load: DEBUG   :Starting new HTTP connection (1): api.worldbank.org:80
load: DEBUG   :http://api.worldbank.org:80 ""GET /v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel HTTP/1.1"" 200 308736
load: DEBUG   :http://api.worldbank.org:80 ""GET /v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel HTTP/1.1"" 200 308736
load: DEBUG   :Starting new HTTP connection (1): api.worldbank.org:80
load: DEBUG   :http://api.worldbank.org:80 ""GET /v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel HTTP/1.1"" 200 308736
load: DEBUG   :http://api.worldbank.org:80 ""GET /v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel HTTP/1.1"" 200 308736
set_types: INFO    :(<dataflows.processors.set_type.set_type object at 0x10a5c79b0>,)
load: INFO    :Processed 264 rows
set_types: INFO    :Processed 264 rows
DONE /Users/adam/code/dhq/specstore/dpp_repo/datapackage_pipelines/specs/../lib/load.py
DONE /Users/adam/code/dhq/specstore/dpp_repo/datapackage_pipelines/specs/../lib/set_types.py
dump_to_zip: INFO    :Processed 264 rows
DONE /Users/adam/code/dhq/specstore/dpp_repo/datapackage_pipelines/manager/../lib/internal/sink.py
DONE /Users/adam/code/dhq/specstore/dpp_repo/datapackage_pipelines/specs/../lib/dump_to_zip.py
DONE V ./worldbank-co2-emissions {'bytes': 692741, 'count_of_rows': 264, 'dataset_name': 'co2-emissions', 'hash': '4dd18effcdfbf5fc267221b4ffc28fa4'}
INFO    :RESULTS:
INFO    :SUCCESS: ./worldbank-co2-emissions {'bytes': 692741, 'count_of_rows': 264, 'dataset_name': 'co2-emissions', 'hash': '4dd18effcdfbf5fc267221b4ffc28fa4'}
```

Alternatively, you could use our [Docker](https://www.docker.com/) image:

```shell
$ docker run -it -v `pwd`:/pipelines:rw \
        frictionlessdata/datapackage-pipelines
<available-pipelines>

$ docker run -it -v `pwd`:/pipelines:rw \
       frictionlessdata/datapackage-pipelines run ./worldbank-co2-emissions
<execution-logs>
```

### The Command Line Interface - `dpp`

Running a pipeline from the command line is done using the `dpp` tool.

Running `dpp` without any argument, will show the list of available pipelines. This is done by scanning the current directory and its subdirectories, searching for `pipeline-spec.yaml` files and extracting the list of pipeline specifications described within.

Each pipeline has an identifier, composed of the path to the `pipeline-spec.yaml` file and the name of the pipeline, as defined within that description file.

In order to run a pipeline, you use `dpp run <pipeline-id>`.

You can also use `dpp run all` for running all pipelines and `dpp run dirty` to run the just the _dirty_ pipelines (more on that later on).

## Deeper look into pipelines

### Processor Resolution

As previously seen, processors are referenced by name.

This name is, in fact, the name of a Python script containing the processing code (minus the `.py` extension). When trying to find where is the actual code that needs to be executed, the processor resolver will search in these predefined locations:

- First of all, it will try to find a custom processor with that name in the directory of the `pipeline-spec.yaml` file.
  Processor names support the dot notation, so you could write `mycode.custom_processor` and it will try to find a processor named `custom_processor.py` in the `mycode` directory, in the same path as the pipeline spec file.
  For this specific resolving phase, if you would write `..custom_processor` it will try to find that processor in the parent directory of the pipeline spec file.
  (read on for instructions on how to write custom processors)
- In case the processor name looks like `myplugin.somename`, it will try to find a processor named `somename` in the `myplugin` plugin. That is - it will see if there's an installed plugin which is called `myplugin`, and if so, whether that plugin publishes a processor called `somename` (more on plugins below).
- If no processor was found until this point, it will try to search for this processor in the processor search path. The processor search path is taken from the environment variable `DPP_PROCESSOR_PATH`. Each of the `:` separated paths in the path is considered as a possible starting point for resolving the processor.
- Finally, it will try to find that processor in the Standard Processor Library which is bundled with this package.

### Excluding directories form scanning for pipeline specs

By default `.*` directories are excluded from scanning, you can add additional directory patterns for
exclusion by creating a `.dpp_spec_ignore` file at the project root. This file has similar syntax
to .gitignore and will exclude directories from scanning based on glob pattern matching.

For example, the following file will ignore `test*` directories including inside subdirectories
and `/docs` directory will only be ignored at the project root directory

```
test*
/docs
```

### Caching

By setting the `cached` property on a specific pipeline step to `True`, this step's output will be stored on disk (in the `.cache` directory, in the same location as the `pipeline-spec.yaml` file).

Rerunning the pipeline will make use of that cache, thus avoiding the execution of the cached step and its precursors.

Internally, a hash is calculated for each step in the pipeline - which is based on the processor's code, it parameters and the hash of its predecessor. If a cache file exists with exactly the same hash as a specific step, then we can remove it (and its predecessors) and use that cache file as an input to the pipeline

This way, the cache becomes invalid in case the code or execution parameters changed (either for the cached processor or in any of the preceding processors).

### Dirty tasks and keeping state

The cache hash is also used for seeing if a pipeline is ""dirty"". When a pipeline completes executing successfully, `dpp` stores the cache hash along with the pipeline id. If the stored hash is different than the currently calculated hash, it means that either the code or the execution parameters were modified, and that the pipeline needs to be re-run.

`dpp` works with two storage backends. For running locally, it uses a python _sqlite DB_ to store the current state of each running task, including the last result and cache hash. The state DB file is stored in a file named `.dpp.db` in the same directory that `dpp` is being run from.

For other installations, especially ones using the task scheduler, it is recommended to work with the _Redis_ backend. In order to enable the Redis connection, simply set the `DPP_REDIS_HOST` environment variable to point to a running Redis instance.

### Pipeline Dependencies

You can declare that a pipeline is dependent on another pipeline or datapackage. This dependency is considered when calculating the cache hashes of a pipeline, which in turn affect the validity of cache files and the ""dirty"" state:
- For pipeline dependencies, the hash of that pipeline is used in the calculation
- For datapackage dependencies, the `hash` property in the datapackage is used in the calculation

If the dependency is missing, then the pipeline is marked as 'unable to be executed'.

Declaring dependencies is done by a `dependencies` property to a pipeline definition in the `pipeline-spec.yaml` file.
This property should contain a list of dependencies, each one is an object with the following formats:
- A single key named `pipeline` whose value is the pipeline id to depend on
- A single key named `datapackage` whose value is the identifier (or URL) for the datapackage to depend on

Example:
```yaml
cat-vs-dog-populations:
  dependencies:
    -
      pipeline: ./geo/region-areal
    -
      datapackage: http://pets.net/data/dogs-per-region/datapackage.json
    -
      datapackage: http://pets.net/data/dogs-per-region
  ...
```

### Validating

Each processor's input is automatically validated for correctness:

- The datapackage is always validated before being passed to a processor, so there's no possibility for a processor to modify a datapackage in a way that renders it invalid.

- Data is not validated against its respective JSON Table Schema, unless explicitly requested by setting the `validate` flag to True in the step's info.
  This is done for two main reasons:

  - Performance wise, validating the data in every step is very CPU intensive
  - In some cases you modify the schema in one step and the data in another, so you would only like to validate the data once all the changes were made

  In any case, when using the `set_types` standard processor, it will validate and transform the input data with the new types..

### Dataflows integration

[Dataflows](https://github.com/datahq/dataflows) is the successor of datapackage-pipelines and provides a more
Pythonic interface to running pipelines. You can integrate dataflows within pipeline specs using the `flow` attribute
instead of `run`. For example, given the following flow file, saved under `my-flow.py`:

```
from dataflows import Flow, dump_to_path, load, update_package

def flow(parameters, datapackage, resources, stats):
    stats['multiplied_fields'] = 0

    def multiply(field, n):
        def step(row):
            row[field] = row[field] * n
            stats['multiplied_fields'] += 1
        return step

    return Flow(update_package(name='my-datapackage'),
                multiply('my-field', 2))
```

And a `pipeline-spec.yaml` in the same directory:

```
my-flow:
  pipeline:
  - run: load_resource
    parameters:
      url: http://example.com/my-datapackage/datapackage.json
      resource: my-resource
  - flow: my-flow
  - run: dump_to_path
```

You can run the pipeline using `dpp run my-flow`.

## The Standard Processor Library

A few built in processors are provided with the library.

### ***`update_package`***

Adds meta-data to the data-package.

_Parameters_:

Any allowed property (according to the [spec]([http://specs.frictionlessdata.io/data-packages/#metadata)) can be provided here.

*Example*:

```yaml
- run: update_package
  parameters:
    name: routes-to-mordor
    license: CC-BY-SA-4
    author: Frodo Baggins <frodo@shire.me>
    contributors:
      - samwise gamgee <samwise1992@yahoo.com>
```

### ***`update_resource`***

Adds meta-data to the resource.

_Parameters_:

- `resources`
  - A name of a resource to operate on
  - A regular expression matching resource names
  - A list of resource names
  - The index of the resource in the package
  - if omitted indicates operation should be done on all resources
- `metadata` - A dictionary containing any allowed property (according to the [spec]([https://frictionlessdata.io/specs/data-resource/#metadata)).

*Example*:

```yaml
- run: update_resource
  parameters:
    resources: ['resource1']
    metadata:
      path: 'new-path.csv'
```

### ***`load`***

Loads data into the package, infers the schema and optionally casts values.

_Parameters_:
- `from` - location of the data that is to be loaded. This can be either:
  - a local path (e.g. /path/to/the/data.csv)
  - a remote URL (e.g. https://path.to/the/data.csv)
  - Other supported links, based on the current support of schemes and formats in [tabulator](https://github.com/frictionlessdata/tabulator-py#schemes)
  - a local path or remote URL to a datapackage.json file (e.g. https://path.to/data_package/datapackage.json)
  - a reference to an environment variable containing the source location, in the form of `env://ENV_VAR`
  - a tuple containing (datapackage_descriptor, resources_iterator)
- `resources` - optional, relevant only if source points to a datapackage.json file or datapackage/resource tuple. Value should be one of the following:
  - Name of a single resource to load
  - A regular expression matching resource names to load
  - A list of resource names to load
  - 'None' indicates to load all resources
  - The index of the resource in the package
- `validate` - Should data be casted to the inferred data-types or not. Relevant only when not loading data from datapackage.
- other options - based on the loaded file, extra options (e.g. sheet for Excel files etc., see the link to tabulator above)

### ***`printer`***

Just prints whatever it sees. Good for debugging.

_Parameters_:
- `num_rows` - modify the number of rows to preview, printer will print multiple samples of this number of rows from different places in the stream
- `last_rows` - how many of the last rows in the stream to print. optional, defaults to the value of num_rows
- `fields` - optional, list of field names to preview
- `resources` - optional, allows to limit the printed resources, same semantics as load processor resources argument

### ***`set_types`***

Sets data types and type options to fields in streamed resources, and make sure that the data still validates with the new types.

This allows to make modifications to the existing table schema, and usually to the default schema from `stream_remote_resources`.

_Parameters_:

-  `resources` - Which resources to modify. Can be:

   - List of strings, interpreted as resource names to stream
   - String, interpreted as a regular expression to be used to match resource names

   If omitted, all resources in datapackage are streamed.

- `regex` - if set to `False` field names will be interpreted as strings not as regular expressions (`True` by default)
-  `types` - A map between field names and field definitions.
   - _field name_ is either simply the name of a field, or a regular expression matching multiple fields.
   - _field definition_ is an object adhering to the [JSON Table Schema spec](http://specs.frictionlessdata.io/table-schema/). You can use `null` instead of an object to remove a field from the schema.


*Example*:

```yaml
- run: add_resources
  parameters:
    name: example-resource
    url: http://example.com/my-csv-file.csv
    encoding: ""iso-8859-2""
- run: stream_remote_resources
- run: set_types
  parameters:
    resources: example-resource
    types:
      age:
        type: integer
      ""yearly_score_[0-9]{4}"":
        type: number
      ""date of birth"":
        type: date
        format: ""%d/%m/%Y""
      ""social security number"": null
```

### ***`load_metadata`***

Loads metadata from an existing data-package.

_Parameters_:

Loads the metadata from the data package located at `url`.

All properties of the loaded datapackage will be copied (except the `resources`)

*Example*:

```yaml
- run: load_metadata
  parameters:
    url: http://example.com/my-datapackage/datapackage.json
```

### ***`load_resource`***

Loads a tabular resource from an existing data-package.

_Parameters_:

Loads the resource specified in the `resource` parameter from the data package located at `url`.
All properties of the loaded resource will be copied - `path` and `schema` included.

- `url` - a URL pointing to the datapackage in which the required resource resides

- `resource` - can be
   - List of strings, interpreted as resource names to load
   - String, interpreted as a regular expression to be used to match resource names
   - an integer, indicating the index of the resource in the data package (0-based)

- `limit-rows` - if provided, will limit the number of rows fetched from the source. Takes an integer value which specifies how many rows of the source to stream.

- `log-progress-rows` - if provided, will log the loading progress. Takes an integer value which specifies the number of rows interval at which to log the progress.

- `stream` - if provided and is set to false, then the resource will be added to the datapackage but not streamed.

- `resources` - can be used instead of `resource` property to support loading resources and modify the output resource metadata
    - Value is a dict containing mapping between source resource name to load and dict containing descriptor updates to apply to the loaded resource

- `required` - if provided and set to false, will not fail if datapackage is not available or resource is missing


*Example*:

```yaml
- run: load_resource
  parameters:
    url: http://example.com/my-datapackage/datapackage.json
    resource: my-resource
- run: load_resource
  parameters:
    url: http://example.com/my-other-datapackage/datapackage.json
    resource: 1
- run: load_resource
  parameters:
    url: http://example.com/my-datapackage/datapackage.json
    resources:
      my-resource:
        name: my-renamed-resource
        path: my-renamed-resource.csv
```


### ***`concatenate`***

Concatenates a number of streamed resources and converts them to a single resource.

_Parameters_:

- `sources` - Which resources to concatenate. Same semantics as `resources` in `stream_remote_resources`.

  If omitted, all resources in datapackage are concatenated.

  Resources to concatenate must appear in consecutive order within the data-package.

- `target` - Target resource to hold the concatenated data. Should define at least the following properties:

  - `name` - name of the resource
  - `path` - path in the data-package for this file.

  If omitted, the target resource will receive the name `concat` and will be saved at `data/concat.csv` in the datapackage.

- `fields` - Mapping of fields between the sources and the target, so that the keys are the _target_ field names, and values are lists of _source_ field names.

  This mapping is used to create the target resources schema.

  Note that the target field name is _always_ assumed to be mapped to itself.

*Example*:

```yaml
- run: concatenate
  parameters:
    target:
      name: multi-year-report
      path: data/multi-year-report.csv
    sources: 'report-year-20[0-9]{2}'
    fields:
      activity: []
      amount: ['2009_amount', 'Amount', 'AMOUNT [USD]', '$$$']
```

In this example we concatenate all resources that look like `report-year-<year>`, and output them to the `multi-year-report` resource.

The output contains two fields:

- `activity` , which is called `activity` in all sources
- `amount`, which has varying names in different resources (e.g. `Amount`, `2009_amount`, `amount` etc.)

### ***`join`***

Joins two streamed resources.

""Joining"" in our case means taking the *target* resource, and adding fields to each of its rows by looking up data in the _source_ resource.

A special case for the join operation is when there is no target stream, and all unique rows from the source are used to create it.
This mode is called _deduplication_ mode - The target resource will be created and  deduplicated rows from the source will be added to it.

_Parameters_:

- `source` - information regarding the _source_ resource
  - `name` - name of the resource
  - `key` - One of
    - List of field names which should be used as the lookup key
    - String, which would be interpreted as a Python format string used to form the key (e.g. `{<field_name_1>}:{field_name_2}`)
  - `delete` - delete from data-package after joining (`False` by default)
- `target` - Target resource to hold the joined data. Should define at least the following properties:
  - `name` - as in `source`
  - `key` - as in `source`, or `null` for creating the target resource and performing _deduplication_.
- `fields` - mapping of fields from the source resource to the target resource.
  Keys should be field names in the target resource.
  Values can define two attributes:
  - `name` - field name in the source (by default is the same as the target field name)

  - `aggregate` - aggregation strategy (how to handle multiple _source_ rows with the same key). Can take the following options:
    - `sum` - summarise aggregated values.
      For numeric values it's the arithmetic sum, for strings the concatenation of strings and for other types will error.

    - `avg` - calculate the average of aggregated values.

      For numeric values it's the arithmetic average and for other types will err.

    - `max` - calculate the maximum of aggregated values.

      For numeric values it's the arithmetic maximum, for strings the dictionary maximum and for other types will error.

    - `min` - calculate the minimum of aggregated values.

      For numeric values it's the arithmetic minimum, for strings the dictionary minimum and for other types will error.

    - `first` - take the first value encountered

    - `last` - take the last value encountered

    - `count` - count the number of occurrences of a specific key
      For this method, specifying `name` is not required. In case it is specified, `count` will count the number of non-null values for that source field.

    - `counters` - count the number of occurrences of distinct values
      Will return an array of 2-tuples of the form `[value, count-of-value]`.

    - `set` - collect all distinct values of the aggregated field, unordered

    - `array` - collect all values of the aggregated field, in order of appearance

    - `any` - pick any value.

    By default, `aggregate` takes the `any` value.

  If neither `name` or `aggregate` need to be specified, the mapping can map to the empty object `{}` or to `null`.
- `full`  - Boolean,
  - If `True` (the default), failed lookups in the source will result in ""null"" values at the source.
  - if `False`, failed lookups in the source will result in dropping the row from the target.

_Important: the ""source"" resource **must** appear before the ""target"" resource in the data-package._

*Examples*:

```yaml
- run: join
  parameters:
    source:
      name: world_population
      key: [""country_code""]
      delete: yes
    target:
      name: country_gdp_2015
      key: [""CC""]
    fields:
      population:
        name: ""census_2015""
    full: true
```

The above example aims to create a package containing the GDP and Population of each country in the world.

We have one resource (`world_population`) with data that looks like:

| country_code | country_name   | census_2000 | census_2015 |
| ------------ | -------------- | ----------- | ----------- |
| UK           | United Kingdom | 58857004    | 64715810    |
| ...          |                |             |             |

And another resource (`country_gdp_2015`) with data that looks like:

| CC   | GDP (£m) | Net Debt (£m) |
| ---- | -------- | ------------- |
| UK   | 1832318  | 1606600       |
| ...  |          |               |

The `join` command will match rows in both datasets based on the `country_code` / `CC` fields, and then copying the value in the `census_2015` field into a new `population` field.

The resulting data package will have the `world_population` resource removed and the `country_gdp_2015` resource looking like:

| CC   | GDP (£m) | Net Debt (£m) | population |
| ---- | -------- | ------------- | ---------- |
| UK   | 1832318  | 1606600       | 64715810   |
| ...  |          |               |            |



A more complex example:

```yaml
- run: join
  parameters:
    source:
      name: screen_actor_salaries
      key: ""{production} ({year})""
    target:
      name: mgm_movies
      key: ""{title}""
    fields:
      num_actors:
        aggregate: 'count'
      average_salary:
        name: salary
        aggregate: 'avg'
      total_salaries:
        name: salary
        aggregate: 'sum'
    full: false
```

This example aims to analyse salaries for screen actors in the MGM studios.

Once more, we have one resource (`screen_actor_salaries`) with data that looks like:

| year | production                  | actor             | salary   |
| ---- | --------------------------- | ----------------- | -------- |
| 2016 | Vertigo 2                   | Mr. T             | 15000000 |
| 2016 | Vertigo 2                   | Robert Downey Jr. | 7000000  |
| 2015 | The Fall - Resurrection     | Jeniffer Lawrence | 18000000 |
| 2015 | Alf - The Return to Melmack | The Rock          | 12000000 |
| ...  |                             |                   |          |

And another resource (`mgm_movies`) with data that looks like:

| title                     | director      | producer     |
| ------------------------- | ------------- | ------------ |
| Vertigo 2 (2016)          | Lindsay Lohan | Lee Ka Shing |
| iRobot - The Movie (2018) | Mr. T         | Mr. T        |
| ...                       |               |              |

The `join` command will match rows in both datasets based on the movie name and production year. Notice how we overcome incompatible fields by using different key patterns.

The resulting dataset could look like:

| title            | director      | producer     | num_actors | average_salary | total_salaries |
| ---------------- | ------------- | ------------ | ---------- | -------------- | -------------- |
| Vertigo 2 (2016) | Lindsay Lohan | Lee Ka Shing | 2          | 11000000       | 22000000       |
| ...              |               |              |            |                |                |


### ***`filter`***

Filter streamed resources.

`filter` accepts equality and inequality conditions and tests each row in the selected resources. If none of the conditions validate, the row will be discarded.

_Parameters_:

- `resources` - Which resources to apply the filter on. Same semantics as `resources` in `stream_remote_resources`.
- `in` - Mapping of keys to values which translate to `row[key] == value` conditions
- `out` - Mapping of keys to values which translate to `row[key] != value` conditions

Both `in` and `out` should be a list of objects. However, `out` should only ever have one element.

*Examples*:

Filtering just American and European countries, leaving out countries whose main language is English:
```yaml
- run: filter
  parameters:
    resources: world_population
    in:
      - continent: america
      - continent: europe
- run: filter
  parameters:
    resources: world_population
    out:
      - language: english
```
To filter `out` by multiple values, you need multiple filter processors, not multiple `out` elements. Otherwise some condition will always validate and no rows will be discareded:

```
- run: filter
  parameters:
    resources: world_population
    out:
      - language: english
- run: filter
  parameters:
    resources: world_population
    out:
      - language: swedish
```

### ***`sort`***

Sort streamed resources by key.

`sort` accepts a list of resources and a key (as a Python format string on row fields).
It will output the rows for each resource, sorted according to the key (in ascending order by default).

_Parameters_:

- `resources` - Which resources to sort. Same semantics as `resources` in `stream_remote_resources`.
- `sort-by` - String, which would be interpreted as a Python format string used to form the key (e.g. `{<field_name_1>}:{field_name_2}`)
- `reverse` - Optional boolean, if set to true - sorts in reverse order

*Examples*:

Filtering just American and European countries, leaving out countries whose main language is English:
```yaml
- run: sort
  parameters:
    resources: world_population
    sort-by: ""{country_name}""
```

### ***`deduplicate`***

Deduplicates rows in resources based on the resources' primary key

`deduplicate` accepts a resource specifier - for each resource, it will output only unique rows (based on the values in the primary key fields). Rows with duplicate primary keys will be ignored.

_Parameters_:

- `resources` - Which resources to sort. Same semantics as `resources` in `stream_remote_resources`.

*Examples*:

Deduplicating rows in the `world-population` resource.

```yaml
- run: deduplicate
  parameters:
    resources: world_population
```


### ***`duplicate`***

Duplicate a resource.

`duplicate` accepts the name of a single resource in the datapackage.
It will then duplicate it in the output datapackage, with a different name and path.
The duplicated resource will appear immediately after its original.

_Parameters_:

- `source` - Which resources to duplicate. The name of the resource.
- `target-name` - Name of the new, duplicated resource.
- `target-path` - Path for the new, duplicated resource.
- `duplicate_to_end` - Make the duplicate resource appear at the end, rather than immediately after its original.

*Examples*:

Filtering just American and European countries, leaving out countries whose main language is English:
```yaml
- run: duplicate
  parameters:
    source: original-resource
    target-name: copy-of-resource
    target-path: data/duplicate.csv
```


### ***`delete_fields`***

Delete fields (columns) from streamed resources

`delete_fields` accepts a list of resources and list of fields to remove

_Note: if multiple resources provided, all of them should contain all fields to delete_

_Parameters_:

- `resources` - Which resources to delete columns from. Same semantics as `resources` in `stream_remote_resources`.
- `fields` - List of field (column) names to be removed (exact names or regular expressions for matching field names)
- `regex` - if set to `False` field names will be interpreted as strings not as regular expressions (`True` by default)

*Examples*:

Deleting `country_name` and `census_2000` columns from `world_population` resource:
```yaml
- run: delete_fields
  parameters:
    resources: world_population
    fields:
      - country_name
      - census_2000
```

### ***`add_computed_field`***

Add field(s) to streamed resources

`add_computed_field` accepts a list of resources and fields to add to existing resource. It will output the rows for each resource with new field(s) (columns) in it. `add_computed_field` allows to perform various operations before inserting value into targeted field.

_Parameters_:

- `resources` - Resources to add field. Same semantics as `resources` in `stream_remote_resources`.
- `fields` - List of operations to be performed on the targeted fields.
  - `operation`: operation to perform on values of pre-defined columns of the same row. available operation:
    - `constant` - add a constant value
    - `sum` - summed value for given columns in a row.
    - `avg` - average value from given columns in a row.
    - `min` - minimum value among given columns in a row.
    - `max` - maximum value among given columns in a row.
    - `multiply` - product of given columns in a row.
    - `join` - joins two or more column values in a row.
    - `format` - Python format string used to form the value Eg:  `my name is {first_name}`.
  - `target` - name of the new field.
  - `source` - list of columns the operations should be performed on (Not required in case of `format` and `constant`).
  - `with` - String passed to `constant`, `format` or `join` operations
    - in `constant` - used as constant value
    - in `format` - used as Python format string with existing column values Eg: `{first_name} {last_name}`
    - in `join` - used as delimiter

*Examples*:

Following example adds 4 new field to `salaries` resource

```yaml
run: add_computed_field
parameters:
  resources: salaries
  fields:
    -
      operation: sum
      target: total
      source:
        - jan
        - feb
        - may
    -
      operation: avg
      target: average
      source:
        - jan
        - feb
        - may
    -
      operation: format
      target: full_name
      with: '{first_name} {last_name}'
    -
      operation: constant
      target: status
      with: single
```

We have one resource (`salaries`) with data that looks like:

| first_name | last_name | jan | feb | mar |
| ---------- | --------- | --- | --- | --- |
| John       | Doe       | 100 | 200 | 300 |
| ...        |           |     |     |     |

The resulting dataset could look like:

| first_name | last_name | last_name | jan | feb | mar | average | total | status |
| ---------- | --------- | --------- | --- | --- | --- | ------- | ----- | ------ |
| John       | Doe       | John Doe  | 100 | 200 | 300 | 200     | 600   | single |
| ...        |           |           |     |     |     |         |       |        |

### ***`find_replace`***

find and replace string or pattern from field(s) values

_Parameters_:

- `resources` - Resources to clean the field values. Same semantics as `resources` in `stream_remote_resources`

- `fields`- list of fields to replace values
  - `name` - name of the field to replace value
  - `patterns` - list of patterns to find and replace from field
    - `find` - String, interpreted as a regular expression to match field value
    - `replace` - String, interpreted as a regular expression to replace matched pattern

*Examples*:

Following example replaces field values using regular expression and exact string patterns

```yaml
run: find_replace
parameters:
  resources: dates
  fields:
    -
      name: year
      patterns:
        -
          find: ([0-9]{4})( \(\w+\))
          replace: \1
    -
      name: quarter
      patterns:
        -
          find: Q1
          replace: '03-31'
        -
          find: Q2
          replace: '06-31'
        -
          find: Q3
          replace: '09-30'
        -
          find: Q4
          replace: '12-31'
```

We have one resource (`dates`) with data that looks like:

|   year   |  quarter  |
| -------- | --------- |
| 2000 (1) | 2000-Q1   |
| ...      |           |

The resulting dataset could look like:

| year |  quarter   |
| ---- | ---------- |
| 2000 | 2000-03-31 |
| ...  |            |

### ***`unpivot`***

Unpivots, transposes tabular data so that there's only one record per row.

_Parameters_:

- `resources` - Resources to unpivot. Same semantics as `resources` in `stream_remote_resources`.
- `extraKeyFields` - List of target field definitions, each definition is an object containing at least these properties (unpivoted column values will go here)
  - `name` - Name of the target field
  - `type` - Type of the target field
- `extraValueField` - Target field definition - an object containing at least these properties (unpivoted cell values will go here)
  - `name` - Name of the target field
  - `type` - Type of the target field
- `unpivot` - List of source field definitions, each definition is an object containing at least these properties
  - `name` - Either simply the name, or a regular expression matching the name of original field to unpivot.
  - `keys` - A Map between target field name and values for original field
    - Keys should be target field names from `extraKeyFields`
    - Values may be either simply the constant value to insert, or a regular expression matching the `name`.

_Examples_:

Following example will unpivot data into 3 new fields: `year`, `direction` and `amount`

```yaml
parameters:
  resources: balance
  extraKeyFields:
    -
      name: year
      type: integer
    -
      name: direction
      type: string
      constraints:
        enum:
          - In
          - Out
  extraValueField:
      name: amount
      type: number
  unpivot:
    -
      name: 2015 incomes
      keys:
        year: 2015
        direction: In
    -
      name: 2015 expenses
      keys:
        year: 2015
        direction: Out
    -
      name: 2016 incomes
      keys:
        year: 2016
        direction: In
    -
      name: 2016 expenses
      keys:
        year: 2016
        direction: Out
```

We have one resource (`balance`) with data that looks like:

| company | 2015 incomes | 2015 expenses | 2016 incomes | 2016 expenses |
| --------| ------------ | ------------- | ------------ | ------------- |
| Inc     | 1000         | 900           | 2000         | 1700          |
| Org     | 2000         | 800           | 3000         | 2000          |
| ...     |              |               |              |               |

The resulting dataset could look like:

| company | year | direction | amount |
| --------| ---- | --------- | ------ |
| Inc     | 2015 | In        | 1000   |
| Inc     | 2015 | Out       | 900    |
| Inc     | 2016 | In        | 2000   |
| Inc     | 2016 | Out       | 1700   |
| Org     | 2015 | In        | 2000   |
| Org     | 2015 | Out       | 800    |
| Org     | 2016 | In        | 3000   |
| Org     | 2016 | Out       | 2000   |
| ...     |      |           |        |

Similar result can be accomplished by defining regular expressions instead of constant values

```yaml
parameters:
  resources: balance
  extraKeyFields:
    -
      name: year
      type: integer
    -
      name: direction
      type: string
      constraints:
        enum:
          - In
          - Out
  extraValueField:
      name: amount
      type: number
  unpivot:
    -
      name: ([0-9]{4}) (\\w+)  # regex for original column
      keys:
        year: \\1  # First member of group from above
        direction: \\2  # Second member of group from above
```

### ***`dump_to_sql`***

Saves the datapackage to an SQL database.

_Parameters_:

- `engine` - Connection string for connecting to the SQL Database (URL syntax)
  Also supports `env://<environment-variable>`, which indicates that the connection string should be fetched from the indicated environment variable.
  If not specified, assumes a default of `env://DPP_DB_ENGINE`
- `tables` - Mapping between resources and DB tables. Keys are table names, values are objects with the following attributes:
  - `resource-name` - name of the resource that should be dumped to the table
  - `mode` - How data should be written to the DB.
    Possible values:
      - `rewrite` (the default) - rewrite the table, all previous data (if any) will be deleted.
      - `append` - write new rows without changing already existing data.
      - `update` - update the table based on a set of ""update keys"".
        For each new row, see if there already an existing row in the DB which can be updated (that is, an existing row
        with the same values in all of the update keys).
        If so - update the rest of the columns in the existing row. Otherwise - insert a new row to the DB.
  - `update_keys` - Only applicable for the `update` mode. A list of field names that should be used to check for row existence.
        If left unspecified, will use the schema's `primaryKey` as default.
  - `indexes` - TBD
- `updated_column` - Optional name of a column that will be added to the spewed data with boolean value
  - `true` - row was updated
  - `false` - row was inserted
- `updated_id_column` - Optional name of a column that will be added to the spewed data and contain the id of the updated row in DB.

### ***`dump_to_path`***
Saves the datapackage to a filesystem path.

_Parameters_:

- `out-path` - Name of the output path where `datapackage.json` will be stored.

  This path will be created if it doesn't exist, as well as internal data-package paths.

  If omitted, then `.` (the current directory) will be assumed.

- `force-format` - Specifies whether to force all output files to be generated with the same format
    - if `True` (the default), all resources will use the same format
    - if `False`, format will be deduced from the file extension. Resources with unknown extensions will be discarded.
- `format` - Specifies the type of output files to be generated (if `force-format` is true): `csv` (the default) or `json`
- `add-filehash-to-path`: Specifies whether to include file md5 hash into the resource path. Defaults to `False`. If `True` Embeds hash in path like so:
    - If original path is `path/to/the/file.ext`
    - Modified path will be `path/to/the/HASH/file.ext`
- `counters` - Specifies whether to count rows, bytes or md5 hash of the data and where it should be stored. An object with the following properties:
    - `datapackage-rowcount`: Where should a total row count of the datapackage be stored (default: `count_of_rows`)
    - `datapackage-bytes`: Where should a total byte count of the datapackage be stored (default: `bytes`)
    - `datapackage-hash`: Where should an md5 hash of the datapackage be stored (default: `hash`)
    - `resource-rowcount`: Where should a total row count of each resource be stored (default: `count_of_rows`)
    - `resource-bytes`: Where should a total byte count of each resource be stored (default: `bytes`)
    - `resource-hash`: Where should an md5 hash of each resource be stored (default: `hash`)
    Each of these attributes could be set to null in order to prevent the counting.
    Each property could be a dot-separated string, for storing the data inside a nested object (e.g. `stats.rowcount`)
- `pretty-descriptor`: Specifies how datapackage descriptor (`datapackage.json`) file will look like:
    - `False` (default) - descriptor will be written in one line.
    - `True` - descriptor will have indents and new lines for each key, so it becomes more human-readable.

### ***`dump_to_zip`***

Saves the datapackage to a zipped archive.

_Parameters_:

- `out-file` - Name of the output file where the zipped data will be stored
- `force-format` and `format` - Same as in `dump_to_path`
- `add-filehash-to-path` - Same as in `dump_to_path`
- `counters` - Same as in `dump_to_path`
- `pretty-descriptor` - Same as in `dump_to_path`

## Deprecated Processors

These processors will be removed in the next major version.

### ***`add_metadata`***

Alias for `update_package`, is kept for backward compatibility reasons.

### ***`add_resource`***

Adds a new external tabular resource to the data-package.

_Parameters_:

You should provide a `name` and `url` attributes, and other optional attributes as defined in the [spec]([http://specs.frictionlessdata.io/data-packages/#resource-information).

`url` indicates where the data for this resource resides. Later on, when `stream_remote_resources` runs, it will use the `url` (which is stored in the resource in the `dpp:streamedFrom` property) to read the data rows and push them into the pipeline.

Note that `url` also supports `env://<environment-variable>`, which indicates that the resource url should be fetched from the indicated environment variable.  This is useful in case you are supplying a string with sensitive information (such as an SQL connection string for streaming from a database table).

Parameters are basically arguments that are passed to a `tabulator.Stream` instance (see the [API](https://github.com/frictionlessdata/tabulator-py#api-reference)).
Other than those, you can pass a `constants` parameter which should be a mapping of headers to string values.
When used in conjunction with `stream_remote_resources`, these constant values will be added to each generated row
(as well as to the default schema).

You may also provide a schema here, or use the default schema generated by the `stream_remote_resources` processor.
In case `path` is specified, it will be used. If not, the `stream_remote_resources` processor will assign a `path` for you with a `csv` extension.

*Example*:

```yaml
- run: add_resource
  parameters:
    url: http://example.com/my-excel-file.xlsx
    sheet: 1
    headers: 2
- run: add_resource
  parameters:
    url: http://example.com/my-csv-file.csv
    encoding: ""iso-8859-2""
```

### ***`stream_remote_resources`***

Converts external resources to streamed resources.

External resources are ones that link to a remote data source (url or file path), but are not processed by the pipeline and are kept as-is.

Streamed resources are ones that can be processed by the pipeline, and their output is saved as part of the resulting datapackage.

In case a resource has no schema, a default one is generated automatically here by creating a `string` field from each column in the data source.

_Parameters_:

- `resources` - Which resources to stream. Can be:

  - List of strings, interpreted as resource names to stream
  - String, interpreted as a regular expression to be used to match resource names

  If omitted, all resources in datapackage are streamed.

- `ignore-missing` - if true, then missing resources won't raise an error but will be treated as 'empty' (i.e. with zero rows).
  Resources with empty URLs will be treated the same (i.e. will generate an 'empty' resource).

- `limit-rows` - if provided, will limit the number of rows fetched from the source. Takes an integer value which specifies how many rows of the source to stream.

*Example*:

```yaml
- run: stream_remote_resources
  parameters:
    resources: ['2014-data', '2015-data']
- run: stream_remote_resources
  parameters:
    resources: '201[67]-data'
```

This processor also supports loading plain-text resources (e.g. html pages) and handling them as tabular data - split into rows with a single ""data"" column.
To enable this behavior, add the following attribute to the resource: `""format"": ""txt""`.

### ***`dump.to_sql`***

Alias for `dump_to_sql`, is kept for backward compatibility reasons.

### ***`dump.to_path`***

Saves the datapackage to a filesystem path.

_Parameters_:

- `out-path` - Name of the output path where `datapackage.json` will be stored.

  This path will be created if it doesn't exist, as well as internal data-package paths.

  If omitted, then `.` (the current directory) will be assumed.

- `force-format` - Specifies whether to force all output files to be generated with the same format
    - if `True` (the default), all resources will use the same format
    - if `False`, format will be deduced from the file extension. Resources with unknown extensions will be discarded.
- `format` - Specifies the type of output files to be generated (if `force-format` is true): `csv` (the default) or `json`
- `handle-non-tabular` - Specifies whether non tabular resources (i.e. resources without a `schema`) should be dumped as well to the resulting datapackage.
    (See note below for more details)
- `add-filehash-to-path`: Specifies whether to include file md5 hash into the resource path. Defaults to `False`. If `True` Embeds hash in path like so:
    - If original path is `path/to/the/file.ext`
    - Modified path will be `path/to/the/HASH/file.ext`
- `counters` - Specifies whether to count rows, bytes or md5 hash of the data and where it should be stored. An object with the following properties:
    - `datapackage-rowcount`: Where should a total row count of the datapackage be stored (default: `count_of_rows`)
    - `datapackage-bytes`: Where should a total byte count of the datapackage be stored (default: `bytes`)
    - `datapackage-hash`: Where should an md5 hash of the datapackage be stored (default: `hash`)
    - `resource-rowcount`: Where should a total row count of each resource be stored (default: `count_of_rows`)
    - `resource-bytes`: Where should a total byte count of each resource be stored (default: `bytes`)
    - `resource-hash`: Where should an md5 hash of each resource be stored (default: `hash`)
    Each of these attributes could be set to null in order to prevent the counting.
    Each property could be a dot-separated string, for storing the data inside a nested object (e.g. `stats.rowcount`)
- `pretty-descriptor`: Specifies how datapackage descriptor (`datapackage.json`) file will look like:
    - `False` (default) - descriptor will be written in one line.
    - `True` - descriptor will have indents and new lines for each key, so it becomes more human-readable.
- `file-formatters`: Specifies custom file format handlers. An object with mapping of format name to Python module and class name.
    - Allows to override the existing `csv` and `json` format handlers or add support for new formats.
    - Note that such changes may make the resulting datapackage incompatible with the frictionlessdata specs and may cause interoperability problems.
    - Example usage: [pipeline-spec.yaml](tests/cli/pipeline-spec.yaml) (under the `custom-formatters` pipeline), [XLSXFormat class](tests/cli/custom_formatters/xlsx_format.py)

### ***`dump.to_zip`***

Saves the datapackage to a zipped archive.

_Parameters_:

- `out-file` - Name of the output file where the zipped data will be stored
- `force-format` and `format` - Same as in `dump_to_path`
- `handle-non-tabular` - Same as in `dump_to_path`
- `add-filehash-to-path` - Same as in `dump_to_path`
- `counters` - Same as in `dump_to_path`
- `pretty-descriptor` - Same as in `dump_to_path`
- `file-formatters` - Same as in `dump_to_path`

#### *Note*

`dump.to_path` and `dump.to_zip` processors will handle non-tabular resources as well.
These resources must have both a `url` and `path` properties, and _must not_ contain a `schema` property.
In such cases, the file will be downloaded from the `url` and placed in the provided `path`.

## Custom Processors

It's quite reasonable that for any non-trivial processing task, you might encounter a problem that cannot be solved using the standard library processors.

For that you might need to write your own processor - here's how it's done.

There are two APIs for writing processors - the high level API and the low level API.

### High Level Processor API

The high-level API is quite useful for most processor kinds:

```python
from datapackage_pipelines.wrapper import process

def modify_datapackage(datapackage, parameters, stats):
    # Do something with datapackage
    return datapackage

def process_row(row, row_index,
                resource_descriptor, resource_index,
                parameters, stats):
    # Do something with row
    return row

process(modify_datapackage=modify_datapackage,
        process_row=process_row)
```

The high level API consists of one method, `process` which takes two functions:

- `modify_datapackage` - which makes changes (if necessary) to the data-package descriptor, e.g. adds metadata, adds resources, modifies resources' schema etc.

  Can also be used for initialization code when needed.

  It has these arguments:

  - `datapackage` is the current data-package descriptor that needs to be modified.
    The modified data-package descriptor needs to be returned.
  - `parameters` is a dict containing the processor's parameters, as provided in the `pipeline-spec.yaml` file.
  - `stats` is a dict which should be modified in order to collect metrics and measurements in the process (e.g. validation checks, row count etc.)

- `process_row` - which modifies a single row in the stream. It receives these arguments:
  - `row` is a dictionary containing the row to process
  - `row_index` is the index of the row in the resource
  - `resource_descriptor` is the descriptor object of the current resource being processed
  - `resource_index` is the index of the resource in the data-package
  - `parameters` is a dict containing the processor's parameters, as provided in the `pipeline-spec.yaml` file.
  - `stats` is a dict which should be modified in order to collect metrics and measurements in the process (e.g. validation checks, row count etc.)

  and yields zero or more processed rows.

#### A few examples

```python
# Add license information
from datapackage_pipelines.wrapper import process

def modify_datapackage(datapackage, parameters, stats):
    datapackage['license'] = 'CC-BY-SA'
    return datapackage

process(modify_datapackage=modify_datapackage)
```

```python
# Add new column with constant value to first resource
# Column name and value are taken from the processor's parameters
from datapackage_pipelines.wrapper import process

def modify_datapackage(datapackage, parameters, stats):
    datapackage['resources'][0]['schema']['fields'].append({
      'name': parameters['column-name'],
      'type': 'string'
    })
    return datapackage

def process_row(row, row_index, resource_descriptor, resource_index, parameters, stats):
    if resource_index == 0:
        row[parameters['column-name']] = parameters['value']
    return row

process(modify_datapackage=modify_datapackage,
        process_row=process_row)
```

```python
# Row counter
from datapackage_pipelines.wrapper import process

def modify_datapackage(datapackage, parameters, stats):
    stats['row-count'] = 0
    return datapackage

def process_row(row, row_index, resource_descriptor, resource_index, parameters, stats):
    stats['row-count'] += 1
    return row

process(modify_datapackage=modify_datapackage,
        process_row=process_row)
```

### Low Level Processor API

In some cases, the high-level API might be too restricting. In these cases you should consider using the low-level API.

```python
from datapackage_pipelines.wrapper import ingest, spew

if __name__ == '__main__':
  with ingest() as ctx:

    # Initialisation code, if needed

    # Do stuff with datapackage
    # ...

    stats = {}

    # and resources:
    def new_resource_iterator(resource_iterator_):
        def resource_processor(resource_):
            # resource_.spec is the resource descriptor
            for row in resource_:
                # Do something with row
                # Perhaps collect some stats here as well
                yield row
        for resource in resource_iterator_:
            yield resource_processor(resource)

    spew(ctx.datapackage,
         new_resource_iterator(ctx.resource_iterator),
         ctx.stats)
```

The above code snippet shows the structure of most low-level processors.

We always start with calling `ingest()` - this method gives us the context, containing the execution parameters, the data-package descriptor (as outputed from the previous step) and an iterator on all streamed resources' rows.

We finish the processing by calling `spew()`, which sends the processed data to the next processor in the pipeline. `spew` receives:
* A modified data-package descriptor;
* A (possibly new) iterator on the resources;
* A stats object which will be added to stats from previous steps and returned to the user upon completion of the pipeline, and;
* Optionally, a `finalizer` function that will be called after it has finished iterating on the resources, but before signalling to other processors that it's finished. You could use it to close any open files, for example.

#### A more in-depth explanation

`spew` writes the data it receives in the following order:

- First, the `datapackage` parameter is written to the stream.
  This means that all modifications to the data-package descriptor must be done _before_ `spew` is called.
  One common pitfall is to modify the data-package descriptor inside the resource iterator - try to avoid that, as the descriptor that the next processor will receive will be wrong.
- Then it starts iterating on the resources. For each resource, it iterates on its rows and writes each row to the stream.
  This iteration process eventually causes an iteration on the original resource iterator (the one that's returned from `ingest`). In turn, this causes the process' input stream to be read. Because of the way buffering in operating systems work, ""slow"" processors will read their input slowly, causing the ones before them to sleep on IO while their more CPU intensive counterparts finish their processing. ""quick"" processors will not work aimlessly, but instead will either sleep while waiting for incoming data or while waiting for their output buffer to drain.
  What is achieved here is that all rows in the data are processed more or less at the same time, and that no processor works too ""far ahead"" on rows that might fail in subsequent processing steps.
- Then the stats are written to the stream. This means that stats can be modified during the iteration, and only the value after the iteration finishes will be used.
- Finally, the `finalizer` method is called (if we received one).

#### A few examples

We'll start with the same processors from above, now implemented with the low level API.

```python
# Add license information
from datapackage_pipelines.wrapper import ingest, spew

if __name__ == '__main__':
  with ingest() as ctx:
    ctx.datapackage['license'] = 'MIT'
    spew(ctx.datapackage, ctx.resource_iterator)
```

```python
# Add new column with constant value to first resource
# Column name and value are taken from the processor's parameters
from datapackage_pipelines.wrapper import ingest, spew

parameters, datapackage, resource_iterator = ingest()

datapackage['resources'][0]['schema']['fields'].append({
   'name': parameters['column-name'],
   'type': 'string'
})

def new_resource_iterator(resource_iterator_):
    def resource_processor(resource_):
        for row in resource_:
            row[parameters['column-name']] = parameters['value']
            yield row

    first_resource = next(resource_iterator_)
    yield(resource_processor(first_resource))

    for resource in resource_iterator_:
        yield resource

spew(datapackage, new_resource_iterator(resource_iterator))
```

```python
# Row counter
from datapackage_pipelines.wrapper import ingest, spew

_, datapackage, resource_iterator = ingest()

stats = {'row-count': 0}

def new_resource_iterator(resource_iterator_):
    def resource_processor(resource_):
        for row in resource_:
            stats['row-count'] += 1
            yield row

    for resource in resource_iterator_:
        yield resource_processor(resource)

spew(datapackage, new_resource_iterator(resource_iterator), stats)
```

This next example shows how to implement a simple web scraper. Although not strictly required, web scrapers are usually the first processor in a pipeline. Therefore, they can ignore the incoming data-package and resource iterator, as there's no previous processor generating data:

```python
# Web Scraper
import requests
from datapackage_pipelines.wrapper import ingest, spew
from datapackage_pipelines.utilities.resources import PROP_STREAMING

parameters, _, _ = ingest()

host = parameters['ckan-instance']
package_list_api = 'https://{host}/api/3/action/package_list'
package_show_api = 'https://{host}/api/3/action/package_show'

def scrape_ckan(host_):
    all_packages = requests.get(package_list_api.format(host=host_))\
                           .json()\
                           .get('result', [])
    for package_id in all_packages:
      params = dict(id=package_id)
      package_info = requests.get(package_show_api.format(host=host_),
                                  params=params)\
                             .json()\
                             .get('result')
      if result is not None:
        yield dict(
            package_id=package_id,
            author=package_info.get('author'),
            title=package_info.get('title'),
        )

datapackage = {
  'resources': [
    {
      PROP_STREAMING: True,   # You must set this property for resources being streamed in the pipeline!
      'name': 'package-list',
      'schema': {
        'fields': [
          {'name': 'package_id', 'type': 'string'},
          {'name': 'author',     'type': 'string'},
          {'name': 'title',      'type': 'string'},
        ]
      }
    }
  ]
}

spew(datapackage, [scrape_ckan(host)])
```

In this example we can see that the initial datapackage is generated from scratch, and the resource iterator is in fact a scraper, yielding rows as they are received from the CKAN instance API.

## Plugins and Source Descriptors

When writing pipelines in a specific problem domain, one might discover that the processing pipelines that are developed follow a certain pattern. Scraping, or fetching source data tends to be similar to one another. Processing, data cleaning, validation are often the same.

In order to ease maintenance and avoid boilerplate, a _`datapackage-pipelines` **plugin**_ can be written.

Plugins are Python modules named `datapackage_pipelines_<plugin-name>`. Plugins can provide two facilities:

- Processor packs - you can pack processors revolving a certain theme or for a specific purpose in a plugin. Any processor `foo` residing under the `datapackage_pipelines_<plugin-name>.processors` module can be used from within a pipeline as `<plugin-name>.foo`.
- Pipeline templates - if the class `Generator` exists in the `datapackage_pipelines_<plugin-name>` module, it will be used to generate pipeline based on templates - which we call ""source descriptors"".

### Source Descriptors

A source descriptor is a yaml file containing information which is used to create a full pipeline.

`dpp` will look for files named `<plugin-name>.source-spec.yaml` , and will treat them as input for the pipeline generating code - which should be implemented in a class called `Generator` in the `datapackage_pipelines_<plugin-name>` module.

This class should inherit from `GeneratorBase` and should implement two methods:

- `generate_pipeline` -
   which receives the source description and returns an iterator of tuples of the form `(id, details)`.
   `id` might be a pipeline id, in which case details would be an object containing the pipeline definition.
   If `id` is of the form `:module:`, then the details are treated as a source spec from the specified module. This way a generator might generate other source specs.
- `get_schema` - which should return a JSON Schema for validating the source description's structure

#### Example

Let's assume we write a `datapackage_pipelines_ckan` plugin, used to pull data out of [CKAN](https://ckan.org) instances.

Here's how such a hypothetical generator would look like:

```python
import os
import json

from datapackage_pipelines.generators import \
    GeneratorBase, slugify, steps, SCHEDULE_MONTHLY

SCHEMA_FILE = os.path.join(os.path.dirname(__file__), 'schema.json')


class Generator(GeneratorBase):

    @classmethod
    def get_schema(cls):
        return json.load(open(SCHEMA_FILE))

    @classmethod
    def generate_pipeline(cls, source):
        pipeline_id = dataset_name = slugify(source['name'])
        host = source['ckan-instance']
        action = source['data-kind']

        if action == 'package-list':
            schedule = SCHEDULE_MONTHLY
            pipeline_steps = steps(*[
                ('ckan.scraper', {
                   'ckan-instance': host
                }),
                ('metadata', {
                  'name': dataset_name
                }),
                ('dump_to_zip', {
                   'out-file': 'ckan-datapackage.zip'
                })])
            pipeline_details = {
                'pipeline': pipeline_steps,
                'schedule': {'crontab': schedule}
            }
            yield pipeline_id, pipeline_details
```

In this case, if we store a `ckan.source-spec.yaml` file looking like this:

```yaml
ckan-instance: example.com
name: example-com-list-of-packages
data-kind: package-list
```

Then when running `dpp` we will see an available pipeline named `./example-com-list-of-packages`

This pipeline would internally be composed of 3 steps: `ckan.scraper`, `metadata` and `dump_to_zip`.

#### Validating Source Descriptors

Source descriptors can have any structure that best matches the parameter domain of the output pipelines. However, it must have a consistent structure, backed by a JSON Schema file. In our case, the Schema might look like this:

```json
{
  ""$schema"": ""http://json-schema.org/draft-04/schema#"",
  ""type"": ""object"",
  ""properties"": {
    ""name"":          { ""type"": ""string"" },
    ""ckan-instance"": { ""type"": ""string"" },
    ""data-kind"":     { ""type"": ""string"" }
  },
  ""required"": [ ""name"", ""ckan-instance"", ""data-kind"" ]
}
```

`dpp` will ensure that source descriptor files conform to that schema before attempting to convert them into pipelines using the `Generator` class.

#### Providing Processor Code

In some cases, a generator would prefer to provide the processor code as well (alongside the pipeline definition).
In order to to that, the generator can add a `code` attribute to any step containing the processor's code. When executed, this step won't try to resolve the processor as usual but will the provided code instead.

## Running on a schedule

`datapackage-pipelines` comes with a celery integration, allowing for pipelines to be run at specific times via a `crontab` like syntax.

In order to enable that, you simply add a `schedule` section to your `pipeline-spec.yaml` file (or return a schedule from the generator class, see above), like so:

```yaml
co2-information-cdiac:
  pipeline:
    -
        ...
  schedule:
    # minute hour day_of_week day_of_month month_of_year
    crontab: '0 * * * *'
```

In this example, this pipeline is set to run every hour, on the hour.

To run the celery daemon, use `celery`'s command line interface to run `datapackage_pipelines.app`. Here's one way to do it:

```shell
$ python -m celery worker -B -A datapackage_pipelines.app
```

Running this server will start by executing all ""dirty"" tasks, and continue by executing tasks based on their schedules.

As a shortcut for starting the scheduler and the dashboard (see below), you can use a prebuilt _Docker_ image:

```bash
$ docker run -v `pwd`:/pipelines:rw -p 5000:5000 \
        frictionlessdata/datapackage-pipelines server
```

And then browse to `http://<docker machine's IP address>:5000/` to see the current execution status dashboard.

## Pipeline Dashboard & Status Badges

When installed on a server or running using the task scheduler, it's often very hard to know exactly what's running and what the status is of each pipeline.

To make things easier, you can spin up the web dashboard to provide an overview of each pipeline's status, its basic info and the result of it latest execution.

To start the web server run `dpp serve` from the command line and browse to http://localhost:5000

The environment variable `DPP_BASE_PATH` will determine whether dashboard will be served from root or from another base path (example value: `/pipelines/`).

The dashboard endpoints can be made to require authentication by adding a username and password with the environment variables `DPP_BASIC_AUTH_USERNAME` and `DPP_BASIC_AUTH_PASSWORD`.

Even simpler pipeline status is available with a status badge, both for individual pipelines, and for pipeline collections. For a single pipeline, add the full pipeline id to the badge endpoint:

```
http://localhost:5000/badge/path_to/pipelines/my-pipeline-id
```

![](https://img.shields.io/badge/pipeline-succeeded%20(30756%20records)-brightgreen.svg)

![](https://img.shields.io/badge/pipeline-invalid-lightgrey.svg)

![](https://img.shields.io/badge/pipeline-failed-red.svg)

![](https://img.shields.io/badge/pipeline-not%20found-lightgray.svg)

Or for a collection of pipelines:

```
http://localhost:5000/badge/collection/path_to/pipelines/
```

![](https://img.shields.io/badge/pipelines-22%20succeeded-brightgreen.svg)

![](https://img.shields.io/badge/pipelines-4%20running%2C%20%201%20succeeded%2C%205%20queued-yellow.svg)

![](https://img.shields.io/badge/pipelines-11%20succeeded%2C%207%20failed%2C%201%20invalid-red.svg)

![](https://img.shields.io/badge/pipelines-not%20found-lightgray.svg)

Note that these badge endpoints will always be exposed regardless of `DPP_BASIC_AUTH_PASSWORD` and `DPP_BASIC_AUTH_USERNAME` settings.

## Integrating with other services

Datapackage-pipelines can call a predefined webhook on any pipeline event. This might allow for potential integrations with other applications.

In order to add a webhook in a specific pipeline, add a `hooks` property in the pipeline definition, which should be a list of URLs.
Whenever that pipeline is queued, starts running or finishes running, all the urls will be POSTed with this payload:
```json
{
  ""pipeline"": ""<pipeline-id>"",
  ""event"": ""queue/start/progress/finish"",
  ""success"": true/false (when applicable),
  ""errors"": [list-of-errors, when applicable]
}
```

## Known Issues

* loading a resource which has a lot of data in a single cell raises an exception ([#112](https://github.com/frictionlessdata/datapackage-pipelines/pull/112#issue-160766294))
",2023-07-07 15:57:42+00:00
galaxy,galaxy,galaxyproject/galaxy,Data intensive science for everyone.,https://galaxyproject.org,False,1129,2023-07-07 09:15:10+00:00,2015-02-23 14:18:06+00:00,890,69,258,4,v23.0.4,2023-06-30 21:13:57+00:00,Other,69393,v23.0.4,207,2023-06-30 21:13:57+00:00,2023-07-07 15:38:05+00:00,2023-07-07 02:40:43+00:00,".. figure:: https://galaxyproject.org/images/galaxy-logos/galaxy_project_logo.jpg
   :alt: Galaxy Logo

The latest information about Galaxy can be found on the `Galaxy Community Hub <https://galaxyproject.org/>`__.

Community support is available at `Galaxy Help <https://help.galaxyproject.org/>`__.

.. image:: https://img.shields.io/badge/chat-gitter-blue.svg
    :target: https://gitter.im/galaxyproject/Lobby
    :alt: Chat on gitter

.. image:: https://img.shields.io/badge/chat-irc.freenode.net%23galaxyproject-blue.svg
    :target: https://webchat.freenode.net/?channels=galaxyproject
    :alt: Chat on irc

.. image:: https://img.shields.io/badge/release-documentation-blue.svg
    :target: https://docs.galaxyproject.org/en/master/
    :alt: Release Documentation

.. image:: https://travis-ci.org/galaxyproject/galaxy.svg?branch=dev
    :target: https://travis-ci.org/galaxyproject/galaxy
    :alt: Inspect the test results

Galaxy Quickstart
=================

Galaxy requires Python 3.7 . To check your Python version, run:

.. code:: console

    $ python -V
    Python 3.7.6

Start Galaxy:

.. code:: console

    $ sh run.sh

Once Galaxy completes startup, you should be able to view Galaxy in your
browser at: http://localhost:8080

For more installation details please see: https://getgalaxy.org/

Documentation is available at: https://docs.galaxyproject.org/

Tutorials on how to use Galaxy, perform scientific analyses with it, develop Galaxy and its tools, and admin a Galaxy server are at: https://training.galaxyproject.org/

Tools
=====

Tools can be either installed from the Tool Shed or added manually.
For details please see the `tutorial <https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/>`__.
Note that not all dependencies for the tools provided in the
``tool_conf.xml.sample`` are included. To install them please visit
""Manage dependencies"" in the admin interface.

Issues and Galaxy Development
=============================

Please see `CONTRIBUTING.md <CONTRIBUTING.md>`_ .
",2023-07-07 15:57:46+00:00
gatk,gatk,broadinstitute/gatk,Official code repository for GATK versions 4 and up,https://software.broadinstitute.org/gatk,False,1461,2023-07-07 11:04:37+00:00,2014-12-02 20:50:36+00:00,545,167,104,49,4.4.0.0,2023-03-16 18:55:06+00:00,Other,4589,last_dataflow,59,2015-10-08 18:01:08+00:00,2023-07-07 15:46:34+00:00,2023-07-02 09:04:46+00:00,"[![Build Status](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml/badge.svg?branch=master)](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml)
[![Maven Central](https://img.shields.io/maven-central/v/org.broadinstitute/gatk.svg)](https://maven-badges.herokuapp.com/maven-central/org.broadinstitute/gatk)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

***Please see the [GATK website](http://www.broadinstitute.org/gatk), where you can download a precompiled executable, read documentation, ask questions, and receive technical support. For GitHub basics, see [here](https://software.broadinstitute.org/gatk/documentation/article?id=23405).***

### GATK 4

This repository contains the next generation of the Genome Analysis Toolkit (GATK). The contents
of this repository are 100% open source and released under the Apache 2.0 license (see [LICENSE.TXT](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)).

GATK4 aims to bring together well-established tools from the [GATK](http://www.broadinstitute.org/gatk) and
[Picard](http://broadinstitute.github.io/picard/) codebases under a streamlined framework,
and to enable selected tools to be run in a massively parallel way on local clusters or in the cloud using
[Apache Spark](http://spark.apache.org/). It also contains many newly developed tools not present in earlier
releases of the toolkit.

## Table of Contents
* [Requirements](#requirements)
* [Quick Start Guide](#quickstart)
* [Downloading GATK4](#downloading)
* [Building GATK4](#building)
* [Running GATK4](#running)
    * [Passing JVM options to gatk](#jvmoptions)
    * [Passing a configuration file to gatk](#configFileOptions)
    * [Running GATK4 with inputs on Google Cloud Storage](#gcs)
    * [Running GATK4 Spark tools locally](#sparklocal)
    * [Running GATK4 Spark tools on a Spark cluster](#sparkcluster)
    * [Running GATK4 Spark tools on Google Cloud Dataproc](#dataproc)
    * [Using R to generate plots](#R)
    * [GATK Tab Completion for Bash](#tab_completion)
* [For GATK Developers](#developers)
    * [General guidelines for GATK4 developers](#dev_guidelines)
    * [Testing GATK4](#testing)
    * [Using Git LFS to download and track large test data](#lfs)
    * [Creating a GATK project in the IntelliJ IDE](#intellij)
    * [Setting up debugging in IntelliJ](#debugging)
    * [Updating the Intellij project when dependencies change](#intellij_gradle_refresh)
    * [Setting up profiling using JProfiler](#jprofiler)
    * [Uploading Archives to Sonatype](#sonatype)
    * [Building GATK4 Docker images](#docker_building)
    * [Releasing GATK4](#releasing_gatk)
    * [Generating GATK4 documentation](#gatkdocs)
    * [Generating GATK4 WDL Wrappers](#gatkwdlgen)
    * [Using Zenhub to track github issues](#zenhub)
* [Further Reading on Spark](#spark_further_reading)
* [How to contribute to GATK](#contribute)
* [Discussions](#discussions)
* [Authors](#authors)
* [License](#license)

## <a name=""requirements"">Requirements</a>
* To run GATK:
    * Java 17 is needed to run or build GATK. 
    We recommend one of the following:
        * Download the Eclipse Foundation's distribution of OpenJDK 17 from [adoptium.net](https://adoptium.net/). Navigate to the [release archive](https://adoptium.net/temurin/archive/?version=17) to find downloads for Java 17.
        * On Mac OS, you can install the [Homebrew package manager](https://brew.sh/) and run `brew tap homebrew/cask-versions` followed by `brew install --cask temurin17` to install the Eclipse Foundation's OpenJDK 17. 
    * Python 2.6 or greater (required to run the `gatk` frontend script)
    * Python 3.6.2, along with a set of additional Python packages, is required to run some tools and workflows.
      See [Python Dependencies](#python) for more information.
    * R 3.2.5 (needed for producing plots in certain tools)
* To build GATK:
    * A Java 17 JDK
    * Git 2.5 or greater
    * [git-lfs](https://git-lfs.github.com/) 1.1.0 or greater. Required to download the large files used to build GATK, and
      test files required to run the test suite. Run `git lfs install` after downloading, followed by `git lfs pull` from
      the root of your git clone to download all of the large files, including those required to run the test suite. The
      full download is approximately 5 gigabytes. Alternatively, if you are just building GATK and not running the test
      suite, you can skip this step since the build itself will use git-lfs to download the minimal set of large `lfs`
      resource files required to complete the build. The test resources will not be downloaded, but this greatly reduces
      the size of the download.
    * Gradle 5.6. We recommend using the `./gradlew` script which will
      download and use an appropriate gradle version automatically (see examples below).
    * R 3.2.5 (needed for running the test suite)
* Pre-packaged Docker images with all needed dependencies installed can be found on
  [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). This requires a recent version of the
   docker client, which can be found on the [docker website](https://www.docker.com/get-docker).
* Python Dependencies:<a name=""python""></a>
    * GATK4 uses the [Conda](https://conda.io/docs/index.html) package manager to establish and manage the
      Python environment and dependencies required by GATK tools that have a Python dependency. This environment also 
      includes the R dependencies used for plotting in some of the tools. The ```gatk``` environment 
      requires hardware with AVX support for tools that depend on TensorFlow (e.g. CNNScoreVariant). The GATK Docker image 
      comes with the ```gatk``` environment pre-configured.
      	* At this time, the only supported platforms are 64-bit Linux distributions. The required Conda environment is not
	  currently supported on OS X/macOS. 
    * To establish the environment when not using the Docker image, a conda environment must first be ""created"", and
      then ""activated"":
        * First, make sure [Miniconda or Conda](https://conda.io/docs/index.html) is installed (Miniconda is sufficient).
        * To ""create"" the conda environment:
            * If running from a zip or tar distribution, run the command ```conda env create -f gatkcondaenv.yml``` to
              create the ```gatk``` environment.
            * If running from a cloned repository, run ```./gradlew localDevCondaEnv```. This generates the Python
              package archive and conda yml dependency file(s) in the build directory, and also creates (or updates)
              the local  ```gatk``` conda environment.
        * To ""activate"" the conda environment (the conda environment must be activated within the same shell from which
          GATK is run):
             * Execute the shell command ```source activate gatk``` to activate the ```gatk``` environment.
        * See the [Conda](https://conda.io/docs/user-guide/tasks/manage-environments.html) documentation for
          additional information about using and managing Conda environments.

## <a name=""quickstart"">Quick Start Guide</a>

* Build the GATK: `./gradlew bundle` (creates `gatk-VERSION.zip` in `build/`)
* Get help on running the GATK: `./gatk --help`
* Get a list of available tools: `./gatk --list`
* Run a tool: `./gatk PrintReads -I src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O output.bam`
* Get help on a particular tool: `./gatk PrintReads --help`

## <a name=""downloading"">Downloading GATK4</a>

You can download and run pre-built versions of GATK4 from the following places:

* A zip archive with everything you need to run GATK4 can be downloaded for each release from the [github releases page](https://github.com/broadinstitute/gatk/releases). We also host unstable archives generated nightly in the Google bucket gs://gatk-nightly-builds.

* You can download a GATK4 docker image from [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). We also host unstable nightly development builds on [this dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk-nightly/).
    * Within the docker image, run gatk commands as usual from the default startup directory (/gatk).

## <a name=""building"">Building GATK4</a>

* **To do a full build of GATK4, first clone the GATK repository using ""git clone"", then run:**

        ./gradlew bundle
        
  Equivalently, you can just type:
  
        ./gradlew
        
    * This creates a zip archive in the `build/` directory with a name like `gatk-VERSION.zip` containing a complete standalone GATK distribution, including our launcher `gatk`, both the local and spark jars, and this README.    
    * You can also run GATK commands directly from the root of your git clone after running this command.
    * Note that you *must* have a full git clone in order to build GATK, including the git-lfs files in src/main/resources. The zipped source code alone is not buildable.

* **Other ways to build:**
    * `./gradlew installDist`  
        * Does a *fast* build that only lets you run GATK tools from inside your git clone, and locally only (not on a cluster). Good for developers! 
    * `./gradlew installAll`
        * Does a *semi-fast* build that only lets you run GATK tools from inside your git clone, but works both locally and on a cluster. Good for developers!
    * `./gradlew localJar`
        * Builds *only* the GATK jar used for running tools locally (not on a Spark cluster). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-local.jar`, and can be used outside of your git clone.
    * `./gradlew sparkJar`
        * Builds *only* the GATK jar used for running tools on a Spark cluster (rather than locally). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-spark.jar`, and can be used outside of your git clone. 
        * This jar will not include Spark and Hadoop libraries, in order to allow the versions of Spark and Hadoop installed on your cluster to be used.

* **To remove previous builds, run:** 

        ./gradlew clean

* For faster gradle operations, add `org.gradle.daemon=true` to your `~/.gradle/gradle.properties` file.
  This will keep a gradle daemon running in the background and avoid the ~6s gradle start up time on every command.

* Gradle keeps a cache of dependencies used to build GATK.  By default this goes in `~/.gradle`.  If there is insufficient free space in your home directory, you can change the location of the cache by setting the `GRADLE_USER_HOME` environment variable.

* The version number is automatically derived from the git history using `git describe`, you can override it by setting the `versionOverride` property.
  ( `./gradlew -DversionOverride=my_weird_version printVersion` )

## <a name=""running"">Running GATK4</a>

* The standard way to run GATK4 tools is via the **`gatk`** wrapper script located in the root directory of a clone of this repository.
    * Requires Python 2.6 or greater (this includes Python 3.x)
    * You need to have built the GATK as described in the [Building GATK4](#building) section above before running this script.
    * There are several ways `gatk` can be run:
        * Directly from the root of your git clone after building
        * By extracting the zip archive produced by `./gradlew bundle` to a directory, and running `gatk` from there
        * Manually putting the `gatk` script within the same directory as fully-packaged GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar`
        * Defining the environment variables `GATK_LOCAL_JAR` and `GATK_SPARK_JAR`, and setting them to the paths to the GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar` 
    * `gatk` can run non-Spark tools as well as Spark tools, and can run Spark tools locally, on a Spark cluster, or on Google Cloud Dataproc.
    * ***Note:*** running with `java -jar` directly and bypassing `gatk` causes several important system properties to not get set, including htsjdk compression level!
    
* For help on using `gatk` itself, run **`./gatk --help`**

* To print a list of available tools, run **`./gatk --list`**.
    * Spark-based tools will have a name ending in `Spark` (eg., `BaseRecalibratorSpark`). Most other tools are non-Spark-based.

* To print help for a particular tool, run **`./gatk ToolName --help`**.

* To run a non-Spark tool, or to run a Spark tool locally, the syntax is: **`./gatk ToolName toolArguments`**.

* Tool arguments that allow multiple values, such as -I, can be supplied on the command line using a file with the extension "".args"". Each line of the file should contain a
  single value for the argument.

* Examples:

  ```
  ./gatk PrintReads -I input.bam -O output.bam
  ```

  ```
  ./gatk PrintReadsSpark -I input.bam -O output.bam
  ```

#### <a name=""jvmoptions"">Passing JVM options to gatk</a>

* To pass JVM arguments to GATK, run `gatk` with the `--java-options` argument: 

    ```
    ./gatk --java-options ""-Xmx4G"" <rest of command>
     
    ./gatk --java-options ""-Xmx4G -XX:+PrintGCDetails"" <rest of command>
    ```
#### <a name=""configFileOptions"">Passing a configuration file to gatk</a>

* To pass a configuration file to GATK, run `gatk` with the `--gatk-config-file` argument: 

	```
	./gatk --gatk-config-file GATKProperties.config <rest of command>
	```

	An example GATK configuration file is packaged with each release as `GATKConfig.EXAMPLE.properties`
	This example file contains all current options that are used by GATK and their default values.

#### <a name=""gcs"">Running GATK4 with inputs on Google Cloud Storage:</a>

* Many GATK4 tools can read BAM or VCF inputs from a Google Cloud Storage bucket. Just use the ""gs://"" prefix:
  ```
  ./gatk PrintReads -I gs://mybucket/path/to/my.bam -L 1:10000-20000 -O output.bam
  ```
* ***Important:*** You must set up your credentials first for this to work! There are three options:
    * Option (a): run in a Google Cloud Engine VM
        * If you are running in a Google VM then your credentials are already in the VM and will be picked up by GATK, you don't need to do anything special.
    * Option (b): use your own account
        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)
        * Log into your account:
        ```
        gcloud auth application-default login
        ```
        * Done! GATK will use the application-default credentials you set up there.
    * Option (c): use a service account
        * Create a new service account on the Google Cloud web page and download the JSON key file
        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)
        * Tell gcloud about the key file:
        ```
        gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""
        ```
        * Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the file
        ```
        export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE""
        ```
        * Done! GATK will pick up the service account. You can also do this in a VM if you'd like to override the default credentials.

#### <a name=""sparklocal"">Running GATK4 Spark tools locally:</a>

* GATK4 Spark tools can be run in local mode (without a cluster). In this mode, Spark will run the tool
  in multiple parallel execution threads using the cores in your CPU. You can control how many threads
  Spark will use via the `--spark-master` argument.
  
* Examples:

  Run `PrintReadsSpark` with 4 threads on your local machine:
  ``` 
    ./gatk PrintReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output.bam \
        -- \
        --spark-runner LOCAL --spark-master 'local[4]'
  ```
  Run `PrintReadsSpark` with as many worker threads as there are logical cores on your local machine:
  ``` 
    ./gatk PrintReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output.bam \
        -- \
        --spark-runner LOCAL --spark-master 'local[*]'
  ```   
  
* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.

#### <a name=""sparkcluster"">Running GATK4 Spark tools on a Spark cluster:</a>

**`./gatk ToolName toolArguments -- --spark-runner SPARK --spark-master <master_url> additionalSparkArguments`**
* Examples:

  ```
  ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \
      -- \
      --spark-runner SPARK --spark-master <master_url>
  ```

    ```
    ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \
      -- \
      --spark-runner SPARK --spark-master <master_url> \
      --num-executors 5 --executor-cores 2 --executor-memory 4g \
      --conf spark.executor.memoryOverhead=600
    ```

* You can also omit the ""--num-executors"" argument to enable [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) if you configure the cluster properly (see the Spark website for instructions).
* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.
* Running a Spark tool on a cluster requires Spark to have been installed from http://spark.apache.org/, since
   `gatk` invokes the `spark-submit` tool behind-the-scenes.
* Note that the examples above use YARN but we have successfully run GATK4 on Mesos as well.

#### <a name=""dataproc"">Running GATK4 Spark tools on Google Cloud Dataproc:</a>
  * You must have a [Google cloud services](https://cloud.google.com/) account, and have spun up a Dataproc cluster
    in the [Google Developer's console](https://console.developers.google.com). You may need to have the ""Allow API access to all Google Cloud services in the same project"" option enabled (settable when you create a cluster).
  * You need to have installed the Google Cloud SDK from [here](https://cloud.google.com/sdk/), since
    `gatk` invokes the `gcloud` tool behind-the-scenes. As part of the installation, be sure
      that you follow the `gcloud` setup instructions [here](https://cloud.google.com/sdk/gcloud/). As this library is frequently updated by Google, we recommend updating your copy regularly to avoid any version-related difficulties.
  * Your inputs to the GATK when running on dataproc are typically in Google Cloud Storage buckets, and should be specified on
    your GATK command line using the syntax `gs://my-gcs-bucket/path/to/my-file`
  * You can run GATK4 jobs on Dataproc from your local computer or from the VM (master node) on the cloud.

  Once you're set up, you can run a Spark tool on your Dataproc cluster using a command of the form:

  **`./gatk ToolName toolArguments -- --spark-runner GCS --cluster myGCSCluster additionalSparkArguments`**

  * Examples:

      ```      
      ./gatk PrintReadsSpark \
          -I gs://my-gcs-bucket/path/to/input.bam \
          -O gs://my-gcs-bucket/path/to/output.bam \
          -- \
          --spark-runner GCS --cluster myGCSCluster
      ```

      ```
      ./gatk PrintReadsSpark \
          -I gs://my-gcs-bucket/path/to/input.bam \
          -O gs://my-gcs-bucket/path/to/output.bam \
          -- \
          --spark-runner GCS --cluster myGCSCluster \
          --num-executors 5 --executor-cores 2 --executor-memory 4g \
          --conf spark.yarn.executor.memoryOverhead=600
      ```
  * When using Dataproc you can access the web interfaces for YARN, Hadoop and HDFS by opening an SSH tunnel and connecting with your browser.  This can be done easily using included `gcs-cluster-ui` script.
  
    ```
    scripts/dataproc-cluster-ui myGCSCluster
    ```
    Or see these [these instructions](https://cloud.google.com/dataproc/cluster-web-interfaces) for more details.
  * Note that the spark-specific arguments are separated from the tool-specific arguments by a `--`.
  * If you want to avoid uploading the GATK jar to GCS on every run, set the `GATK_GCS_STAGING`
    environment variable to a bucket you have write access to (eg., `export GATK_GCS_STAGING=gs://<my_bucket>/`)
  * Dataproc Spark clusters are configured with [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) so you can omit the ""--num-executors"" argument and let YARN handle it automatically.

#### <a name=""R"">Using R to generate plots</a>
Certain GATK tools may optionally generate plots using the R installation provided within the conda environment.  If you are uninterested in plotting, R is still required by several of the unit tests.  Plotting is currently untested and should be viewed as a convenience rather than a primary output.

#### <a name=""tab_completion"">Bash Command-line Tab Completion (BETA)</a>

* A tab completion bootstrap file for the bash shell is now included in releases.  This file allows the command-line shell to complete GATK run options in a manner equivalent to built-in command-line tools (e.g. grep).  

* This tab completion functionality has only been tested in the bash shell, and is released as a beta feature.

* To enable tab completion for the GATK, open a terminal window and source the included tab completion script:

```
source gatk-completion.sh
```

* Sourcing this file will allow you to press the tab key twice to get a list of options available to add to your current GATK command.  By default you will have to source this file once in each command-line session, then for the rest of the session the GATK tab completion functionality will be available.  GATK tab completion will be available in that current command-line session only.

* Note that you must have already started typing an invocation of the GATK (using gatk) for tab completion to initiate:

```
./gatk <TAB><TAB>
```

* We recommend adding a line to your bash settings file (i.e. your ~/.bashrc file) that sources the tab completion script.  To add this line to your bash settings / bashrc file you can use the following command:

```
echo ""source <PATH_TO>/gatk-completion.sh"" >> ~/.bashrc
```

* Where ```<PATH_TO>``` is the fully qualified path to the ```gatk-completion.sh``` script.

## <a name=""developers"">For GATK Developers</a>

#### <a name=""dev_guidelines"">General guidelines for GATK4 developers</a>

* **Do not put private or restricted data into the repo.**

* **Try to keep datafiles under 100kb in size.** Larger test files should go into `src/test/resources/large` (and subdirectories) so that they'll be stored and tracked by git-lfs as described [above](#lfs).

* GATK4 is Apache 2.0 licensed.  The license is in the top level LICENSE.TXT file.  Do not add any additional license text or accept files with a license included in them.

* Each tool should have at least one good end-to-end integration test with a check for expected output, plus high-quality unit tests for all non-trivial utility methods/classes used by the tool. Although we have no specific coverage target, coverage should be extensive enough that if tests pass, the tool is guaranteed to be in a usable state.

* All newly written code must have good test coverage (>90%).

* All bug fixes must be accompanied by a regression test.

* All pull requests must be reviewed before merging to master (even documentation changes).

* Don't issue or accept pull requests that introduce warnings. Warnings must be addressed or suppressed.

* Don't issue or accept pull requests that significantly decrease coverage (less than 1% decrease is sort of tolerable). 

* Don't use `toString()` for anything other than human consumption (ie. don't base the logic of your code on results of `toString()`.)

* Don't override `clone()` unless you really know what you're doing. If you do override it, document thoroughly. Otherwise, prefer other means of making copies of objects.

* For logging, use [org.apache.logging.log4j.Logger](https://logging.apache.org/log4j/2.0/log4j-api/apidocs/org/apache/logging/log4j/Logger.html)

* We mostly follow the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)

* Git: Don't push directly to master - make a pull request instead. 

* Git: Rebase and squash commits when merging.

* If you push to master or mess up the commit history, you owe us 1 growler or tasty snacks at happy hour. If you break the master build, you owe 3 growlers (or lots of tasty snacks). Beer may be replaced by wine (in the color and vintage of buyer's choosing) in proportions of 1 growler = 1 bottle. 

#### <a name=""testing"">Testing GATK</a>

* Before running the test suite, be sure that you've installed `git lfs` and downloaded the large test data, following the [git lfs setup instructions](#lfs)

* To run the test suite, run **`./gradlew test`**.
    * Test report is in `build/reports/tests/test/index.html`.
    * What will happen depends on the value of the `TEST_TYPE` environment variable: 
       * unset or any other value         : run non-cloud unit and integration tests, this is the default
       * `cloud`, `unit`, `integration`, `conda`, `spark`   : run only the cloud, unit, integration, conda (python + R), or Spark tests
       * `all`                            : run the entire test suite
    * Cloud tests require being logged into `gcloud` and authenticated with a project that has access
      to the cloud test data.  They also require setting several certain environment variables.
      * `HELLBENDER_JSON_SERVICE_ACCOUNT_KEY` : path to a local JSON file with [service account credentials](https://cloud.google.com/storage/docs/authentication#service_accounts) 
      * `HELLBENDER_TEST_PROJECT` : your google cloud project 
      * `HELLBENDER_TEST_STAGING` : a gs:// path to a writable location
      * `HELLBENDER_TEST_INPUTS` : path to cloud test data, ex: gs://hellbender/test/resources/ 
    * Setting the environment variable `TEST_VERBOSITY=minimal` will produce much less output from the test suite 

* To run a subset of tests, use gradle's test filtering (see [gradle doc](https://docs.gradle.org/current/userguide/java_plugin.html)):
    * You can use `--tests` with a wildcard to run a specific test class, method, or to select multiple test classes:
        * `./gradlew test --tests *SomeSpecificTestClass`
        * `./gradlew test --tests *SomeTest.someSpecificTestMethod`
        * `./gradlew test --tests all.in.specific.package*`

* To run tests and compute coverage reports, run **`./gradlew jacocoTestReport`**. The report is then in `build/reports/jacoco/test/html/index.html`.
  (IntelliJ has a good coverage tool that is preferable for development).

* We use [Github Actions](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml) as our continuous integration provider.

    * Before merging any branch make sure that all required tests pass on Github.
    * Every Actions build will upload the test results to our GATK Google Cloud Storage bucket and a zipped artifact upload.
      A link to the uploaded report will appear at the very bottom of the github actions log.
      Look for the line that says `See the test report at`.
      Test github actions test artifacts will not show up on the webpage until the entire test has concluded.
      If TestNG itself crashes there will be no report generated.

* We use [Broad Jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) for our long-running tests and performance tests.
    * To add a performance test (requires Broad-ID), you need to make a ""new item"" in Jenkins and make it a ""copy"" instead of a blank project. You need to base it on either the ""-spark-"" jobs or the other kind of jobs and alter the commandline. 

* To output stack traces for `UserException` set the environment variable `GATK_STACKTRACE_ON_USER_EXCEPTION=true`

#### <a name=""lfs"">Using Git LFS to download and track large test data</a>

We use [git-lfs](https://git-lfs.github.com/) to version and distribute test data that is too large to check into our repository directly. You must install and configure it in order to be able to run our test suite.

* After installing [git-lfs](https://git-lfs.github.com/), run `git lfs install`
    * This adds hooks to your git configuration that will cause git-lfs files to be checked out for you automatically in the future.
    
* To manually retrieve the large test data, run `git lfs pull` from the root of your GATK git clone.
    * The download size is approximately 5 gigabytes.
    
* To add a new large file to be tracked by git-lfs, simply:
    * Put the new file(s) in `src/test/resources/large` (or a subdirectory)
    * `git add` the file(s), then `git commit -a`
    * That's it! Do ***not*** run `git lfs track` on the files manually: all files in `src/test/resources/large` are tracked by git-lfs automatically. 

#### <a name=""intellij"">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>

* Ensure that you have `gradle` and the Java 17 JDK installed

* You may need to install the TestNG and Gradle plugins (in preferences)

* Clone the GATK repository using git

* In IntelliJ, click on ""Import Project"" in the home screen or go to File -> New... -> Project From Existing Sources...

* Select the root directory of your GATK clone, then click on ""OK""

* Select ""Import project from external model"", then ""Gradle"", then click on ""Next""

* Ensure that ""Gradle project"" points to the build.gradle file in the root of your GATK clone

* Select ""Use auto-import"" and ""Use default gradle wrapper"".

* Make sure the Gradle JVM points to Java 17. You may need to set this manually after creating the project, to do so find the gradle settings by clicking the wrench icon in the gradle tab on the right bar, from there edit ""Gradle JVM"" argument to point to Java 17.

* Click ""Finish""

* After downloading project dependencies, IntelliJ should open a new window with your GATK project

* Make sure that the Java version is set correctly by going to File -> ""Project Structure"" -> ""Project"". Check that the ""Project SDK"" is set to your Java 17 JDK, and ""Project language level"" to 17 (you may need to add your Java 17 JDK under ""Platform Settings"" -> SDKs if it isn't there already). Then click ""Apply""/""Ok"".

#### <a name=""debugging"">Setting up debugging in IntelliJ</a>

* Follow the instructions above for creating an IntelliJ project for GATK

* Go to Run -> ""Edit Configurations"", then click ""+"" and add a new ""Application"" configuration

* Set the name of the new configuration to something like ""GATK debug""

* For ""Main class"", enter `org.broadinstitute.hellbender.Main`

* Ensure that ""Use classpath of module:"" is set to use the ""gatk"" module's classpath

* Enter the arguments for the command you want to debug in ""Program Arguments""

* Click ""Apply""/""Ok""

* Set breakpoints, etc., as desired, then select ""Run"" -> ""Debug"" -> ""GATK debug"" to start your debugging session

* In future debugging sessions, you can simply adjust the ""Program Arguments"" in the ""GATK debug"" configuration as needed

#### <a name=""intellij_gradle_refresh"">Updating the Intellij project when dependencies change</a>
If there are dependency changes in `build.gradle` it is necessary to refresh the gradle project. This is easily done with the following steps.

* Open the gradle tool window  ( ""View"" -> ""Tool Windows"" -> ""Gradle"" )
* Click the refresh button in the Gradle tool window.  It is in the top left of the gradle view and is represented by two blue arrows.

#### <a name=""jprofiler"">Setting up profiling using JProfiler</a>

   * Running JProfiler standalone:
       * Build a full GATK4 jar using `./gradlew localJar`
       * In the ""Session Settings"" window, select the GATK4 jar, eg. `~/gatk/build/libs/gatk-package-4.alpha-196-gb542813-SNAPSHOT-local.jar` for ""Main class or executable JAR"" and enter the right ""Arguments""
       * Under ""Profiling Settings"", select ""sampling"" as the ""Method call recording"" method.

   * Running JProfiler from within IntelliJ:
       * JProfiler has great integration with IntelliJ (we're using IntelliJ Ultimate edition) so the setup is trivial.   
       * Follow the instructions [above](#intellij) for creating an IntelliJ project for GATK  
       * Right click on a test method/class/package and select ""Profile"" 

#### <a name=""sonatype"">Uploading Archives to Sonatype (to make them available via maven central)</a>
To upload snapshots to Sonatype you'll need the following:

* You must have a registered account on the sonatype JIRA (and be approved as a gatk uploader)
* You need to configure several additional properties in your `/~.gradle/gradle.properties` file

* If you want to upload a release instead of a snapshot you will additionally need to have access to the gatk signing key and password

```
#needed for snapshot upload
sonatypeUsername=<your sonatype username>
sonatypePassword=<your sonatype password>

#needed for signing a release
signing.keyId=<gatk key id>
signing.password=<gatk key password>
signing.secretKeyRingFile=/Users/<username>/.gnupg/secring.gpg
```

To perform an upload, use
```
./gradlew uploadArchives
```

Builds are considered snapshots by default.  You can mark a build as a release build by setting `-Drelease=true`.  
The archive name is based off of `git describe`.

#### <a name=""docker_building"">Building GATK4 Docker images</a>

Please see the [the Docker README](scripts/docker/README.md) in ``scripts/docker``.  This has instructions for the Dockerfile in the root directory.

#### <a name=""releasing_gatk"">Releasing GATK4</a>

Please see the [How to release GATK4](https://github.com/broadinstitute/gatk/wiki/How-to-release-GATK4) wiki article for instructions on releasing GATK4.

#### <a name=""gatkdocs"">Generating GATK4 documentation</a>

To generate GATK documentation, run `./gradlew gatkDoc`

* Generated docs will be in the `build/docs/gatkdoc` directory.

#### <a name=""gatkwdlgen"">Generating GATK4 WDL Wrappers</a>

* A WDL wrapper can be generated for any GATK4 tool that is annotated for WDL generation (see the wiki article
[How to Prepare a GATK tool for WDL Auto Generation](https://github.com/broadinstitute/gatk/wiki/How-to-Prepare-a-GATK-tool-for-WDL-Auto-Generation))
to learn more about WDL annotations.

* To generate the WDL Wrappers, run `./gradlew gatkWDLGen`. The generated WDLs and accompanying JSON input files can
be found in the `build/docs/wdlGen` folder.

* To generate WDL Wrappers and validate the resulting outputs, run `./gradlew gatkWDLGenValidation`.
Running this task requires a local [cromwell](https://github.com/broadinstitute/cromwell) installation, and environment
variables `CROMWELL_JAR` and `WOMTOOL_JAR` to be set to the full pathnames of the `cromwell` and `womtool` jar files.
If no local install is available, this task will run automatically on github actions in a separate job whenever a PR is submitted.

* WDL wrappers for each GATK release are published to the [gatk-tool-wdls](https://github.com/broadinstitute/gatk-tool-wdls) repository.
Only tools that have been annotated for WDL generation will show up there. 

#### <a name=""zenhub"">Using Zenhub to track github issues</a>

We use [Zenhub](https://www.zenhub.com/) to organize and track github issues.

* To add Zenhub to github, go to the [Zenhub home page](https://www.zenhub.com/) while logged in to github, and click ""Add Zenhub to Github""

* Zenhub allows the GATK development team to assign time estimates to issues, and to mark issues as Triaged/In Progress/In Review/Blocked/etc.

## <a name=""spark_further_reading"">Further Reading on Spark</a>

[Apache Spark](https://spark.apache.org/) is a fast and general engine for large-scale data processing.
GATK4 can run on any Spark cluster, such as an on-premise Hadoop cluster with HDFS storage and the Spark
runtime, as well as on the cloud using Google Dataproc.

In a cluster scenario, your input and output files reside on HDFS, and Spark will run in a distributed fashion on the cluster.
The Spark documentation has a good [overview of the architecture](https://spark.apache.org/docs/latest/cluster-overview.html).

Note that if you don't have a dedicated cluster you can run Spark in
[standalone mode](https://spark.apache.org/docs/latest/spark-standalone.html) on a single machine, which exercises
the distributed code paths, albeit on a single node.

While your Spark job is running, the [Spark UI](http://spark.apache.org/docs/latest/monitoring.html) is an excellent place to monitor the  progress.
Additionally, if you're running tests, then by adding `-Dgatk.spark.debug=true` you can run a single Spark test and
look at the Spark UI (on [http://localhost:4040/](http://localhost:4040/)) as it runs.

You can find more information about tuning Spark and choosing good values for important settings such as the number
of executors and memory settings at the following:

* [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)
* [How-to: Tune Your Apache Spark Jobs (Part 1)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)
* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)

## <a name=""contribute"">How to contribute to GATK</a>
(Note: section inspired by, and some text copied from, [Apache Parquet](https://github.com/apache/parquet-mr))
 
We welcome all contributions to the GATK project. The contribution can be a [issue report]( https://github.com/broadinstitute/gatk/issues) 
or a [pull request](https://github.com/broadinstitute/gatk/pulls). If you're not a committer, you will 
need to [make a fork](https://help.github.com/articles/fork-a-repo/) of the gatk repository 
and [issue a pull request](https://help.github.com/articles/be-social/) from your fork.

For ideas on what to contribute, check issues labeled [""Help wanted (Community)""](https://github.com/broadinstitute/gatk/issues?q=is%3Aopen+is%3Aissue+label%3A%22Help+Wanted+%28Community%29%22). Comment on the issue to indicate you're interested in contibuting code and for sharing your questions and ideas.

To contribute a patch:
* Break your work into small, single-purpose patches if possible. It’s much harder to merge in a large change with a lot of disjoint features.
* Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on [forking a repo](https://help.github.com/articles/fork-a-repo/) and [sending a pull request](https://help.github.com/articles/be-social/). If applicable, include the issue number in the pull request name.
* Make sure that your code passes all our tests. You can run the tests with `./gradlew test` in the root directory.
* Add tests for all new code you've written. We prefer unit tests but high quality integration tests that use small amounts of data are acceptable.
* Follow the [**General guidelines for GATK4 developers**](https://github.com/broadinstitute/gatk#general-guidelines-for-gatk4-developers).

We tend to do fairly close readings of pull requests, and you may get a lot of comments. Some things to consider:
* Write tests for all new code.
* Document all classes and public methods.
* For all public methods, check validity of the arguments and throw `IllegalArgumentException` if invalid.
* Use braces for control constructs, `if`, `for` etc.
* Make classes, variables, parameters etc `final` unless there is a strong reason not to.
* Give your operators some room. Not `a+b` but `a + b` and not `foo(int a,int b)` but `foo(int a, int b)`.
* Generally speaking, stick to the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)

Thank you for getting involved!

## <a name=""discussions"">Discussions</a>
* [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics) for general discussions on how to use the GATK and support questions.
* [Issue tracker](https://github.com/broadinstitute/gatk/issues) to report errors and enhancement ideas. 
* Discussions also take place in [GATK pull requests](https://github.com/broadinstitute/gatk/pulls)

## <a name=""authors"">Authors</a>
The authors list is maintained in the [AUTHORS](https://github.com/broadinstitute/gatk/edit/master/AUTHORS) file. 
See also the [Contributors](https://github.com/broadinstitute/gatk/graphs/contributors) list at github. 

## <a name=""license"">License</a>
Licensed under the Apache 2.0 License. See the [LICENSE.txt](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT) file.
",2023-07-07 15:57:51+00:00
gc3pie,gc3pie,gc3pie/gc3pie,Python libraries and tools for running applications on diverse Grids and clusters,http://gc3pie.readthedocs.org/,False,42,2023-06-19 02:26:07+00:00,2015-08-06 13:51:20+00:00,23,11,17,0,,,GNU Lesser General Public License v2.1,3876,v2.6.9,64,2021-08-10 16:17:56+00:00,2023-06-19 02:26:07+00:00,2021-08-07 19:57:39+00:00,"========================================================================
    GC3Pie |gitter|
========================================================================

.. |gitter| image:: https://badges.gitter.im/gc3pie/chat.svg
   :alt: Join the chat at https://gitter.im/gc3pie/chat
   :target: https://gitter.im/gc3pie/chat?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. This file follows reStructuredText markup syntax; see
   http://docutils.sf.net/rst.html for more information

GC3Pie is a python package for executing computational workflows
consisting of tasks with complex inter-dependencies. GC3Pie accomplishes
this by defining a run-time task list in which a task can only be
executed once all upstream task dependencies have been successfullly
completed. In contrast to other workflow managers, GC3Pie accplications
are written in python and not a markup language. The advantage is that
this makes it trivial to write highly complex workflows. GC3Pies roots
are in shared-nothing achitectures (server-less for example) and can be
configured to use backends such as batch clusters or clouds.

GC3Pie is a suite of Python classes (and command-line tools built
upon them) to aid in submitting and controlling batch jobs to clusters
and grid resources seamlessly. GC3Pie aims at providing the
building blocks by which Python scripts that combine several
applications in a dynamic workflow can be quickly developed.

The GC3Pie suite is comprised of three main components:

 * GC3Libs: A python package for controlling the life-cycle of a Grid or batch computational job
 * GC3Utils: Command-line tools exposing the main functionality provided by GC3Libs
 * GC3Apps: Driver scripts to run large job campaigns


GC3Libs
=======

GC3Libs provides services for submitting computational jobs to Grids
and batch systems and controlling their execution, persisting job
information, and retrieving the final output.

GC3Libs takes an application-oriented approach to batch computing. A
generic ``Application`` class provides the basic operations for
controlling remote computations, but different ``Application``
subclasses can expose adapted interfaces, focusing on the most
relevant aspects of the application being represented. Specific
interfaces are already provided for the GAMESS_ and Rosetta_ suites;
new ones can be easily created by subclassing the generic
``Application`` class.


GC3Utils
========

Most of the time users have lots of different accounts on several
diverse resources. The idea underlying GC3Utils is that a user can
submit and control a computational job from one single place with a few
simple commands.

Commands are provided to submit a job (``gsub``), check its running
status (``gstat``), get a snapshot of the output files (``gget``,
``gtail``), or cancel it (``gkill``).


GC3Apps
=======

There is a need in some scientific communities, to run large job
campaigns to analyze a vast number of data files with the same
application. The single-job level of control implemented by GC3Utils
in this case is not enough: you would have to implement ""glue scripts""
to control hundreds or thousand scripts at once. GC3Pie has provisons
for this, in the form of re-usable Python classes that implement a
single point of control for job families.

The GC3Apps scripts are driver scripts that run job campaigns using
the supported applications on a large set of input data. They can be
used in production as-is, or adapted to suit your data processing needs.

A Simple Example
================

There are several examples and a tutorial in the examples directory.

.. code-block:: python

    import sys
    import time
    import gc3libs

    class GdemoSimpleApp(gc3libs.Application):
        """"""
        This simple application will run `/bin/hostname`:file: on the remote host,
        and retrieve the output in a file named `stdout.txt`:file: into a
        directory `GdemoSimpleApp_output`:file: inside the current directory.
        """"""
        def __init__(self):
            gc3libs.Application.__init__(
                self,
                # the following arguments are mandatory:
                arguments = [""/bin/hostname""],
                inputs = [],
                outputs = [],
                output_dir = ""./GdemoSimpleApp_output"",
                # the rest is optional and has reasonable defaults:
                stdout = ""stdout.txt"",)

    # Create an instance of GdemoSimpleApp
    app = GdemoSimpleApp()

    # Create an instance of `Engine` using the configuration file present
    # in your home directory.
    engine = gc3libs.create_engine()

    # Add your application to the engine. This will NOT submit your
    # application yet, but will make the engine awere *aware* of the
    # application.
    engine.add(app)

    # in case you want to select a specific resource, call
    # `Engine.select_resource(<resource_name>)`
    if len(sys.argv)>1:
        engine.select_resource(sys.argv[1])

    # Periodically check the status of your application.
    while app.execution.state != gc3libs.Run.State.TERMINATED:
        print ""Job in status %s "" % app.execution.state
        # `Engine.progress()` will do the GC3Pie magic:
        # submit new jobs, update status of submitted jobs, get
        # results of terminating jobs etc...
        engine.progress()

        # Wait a few seconds...
        time.sleep(1)

    print ""Job is now terminated.""
    print ""The output of the application is in `%s`."" %  app.output_dir

This is what it looks like when the code is run:

.. code-block:: text

    $ python gdemo_simple.py localhost
    gdemo_simple.py: [2019-01-21 14:37:53] INFO    : Computational resource 'localhost' initialized successfully.
    Job in status NEW
    gdemo_simple.py: [2019-01-21 14:37:55] INFO    : Successfully submitted GdemoSimpleApp@7f07aa094a90 to: localhost
    Job in status SUBMITTED
    Job is now terminated.
    The output of the application is in `./GdemoSimpleApp_output`.

The output file looks as follows:

.. code-block:: text

    $ cat GdemoSimpleApp_output/stdout.txt
    93607d089233


Installation instructions and further reading
=============================================

For up-to-date information, please read the GC3Pie documentation at:
http://gc3pie.readthedocs.io/

Installation instructions are in the `INSTALL.rst`_ file (in this
same directory), or can be read online at:
http://gc3pie.readthedocs.io/en/latest/users/install.html

.. _`INSTALL.rst`: https://github.com/uzh/gc3pie/blob/master/docs/users/install.rst


License
=======

The GC3Pie library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the `GNU
Lesser General Public License`_ for more details.

A copy of the GNU Lesser General Public License is in file `LICENSE.md`_.


.. References

.. _GC3Pie: http://gc3pie.googlecode.com/
.. _GAMESS: http://www.msg.chem.iastate.edu/gamess/
.. _Rosetta: http://www.rosettacommons.org/
.. _`GNU Lesser General Public License`: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html
.. _`LICENSE.md`: https://github.com/uzh/gc3pie/blob/master/LICENSE.md

.. (for Emacs only)
..
  Local variables:
  mode: rst
  End:
",2023-07-07 15:57:56+00:00
genepattern,genepattern-notebook,genepattern/genepattern-notebook,Platform for integrating genomic analysis with Jupyter Notebooks.,http://genepattern-notebook.org,False,41,2023-07-06 22:49:45+00:00,2014-10-28 16:42:05+00:00,12,15,5,42,v23.04,2023-04-17 19:06:43+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",872,v23.04,43,2023-04-17 19:06:43+00:00,2023-06-07 16:03:55+00:00,2023-04-17 19:06:43+00:00,"[![Version](https://img.shields.io/pypi/v/genepattern-notebook.svg)](https://pypi.python.org/pypi/genepattern-notebook)
[![Build](https://travis-ci.org/genepattern/genepattern-notebook.svg?branch=master)](https://travis-ci.org/genepattern/genepattern-notebook)
[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](http://genepattern-notebook.org/programmatic/)
[![Docker Pulls](https://img.shields.io/docker/pulls/genepattern/genepattern-notebook.svg)](https://hub.docker.com/r/genepattern/genepattern-notebook/)
[![Join the chat at https://gitter.im/genepattern](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/genepattern/genepattern-notebook?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

GenePattern Notebook for JupyterLab
====================

The [GenePattern Notebook](http://www.genepattern-notebook.org) 
environment gives GenePattern users the ability to interleave text, graphics, and code with 
their GenePattern analyses to create ""notebooks"" that can be edited, shared, and published. 

GenePattern Notebooks are built on the [Jupyter](https://jupyter.org/) environment 
and extend it so that users can take advantage of its ease of use and ability to encapsulate 
an entire scientific research narrative, without the need to write code. They are a core 
component of the [g2nb](https://github.com/g2nb) project.

> ### **Looking for classic Jupyter Notebook support?**
> **Jupyter Notebook support in is available, albeit no longer in active development. You can 
> find it in its own branch. [Just click here!](https://github.com/genepattern/genepattern-notebook/tree/notebook)**


### **Prerequisites**

* JupyterLab >= 3.0.0
* ipywidgets >= 7.0.0

# Docker

A Docker image with nbtools and the full JupyterLab stack is available through DockerHub.

```bash
docker pull genepattern/lab
docker run --rm -p 8888:8888 genepattern/lab
```

# Installation

Full installation instructions for casual use are detailed on the 
[GenePattern Notebook website](http://www.genepattern-notebook.org/install/). Users should 
also consider the [g2nb Notebook Repository](https://notebook.genepattern.org), which 
provides an install-free cloud deployment of the full suite of g2nb tools, including GenePattern Notebook.

## Development Install

The installation instructions below are intended for developers who want to install the 
project from PIP or GitHub for the purposes of software development.

### Install Python

In order to get the GenePattern Notebook working you will first need to install a compatible 
version of Python. This means you will need Python 3.6+. We recommend using the 
[Anaconda](https://www.anaconda.com/download/#macos) Python distribution. This is 
a scientific version of Python that ships with many of the most popular Python packages for 
science, math and data analysis (ex: NumPy, SciPy, Pandas, Matplotlib, IPython, etc.).

**Note for Mac Users:** Mac comes with Python, but if you have an older version of the OS 
you will need to install a newer version, as many older versions of MacOS ship with Python 2.

### Install GenePattern Notebook from GitHub

Copy the contents of genepattern-notebook/extension to your development computer and ensure 
that the resulting directory if on your Python path. To test this, open Python and try to 
*import genepattern*. If this is successful, you have a copy of the extension available.

If you don't already have Jupyter installed, you can install it from PIP by running:

> pip install jupyter

From here go to the ""Load the GenePattern extension"" step below.

### Install GenePattern Notebook from PIP or Conda

The easiest way to install GenePattern Notebook is through either PIP or conda. It can be installed by 
executing one of the following commands:

```bash
pip install genepattern-notebook
```

*or*

```bash
conda install -c genepattern genepattern-notebook
```

### Load the nbtools extension

```bash
# Install ipywidgets, if you haven't already
jupyter nbextension enable --py widgetsnbextension
jupyter labextension install @jupyter-widgets/jupyterlab-manager

# Clone the nbtools repository
git clone https://github.com/genepattern/nbtools-lab-prototype.git
cd nbtools-lab-prototype

# Install the nbtools JupyterLab prototype
pip install .
jupyter labextension install .
jupyter nbextension install --py nbtools --sys-prefix
jupyter nbextension enable --py nbtools --sys-prefix
```

### Launch Jupyter

Finally, you may launch JupyterLab by issuing the following command at the terminal:

> jupyter lab

This will start up the notebook kernel and launch your web browser pointing to the Notebook.

# Related Repositories

The following GitHub repositories contain code or other data related to the GenePattern 
Notebook environment.

* [g2nb](https://github.com/g2nb/g2nb): A meta-package which installs all of the g2nb tools and extensions.
* [genepattern-python](https://github.com/genepattern/genepattern-python): The GenePattern 
    Library allows for programmatic access to GenePattern from Python, and is used by 
    GenePattern Notebook behind the scenes.
* [nbtools](https://github.com/g2nb/nbtools): The Notebook Tool Manager 
    is a tool-agnostic interface and registry for searching, browsing and launching available 
    notebook tools in a Jupyter environment.
* [jupyter-wysiwyg](https://github.com/g2nb/jupyter-wysiwyg): A WYSIWYG editor for 
    markdown cells.
* [example-notebooks](https://github.com/g2nb/example-notebooks): A repository of example notebooks that 
    demonstrate functionality or analysis techniques in the GenePattern Notebook environment.
* [workspace](https://github.com/g2nb/workspace): Scripts, services 
    and other infrastructure used in the operation of the GenePattern Notebook Repository.

# Known Issues

**The current version of the code only works with GenePattern 3.9.3 and up!**

Users using the GenePattern Notebook with an older version of GenePattern (3.9.3 or 3.9.4) may
need to log into the GenePattern UI before making use of the notebook. The server status 
message and child jobs will also be unavailable. If you are using one of these older versions,
we recommend that you upgrade to the latest version of GenePattern.

# Code of Conduct

We are dedicated to providing a harassment-free experience for all members of the GenePattern community, regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, age, race, or religion. We do not tolerate harassment of participants in any form. This code of conduct applies to all GenePattern spaces, including the Google Group, our Git repositories, and our social media accounts, both online and off. Anyone who violates this code of conduct may be sanctioned or expelled from these spaces at the discretion of the GenePattern team.

For more details, see our [Code of Conduct](https://github.com/genepattern/genepattern-notebook/blob/master/Code_of_Conduct.md).
",2023-07-07 15:58:00+00:00
genomevip,GenomeVIP,ding-lab/GenomeVIP,,,False,19,2022-10-17 05:25:05+00:00,2015-09-29 15:06:06+00:00,5,12,2,3,v1.2.1,2017-01-05 18:00:12+00:00,Other,104,v1.2.1,9,2017-01-05 18:00:12+00:00,,2017-02-24 18:34:10+00:00,"# GenomeVIP

GenomeVIP is a web application platform for performing variant discovery on Amazon's Web Service (AWS) cloud or on local high-performance computing clusters.

Documentation is online at http://genomevip.readthedocs.io/.

",2023-07-07 15:58:04+00:00
geoweaver,Geoweaver,ESIPFed/Geoweaver,"a web system to allow users to automatically record history and manage complicated scientific workflows in web browsers involving the online spatial data facilities, high-performance computation platforms, and open-source libraries.",,False,38,2023-06-05 09:27:40+00:00,2018-08-14 13:24:52+00:00,33,9,17,39,latest,2023-06-15 01:26:09+00:00,MIT License,1369,v1.0.0-rc12,45,2023-06-15 01:26:09+00:00,2023-06-23 14:39:13+00:00,2023-06-15 01:26:09+00:00,"[![CircleCI](https://circleci.com/gh/ZihengSun/Geoweaver/tree/master.svg?style=svg)](https://circleci.com/gh/ZihengSun/Geoweaver/tree/master) [![License](https://img.shields.io/github/license/ESIPFed/Geoweaver.svg)](https://github.com/ESIPFed/Geoweaver/blob/master/LICENSE) [![Stars](https://img.shields.io/github/stars/ESIPFed/Geoweaver.svg)](https://github.com/ESIPFed/Geoweaver/stargazers) [![Forks](https://img.shields.io/github/forks/ESIPFed/Geoweaver.svg)](https://github.com/ESIPFed/Geoweaver/network/members) [![Issues](https://img.shields.io/github/issues/ESIPFed/Geoweaver.svg)](https://github.com/ESIPFed/Geoweaver/issues) [![Coverage](https://img.shields.io/badge/coverage-100%25-success.svg)](https://codecov.io/) [![Discourse status](https://img.shields.io/discourse/status?server=https%3A%2F%2Fgeoweaver.discourse.group)](https://geoweaver.discourse.group)

# [Geoweaver](https://esipfed.github.io/Geoweaver/)

Geoweaver is an in-browser software allowing users to easily compose and execute full-stack data processing workflows via taking advantage of online spatial data facilities, high-performance computation platforms, and open-source deep learning libraries. It provides all-in-one capacity covering server management, code repository, workflow orchestration software, and history recorder. 

It can be run from both local and remote (distributed) machines.

GeoWeaver is a community effort. Any contribution is welcome and greatly appreciated! 

# Software Goals

Only two things basically:

1. Make it time affordable for less-coder scientists (who know nothing about WfMS) to manage their data processing workflows

2. Preserve all the model run history and share them along with the code files

# [Installation](docs/install.md)

# [Tutorial](https://zihengsun.github.io/Geoweaver/)

* [Create and Manipulate Hosts](docs/host.md)

* [Create and Run Processes](docs/process.md)

* [Create, Run, and Export Workflows](docs/workflow.md)

* [Jupyter Recording](https://andrewmagill.github.io/#/)

# Demo

[A live demo site](https://geobrain.csiss.gmu.edu/Geoweaver) is available.

# Citation

If you found Geoweaver helpful in your research, please cite: 

Sun, Z. et al., ""Geoweaver: Advanced cyberinfrastructure for managing hybrid geoscientific AI workflows."" ISPRS International Journal of Geo-Information 9, no. 2 (2020): 119.

# Dependencies

This project is impossible without the support of several fantastic open source libraries.

[d3.js](https://github.com/d3/d3) - BSD 3-Clause

[graph-creator](https://github.com/cjrd/directed-graph-creator) - MIT License

[bootstrap](https://github.com/twbs/bootstrap) - MIT License

[CodeMirror](https://github.com/codemirror/CodeMirror) - MIT License

[JQuery Terminal](https://github.com/jcubic/jquery.terminal) - MIT License

# [Community](docs/authors.md)

# License

MIT


",2023-07-07 15:58:08+00:00
giraffetools,GiraffeTools,GiraffeTools/GiraffeTools,a Graphical Interface for Reproducible Analysis oF workFlow Experiments,https://giraffe.tools,False,47,2023-04-06 02:04:43+00:00,2018-03-22 11:35:24+00:00,23,6,12,0,,,GNU General Public License v3.0,1002,,0,,2023-05-25 17:31:37+00:00,2021-11-18 21:52:19+00:00,"[![Codacy Badge](https://api.codacy.com/project/badge/Grade/9a612459dbc54089b8a66453ace3f024)](https://www.codacy.com/app/TimVanMourik/GiraffeTools?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=GiraffeTools/GiraffeTools&amp;utm_campaign=Badge_Grade)
[![Build Status](https://travis-ci.org/GiraffeTools/GiraffeTools.svg?branch=master)](https://travis-ci.org/GiraffeTools/GiraffeTools)
[![codecov](https://codecov.io/gh/TimVanMourik/GiraffeTools/branch/master/graph/badge.svg)](https://codecov.io/gh/TimVanMourik/GiraffeTools)

[https://giraffe.tools](https://giraffe.tools)
# GiraffeTools <img src=""backend/giraffe/static/img/giraffetools_logo.png"" width=""50"">
 GiraffeTools: **Tools** for **G**raphical **I**nterface for **R**eproducible **A**nalysis o**F** work**F**low **E**xperiments

GiraffeTools provides tools for interactive workflow development Currently the main tool is the [Porcupine](https://doi.org/10.1371/journal.pcbi.1006064) workflow editor. You can visually build a node graph that represents a workflow and read and write it from and to GitHub.

*Note: this software was originally made for neuroimaging pipeline analysis, but is now expanding to different toolboxes*
## Intended usage
Set up any GitHub repository as a GiraffeTools project by putting a **Giraffe.yml** file in the root of the project. GiraffeTools reads information straight from an analysis repository, just go to:
https://giraffe.tools/github/ **[username]** / **[repository]** / **[branch]**
to find a dashboard of the project. A **Giraffe.yml** looks like this:
```
# list of tools
tools:
  workflow:
    # A file in your repository to which the UI state is saved
    # Currently only a single file is supported
    files:
      - GIRAFFE/keras.pork
    # A list of nodes to load into the editor
    # The path is either relative to the root or a full URL
    nodes:
      - https://raw.githubusercontent.com/TimVanMourik/keras/giraffe-tools/keras_nodes.json
    # You can load your own JavaScript code generator. Documentation will follow soon
    # The path is either relative to the root or a full URL
    grammars:
      - language: TvM
        script: GIRAFFE/test.js
        format: matlab
```
Example: [https://giraffe.tools/github/TimVanMourik/SomeGiraffeExample/master](https://giraffe.tools/github/TimVanMourik/SomeGiraffeExample/master)

## Development
Download this repository *with submodules*, such that the code from linked repositories are included:
```
git clone https://github.com/GiraffeTools/GiraffeTools --recursive
```
or
```
git clone https://github.com/GiraffeTools/GiraffeTools
git submodule update --init
```
The website can locally be deployed with [Docker](https://www.docker.com) and `docker-compose`. Starting local development is as easy as typing in the command line:
```
docker-compose up
```
If you want to customise your settings, specify this in a `.env` file in the root of the project. You can start by renaming the `env.sample`.

You can run the site in three different modes, MODE=watch|development|production. Default is `watch`. This automatically enables *hot reloading* and updates your browser pages upon saving a source file. In development mode, you can still use debug statement, but there is no 'hot reloading'. The production mode is the way it's gonna be like online.

## Slack
Developers and users alike, join us on [Slack](https://giraffe.tools/slack)!


## Contributing
Any contributions are much appreciated! In the form of issues about bugs, feature requests, enhancement ideas, or even pull requests! If you like more details, you can find them in CONTRIBUTING.md.
",2023-07-07 15:58:12+00:00
giraph,giraph,apache/giraph,Mirror of Apache Giraph,,False,612,2023-07-04 17:37:06+00:00,2011-08-28 07:00:33+00:00,302,68,26,0,,,Apache License 2.0,1138,release-1.1.0,12,2014-11-01 05:25:16+00:00,2023-07-04 17:37:06+00:00,2022-03-09 03:51:20+00:00,"Giraph : Large-scale graph processing on Hadoop

Web and online social graphs have been rapidly growing in size and
scale during the past decade.  In 2008, Google estimated that the
number of web pages reached over a trillion.  Online social networking
and email sites, including Yahoo!, Google, Microsoft, Facebook,
LinkedIn, and Twitter, have hundreds of millions of users and are
expected to grow much more in the future.  Processing these graphs
plays a big role in relevant and personalized information for users,
such as results from a search engine or news in an online social
networking site.

Graph processing platforms to run large-scale algorithms (such as page
rank, shared connections, personalization-based popularity, etc.) have
become quite popular.  Some recent examples include Pregel and HaLoop.
For general-purpose big data computation, the map-reduce computing
model has been well adopted and the most deployed map-reduce
infrastructure is Apache Hadoop.  We have implemented a
graph-processing framework that is launched as a typical Hadoop job to
leverage existing Hadoop infrastructure, such as Amazon’s EC2.  Giraph
builds upon the graph-oriented nature of Pregel but additionally adds
fault-tolerance to the coordinator process with the use of ZooKeeper
as its centralized coordination service.

Giraph follows the bulk-synchronous parallel model relative to graphs
where vertices can send messages to other vertices during a given
superstep.  Checkpoints are initiated by the Giraph infrastructure at
user-defined intervals and are used for automatic application restarts
when any worker in the application fails.  Any worker in the
application can act as the application coordinator and one will
automatically take over if the current application coordinator fails.

-------------------------------

Hadoop versions for use with Giraph:

Secure Hadoop versions:

- Apache Hadoop 1 (latest version: 1.2.1)

  This is the default version used by Giraph: if you do not specify a
  profile with the -P flag, maven will use this version. You may also
  explicitly specify it with ""mvn -Phadoop_1 <goals>"".

- Apache Hadoop 2 (latest version: 2.5.1)

  This is the latest version of Hadoop 2 (supporting YARN in addition
  to MapReduce) Giraph could use. You may tell maven to use this version
  with ""mvn -Phadoop_2 <goals>"".

- Apache Hadoop Yarn with 2.2.0

  You may tell maven to use this version with ""mvn -Phadoop_yarn -Dhadoop.version=2.2.0 <goals>"".

- Apache Hadoop 3.0.0-SNAPSHOT

  You may tell maven to use this version with ""mvn -Phadoop_snapshot <goals>"".

Unsecure Hadoop versions:

- Facebook Hadoop releases: https://github.com/facebook/hadoop-20, Master branch

  You may tell maven to use this version with ""mvn -Phadoop_facebook <goals>""

-- Other versions reported working include:
---  Cloudera CDH3u0, CDH3u1

While we provide support for unsecure and Facebook versions of Hadoop
with the maven profiles 'hadoop_non_secure' and 'hadoop_facebook',
respectively, we have been primarily focusing on secure Hadoop releases
at this time.

-------------------------------

Building and testing:

You will need the following:
- Java 1.8
- Maven 3 or higher. Giraph uses the munge plugin
  (http://sonatype.github.com/munge-maven-plugin/),
  which requires Maven 3, to support multiple versions of Hadoop. Also, the
  web site plugin requires Maven 3.

Use the maven commands with secure Hadoop to:
- compile (i.e mvn compile)
- package (i.e. mvn package)
- test (i.e. mvn test)

For the non-secure versions of Hadoop, run the maven commands with the
additional argument '-Phadoop_non_secure'.
Example compilation commands is 'mvn -Phadoop_non_secure compile'.

For the Facebook Hadoop release, run the maven commands with the
additional arguments '-Phadoop_facebook'.
Example compilation commands is 'mvn -Phadoop_facebook compile'.

-------------------------------

Developing:

Giraph is a multi-module maven project. The top level generates a POM that
carries information common to all the modules. Each module creates a jar with
the code contained in it.

The giraph/ module contains the main giraph code. If you just want to work on
the main code only you can do all your work inside this subdirectory.
Specifically you would do something like:

  giraph-root/giraph/ $ mvn verify            # build from current state
  giraph-root/giraph/ $ mvn clean             # wipe out build files
  giraph-root/giraph/ $ mvn clean verify      # build from fresh state
  giraph-root/giraph/ $ mvn install           # install jar to local repository

The giraph-formats/ module contains hooks to read/write from various
formats (e.g. Accumulo, HBase, Hive). It depends on the giraph module. This
means if you make local changes to the giraph codebase you will first need to
install the giraph/ jar locally so that giraph-formats/ will pick it up.
In other words something like this:

  giraph-root/giraph/ $ mvn install
  giraph-root/giraph-formats $ mvn verify

To build everything at once you can issue the maven commands at the top level.
Note that we use the ""install"" target so that if you have any local changes to
giraph/ which formats needs it will get picked up because it will install
locally first.

  giraph-root/ $ mvn clean install

-------------------------------

Scripting:

Giraph has support for writing user logic in languages other than Java. A Giraph
job involves at the very least a Computation and Input/Output Formats. There are
other optional pieces as well like Aggregators and Combiners.

As of this writing we support writing the Computation logic in Jython. The
Computation class is at the core of the algorithm so it was a natural starting
point. Eventually it is our goal to allow users to write any / all components of
their algorithms in any language they desire.

To use Jython with our job launcher, GiraphRunner, pass the path to the script
as the Computation class argument. Additionally, you should set the -jythonClass
option to let Giraph know the name of your Jython Computation class. Lastly, you
will need to set -typesHolder to a class that extends Giraph's TypesHolder so
that Giraph can infer the types you use. Look at page-rank.py as an example.

-------------------------------

How to run the unittests on a local pseudo-distributed Hadoop instance:

As mentioned earlier, Giraph supports several versions of Hadoop.  In
this section, we describe how to run the Giraph unittests against a single
node instance of Apache Hadoop 0.20.203.

Download Apache Hadoop 0.20.203 (hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz)
from a mirror picked at http://www.apache.org/dyn/closer.cgi/hadoop/common/
and unpack it into a local directory

Follow the guide at
http://hadoop.apache.org/common/docs/r0.20.2/quickstart.html#PseudoDistributed
to setup a pseudo-distributed single node Hadoop cluster.

Giraph’s code assumes that you can run at least 4 mappers at once,
unfortunately the default configuration allows only 2. Therefore you need
to update conf/mapred-site.xml:

<property>
  <name>mapred.tasktracker.map.tasks.maximum</name>
  <value>4</value>
</property>

<property>
  <name>mapred.map.tasks</name>
  <value>4</value>
</property>

After preparing the local filesystem with:

rm -rf /tmp/hadoop-<username>
/path/to/hadoop/bin/hadoop namenode -format

you can start the local hadoop instance:

/path/to/hadoop/bin/start-all.sh

and finally run Giraph’s unittests:

mvn clean test -Dprop.mapred.job.tracker=localhost:9001

Now you can open a browser, point it to http://localhost:50030 and watch the
Giraph jobs from the unittests running on your local Hadoop instance!


Notes:
Counter limit: In Hadoop 0.20.203.0 onwards, there is a limit on the number of
counters one can use, which is set to 120 by default. This limit restricts the
number of iterations/supersteps possible in Giraph. This limit can be increased
by setting a parameter ""mapreduce.job.counters.limit"" in job tracker's config
file mapred-site.xml.

",2023-07-07 15:58:16+00:00
globuscompute,funcX,funcx-faas/funcX,funcX: High Performance Function Serving for Science,https://funcx.org,False,123,2023-06-20 21:48:24+00:00,2019-05-16 21:31:24+00:00,36,17,35,7,2.1.0,2023-05-26 20:17:22+00:00,Apache License 2.0,2392,v2.0.0,65,2023-04-13 13:56:52+00:00,2023-07-06 22:04:59+00:00,2023-07-06 22:04:57+00:00,"Globus Compute - Fast Function Serving
=============================
|licence| |build-status| |docs| |NSF-2004894| |NSF-2004932|

Globus Compute (formerly funcX) is a high-performance function-as-a-service (FaaS)
platform that enables intuitive, flexible, efficient, scalable, and performant remote
function execution on existing infrastructure including clouds, clusters, and supercomputers.

.. |licence| image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg
   :target: https://github.com/funcx-faas/funcX/blob/master/LICENSE
   :alt: Apache Licence V2.0
.. |build-status| image:: https://travis-ci.com/funcx-faas/funcX.svg?branch=master
   :target: https://travis-ci.com/funcx-faas/funcX
   :alt: Build status
.. |docs| image:: https://readthedocs.org/projects/funcx/badge/?version=latest
   :target: https://funcx.readthedocs.io/en/latest/
   :alt: Documentation Status
.. |NSF-2004894| image:: https://img.shields.io/badge/NSF-2004894-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=2004894
   :alt: NSF award info
.. |NSF-2004932| image:: https://img.shields.io/badge/NSF-2004932-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=2004932
   :alt: NSF award info


.. image:: docs/_static/images/globus-300x300-blue.png
  :target: https://www.funcx.org
  :width: 200

Website: https://www.funcx.org

Documentation: https://globus-compute.readthedocs.io/en/latest/

Quickstart
==========

Globus Compute is currently available on PyPI.

To install Globus Compute, please ensure you have python3.7+.::

   $ python3 --version

Install using Pip::

   $ pip install globus-compute-sdk

To use our example notebooks you will need Jupyter.::

   $ pip install jupyter

.. note:: The Globus Compute client is supported on MacOS, Linux and Windows.
          The Globus Compute endpoint is only supported on Linux.

Documentation
=============

Complete documentation for Globus Compute is available `here <https://funcx.readthedocs.io>`_

",2023-07-07 15:58:20+00:00
graphlabcreate,turicreate,apple/turicreate,Turi Create simplifies the development of custom machine learning models.,,False,11044,2023-07-06 17:40:29+00:00,2017-12-01 00:42:04+00:00,1158,348,71,30,6.4.1,2020-09-29 22:20:30+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",1571,v4.3.2,30,2018-04-17 22:17:32+00:00,2023-07-05 19:35:44+00:00,2021-11-29 19:55:31+00:00,"Quick Links: [Installation](#supported-platforms) | [Documentation](#documentation)

[![Build Status](https://travis-ci.com/apple/turicreate.svg?branch=master)](#)
[![PyPI Release](https://img.shields.io/pypi/v/turicreate.svg)](#)
[![Python Versions](https://img.shields.io/pypi/pyversions/turicreate.svg)](#)

[<img align=""right"" src=""https://docs-assets.developer.apple.com/turicreate/turi-dog.svg"" alt=""Turi Create"" width=""100"">](#)

# Turi Create 

Turi Create simplifies the development of custom machine learning models. You
don't have to be a machine learning expert to add recommendations, object
detection, image classification, image similarity or activity classification to
your app.

* **Easy-to-use:** Focus on tasks instead of algorithms
* **Visual:** Built-in, streaming visualizations to explore your data
* **Flexible:** Supports text, images, audio, video and sensor data
* **Fast and Scalable:** Work with large datasets on a single machine
* **Ready To Deploy:** Export models to Core ML for use in iOS, macOS, watchOS, and tvOS apps

With Turi Create, you can accomplish many common ML tasks:

| ML Task                 | Description                      |
|:------------------------:|:--------------------------------:|
| [Recommender](https://apple.github.io/turicreate/docs/userguide/recommender/)             | Personalize choices for users    |
| [Image Classification](https://apple.github.io/turicreate/docs/userguide/image_classifier/)    | Label images                     |
| [Drawing Classification](https://apple.github.io/turicreate/docs/userguide/drawing_classifier)  | Recognize Pencil/Touch Drawings and Gestures                     |
| [Sound Classification](https://apple.github.io/turicreate/docs/userguide/sound_classifier)  | Classify sounds                     |
| [Object Detection](https://apple.github.io/turicreate/docs/userguide/object_detection/)        | Recognize objects within images  |
| [One Shot Object Detection](https://apple.github.io/turicreate/docs/userguide/one_shot_object_detection/)    | Recognize 2D objects within images using a single example  |
| [Style Transfer](https://apple.github.io/turicreate/docs/userguide/style_transfer/)        | Stylize images |
| [Activity Classification](https://apple.github.io/turicreate/docs/userguide/activity_classifier/) | Detect an activity using sensors |
| [Image Similarity](https://apple.github.io/turicreate/docs/userguide/image_similarity/)        | Find similar images              |
| [Classifiers](https://apple.github.io/turicreate/docs/userguide/supervised-learning/classifier.html)             | Predict a label           |
| [Regression](https://apple.github.io/turicreate/docs/userguide/supervised-learning/regression.html)              | Predict numeric values           |
| [Clustering](https://apple.github.io/turicreate/docs/userguide/clustering/)              | Group similar datapoints together|
| [Text Classifier](https://apple.github.io/turicreate/docs/userguide/text_classifier/)         | Analyze sentiment of messages    |


Example: Image classifier with a few lines of code
--------------------------------------------------

If you want your app to recognize specific objects in images, you can build your own model with just a few lines of code:

```python
import turicreate as tc

# Load data 
data = tc.SFrame('photoLabel.sframe')

# Create a model
model = tc.image_classifier.create(data, target='photoLabel')

# Make predictions
predictions = model.predict(data)

# Export to Core ML
model.export_coreml('MyClassifier.mlmodel')
```
 
It's easy to use the resulting model in an [iOS application](https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml):

<p align=""center""><img src=""https://docs-assets.developer.apple.com/published/a2c37bce1f/689f61a6-1087-4112-99d9-bbfb326e3138.png"" alt=""Turi Create"" width=""600""></p>

Supported Platforms
-------------------

Turi Create supports:

* macOS 10.12+
* Linux (with glibc 2.10+)
* Windows 10 (via WSL)

System Requirements
-------------------

Turi Create requires:

* Python 2.7, 3.5, 3.6, 3.7, 3.8
* x86\_64 architecture
* At least 4 GB of RAM

Installation
------------

For detailed instructions for different varieties of Linux see [LINUX\_INSTALL.md](LINUX_INSTALL.md).
For common installation issues see [INSTALL\_ISSUES.md](INSTALL_ISSUES.md).

We recommend using virtualenv to use, install, or build Turi Create. 

```shell
pip install virtualenv
```

The method for installing *Turi Create* follows the
[standard python package installation steps](https://packaging.python.org/installing/).
To create and activate a Python virtual environment called `venv` follow these steps:

```shell
# Create a Python virtual environment
cd ~
virtualenv venv

# Activate your virtual environment
source ~/venv/bin/activate
```
Alternatively, if you are using [Anaconda](https://www.anaconda.com/what-is-anaconda/), you may use its virtual environment:
```shell
conda create -n virtual_environment_name anaconda
conda activate virtual_environment_name
```

To install `Turi Create` within your virtual environment:
```shell
(venv) pip install -U turicreate
```

Documentation
-------------

The package [User Guide](https://apple.github.io/turicreate/docs/userguide) and [API Docs](https://apple.github.io/turicreate/docs/api) contain
more details on how to use Turi Create.

GPU Support
-----------

Turi Create **does not require a GPU**, but certain models can be accelerated 9-13x by utilizing a GPU.

| Linux                     | macOS 10.13+         | macOS 10.14+ discrete GPUs, macOS 10.15+ integrated GPUs |
| :-------------------------|:---------------------|:---------------------------------------------------------|
| Activity Classification   | Image Classification | Activity Classification                                  |
| Drawing Classification    | Image Similarity     | Object Detection                                         |
| Image Classification      | Sound Classification | One Shot Object Detection                                |
| Image Similarity          |                      | Style Transfer                                           |
| Object Detection          |                      |                                                          |
| One Shot Object Detection |                      |                                                          |
| Sound Classification      |                      |                                                          |
| Style Transfer            |                      |                                                          |

macOS GPU support is automatic. For Linux GPU support, see [LinuxGPU.md](LinuxGPU.md).

Building From Source
---------------------

If you want to build Turi Create from source, see [BUILD.md](BUILD.md).

Contributing
------------

Prior to contributing, please review [CONTRIBUTING.md](CONTRIBUTING.md) and do
not provide any contributions unless you agree with the terms and conditions
set forth in [CONTRIBUTING.md](CONTRIBUTING.md).

We want the Turi Create community to be as welcoming and inclusive as possible, and have adopted a [Code of Conduct](CODE_OF_CONDUCT.md) that we expect all community members, including contributors, to read and observe.
",2023-07-07 15:58:24+00:00
great_expectations,great_expectations,great-expectations/great_expectations,Always know what to expect from your data.,https://docs.greatexpectations.io/,False,8575,2023-07-07 09:03:34+00:00,2017-09-11 00:18:46+00:00,1346,77,355,211,0.17.2,2023-06-29 19:12:03+00:00,Apache License 2.0,11192,v0.7.10,245,2019-09-19 20:38:53+00:00,2023-07-07 15:57:21+00:00,2023-07-07 15:54:18+00:00,"
[![PyPI](https://img.shields.io/pypi/v/great_expectations)](https://pypi.org/project/great-expectations/#history)
[![PyPI Downloads](https://img.shields.io/pypi/dm/great-expectations)](https://pypistats.org/packages/great-expectations)
[![Build Status](https://img.shields.io/azure-devops/build/great-expectations/bedaf2c2-4c4a-4b37-87b0-3877190e71f5/1)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&branchName=develop)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5683574.svg)](https://doi.org/10.5281/zenodo.5683574)
[![Twitter Follow](https://img.shields.io/twitter/follow/expectgreatdata?style=social)](https://twitter.com/expectgreatdata)
[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://greatexpectations.io/slack)
[![Contributors](https://img.shields.io/github/contributors/great-expectations/great_expectations)](https://github.com/great-expectations/great_expectations/graphs/contributors)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)

<!-- <<<Super-quickstart links go here>>> -->



<img align=""right"" src=""./docs/docusaurus/static/img/gx-mark-160.png"">

Great Expectations
================================================================================

*Always know what to expect from your data.*

What is GX?
--------------------------------------------------------------------------------

Great Expectations (GX) helps data teams build a shared understanding of their data through quality testing, documentation, and profiling.

Data practitioners know that testing and documentation are essential for managing complex data pipelines. GX makes it possible for data science and engineering teams to quickly deploy extensible, flexible data quality testing into their data stacks. Its human-readable documentation makes the results accessible to technical and nontechnical users.

[See Down with Pipeline Debt!](https://greatexpectations.io/blog/down-with-pipeline-debt-introducing-great-expectations/) for an introduction to our philosophy of pipeline data quality testing.



<!--
--------------------------------------------------
<<<A bunch of logos go here for social proof>>>

--------------------------------------------------
-->

Key features
--------------------------------------------------

### Seamless operation

GX fits into your existing tech stack, and can integrate with your CI/CD pipelines to add data quality exactly where you need it. Connect to and validate your data wherever it already is, so you can focus on honing your Expectation Suites to perfectly meet your data quality needs.

### Start fast

Get useful results quickly even for large data volumes. GX’s Data Assistants provide curated Expectations for different domains, so you can accelerate your data discovery to rapidly deploy data quality throughout your pipelines. Auto-generated Data Docs ensure your DQ documentation will always be up-to-date.



![data_assistant_plot_expectations_and_metrics](./docs/readme_assets/data_assistant_plot_expectations_and_metrics.png)

### Unified understanding

Expectations are GX’s workhorse abstraction: each Expectation declares an expected state of the data. The Expectation library provides a flexible, extensible vocabulary for data quality—one that’s human-readable, meaningful for technical and nontechnical users alike. Bundled into Expectation Suites, Expectations are the ideal tool for characterizing exactly what you expect from your data.

- `expect_column_values_to_not_be_null`
- `expect_column_values_to_match_regex`
- `expect_column_values_to_be_unique`
- `expect_column_values_to_match_strftime_format`
- `expect_table_row_count_to_be_between`
- `expect_column_median_to_be_between`
- ...and [many more](https://greatexpectations.io/expectations)

### Secure and transparent

GX doesn’t ask you to exchange security for your insight. It processes your data in place, on your systems, so your security and governance procedures can maintain control at all times. And because GX’s core is and always will be open source, its complete transparency is the opposite of a black box.

### Data contracts support

Checkpoints are a transparent, central, and automatable mechanism for testing Expectations and evaluating your data quality. Every Checkpoint run produces human-readable Data Docs reporting the results. You can also configure Checkpoints to take Actions based on the results of the evaluation, like sending alerts and preventing low-quality data from moving further in your pipelines.

![Image of data contact support](./docs/readme_assets/data-contract-support.png)

### Readable for collaboration

Everyone stays on the same page about your data quality with GX’s inspectable, shareable, and human-readable Data Docs. You can publish Data Docs to the locations where you need them in a variety of formats, making it easy to integrate Data Docs into your existing data catalogs, dashboards, and other reporting and data governance tools.

![Image of data docs](./docs/readme_assets/datadocs-update.jpg)

Quick start
-------------------------------------------------------------

To see Great Expectations in action on your own data:

You can install it using pip
```
pip install great_expectations
```
and then run

```
import great_expectations as gx

data_context = gx.get_context()
```

(We recommend deploying within a virtual environment. If you’re not familiar with pip, virtual environments, notebooks, or git, you may want to check out the [Supporting Resources](https://docs.greatexpectations.io/docs/terms/supporting_resource/), which will teach you how to get up and running in minutes.)

For full documentation, visit [https://docs.greatexpectations.io/](https://docs.greatexpectations.io/).

If you need help, hop into our [Slack channel](https://greatexpectations.io/slack)&mdash;there are always contributors and other users there.


Integrations
-------------------------------------------------------------------------------
Great Expectations works with the tools and systems that you're already using with your data, including:

<table style=""background-color: #fff;"">
	<thead>
		<tr>
			<th colspan=""2"">Integration</th>
			<th>Notes</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/datahub_logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/integrations/integration_datahub/"">
					DataHub
				</a>
			</td>
			<td>
				Data Catalog
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/aws-glue-logo__1_.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_aws_glue/"">
					AWS Glue
				</a>
			</td>
			<td>
				Data Integration
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/Athena_logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/athena/"">
					Athena
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/datasource_redshift.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/redshift/"">
					AWS Redshift
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/awss3.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_amazon_s3/"">
					AWS S3
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/bigquery.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_google_cloud_platform_and_bigquery/"">
					BigQuery
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/og-databricks.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/"">
					Databricks
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/deepnote_logo_400x.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_deepnote/"">
					Deepnote
				</a>
			</td>
			<td>
				Collaborative data notebook
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/Google_Cloud_logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_google_cloud_platform_and_bigquery/"">
					Google Cloud Platform &#40;GCP&#41;
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/microsoft-azure-blob-storage-logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_azure_blob_storage/"">
					Microsoft Azure Blob Storage
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/microsoft-sql-server-logo-96AF49E2B3-seeklogo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/mssql"">
					Microsoft SQL Server
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/mysql.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/mysql/"">
					MySQL
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/pandas.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/in_memory/pandas/"">
					Pandas
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/postgres.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/postgres/"">
					PostgreSQL
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/snowflake.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/snowflake/"">
					Snowflake
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/spark.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_instantiate_a_data_context_on_an_emr_spark_cluster/"">
					Spark
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/SQLite370.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/sqlite/"">
					SQLite
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/trino-og.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/database/trino/"">
					Trino
				</a>
			</td>
			<td>
				Data Source
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/Apache_Airflow.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_airflow/"">
					Apache Airflow
				</a>
			</td>
			<td>
				Orchestrator
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/flyte_logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_flyte/"">
					Flyte
				</a>
			</td>
			<td>
				Orchestrator
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/meltano-logo.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_meltano/"">
					Meltano
				</a>
			</td>
			<td>
				Orchestrator
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/prefect-logo-gradient-navy.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_prefect/"">
					Prefect
				</a>
			</td>
			<td>
				Orchestrator
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/zenml.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/integrations/integration_zenml/"">
					ZenML
				</a>
			</td>
			<td>
				Orchestrator
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/slack.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/guides/validation/validation_actions/how_to_trigger_slack_notifications_as_a_validation_action/"">
					Slack
				</a>
			</td>
			<td>
				Plugin
			</td>
		</tr>
		<tr>
			<td style=""text-align: center; height: 40px; background-color: #fff;"">
				<img height=""40"" src=""./docs/readme_assets/jupyter.jpg"" />
			</td>
			<td style=""width: 200px;"">
				<a href=""https://docs.greatexpectations.io/docs/tutorials/getting_started/tutorial_create_expectations/#creating-expectations-in-jupyter-notebooks"">
					Jupyter Notebooks
				</a>
			</td>
			<td>
				Utility
			</td>
		</tr>
	</tbody>
</table>


What is GX _not_?
-------------------------------------------------------------

Great Expectations is _not_ a pipeline execution framework. Instead, it integrates seamlessly with DAG execution tools like [Spark]( https://spark.apache.org/), [Airflow](https://airflow.apache.org/), [dbt]( https://www.getdbt.com/), [prefect](https://www.prefect.io/), [dagster]( https://github.com/dagster-io/dagster), [Kedro](https://github.com/quantumblacklabs/kedro), [Flyte](https://flyte.org/), etc. GX carries out your data quality pipeline testing while these tools execute the pipelines.

Great Expectations is _not_ a database or storage software. It processes your data in place, on your existing systems. Expectations and Validation Results that GX produces are metadata about your data.

Great Expectations is _not_ a data versioning tool. If you want to bring your data itself under version control, check out tools like [DVC](https://dvc.org/), [Quilt](https://github.com/quiltdata/quilt), and [lakeFS](https://github.com/treeverse/lakeFS/).

Great Expectations is _not_ a language-agnostic platform. Instead, it follows the philosophy of “take the compute to the data” by using the popular Python language to support native execution of Expectations in pandas, SQL (via SQLAlchemy), and Spark environments.

Great Expectations is _not_ exclusive to Python programming environments. It can be invoked from the command line without a Python environment. However, if you’re working into another ecosystem, you may want to explore ecosystem-specific alternatives such as [assertR](https://github.com/ropensci/assertr) (for R environments) or [TFDV](https://www.tensorflow.org/tfx/guide/tfdv) (for Tensorflow environments).


Who maintains Great Expectations?
-------------------------------------------------------------

Great Expectations OSS is under active development by GX Labs and the Great Expectations community.

What's the best way to get in touch with the Great Expectations team?
--------------------------------------------------------------------------------

If you have questions, comments, or just want to have a good old-fashioned chat about data quality, please hop on our public [Slack channel](https://greatexpectations.io/slack) channel or post in our [GitHub Discussions](https://github.com/great-expectations/great_expectations/discussions).


Can I contribute to the library?
--------------------------------------------------------------------------------

Absolutely. Yes, please. See [Contributing code](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_CODE.md), [Contributing Expectations](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_EXPECTATIONS.md), [Contributing packages](https://github.com/great-expectations/great_expectations/blob/develop/CONTRIBUTING_PACKAGES.md), or [Contribute to Great Expectations documentation](https://github.com/great-expectations/great_expectations/tree/develop/docs), and please don't be shy with questions.
",2023-07-07 15:58:29+00:00
gwf,gwf,gwforg/gwf,"A flexible, pragmatic workflow tool.",https://gwf.app/,False,28,2023-05-26 08:34:56+00:00,2013-06-22 14:50:17+00:00,10,7,8,4,v2.0.2,2023-05-22 11:51:15+00:00,GNU General Public License v3.0,1918,v2.0.2,62,2023-05-22 11:51:15+00:00,2023-06-01 13:13:28+00:00,2023-05-30 18:21:25+00:00,"===
gwf
===

A flexible, pragmatic workflow tool.

|anacondaversion| |anacondadownloads| |cistatus|

*gwf* is a flexible, pragmatic workflow tool for building and running large,
scientific workflows. It runs on Python 3.7+ and is developed at GenomeDK, 
Aarhus University.

Examples
  To get a feeling for what a *gwf* workflow looks like, have a look at a few
  `examples`_.

Getting started
  To quickly get started writing workflows in *gwf* you can read the
  `User's Guide`_.

.. _examples: https://github.com/gwforg/gwf/tree/master/examples
.. _User's Guide: https://gwf.app/guide/tutorial/


.. |cistatus| image:: https://img.shields.io/github/actions/workflow/status/gwforg/gwf/test.yml?label=tests&style=for-the-badge
    :target: https://github.com/gwforg/gwf/actions?query=workflow%3A%22Run+tests%22
    :alt: Build status
.. |anacondaversion| image:: https://img.shields.io/conda/v/gwforg/gwf?style=for-the-badge
    :target: https://anaconda.org/gwforg/gwf
    :alt: Version of Conda package
.. |anacondadownloads| image:: https://img.shields.io/conda/dn/gwforg/gwf?style=for-the-badge
    :target: https://anaconda.org/gwforg/gwf
    :alt: Downloads with Conda
",2023-07-07 15:58:33+00:00
h2o,h2o-3,h2oai/h2o-3,"H2O is an Open Source, Distributed, Fast & Scalable Machine Learning Platform: Deep Learning, Gradient Boosting (GBM) & XGBoost, Random Forest, Generalized Linear Modeling (GLM with Elastic Net), K-Means, PCA, Generalized Additive Models (GAM), RuleFit, Support Vector Machine (SVM), Stacked Ensembles, Automatic Machine Learning (AutoML), etc.",http://h2o.ai,False,6356,2023-07-07 14:56:59+00:00,2014-03-03 16:08:07+00:00,1980,390,163,0,,,Apache License 2.0,32006,last_OK,3641,2017-03-03 23:55:10+00:00,2023-07-07 15:57:51+00:00,2023-07-03 15:52:58+00:00,"# H2O

[![Join the chat at https://gitter.im/h2oai/h2o-3](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/h2oai/h2o-3?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

H2O is an in-memory platform for distributed, scalable machine learning. H2O uses familiar interfaces like R, Python, Scala, Java, JSON and the Flow notebook/web interface, and works seamlessly with big data technologies like Hadoop and Spark. H2O provides implementations of many popular [algorithms](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html) such as Generalized Linear Models (GLM), Gradient Boosting Machines (including XGBoost), Random Forests, Deep Neural Networks, Stacked Ensembles, Naive Bayes, Generalized Additive Models (GAM), Cox Proportional Hazards, K-Means, PCA, Word2Vec, as well as a fully automatic machine learning algorithm ([H2O AutoML](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)). 

H2O is extensible so that developers can add data transformations and custom algorithms of their choice and access them through all of those clients.  H2O models can be [downloaded](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html) and loaded into H2O memory for scoring, or exported into POJO or MOJO format for extemely fast scoring in [production](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html).  More information can be found in the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html).

H2O-3 (this repository) is the third incarnation of H2O, and the successor to [H2O-2](https://github.com/h2oai/h2o-2).  

### Table of Contents

* [Downloading H2O-3](#Downloading)
* [Open Source Resources](#Resources)
    * [Issue Tracking and Feature Requests](#IssueTracking)
    * [List of H2O Resources](#OpenSourceResources)
* [Using H2O-3 Code Artifacts (libraries)](#Artifacts)
* [Building H2O-3](#Building)
* [Launching H2O after Building](#Launching)
* [Building H2O on Hadoop](#BuildingHadoop)
* [Sparkling Water](#Sparkling)
* [Documentation](#Documentation)
* [Citing H2O](#Citing)
* [Community](#Community) / [Advisors](#Advisors) / [Investors](#Investors)

<a name=""Downloading""></a>
## 1. Downloading H2O-3

While most of this README is written for developers who do their own builds, most H2O users just download and use a pre-built version.  If you are a Python or R user, the easiest way to install H2O is via [PyPI](https://pypi.python.org/pypi/h2o) or [Anaconda](https://anaconda.org/h2oai/h2o) (for Python) or [CRAN](https://CRAN.R-project.org/package=h2o) (for R):  

### Python

```bash
pip install h2o
```

### R

```r
install.packages(""h2o"")
```

For the latest stable, nightly, Hadoop (or Spark / Sparkling Water) releases, or the stand-alone H2O jar, please visit: [https://h2o.ai/download](https://h2o.ai/download)

More info on downloading & installing H2O is available in the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html).

<a name=""Resources""></a>
## 2. Open Source Resources

Most people interact with three or four primary open source resources:  **GitHub** (which you've already found), **JIRA** (for bug reports and issue tracking), **Stack Overflow** for H2O code/software-specific questions, and **h2ostream** (a Google Group / email discussion forum) for questions not suitable for Stack Overflow.  There is also a **Gitter** H2O developer chat group, however for archival purposes & to maximize accessibility, we'd prefer that standard H2O Q&A be conducted on Stack Overflow.

<a name=""IssueTracking""></a>
### 2.1 Issue Tracking and Feature Requests

> (Note: There is only one issue tracking system for the project.  GitHub issues are not enabled; you must use JIRA.)

You can browse and create new issues in our open source **JIRA**:  <http://jira.h2o.ai>

*  You can **browse** and search for **issues** without logging in to JIRA:
    1.  Click the `Issues` menu
    1.  Click `Search for issues`
*  To **create** an **issue** (either a bug or a feature request), please create yourself an account first:
    1.  Click the `Log In` button on the top right of the screen
    1.  Click `Create an acccount` near the bottom of the login box
    1.  Once you have created an account and logged in, use the `Create` button on the menu to create an issue
    1.  Create H2O-3 issues in the [PUBDEV](https://0xdata.atlassian.net/projects/PUBDEV/issues) project.  (Note: Sparkling Water questions should be filed under the [SW](https://0xdata.atlassian.net/projects/SW/issues) project.)
*	You can also vote for feature requests and/or other issues. Voting can help H2O prioritize the features that are included in each release.
	1. Go to the [H2O JIRA page](https://0xdata.atlassian.net/).
	2. Click **Log In** to either log in or create an account if you do not already have one.
	3. Search for the feature that you want to prioritize, or create a new feature.
	4. Click on the **Vote for this issue** link. This is located on the right side of the issue under the **People** section.

<a name=""OpenSourceResources""></a>
### 2.2 List of H2O Resources

*  GitHub
    * <https://github.com/h2oai/h2o-3>
*  JIRA -- file bug reports / track issues here
    * The [PUBDEV](https://0xdata.atlassian.net/projects/PUBDEV/issues) project contains issues for the current H2O-3 project)
*  Stack Overflow -- ask all code/software questions here
    * <http://stackoverflow.com/questions/tagged/h2o>
*  Cross Validated (Stack Exchange) -- ask algorithm/theory questions here
    * <https://stats.stackexchange.com/questions/tagged/h2o>
*  h2ostream Google Group -- ask non-code related questions here
    * Web: <https://groups.google.com/d/forum/h2ostream>
    * Mail to: [h2ostream@googlegroups.com](mailto:h2ostream@googlegroups.com)
* Gitter H2O Developer Chat
    * <https://gitter.im/h2oai/h2o-3>    
*  Documentation
    * H2O User Guide (main docs): <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html>
    * All H2O documenation links: <http://docs.h2o.ai>
    * Nightly build page (nightly docs linked in page): <https://s3.amazonaws.com/h2o-release/h2o/master/latest.html>
*  Download (pre-built packages)
    * <http://h2o.ai/download>
*  Jenkins (H2O build and test system)
    * <http://test.h2o.ai>
*  Website
    * <http://h2o.ai>
*  Twitter -- follow us for updates and H2O news!
    * <https://twitter.com/h2oai>

*  Awesome H2O -- share your H2O-powered creations with us
   * <https://github.com/h2oai/awesome-h2o>


<a name=""Artifacts""></a>
## 3. Using H2O-3 Artifacts

Every nightly build publishes R, Python, Java, and Scala artifacts to a build-specific repository.  In particular, you can find Java artifacts in the maven/repo directory.

Here is an example snippet of a gradle build file using h2o-3 as a dependency.  Replace x, y, z, and nnnn with valid numbers.

```
// h2o-3 dependency information
def h2oBranch = 'master'
def h2oBuildNumber = 'nnnn'
def h2oProjectVersion = ""x.y.z.${h2oBuildNumber}""

repositories {
  // h2o-3 dependencies
  maven {
    url ""https://s3.amazonaws.com/h2o-release/h2o-3/${h2oBranch}/${h2oBuildNumber}/maven/repo/""
  }
}

dependencies {
  compile ""ai.h2o:h2o-core:${h2oProjectVersion}""
  compile ""ai.h2o:h2o-algos:${h2oProjectVersion}""
  compile ""ai.h2o:h2o-web:${h2oProjectVersion}""
  compile ""ai.h2o:h2o-app:${h2oProjectVersion}""
}
```

Refer to the latest H2O-3 bleeding edge [nightly build page](http://s3.amazonaws.com/h2o-release/h2o-3/master/latest.html) for information about installing nightly build artifacts.

Refer to the [h2o-droplets GitHub repository](https://github.com/h2oai/h2o-droplets) for a working example of how to use Java artifacts with gradle.

> Note: Stable H2O-3 artifacts are periodically published to Maven Central ([click here to search](http://search.maven.org/#search%7Cga%7C1%7Cai.h2o)) but may substantially lag behind H2O-3 Bleeding Edge nightly builds.


<a name=""Building""></a>
## 4. Building H2O-3

Getting started with H2O development requires [JDK 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements), [Node.js](https://nodejs.org/), [Gradle](https://gradle.org/), [Python](https://www.python.org/) and [R](http://www.r-project.org/).  We use the Gradle wrapper (called `gradlew`) to ensure up-to-date local versions of Gradle and other dependencies are installed in your development directory.

### 4.1. Before building

Building `h2o` requires a properly set up R environment with [required packages](#InstallRPackagesInUnix) and Python environment with the following packages:

```
grip
tabulate
requests
wheel
```

To install these packages you can use [pip](https://pip.pypa.io/en/stable/installing/) or [conda](https://conda.io/).
If you have troubles installing these packages on *Windows*, please follow section [Setup on Windows](#SetupWin) of this guide.
> (Note: It is recommended to use some virtual environment such as [VirtualEnv](https://virtualenv.pypa.io/), to install all packages. )


### 4.2. Building from the command line (Quick Start)

To build H2O from the repository, perform the following steps.


#### Recipe 1: Clone fresh, build, skip tests, and run H2O

```
# Build H2O
git clone https://github.com/h2oai/h2o-3.git
cd h2o-3
./gradlew build -x test

You may encounter problems: e.g. npm missing. Install it:
brew install npm

# Start H2O
java -jar build/h2o.jar

# Point browser to http://localhost:54321

```

#### Recipe 2: Clone fresh, build, and run tests (requires a working install of R)

```
git clone https://github.com/h2oai/h2o-3.git
cd h2o-3
./gradlew syncSmalldata
./gradlew syncRPackages
./gradlew build
```

>**Notes**:
>
> - Running tests starts five test JVMs that form an H2O cluster and requires at least 8GB of RAM (preferably 16GB of RAM).
> - Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE} can be any path):
>  
> ```
> mkdir -p ${WORKSPACE}/Rlibrary
> export R_LIBS_USER=${WORKSPACE}/Rlibrary
> ```

#### Recipe 3:  Pull, clean, build, and run tests

```
git pull
./gradlew syncSmalldata
./gradlew syncRPackages
./gradlew clean
./gradlew build
```

#### Notes

 - We recommend using `./gradlew clean` after each `git pull`.

- Skip tests by adding `-x test` at the end the gradle build command line.  Tests typically run for 7-10 minutes on a Macbook Pro laptop with 4 CPUs (8 hyperthreads) and 16 GB of RAM.

- Syncing smalldata is not required after each pull, but if tests fail due to missing data files, then try `./gradlew syncSmalldata` as the first troubleshooting step.  Syncing smalldata downloads data files from AWS S3 to the smalldata directory in your workspace.  The sync is incremental.  Do not check in these files.  The smalldata directory is in .gitignore.  If you do not run any tests, you do not need the smalldata directory.
- Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE}` can be any path):

  ```
  mkdir -p ${WORKSPACE}/Rlibrary
  export R_LIBS_USER=${WORKSPACE}/Rlibrary
  ```

#### Recipe 4:  Just building the docs

```
./gradlew clean && ./gradlew build -x test && (export DO_FAST=1; ./gradlew dist)
open target/docs-website/h2o-docs/index.html
```

#### Recipe 5:  Building using a Makefile

Root of the git repository contains a Makefile with convenient shortcuts for frequent build targets used in development.
To build `h2o.jar` while skipping tests and also the building of alternative assemblies, execute 

```
make
```

To build `h2o.jar` using the minimal assembly, run
```
make minimal
```

The minimal assembly is well suited for developement of H2O machine learning algorithms. It doesn't bundle some heavyweight
dependencies (like Hadoop) and using it saves build time as well as need to download large libraries from Maven repositories.

<a name=""SetupWin""></a>
### 4.3. Setup on Windows

##### Step 1: Download and install [WinPython](https://winpython.github.io).
  From the command line, validate `python` is using the newly installed package by using `which python` (or `sudo which python`). [Update the Environment variable](https://github.com/winpython/winpython/wiki/Environment) with the WinPython path.

##### Step 2: Install required Python packages:

    pip install grip tabulate wheel

##### Step 3: Install JDK

Install [Java 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements) and add the appropriate directory `C:\Program Files\Java\jdk1.7.0_65\bin` with java.exe to PATH in Environment Variables. To make sure the command prompt is detecting the correct Java version, run:

    javac -version

The CLASSPATH variable also needs to be set to the lib subfolder of the JDK:

    CLASSPATH=/<path>/<to>/<jdk>/lib

##### Step 4. Install Node.js

Install [Node.js](http://nodejs.org/download/) and add the installed directory `C:\Program Files\nodejs`, which must include node.exe and npm.cmd to PATH if not already prepended.

##### Step 5. Install R, the required packages, and Rtools:

Install [R](http://www.r-project.org/) and add the bin directory to your PATH if not already included.

<a name=""InstallRPackagesInUnix""></a>
Install the following R packages:

- [RCurl](http://cran.r-project.org/package=RCurl)
- [jsonlite](http://cran.r-project.org/package=jsonlite)
- [statmod](http://cran.r-project.org/package=statmod)
- [devtools](http://cran.r-project.org/package=devtools)
- [roxygen2](http://cran.r-project.org/package=roxygen2)
- [testthat](http://cran.r-project.org/package=testthat)

To install these packages from within an R session:

```r
pkgs <- c(""RCurl"", ""jsonlite"", ""statmod"", ""devtools"", ""roxygen2"", ""testthat"")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) install.packages(pkg)
}
```
Note that [libcurl](http://curl.haxx.se) is required for installation of the **RCurl** R package.

Note that this packages don't cover running tests, they for building H2O only.

Finally, install [Rtools](http://cran.r-project.org/bin/windows/Rtools/), which is a collection of command line tools to facilitate R development on Windows.
>**NOTE**: During Rtools installation, do **not** install Cygwin.dll.

##### Step 6. Install [Cygwin](https://cygwin.com/setup-x86_64.exe)
**NOTE**: During installation of Cygwin, deselect the Python packages to avoid a conflict with the Python.org package.

###### Step 6b. Validate Cygwin
If Cygwin is already installed, remove the Python packages or ensure that Native Python is before Cygwin in the PATH variable.

##### Step 7. Update or validate the Windows PATH variable to include R, Java JDK, Cygwin.

##### Step 8. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)

If you don't already have a Git client, please install one.  The default one can be found here http://git-scm.com/downloads.  Make sure that command prompt support is enabled before the installation.

Download and update h2o-3 source codes:

    git clone https://github.com/h2oai/h2o-3

##### Step 9. Run the top-level gradle build:

    cd h2o-3
    ./gradlew.bat build

> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.


### 4.4. Setup on OS X

If you don't have [Homebrew](http://brew.sh/), we recommend installing it.  It makes package management for OS X easy.

##### Step 1. Install JDK

Install [Java 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirementsl). To make sure the command prompt is detecting the correct Java version, run:

    javac -version

##### Step 2. Install Node.js:

Using Homebrew:

    brew install node

Otherwise, install from the [NodeJS website](http://nodejs.org/download/).

##### Step 3. Install R and the required packages:

Install [R](http://www.r-project.org/) and add the bin directory to your PATH if not already included.

<a name=""InstallRPackagesInUnix""></a>
Install the following R packages:

- [RCurl](http://cran.r-project.org/package=RCurl)
- [jsonlite](http://cran.r-project.org/package=jsonlite)
- [statmod](http://cran.r-project.org/package=statmod)
- [devtools](http://cran.r-project.org/package=devtools)
- [roxygen2](http://cran.r-project.org/package=roxygen2)
- [testthat](http://cran.r-project.org/package=testthat)

To install these packages from within an R session:

```r
pkgs <- c(""RCurl"", ""jsonlite"", ""statmod"", ""devtools"", ""roxygen2"", ""testthat"")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) install.packages(pkg)
}
```
Note that [libcurl](http://curl.haxx.se) is required for installation of the **RCurl** R package.

Note that this packages don't cover running tests, they for building H2O only.

##### Step 4. Install python and the required packages:

Install python:

    brew install python

Install pip package manager:

    sudo easy_install pip

Next install required packages:

    sudo pip install wheel requests tabulate  

##### Step 5. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)

OS X should already have Git installed. To download and update h2o-3 source codes:

    git clone https://github.com/h2oai/h2o-3

##### Step 6. Run the top-level gradle build:

    cd h2o-3
    ./gradlew build

Note: on a regular machine it may take very long time (about an hour) to run all the tests.

> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.

### 4.5. Setup on Ubuntu 14.04

##### Step 1. Install Node.js

    curl -sL https://deb.nodesource.com/setup_0.12 | sudo bash -
    sudo apt-get install -y nodejs

##### Step 2. Install JDK:

Install [Java 8](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements). Installation instructions can be found here [JDK installation](http://askubuntu.com/questions/56104/how-can-i-install-sun-oracles-proprietary-java-jdk-6-7-8-or-jre). To make sure the command prompt is detecting the correct Java version, run:

    javac -version

##### Step 3. Install R and the required packages:

Installation instructions can be found here [R installation](http://cran.r-project.org).  Click “Download R for Linux”.  Click “ubuntu”.  Follow the given instructions.

To install the required packages, follow the [same instructions as for OS X above](#InstallRPackagesInUnix).

>**Note**: If the process fails to install RStudio Server on Linux, run one of the following:
>
>`sudo apt-get install libcurl4-openssl-dev`
>
>or
>
>`sudo apt-get install libcurl4-gnutls-dev`

##### Step 4. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)

If you don't already have a Git client:

    sudo apt-get install git

Download and update h2o-3 source codes:

    git clone https://github.com/h2oai/h2o-3

##### Step 5. Run the top-level gradle build:

    cd h2o-3
    ./gradlew build

> If you encounter errors, run again using `--stacktrace` for more instructions on missing dependencies.

> Make sure that you are not running as root, since `bower` will reject such a run.

### 4.6. Setup on Ubuntu 13.10

##### Step 1. Install Node.js

    curl -sL https://deb.nodesource.com/setup_16.x | sudo bash -
    sudo apt-get install -y nodejs

##### Steps 2-4. Follow steps 2-4 for Ubuntu 14.04 (above)

### 4.7. Setup on CentOS 7

```
cd /opt
sudo wget --no-cookies --no-check-certificate --header ""Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie"" ""http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz""

sudo tar xzf jdk-7u79-linux-x64.tar.gz
cd jdk1.7.0_79

sudo alternatives --install /usr/bin/java java /opt/jdk1.7.0_79/bin/java 2

sudo alternatives --install /usr/bin/jar jar /opt/jdk1.7.0_79/bin/jar 2
sudo alternatives --install /usr/bin/javac javac /opt/jdk1.7.0_79/bin/javac 2
sudo alternatives --set jar /opt/jdk1.7.0_79/bin/jar
sudo alternatives --set javac /opt/jdk1.7.0_79/bin/javac

cd /opt

sudo wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
sudo rpm -ivh epel-release-7-5.noarch.rpm

sudo echo ""multilib_policy=best"" >> /etc/yum.conf
sudo yum -y update

sudo yum -y install R R-devel git python-pip openssl-devel libxml2-devel libcurl-devel gcc gcc-c++ make openssl-devel kernel-devel texlive texinfo texlive-latex-fonts libX11-devel mesa-libGL-devel mesa-libGL nodejs npm python-devel numpy scipy python-pandas

sudo pip install scikit-learn grip tabulate statsmodels wheel

mkdir ~/Rlibrary
export JAVA_HOME=/opt/jdk1.7.0_79
export JRE_HOME=/opt/jdk1.7.0_79/jre
export PATH=$PATH:/opt/jdk1.7.0_79/bin:/opt/jdk1.7.0_79/jre/bin
export R_LIBS_USER=~/Rlibrary

# install local R packages
R -e 'install.packages(c(""RCurl"",""jsonlite"",""statmod"",""devtools"",""roxygen2"",""testthat""), dependencies=TRUE, repos=""http://cran.rstudio.com/"")'

cd
git clone https://github.com/h2oai/h2o-3.git
cd h2o-3

# Build H2O
./gradlew syncSmalldata
./gradlew syncRPackages
./gradlew build -x test

```


<a name=""Launching""></a>

## 5. Launching H2O after Building

To start the H2O cluster locally, execute the following on the command line:

    java -jar build/h2o.jar

A list of available start-up JVM and H2O options (e.g. `-Xmx`, `-nthreads`, `-ip`), is available in the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html#from-the-command-line).

<a name=""BuildingHadoop""></a>
## 6. Building H2O on Hadoop

Pre-built H2O-on-Hadoop zip files are available on the [download page](http://h2o.ai/download).  Each Hadoop distribution version has a separate zip file in h2o-3.

To build H2O with Hadoop support yourself, first install sphinx for python: `pip install sphinx`
Then start the build by entering  the following from the top-level h2o-3 directory:

    export BUILD_HADOOP=1;
    ./gradlew build -x test;
    ./gradlew dist;

This will create a directory called 'target' and generate zip files there.  Note that `BUILD_HADOOP` is the default behavior when the username is `jenkins` (refer to `settings.gradle`); otherwise you have to request it, as shown above.

To build the zip files only for selected distributions use the `H2O_TARGET` env variable together with `BUILD_HADOOP`, for example:

    export BUILD_HADOOP=1;
    export H2O_TARGET=hdp2.5,hdp2.6
    ./gradlew build -x test;
    ./gradlew dist;

### Adding support for a new version of Hadoop

In the `h2o-hadoop` directory, each Hadoop version has a build directory for the driver and an assembly directory for the fatjar.

You need to:

1.  Add a new driver directory and assembly directory (each with a `build.gradle` file) in `h2o-hadoop`
2.  Add these new projects to `h2o-3/settings.gradle`
3.  Add the new Hadoop version to `HADOOP_VERSIONS` in `make-dist.sh`
4.  Add the new Hadoop version to the list in `h2o-dist/buildinfo.json`


### Secure user impersonation

Hadoop supports [secure user impersonation](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/Superusers.html) through its Java API.  A kerberos-authenticated user can be allowed to proxy any username that meets specified criteria entered in the NameNode's core-site.xml file.  This impersonation only applies to interactions with the Hadoop API or the APIs of Hadoop-related services that support it (this is not the same as switching to that user on the machine of origin).

Setting up secure user impersonation (for h2o):

1.  Create or find an id to use as proxy which has limited-to-no access to HDFS or related services; the proxy user need only be used to impersonate a user
2.  (Required if not using h2odriver) If you are not using the driver (e.g. you wrote your own code against h2o's API using Hadoop), make the necessary code changes to impersonate users (see [org.apache.hadoop.security.UserGroupInformation](http://hadoop.apache.org/docs/r2.8.0/api/org/apache/hadoop/security/UserGroupInformation.html))
3.  In either of Ambari/Cloudera Manager or directly on the NameNode's core-site.xml file, add 2/3 properties for the user we wish to use as a proxy (replace <proxyusername> with the simple user name - not the fully-qualified principal name).
    * `hadoop.proxyuser.<proxyusername>.hosts`: the hosts the proxy user is allowed to perform impersonated actions on behalf of a valid user from
    * `hadoop.proxyuser.<proxyusername>.groups`: the groups an impersonated user must belong to for impersonation to work with that proxy user
    * `hadoop.proxyuser.<proxyusername>.users`: the users a proxy user is allowed to impersonate
    * Example: ```
               <property>
                 <name>hadoop.proxyuser.myproxyuser.hosts</name>
                 <value>host1,host2</value>
               </property>
               <property>
                 <name>hadoop.proxyuser.myproxyuser.groups</name>
                 <value>group1,group2</value>
               </property>
               <property>
                 <name>hadoop.proxyuser.myproxyuser.users</name>
                 <value>user1,user2</value>
               </property>
               ```
4.  Restart core services such as HDFS & YARN for the changes to take effect

Impersonated HDFS actions can be viewed in the hdfs audit log ('auth:PROXY' should appear in the `ugi=` field in entries where this is applicable).  YARN similarly should show 'auth:PROXY' somewhere in the Resource Manager UI.


To use secure impersonation with h2o's Hadoop driver:

*Before this is attempted, see Risks with impersonation, below*

When using the h2odriver (e.g. when running with `hadoop jar ...`), specify `-principal <proxy user kerberos principal>`, `-keytab <proxy user keytab path>`, and `-run_as_user <hadoop username to impersonate>`, in addition to any other arguments needed.  If the configuration was successful, the proxy user will log in and impersonate the `-run_as_user` as long as that user is allowed by either the users or groups configuration property (configured above); this is enforced by HDFS & YARN, not h2o's code.  The driver effectively sets its security context as the impersonated user so all supported Hadoop actions will be performed as that user (e.g. YARN, HDFS APIs support securely impersonated users, but others may not).

#### Precautions to take when leveraging secure impersonation

*  The target use case for secure impersonation is applications or services that pre-authenticate a user and then use (in this case) the h2odriver on behalf of that user.  H2O's Steam is a perfect example: auth user in web app over SSL, impersonate that user when creating the h2o YARN container.
*  The proxy user should have limited permissions in the Hadoop cluster; this means no permissions to access data or make API calls.  In this way, if it's compromised it would only have the power to impersonate a specific subset of the users in the cluster and only from specific machines.
*  Use the `hadoop.proxyuser.<proxyusername>.hosts` property whenever possible or practical.
*  Don't give the proxyusername's password or keytab to any user you don't want to impersonate another user (this is generally *any* user).  The point of impersonation is not to allow users to impersonate each other.  See the first bullet for the typical use case.
*  Limit user logon to the machine the proxying is occurring from whenever practical.
*  Make sure the keytab used to login the proxy user is properly secured and that users can't login as that id (via `su`, for instance)
*  Never set hadoop.proxyuser.<proxyusername>.{users,groups} to '*' or 'hdfs', 'yarn', etc.  Allowing any user to impersonate hdfs, yarn, or any other important user/group should be done with extreme caution and *strongly* analyzed before it's allowed.

#### Risks with secure impersonation

*  The id performing the impersonation can be compromised like any other user id.
*  Setting any `hadoop.proxyuser.<proxyusername>.{hosts,groups,users}` property to '*' can greatly increase exposure to security risk.
*  When users aren't authenticated before being used with the driver (e.g. like Steam does via a secure web app/API), auditability of the process/system is difficult.


```
$ git diff
diff --git a/h2o-app/build.gradle b/h2o-app/build.gradle
index af3b929..097af85 100644
--- a/h2o-app/build.gradle
+++ b/h2o-app/build.gradle
@@ -8,5 +8,6 @@ dependencies {
   compile project("":h2o-algos"")
   compile project("":h2o-core"")
   compile project("":h2o-genmodel"")
+  compile project("":h2o-persist-hdfs"")
 }

diff --git a/h2o-persist-hdfs/build.gradle b/h2o-persist-hdfs/build.gradle
index 41b96b2..6368ea9 100644
--- a/h2o-persist-hdfs/build.gradle
+++ b/h2o-persist-hdfs/build.gradle
@@ -2,5 +2,6 @@ description = ""H2O Persist HDFS""

 dependencies {
   compile project("":h2o-core"")
-  compile(""org.apache.hadoop:hadoop-client:2.0.0-cdh4.3.0"")
+  compile(""org.apache.hadoop:hadoop-client:2.4.1-mapr-1408"")
+  compile(""org.json:org.json:chargebee-1.0"")
 }
```

<a name=""Sparkling""></a>
## 7. Sparkling Water

Sparkling Water combines two open-source technologies: Apache Spark and the H2O Machine Learning platform.  It makes H2O’s library of advanced algorithms, including Deep Learning, GLM, GBM, K-Means, and Distributed Random Forest, accessible from Spark workflows. Spark users can select the best features from either platform to meet their Machine Learning needs.  Users can combine Spark's RDD API and Spark MLLib with H2O’s machine learning algorithms, or use H2O independently of Spark for the model building process and post-process the results in Spark.

**Sparkling Water Resources**:

* [Download page for pre-built packages](http://h2o.ai/download/)
* [Sparkling Water GitHub repository](https://github.com/h2oai/sparkling-water)  
* [README](https://github.com/h2oai/sparkling-water/blob/master/README.md)
* [Developer documentation](https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md)

<a name=""Documentation""></a>
## 8. Documentation

### Documenation Homepage

The main H2O documentation is the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html).  Visit <http://docs.h2o.ai> for the top-level introduction to documentation on H2O projects.


### Generate REST API documentation

To generate the REST API documentation, use the following commands:

    cd ~/h2o-3
    cd py
    python ./generate_rest_api_docs.py  # to generate Markdown only
    python ./generate_rest_api_docs.py --generate_html  --github_user GITHUB_USER --github_password GITHUB_PASSWORD # to generate Markdown and HTML

The default location for the generated documentation is `build/docs/REST`.

If the build fails, try `gradlew clean`, then `git clean -f`.

### Bleeding edge build documentation

Documentation for each bleeding edge nightly build is available on the [nightly build page](http://s3.amazonaws.com/h2o-release/h2o/master/latest.html).


<a name=""Citing""></a>
## 9. Citing H2O

If you use H2O as part of your workflow in a publication, please cite your H2O resource(s) using the following BibTex entry:

### H2O Software

	@Manual{h2o_package_or_module,
	    title = {package_or_module_title},
	    author = {H2O.ai},
	    year = {year},
	    month = {month},
	    note = {version_information},
	    url = {resource_url},
	}

**Formatted H2O Software citation examples**:

- H2O.ai (Oct. 2016). _Python Interface for H2O_, Python module version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).
- H2O.ai (Oct. 2016). _R Interface for H2O_, R package version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).
- H2O.ai (Oct. 2016). _H2O_, H2O version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).

### H2O Booklets

H2O algorithm booklets are available at the [Documentation Homepage](http://docs.h2o.ai/h2o/latest-stable/index.html).

	@Manual{h2o_booklet_name,
	    title = {booklet_title},
	    author = {list_of_authors},
	    year = {year},
	    month = {month},
	    url = {link_url},
	}

**Formatted booklet citation examples**:

Arora, A., Candel, A., Lanford, J., LeDell, E., and Parmar, V. (Oct. 2016). _Deep Learning with H2O_. <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf>.

Click, C., Lanford, J., Malohlava, M., Parmar, V., and Roark, H. (Oct. 2016). _Gradient Boosted Models with H2O_. <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/GBMBooklet.pdf>.

<a name=""Community""></a>
## 10. Community

H2O has been built by a great many number of contributors over the years both within H2O.ai (the company) and the greater open source community.  You can begin to contribute to H2O by answering [Stack Overflow](http://stackoverflow.com/questions/tagged/h2o) questions or [filing bug reports](https://0xdata.atlassian.net/projects/PUBDEV/issues).  Please join us!  


### Team & Committers

```
SriSatish Ambati
Cliff Click
Tom Kraljevic
Tomas Nykodym
Michal Malohlava
Kevin Normoyle
Spencer Aiello
Anqi Fu
Nidhi Mehta
Arno Candel
Josephine Wang
Amy Wang
Max Schloemer
Ray Peck
Prithvi Prabhu
Brandon Hill
Jeff Gambera
Ariel Rao
Viraj Parmar
Kendall Harris
Anand Avati
Jessica Lanford
Alex Tellez
Allison Washburn
Amy Wang
Erik Eckstrand
Neeraja Madabhushi
Sebastian Vidrio
Ben Sabrin
Matt Dowle
Mark Landry
Erin LeDell
Andrey Spiridonov
Oleg Rogynskyy
Nick Martin
Nancy Jordan
Nishant Kalonia
Nadine Hussami
Jeff Cramer
Stacie Spreitzer
Vinod Iyengar
Charlene Windom
Parag Sanghavi
Navdeep Gill
Lauren DiPerna
Anmol Bal
Mark Chan
Nick Karpov
Avni Wadhwa
Ashrith Barthur
Karen Hayrapetyan
Jo-fai Chow
Dmitry Larko
Branden Murray
Jakub Hava
Wen Phan
Magnus Stensmo
Pasha Stetsenko
Angela Bartz
Mateusz Dymczyk
Micah Stubbs
Ivy Wang
Terone Ward
Leland Wilkinson
Wendy Wong
Nikhil Shekhar
Pavel Pscheidl
Michal Kurka
Veronika Maurerova
Jan Sterba
Jan Jendrusak
Sebastien Poirier
Tomáš Frýda
Ard Kelmendi
Yuliia Syzon
```

<a name=""Advisors""></a>
## Advisors

Scientific Advisory Council

```
Stephen Boyd
Rob Tibshirani
Trevor Hastie
```

Systems, Data, FileSystems and Hadoop

```
Doug Lea
Chris Pouliot
Dhruba Borthakur
```

<a name=""Investors""></a>
## Investors

```
Jishnu Bhattacharjee, Nexus Venture Partners
Anand Babu Periasamy
Anand Rajaraman
Ash Bhardwaj
Rakesh Mathur
Michael Marks
Egbert Bierman
Rajesh Ambati
```
",2023-07-07 15:58:37+00:00
hadoopmapreduce,hadoop,apache/hadoop,Apache Hadoop,https://hadoop.apache.org/,False,13645,2023-07-07 10:57:39+00:00,2014-08-28 07:00:08+00:00,8444,998,301,0,,,Apache License 2.0,26735,submarine-0.2.0-RC0,374,2019-06-05 02:54:42+00:00,2023-07-07 15:43:28+00:00,2023-07-06 08:18:14+00:00,"For the latest information about Hadoop, please visit our website at:

   http://hadoop.apache.org/

and our wiki, at:

   https://cwiki.apache.org/confluence/display/HADOOP/
",2023-07-07 15:58:42+00:00
harvester,harvester,HSF/harvester,Harvester project,,False,9,2022-11-10 19:19:09+00:00,2019-07-11 15:15:21+00:00,14,11,13,33,v0.2.33,2023-06-05 22:13:41+00:00,Apache License 2.0,2864,v0.2.33,65,2023-06-05 22:13:41+00:00,2023-06-22 13:50:03+00:00,2023-06-22 13:50:01+00:00,"# Harvester

Harvester is a resource-facing service between WFMS and the collection of pilots.
It is a lightweight stateless service running on a VObox or an edge node of HPC centers
to provide a uniform view for various resources.

For a detailed description and installation instructions, please check out this project's wiki tab:
https://github.com/HSF/harvester/wiki

----------
",2023-07-07 15:58:47+00:00
hi-way,Hi-WAY,marcbux/Hi-WAY,Heterogeneity-incorporating Workflow ApplicationMaster for YARN,,False,25,2021-12-03 17:49:51+00:00,2014-02-13 10:22:16+00:00,5,3,1,1,1.0.0-beta,2015-03-09 17:16:51+00:00,Apache License 2.0,176,1.0.0-beta,1,2015-03-09 17:16:51+00:00,,2017-10-31 08:08:06+00:00,"Hi-WAY
======

<p>
The <b>Hi</b>-WAY <b>W</b>orkflow <b>A</b>pplicationMaster for <b>Y</b>ARN provides the means to execute arbitrary scientific workflows on top of <a href=""http://hadoop.apache.org/"">Apache Hadoop 2 (YARN)</a>.
In this context, scientific workflows are directed acyclic graphs (DAGs), in which nodes are black-box tasks (e.g. Bash scripts, Java programs, Python scripts, compiled C++ executables) processing unstructured data (arbitrary files).
Edges in the graph represent data dependencies between the tasks.
Hi-WAY uses Hadoop's distributed file system HDFS to store the workflow's input, output and intermediate data.
</p>

<p>
Hi-WAY currently supports the workflow languages <a href=""https://github.com/joergen7/cuneiform"">Cuneiform</a>, <a href=""https://usegalaxy.org/"">Galaxy</a>, and <a href=""http://pegasus.isi.edu/wms/docs/latest/creating_workflows.php"">Pegasus DAX</a>, yet can be easily extended to support other workflow languages.
A number of general-purpose and more specialized schedulers are provided, which can take into account data locality when assigning tasks to machines to reduce data transfer times during workflow execution.
When running workflows, Hi-WAY captures comprehensive provenance information, which can be stored as files in HDFS, as well as in a MySQL or Couchbase database.
These provenance traces are evaluated by the scheduler for performance estimation and can be used to re-execute previous workflow runs.
The ApplicationMaster has been tested to scale to more than 600 concurrent tasks and is fault-tolerant in that it is able to restart failed tasks.
</p>

<p>
The Installation instructions can be found <a href=""https://github.com/marcbux/Hi-WAY/wiki/Installation-Instructions"">here</a>.
</p>
",2023-07-07 15:58:51+00:00
apachehive,hive,apache/hive,Apache Hive,https://hive.apache.org/,False,4888,2023-07-07 10:07:44+00:00,2009-05-21 02:31:01+00:00,4446,334,255,0,,,Apache License 2.0,16855,storage-release-2.8.1-rc2,84,2021-07-29 21:42:29+00:00,2023-07-07 15:12:34+00:00,2023-07-07 15:11:03+00:00,"Apache Hive (TM)
================
[![Master Build Status](https://travis-ci.org/apache/hive.svg?branch=master)](https://travis-ci.org/apache/hive/branches)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.hive/hive/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.hive%22)

The Apache Hive (TM) data warehouse software facilitates reading,
writing, and managing large datasets residing in distributed storage
using SQL. Built on top of Apache Hadoop (TM), it provides:

* Tools to enable easy access to data via SQL, thus enabling data
  warehousing tasks such as extract/transform/load (ETL), reporting,
  and data analysis

* A mechanism to impose structure on a variety of data formats

* Access to files stored either directly in Apache HDFS (TM) or in other
  data storage systems such as Apache HBase (TM)

* Query execution using Apache Hadoop MapReduce or Apache Tez frameworks.

Hive provides standard SQL functionality, including many of the later
2003 and 2011 features for analytics.  These include OLAP functions,
subqueries, common table expressions, and more.  Hive's SQL can also be
extended with user code via user defined functions (UDFs), user defined
aggregates (UDAFs), and user defined table functions (UDTFs).

Hive users have a choice of 3 runtimes when executing SQL queries.
Users can choose between Apache Hadoop MapReduce or Apache Tez
frameworks as their execution backend. MapReduce is a
mature framework that is proven at large scales. However, MapReduce
is a purely batch framework, and queries using it may experience
higher latencies (tens of seconds), even over small datasets. Apache
Tez is designed for interactive query, and has substantially reduced
overheads versus MapReduce.

Users are free to switch back and forth between these frameworks
at any time. In each case, Hive is best suited for use cases
where the amount of data processed is large enough to require a
distributed system.

Hive is not designed for online transaction processing. It is best used
for traditional data warehousing tasks.  Hive is designed to maximize
scalability (scale out with more machines added dynamically to the Hadoop
cluster), performance, extensibility, fault-tolerance, and
loose-coupling with its input formats.


General Info
============

For the latest information about Hive, please visit out website at:

  http://hive.apache.org/


Getting Started
===============

- Installation Instructions and a quick tutorial:
  https://cwiki.apache.org/confluence/display/Hive/GettingStarted

- Instructions to build Hive from source:
  https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-BuildingHivefromSource

- A longer tutorial that covers more features of HiveQL:
  https://cwiki.apache.org/confluence/display/Hive/Tutorial

- The HiveQL Language Manual:
  https://cwiki.apache.org/confluence/display/Hive/LanguageManual


Requirements
============

Java
------

| Hive Version  | Java Version  |
| ------------- |:-------------:|
| Hive 1.0      | Java 6        |
| Hive 1.1      | Java 6        |
| Hive 1.2      | Java 7        |
| Hive 2.x      | Java 7        |
| Hive 3.x      | Java 8        |
| Hive 4.x      | Java 8        |


Hadoop
------

- Hadoop 1.x, 2.x
- Hadoop 3.x (Hive 3.x)


Upgrading from older versions of Hive
=====================================

- Hive includes changes to the MetaStore schema. If
  you are upgrading from an earlier version of Hive it is imperative
  that you upgrade the MetaStore schema by running the appropriate
  schema upgrade scripts located in the scripts/metastore/upgrade
  directory.

- We have provided upgrade scripts for MySQL, PostgreSQL, Oracle,
  Microsoft SQL Server, and Derby databases. If you are using a
  different database for your MetaStore you will need to provide
  your own upgrade script.

Useful mailing lists
====================

1. user@hive.apache.org - To discuss and ask usage questions. Send an
   empty email to user-subscribe@hive.apache.org in order to subscribe
   to this mailing list.

2. dev@hive.apache.org - For discussions about code, design and features.
   Send an empty email to dev-subscribe@hive.apache.org in order to
   subscribe to this mailing list.

3. commits@hive.apache.org - In order to monitor commits to the source
   repository. Send an empty email to commits-subscribe@hive.apache.org
   in order to subscribe to this mailing list.
",2023-07-07 15:58:55+00:00
hivemall,incubator-hivemall,apache/incubator-hivemall,Mirror of Apache Hivemall (incubating),,True,310,2023-04-12 06:36:00+00:00,2016-09-15 07:00:08+00:00,121,34,29,0,,,Apache License 2.0,2037,v0.6.0-rc1,10,2019-11-28 16:43:43+00:00,2023-04-23 15:03:20+00:00,2022-09-06 15:55:16+00:00,"<!--
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  ""License""); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->

Apache Hivemall: Hive scalable machine learning library
=======================================================
[![Build Status](https://travis-ci.org/apache/incubator-hivemall.svg?branch=master)](https://travis-ci.org/apache/incubator-hivemall)
[![Documentation Status](https://img.shields.io/:docs-latest-green.svg)](http://hivemall.incubator.apache.org/userguide/)
[![License](http://img.shields.io/:license-Apache_v2-blue.svg)](https://github.com/apache/incubator-hivemall/blob/master/LICENSE)
[![Coverage Status](https://coveralls.io/repos/github/apache/incubator-hivemall/badge.svg?branch=master)](https://coveralls.io/github/apache/incubator-hivemall?branch=master)
[![Twitter Follow](https://img.shields.io/twitter/follow/ApacheHivemall.svg?style=social&label=Follow)](https://twitter.com/ApacheHivemall)

[Apache Hivemall](http://hivemall.incubator.apache.org/) is a scalable machine learning library that runs on Apache Hive, Apache Spark, and Apache Pig. Hivemall is designed to be scalable to the number of training instances as well as the number of training features.

[<img src=""src/site/resources/images/apache-incubator-logo.png"" alt=""Apache Incubator"" width=200>](http://hivemall.incubator.apache.org/)

Usage
-----

[![Hivemall](https://gist.githubusercontent.com/myui/d29241262f9313dec706/raw/caead313efd829b42a4a4183285e8b53cf26ab62/hadoopsummit14_slideshare.png)](http://www.slideshare.net/myui/dots20161029-myui/11)

Find more examples on [our user guide](http://hivemall.incubator.apache.org/userguide/index.html) and find a brief introduction to Hivemall in [this slide](http://www.slideshare.net/myui/hadoopsummit16-myui).

Support
-------

Support is through [user@hivemall.incubator.apache.org](http://hivemall.incubator.apache.org/mail-lists.html), not by a direct e-mail.

Contributing
------------

If you are planning to contribute to this repository, we first request you to create an issue at [our JIRA page](https://issues.apache.org/jira/projects/HIVEMALL) even if the topic is not related to source code itself (e.g., documentation, new idea and proposal).

All Hivemall functions are defined under [resources/ddl](resources/ddl). In order to update the definition files, the following script helps inserting function name and class path of your new UDF:

```
$ ./bin/update_ddls.sh
```

Moreover, don't forget to update function list in the document as well:

```
$ ./bin/update_func_md.sh
```

Note that, before creating a pull request including Java code, please make sure your code follows our coding conventions by applying formatter:

```
$ ./bin/format_code.sh
```
",2023-07-07 15:59:00+00:00
hpcflow,hpcflow,LightForm-group/hpcflow,"An automated simulate, process, archive workflow on high performance computing (HPC) systems.",,False,1,2023-05-03 13:36:07+00:00,2019-11-06 08:43:05+00:00,0,4,2,14,,,,281,v0.1.16,16,2021-06-06 16:54:57+00:00,2023-05-03 13:36:07+00:00,2021-06-06 16:54:57+00:00,"# hpcflow

## Installation

### Package installation

#### Install with `pip`

`hpcflow` is a Python package that can be installed from PyPI by typing:

`pip install --user hpcflow`

### Set up

Some console entry points (scripts/executables) will be added to your system as part of the package installation. These are:

1. `hpcflow`, and all of its sub-commands (found by typing `hpcflow --help` at the command line)
2. `hfmake`, which is an alias for `hpcflow make`.
3. `hfsub`, which is an alias for `hpcflow submit`.
4. `hfstat`, which is an alias for `hpcflow stat`.

Global settings are stored in a YAML file named `_config.yml`, which is stored in the `hpcflow` data directory. If it does not exist, the data directory is generated whenever `hpcflow` is run. By default, the data directory is placed under the user's home directory, as: `~/.hpcflow`, but this can be customised using the environment variable: `HPCFLOW_DATA_DIR`.

Inheritable YAML profile files can be added to the data directory under a sub-directory named `profiles`, i.e. at `~/.hpcflow/profiles/` if the data directory is located in the default place.

## Workflow parametrisation

There are four equivalent methods to generate a new `Workflow`, all of which are accessible via the API, and three of which are accessible via the CLI:

1. YAML profile files (API and CLI) -- *recommended*
2. JSON file (API and CLI)
3. JSON string (API and CLI)
4. Python `dict` (API only)

In general, instantiation of a `Workflow` requires two parts: a list of command groups, and a list of variable definitions that are referenced within the command groups. Each command group can have the following keys. More details can be found in the docstring of the `CommandGroup` constructor (`hpcflow.models.CommandGroup.__init__`).

|     Key      | Required |                                      Description                                       | 
| ------------ | -------- | -------------------------------------------------------------------------------------- |
| `commands`   | ✅        | List of commands to execute within one jobscript.                                      |
| `exec_order` | -        | Execution order of this command group.                                                 |
| `sub_order`  | -        | Indexes a command group that has sibling command groups with the same execution order. |
| `options`    | -        | Scheduler options to be passed directly to the jobscript.                                        |
| `directory` | - | The working directory for this command group. |
| `modules` | - | List of modules to load. |
| `job_array` | - | Whether this command group is submitted as a job array. |
| `profile_name` | - | If this command group was submitted as part of a job profile file, this is the `profile_name` of that job profile. |
| `profile_order` | - | If this command group was submitted as part of a job profile file, this is the `profile_order` of that job profile. |

### Workflow channels

If a command group shares its execution order index (`exec_order`) with other command groups, this set of command groups must be distinguished using the `sub_order` key. We use the term ""channel"" to denote a distinct `sub_order` index. So, if, for a given execution order, there are two command groups (with `sub_order`s of `0` and `1`), we say the Workflow has two channels at that execution order. The maximum number of channels in the Workflow is the number of channels for the initial command groups (i.e. with `exec_order=0`). Channels may merge together in subsequent command groups, but can never split apart.

TODO: rename `sub_order` key to `channel`? Is there any value in maintaining these two terms?

### Generating a `Workflow` with YAML profile files (recommended)

Using profile files is a convenient way to use `hpcflow` from the command line, since you can set up a hierarchy of common, reusable profile files that are inheritable. For instance, in your working directory (where, for example, your simulation input files reside), you may have two profiles: `run.yml` and `process.yml`. Perhaps the software you need to use to run the simulations (say, `DAMASK`) is also required for the processing of results. In this case you could add to your `hpcflow` data directory a common profile, say `damask.yml`, which includes things like loading the correct modules for DAMASK. Then in both of the profiles in your working directory (`run.yml` and `process.yml`), you can import/inherit from the common `DAMASK` profile by adding the YAML line: `inherit: damask`.

There are two additional settings that need to be specified when using YAML profile files instead of passing the workflow `dict` directly to the `Workflow` constructor. These are the `profile_name` and the `profile_order` parameters. These two settings can optionally be encoded in the file names of the profile files themselves. The format of profile file names is customisable, using the `profile_filename_fmt` key in the configuration file (`_config.yml`). When `hpcflow` searches for profiles in the working directory, the file names must match this format. All profiles must follow this format, including inheritable profiles that are contained in the `hpcflow` data directory. If either of `profile_name` or `profile_order` are note encoded in the profile file names, then they must be specified within the profile itself.

#### Specifying task ranges

Using the command-line interface, the command `hpcflow submit` (or `hfsub`) has an option `-t` (or `--task-ranges`), which allows us to specify which tasks should be submitted. If the `-t` option is omitted, then all tasks will be submitted. If the `-t` option is specified, then it must be a list of task ranges, where each task range has the format `n[-m[:s]]`, where `n` is the first task index to submit, `m` (optional) is the last, and `s` (optional) is the step size. The number of task ranges specified must be equal to the number of channels in the Workflow.

If multiple channels merge into one command group, the channel of the resulting command group will be the lowest of the channels of the parent command groups.

#### Submit-time versus run-time

There are two times of importance in the life cycle of a Workflow command group. To enable automation of the Workflow, all command groups are generally submitted at the same time, with holding rules that prevent their execution until (part of) the parent command group has completed.

#### Profile settings

|      Name       | Required |                                                    Description                                                     | Filename encodable? |
| --------------- | -------- | ------------------------------------------------------------------------------------------------------------------ | ------------------- |
| `profile_name`  | ️️✅    | A short name for the profile. For instance, `run` or `process`.                                                    | ✅                  |
| `profile_order` | ✅       | The relative execution order of the profile.                                                                       | ✅                  |
| `options`       | -        | Common scheduler options to be passed directly to the jobscripts for each command group belonging to this profile. | -                   |
| `directory`     | -        | The working directory for all command groups belonging to this profile.                                            | -                   |
| `variable_definitions` | - | Definitions for variables that appear in the command group commands. | - |
| `variable_scope` | - | Variable scope within the variable lookup file for this job profile  | - |
| `modules` | - | List of modules to load. |
| `job_array` | - | Whether this command group is submitted as a job array. |

Note that the `options`, `directory`, `modules` and `job_array` keys may be specified at the *profile* level in addition to the *command group* level. This is a useful convenience. If these keys are also specified within a command group, the command group keys take precedence.

The code associated with generating `Workflow`s from YAML profile files is mostly found in the `hpcflow.profiles.py` module.

### Generating a `Workflow` with a JSON file, JSON string or from a Python `dict`

Since JSON objects closely match the structure of Python `dict`s, these cases are all similar.

## Command-line interface

Here is a list of `hpcflow` (sub-)commands. Type `<command> --help` to show the options/arguments for a given sub-command.

|       Command       | Alias | Implemented |                                               Description                                                |
| ------------------- | ----- | ----------- | -------------------------------------------------------------------------------------------------------- |
| `hcpflow --help`    | -     | ✅          | Show the command-line help.                                                                              |
| `hpcflow --version` | -     | ️️️️️️✅    | Show the version of `hpcflow`.                                                                           |
| `hpcflow --info`    | -     | ❌           | Show information about the current `hpcflow` installation, including where the data directory is located. |
| `hpcflow --config`  | -     | ❌           | Show the contents and location of the `hpcflow` global configuration file.                               |
| `hpcflow make`      |`hfmake`| ✅           | Generate a Workflow. | 
| `hpcflow submit`    |`hfsub` | ✅           | Generate a Workflow if it doesn't exist and then submit (write jobscripts and execute) all command groups in the current working directory. | 
|`hpcflow install-example` | - | ❌           | Install an example set of profiles from the `examples` directory (files with the same name will be overwritten). |
| `hpcflow add-inputs` | - | ❌           | Add an example set of input files to be used with the example profile the `examples` directory. This involves merging `_variable_lookup.yml` and `_archive_locations.yml` from the example into the user's profile directory. | 
| `hpcflow write-cmd` | - | ✅           | Write the command file for a given jobscript. This script is invoked within jobscripts at execution-time and is not expected to be invoked by the user. The `write-cmd` process involves opening the JSON representation of the profile set and resolving variables for the set of commands within a given command group. |
| `hpcflow show-stats` | - | ❌           | Show statistics like CPU walltime for the profile set in the current directory. |
| `hpcflow clean` | - | ✅           | Remove all `hpcflow`-generated files from the current directory (use confirmation?). |
| `hpcflow stat` | `hfstat` | ❌           | Show status of running tasks and how many completed tasks within this directory. |
| `hpcflow kill` | `hfkill` | ❌           | Kill one or more jobscripts associated with a workflow. |
| `hpcflow archive` | - | ✅           | Archive the working directory of a given command group. |

### Commands that interact with the local database

- Of the above commands, the following interact with the local database:
    - `hpcflow make` (or `hfmake`)
    - `hpcflow submit` (or `hfsub`)
    - `hpcflow write-cmd`
    - `hpcflow show-stats`
    - `hpcflow stat` (or `hfstat`)
- Invoking any of these commands should therefore set up the relevant database connection.
- Only `hpcflow make` and `hpcflow submit` may invoke the `create_all(engine)` method, all other commands should fail if no database exists.

## Other notes:

- If using Dropbox archiving, make sure, if necessary, a proxy is correctly configured to allow the machine to communicate with the outside world.
- If using Windows, Windows must be at least version 1703, and switched to ""Developer mode"" (reason is creating a symlink; this could be disabled, but we also need developer mode for WSL I think.)

## Database schema

Here is a [link](https://app.sqldbm.com/MySQL/Share/zcKfw4XqIlAxd5gBe-VZtEGFrngIE8md_DYjF4jNYw0) to view the local SQLite database schema using the `sqldbm.com` web app.

## Glossary

**PyPI**: Python package index --- a public repository for Python packages and the default source location when packages are installed with `pip install <package_name>`.

**API**: Application programming interface --- an interface that allows other Python packages to conveniently interact with this package (`hpcflow`).

**CLI**: Command-line interface --- the interface that allows us to interact with `hpcflow` from the command line (i.e. shell).

**YAML**: YAML Ain't Markup Language (pronounced to rhyme with ""camel"") --- a human-readable data-serialisation language, commonly used for configuration files. It is a superset of JSON.

**JSON**: JavaScript Object Notation (pronounced like the male name ""Jason"") --- a human-readable data-serialisation language.
",2023-07-07 15:59:04+00:00
hstreamdb,hstream,hstreamdb/hstream,"HStreamDB is an open-source, cloud-native streaming database for IoT and beyond. Modernize your data stack for real-time applications.",https://hstream.io/,False,618,2023-07-07 10:39:42+00:00,2020-08-31 09:42:17+00:00,55,23,16,16,v0.16.0,2023-07-07 04:12:29+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",1387,v0.16.0,41,2023-07-07 04:12:29+00:00,2023-07-07 10:41:43+00:00,2023-07-07 09:39:33+00:00,"[![GitHub top language](https://img.shields.io/github/languages/top/hstreamdb/hstream)](https://www.haskell.org/)
[![ci](https://github.com/hstreamdb/hstream/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/hstreamdb/hstream/actions/workflows/ci.yml)
[![Docker Pulls](https://img.shields.io/docker/pulls/hstreamdb/hstream)](https://hub.docker.com/r/hstreamdb/hstream)
[![Slack](https://img.shields.io/badge/Slack-HStreamDB-39AE85?logo=slack)](https://slack-invite.hstream.io/)
[![Twitter](https://img.shields.io/badge/Follow-HStreamDB-1DA1F2?logo=twitter)](https://twitter.com/HStreamDB)
[![Community](https://img.shields.io/badge/Community-HStreamDB-yellow?logo=github)](https://github.com/hstreamdb/hstream/discussions)
[![YouTube](https://img.shields.io/badge/Subscribe-EMQ-FF0000?logo=youtube)](https://www.youtube.com/channel/UC5FjR77ErAxvZENEWzQaO5Q)

# HStreamDB

HStreamDB is an open-source, cloud-native streaming database for IoT and beyond.
Modernize your data stack for real-time applications.

![hstream-db](https://assets.emqx.com/images/hstreamdb-hstream-github-readme-2022121402.png)

## Main Features

- **Push real-time data to your apps**

  By subscribing to streams in HStreamDB, any update of the data stream will be
  pushed to your apps in real-time, and this promotes your apps to be more
  responsive.

  You can also replace message brokers with HStreamDB and everything you do with
  message brokers can be done better with HStreamDB.

- **Stream processing with familiar SQL**

  HStreamDB provides built-in support for event time-based stream processing.
  You can use your familiar SQL to perform basic filtering and transformation
  operations, statistics and aggregation based on multiple kinds of time windows
  and even joining between multiple streams.

- **Easy integration with a variety of external systems**

  With connectors provided, you can easily integrate HStreamDB with other
  external systems, such as MQTT Broker, MySQL, Redis and ElasticSearch. More
  connectors will be added.

- **Real-time query based on live materialized views**

  With maintaining materialized views incrementally, HStreamDB enables you to
  gain ahead-of-the-curve data insights that respond to your business quickly.

- **Reliable persistent storage with low latency**

  With an optimized storage design based on [LogDevice](https://logdevice.io/),
  not only can HStreamDB provide reliable and persistent storage but also
  guarantee excellent performance despite large amounts of data written to it.

- **Seamless scaling and high availability**

  With the architecture that separates compute from storage, both compute and
  storage layers of HStreamDB can be independently scaled seamlessly. And with
  the consensus algorithm based on the optimized Paxos, data is securely
  replicated to multiple nodes which ensures the high availability of our
  system.

For more information, please visit [HStreamDB homepage](https://hstream.io).

## Quickstart

**For detailed instructions, follow
[HStreamDB quickstart](https://docs.hstream.io/start/quickstart-with-docker.html).**

1. [Install HStreamDB](https://docs.hstream.io/start/quickstart-with-docker.html#installation).
2. [Start a local standalone HStream server](https://docs.hstream.io/start/quickstart-with-docker.html#start-hstreamdb-services).
3. [Start HStreamDB's interactive CLI](https://docs.hstream.io/start/quickstart-with-docker.html#start-hstreamdb-s-interactive-sql-cli)
   and
   [create your first stream](https://docs.hstream.io/start/quickstart-with-docker.html#create-a-stream).
4. [Run a continuous query](https://docs.hstream.io/start/quickstart-with-docker.html#run-a-continuous-query-over-the-stream).
5. [Start another interactive CLI](https://docs.hstream.io/start/quickstart-with-docker.html#start-another-cli-session),
   then
   [insert some data into the stream and get query results](https://docs.hstream.io/start/quickstart-with-docker.html#insert-data-into-the-stream).

## Documentation

Check out [the documentation](https://hstream.io/docs/en/latest/).

## Community, Discussion, Construction and Support

You can reach the HStreamDB community and developers via the following channels:

- [Slack](https://slack-invite.hstream.io)
- [Twitter](https://twitter.com/HStreamDB)
- [Reddit](https://www.reddit.com/r/HStreamDB)

Please submit any bugs, issues, and feature requests to
[hstreamdb/hstream](https://github.com/hstreamdb/hstream/issues).

## How to build (for developers only)

**Pre-requirements**

1. You have `python3` and `docker` installed.
2. [Optional] You can run `docker` without `sudo`. For details, see
   [this docs](https://docs.docker.com/engine/install/linux-postinstall/)
3. [Optional] You can clone the GitHub repository by ssh key.

**Get the source code**

```sh
git clone --recursive git@github.com:hstreamdb/hstream.git
cd hstream/
```

**Update images**

```sh
script/dev-tools update-images
```

**Start all required services**

You must have all required services started before entering an interactive shell
to do further development (especially for running tests).

```sh
script/dev-tools start-services
```

To see information about all started services, run

```sh
script/dev-tools info
```

> _All datas are stored under `your-project-root/local-data`_

**Enter in an interactive shell**

```sh
script/dev-tools shell
```

**Build as other Haskell projects**

_Inside the interactive shell, you have all extra dependencies installed._

```
I have no name!@649bc6bb75ed:~$ cabal update
I have no name!@649bc6bb75ed:~$ make
I have no name!@649bc6bb75ed:~$ cabal build all
```

## License

HStreamDB is under the BSD 3-Clause license. See the
[LICENSE](https://github.com/hstreamdb/hstream/blob/master/LICENSE) file for
details.

## Acknowledgments

- Thanks [LogDevice](https://logdevice.io/) for the powerful storage engine.
",2023-07-07 15:59:08+00:00
htbac,htbac,radical-cybertools/htbac,High throughput binding affinity calculator,,False,2,2019-12-21 22:27:33+00:00,2018-01-08 21:14:27+00:00,1,6,4,1,,,MIT License,636,v0.2,1,2018-08-12 16:18:05+00:00,,2019-04-15 20:40:08+00:00,"
# High Throughput Binding Affinity Calculator 

* High performance bio-simulation framework for running molecular dynamics simulations locally or
on supercomputers. Create a workflow for your specific MD requirements and submit jobs to a
cluster of [choice](https://radicalpilot.readthedocs.io/en/latest/resources.html#chapter-resources) 

* For documentation please visit [readthedocs](https://htbac.readthedocs.io/en/latest/)

## Features:

* HTBAC supports combinatorial variable assignment: any parameter in the simulation can
take a list of values and a product of all variable combinations will be executed. For 
example you can have `system` attribute be a list of systems like `[""protein-drug-1"", 
""protein-drug-2""]` and your protocol will run for both systems. Or you want to experiment 
with certain parameters, like restrain strength: just tell the framework that your 
property will be a variable. 

## Examples:

* [Jupyter Notebook](https://github.com/radical-cybertools/htbac/blob/master/examples/htbac.ipynb)
* Example scripts can be found [here](https://github.com/kristofarkas/abigail-experiments).
",2023-07-07 15:59:13+00:00
apachehudi,hudi,apache/hudi,"Upserts, Deletes And Incremental Processing on Big Data.",https://hudi.apache.org/,False,4339,2023-07-07 07:05:13+00:00,2016-12-14 15:53:41+00:00,2004,1182,376,23,release-0.13.1,2023-05-23 06:44:01+00:00,Apache License 2.0,4418,release-0.13.1,88,2023-05-23 06:21:44+00:00,2023-07-07 15:18:53+00:00,2023-07-07 03:16:51+00:00,"
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the ""License""); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

# Apache Hudi

Apache Hudi (pronounced Hoodie) stands for `Hadoop Upserts Deletes and Incrementals`. Hudi manages the storage of large
analytical datasets on DFS (Cloud stores, HDFS or any Hadoop FileSystem compatible storage).

<img src=""https://hudi.apache.org/assets/images/hudi-logo-medium.png"" alt=""Hudi logo"" height=""80px"" align=""right"" />

<https://hudi.apache.org/>

[![Build](https://github.com/apache/hudi/actions/workflows/bot.yml/badge.svg)](https://github.com/apache/hudi/actions/workflows/bot.yml)
[![Test](https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_apis/build/status/apachehudi-ci.hudi-mirror?branchName=master)](https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_build/latest?definitionId=3&branchName=master)
[![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.hudi/hudi/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.hudi%22)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/apache/hudi)
[![Join on Slack](https://img.shields.io/badge/slack-%23hudi-72eff8?logo=slack&color=48c628&label=Join%20on%20Slack)](https://join.slack.com/t/apache-hudi/shared_invite/zt-1e94d3xro-JvlNO1kSeIHJBTVfLPlI5w)
![Twitter Follow](https://img.shields.io/twitter/follow/ApacheHudi)

## Features

* Upsert support with fast, pluggable indexing
* Atomically publish data with rollback support
* Snapshot isolation between writer & queries
* Savepoints for data recovery
* Manages file sizes, layout using statistics
* Async compaction of row & columnar data
* Timeline metadata to track lineage
* Optimize data lake layout with clustering
 
Hudi supports three types of queries:
 * **Snapshot Query** - Provides snapshot queries on real-time data, using a combination of columnar & row-based storage (e.g [Parquet](https://parquet.apache.org/) + [Avro](https://avro.apache.org/docs/current/mr.html)).
 * **Incremental Query** - Provides a change stream with records inserted or updated after a point in time.
 * **Read Optimized Query** - Provides excellent snapshot query performance via purely columnar storage (e.g. [Parquet](https://parquet.apache.org/)).

Learn more about Hudi at [https://hudi.apache.org](https://hudi.apache.org)

## Building Apache Hudi from source

Prerequisites for building Apache Hudi:

* Unix-like system (like Linux, Mac OS X)
* Java 8 (Java 9 or 10 may work)
* Git
* Maven (>=3.3.1)

```
# Checkout code and build
git clone https://github.com/apache/hudi.git && cd hudi
mvn clean package -DskipTests

# Start command
spark-3.2.3-bin-hadoop3.2/bin/spark-shell \
  --jars `ls packaging/hudi-spark-bundle/target/hudi-spark3.2-bundle_2.12-*.*.*-SNAPSHOT.jar` \
  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'
```

To build for integration tests that include `hudi-integ-test-bundle`, use `-Dintegration-tests`.

To build the Javadoc for all Java and Scala classes:
```
# Javadoc generated under target/site/apidocs
mvn clean javadoc:aggregate -Pjavadocs
```

### Build with different Spark versions

The default Spark 2.x version supported is 2.4.4. The default Spark 3.x version, corresponding to `spark3` profile is
3.4.0. The default Scala version is 2.12. Refer to the table below for building with different Spark and Scala versions.

| Maven build options       | Expected Spark bundle jar name               | Notes                                            |
|:--------------------------|:---------------------------------------------|:-------------------------------------------------|
| (empty)                   | hudi-spark3.2-bundle_2.12                    | For Spark 3.2.x and Scala 2.12 (default options) |
| `-Dspark2.4 -Dscala-2.11` | hudi-spark2.4-bundle_2.11                    | For Spark 2.4.4 and Scala 2.11                   |
| `-Dspark3.0`              | hudi-spark3.0-bundle_2.12                    | For Spark 3.0.x and Scala 2.12                   |
| `-Dspark3.1`              | hudi-spark3.1-bundle_2.12                    | For Spark 3.1.x and Scala 2.12                   |
| `-Dspark3.2`              | hudi-spark3.2-bundle_2.12                    | For Spark 3.2.x and Scala 2.12 (same as default) |
| `-Dspark3.3`              | hudi-spark3.3-bundle_2.12                    | For Spark 3.3.x and Scala 2.12                   |
| `-Dspark3.4`              | hudi-spark3.4-bundle_2.12                    | For Spark 3.4.x and Scala 2.12                   |
| `-Dspark2 -Dscala-2.11`   | hudi-spark-bundle_2.11 (legacy bundle name)  | For Spark 2.4.4 and Scala 2.11                   |
| `-Dspark2 -Dscala-2.12`   | hudi-spark-bundle_2.12 (legacy bundle name)  | For Spark 2.4.4 and Scala 2.12                   |
| `-Dspark3`                | hudi-spark3-bundle_2.12 (legacy bundle name) | For Spark 3.4.x and Scala 2.12                   |

For example,
```
# Build against Spark 3.2.x
mvn clean package -DskipTests

# Build against Spark 3.4.x
mvn clean package -DskipTests -Dspark3.4

# Build against Spark 2.4.4 and Scala 2.11
mvn clean package -DskipTests -Dspark2.4 -Dscala-2.11
```

#### What about ""spark-avro"" module?

Starting from versions 0.11, Hudi no longer requires `spark-avro` to be specified using `--packages`

### Build with different Flink versions

The default Flink version supported is 1.17. The default Flink 1.17.x version, corresponding to `flink1.17` profile is 1.17.0.
Flink is Scala-free since 1.15.x, there is no need to specify the Scala version for Flink 1.15.x and above versions.
Refer to the table below for building with different Flink and Scala versions.

| Maven build options        | Expected Flink bundle jar name | Notes                            |
|:---------------------------|:-------------------------------|:---------------------------------|
| (empty)                    | hudi-flink1.17-bundle          | For Flink 1.17 (default options) |
| `-Dflink1.17`              | hudi-flink1.17-bundle          | For Flink 1.17 (same as default) |
| `-Dflink1.16`              | hudi-flink1.16-bundle          | For Flink 1.16                   |
| `-Dflink1.15`              | hudi-flink1.15-bundle          | For Flink 1.15                   |
| `-Dflink1.14`              | hudi-flink1.14-bundle          | For Flink 1.14 and Scala 2.12    |
| `-Dflink1.14 -Dscala-2.11` | hudi-flink1.14-bundle          | For Flink 1.14 and Scala 2.11    |
| `-Dflink1.13`              | hudi-flink1.13-bundle          | For Flink 1.13 and Scala 2.12    |
| `-Dflink1.13 -Dscala-2.11` | hudi-flink1.13-bundle          | For Flink 1.13 and Scala 2.11    |

For example,
```
# Build against Flink 1.15.x
mvn clean package -DskipTests -Dflink1.15

# Build against Flink 1.14.x and Scala 2.11
mvn clean package -DskipTests -Dflink1.14 -Dscala-2.11

# Build against Flink 1.13.x and Scala 2.12
mvn clean package -DskipTests -Dflink1.13
```

## Running Tests

Unit tests can be run with maven profile `unit-tests`.
```
mvn -Punit-tests test
```

Functional tests, which are tagged with `@Tag(""functional"")`, can be run with maven profile `functional-tests`.
```
mvn -Pfunctional-tests test
```

Integration tests can be run with maven profile `integration-tests`.
```
mvn -Pintegration-tests verify
```

To run tests with spark event logging enabled, define the Spark event log directory. This allows visualizing test DAG and stages using Spark History Server UI.
```
mvn -Punit-tests test -DSPARK_EVLOG_DIR=/path/for/spark/event/log
```

## Quickstart

Please visit [https://hudi.apache.org/docs/quick-start-guide.html](https://hudi.apache.org/docs/quick-start-guide.html) to quickly explore Hudi's capabilities using spark-shell. 

## Contributing

Please check out our [contribution guide](https://hudi.apache.org/contribute/how-to-contribute) to learn more about how to contribute.
For code contributions, please refer to the [developer setup](https://hudi.apache.org/contribute/developer-setup).
",2023-07-07 15:59:17+00:00
hyperflow,hyperflow,hyperflow-wms/hyperflow,HyperFlow: a scientific workflow engine ,,False,63,2023-04-24 07:23:04+00:00,2013-02-19 13:48:10+00:00,30,11,13,14,v1.6.0,2022-09-17 08:53:45+00:00,MIT License,1000,v1.6.1,65,2022-09-18 10:11:30+00:00,2023-05-30 15:39:01+00:00,2022-09-18 10:46:29+00:00,"# HyperFlow: a scientific workflow management system


![HyperFLow](https://img.shields.io/docker/v/hyperflowwms/hyperflow)
## Description

HyperFlow is a Workflow Management System (WMS) dedicated for scientific workflows. 

Browse the [wiki pages](https://github.com/balis/hyperflow/wiki) to learn more about the HyperFlow workflow model. 

## Getting started

### Installation
* Install Node.js (http://nodejs.org)
* Install Redis (http://redis.io) 
* Install HyperFlow: 
  * From npm package: `npm install -g @hyperflow/hyperflow`
  * From github repository:<br>`npm install https://github.com/hyperflow-wms/hyperflow/archive/{version}.tar.gz`<br>(where `{version}` is for example `v1.5.0`) 
  * From the master branch: <br>`npm install https://github.com/hyperflow-wms/hyperflow/archive/master.tar.gz`

### Running locally
* Start the redis server: `redis-server`
* Run example workflows using command `hflow run <wf_directory>`, for example:<br>```hflow run ./examples/Sqrsum```

### Running locally using Docker images
* Use the latest Docker image for the HyperFlow engine, published in Docker Hub as `hyperflowwms/hyperflow` 
* You can build the image yourself: `make container`
* Start redis container:<br> 
```docker run -d --name redis redis --bind 127.0.0.1```
* Run workflow via HyperFlow container, for example:
```
docker run -a stdout -a stderr --rm --network container:redis \
       -e HF_VAR_WORKER_CONTAINER=""hyperflowwms/soykb-workflow-worker"" \ 
       -e HF_VAR_WORK_DIR=""$PWD/input"" \ 
       -e HF_VAR_HFLOW_IN_CONTAINER=""true"" \
       -e HF_VAR_function=""redisCommand"" \
       -e REDIS_URL=""redis://127.0.0.1:6379"" \
       --name hyperflow \
       -v /var/run/docker.sock:/var/run/docker.sock \
       -v $PWD:/wfdir \
       --entrypoint ""/bin/sh"" hyperflowwms/hyperflow -c ""apk add docker && hflow run /wfdir""
```
Where
* `hyperflowwms/soykb-worker` is the name of the workflow worker container ([Soykb](https://github.com/hyperflow-wms/soykb-workflow) in this case)
* current directory contains `workflow.json`
* subdirectory `inputs` contains workflow input data 

Outputs:
* Directory `inputs` will contain files generated by the workflow run
* Directory `inputs/logs-hf` will contain logs of all workflow jobs

### Running in a Kubernetes cluster
See [HyperFlow Kubernetes deployment](https://github.com/hyperflow-wms/hyperflow-k8s-deployment) project for more information. 

### Running in a distributed infrastructure using the RabbitMQ executor (not maintained)
* Start the RabbitMQ container: `docker run -d --name rabbitmq rabbitmq:3`
* Add option `-e AMQP_URL=amqp://rabbitmq`
* More information in the [hyperflow-amqp-executor](https://github.com/hyperflow-wms/hyperflow-amqp-executor) project
* **Warning**: currently not maintained and not tested with latest HyperFlow versions

## Local configuration files
You can provide workflow configuration through local configuration files:
* `workflow.config.json` -- main configuration file 
* `workflow.config.{name}.json` -- any number of secondary configuration files

The content from all configuration files will be merged and passed to workflow functions via `context.appConfig`. For example for files:
```
workflow.config.json:
{
  ""main"": ""mainValue""
}

workflow.config.foo.json:
{
   ""secondary"": ""secondaryValue""
}
```
The following will be passed in `context.appConfig`:
```
{
  ""main"": ""mainValue"",
  ""foo"": {
     ""secondary"": ""secondaryValue""
  }
}
```

## HyperFlow server
The HyperFlow engine can be started in a server mode using command: ```hflow start-server```

If succesfully started, the server prints its URL:
```
HyperFlow server started at http://localhost:38775
```

Workflows can be run through the HyperFlow server as follows:

```
hflow run --submit=<hyperflow_server_url> <workflow_dir>
```

Currently `<workflow_dir>` must be a local directory accessbile by the server. This allows running multiple workflows (concurrently) using the same instance of the HyperFlow engine.
",2023-07-07 15:59:22+00:00
hypershell,hyper-shell,glentner/hyper-shell,"A cross-platform, high-performance computing utility for processing shell commands over a distributed, asynchronous queue.",https://hyper-shell.readthedocs.io,False,11,2023-06-24 23:00:24+00:00,2019-12-15 20:15:21+00:00,2,3,1,7,2.4.0,2023-06-08 20:55:28+00:00,Apache License 2.0,553,2.4.0,38,2023-06-08 20:55:21+00:00,2023-06-24 23:00:24+00:00,2023-06-08 20:55:21+00:00,"HyperShell v2: Distributed Task Execution for HPC
=================================================

.. image:: https://img.shields.io/badge/license-Apache-blue.svg?style=flat
    :target: https://www.apache.org/licenses/LICENSE-2.0
    :alt: License

.. image:: https://img.shields.io/pypi/v/hyper-shell.svg?style=flat&color=blue
    :target: https://pypi.org/project/hyper-shell
    :alt: PyPI Version

.. image:: https://img.shields.io/pypi/pyversions/hyper-shell.svg?logo=python&logoColor=white&style=flat
    :target: https://pypi.org/project/hyper-shell
    :alt: Python Versions

.. image:: https://readthedocs.org/projects/hyper-shell/badge/?version=latest&style=flat
    :target: https://hyper-shell.readthedocs.io
    :alt: Documentation

.. image:: https://pepy.tech/badge/hyper-shell
    :target: https://pepy.tech/badge/hyper-shell
    :alt: Downloads

|

*HyperShell* is an elegant, cross-platform, high-performance computing utility for
processing shell commands over a distributed, asynchronous queue. It is a highly
scalable workflow automation tool for *many-task* scenarios.

Several tools offer similar functionality but not all together in a single tool with
the ergonomics we provide. Novel design elements include but are not limited to
(1) cross-platform, (2) client-server design, (3) staggered launch for large scales,
(4) persistent hosting of the server, and optionally (5) a database in-the-loop for
persisting task metadata and automated retries.

*HyperShell* is pure Python and is tested on Linux, macOS, and Windows 10 in
Python 3.9+ environments. The server and client don't even need to use the same
platform simultaneously.


Documentation
-------------

Documentation is available at `hyper-shell.readthedocs.io <https://hyper-shell.readthedocs.io>`_.
For basic usage information on the command line use: ``hyper-shell --help``. For a more 
comprehensive usage guide on the command line you can view the manual page with 
``man hyper-shell``.


Contributions
-------------

Contributions are welcome. If you find bugs or have questions, open an *Issue* here.
We've added a Code of Conduct recently, adapted from the
`Contributor Covenant <https://www.contributor-covenant.org/>`_, version 2.0.


Citation
--------

If *HyperShell* has helped in your research please consider citing us.

.. code-block:: bibtex

    @inproceedings{lentner_2022,
        author = {Lentner, Geoffrey and Gorenstein, Lev},
        title = {HyperShell v2: Distributed Task Execution for HPC},
        year = {2022},
        isbn = {9781450391610},
        publisher = {Association for Computing Machinery},
        url = {https://doi.org/10.1145/3491418.3535138},
        doi = {10.1145/3491418.3535138},
        booktitle = {Practice and Experience in Advanced Research Computing},
        articleno = {80},
        numpages = {3},
        series = {PEARC '22}
    }
",2023-07-07 15:59:27+00:00
iceprod,iceprod,WIPACrepo/iceprod,IceCube dataset management system,,False,4,2023-05-19 17:04:25+00:00,2016-05-06 21:10:09+00:00,3,11,11,65,v2.6.17,2023-06-12 21:51:56+00:00,MIT License,2131,v2.6.17,66,2023-06-12 21:51:56+00:00,2023-07-06 18:02:57+00:00,2023-06-12 21:51:56+00:00,"IceProd
=======

.. image:: https://zenodo.org/badge/58235078.svg
   :target: https://zenodo.org/badge/latestdoi/58235078

IceProd is a Python framework for distributed management of batch jobs.
It runs as a layer on top of other batch middleware, such as HTCondor,
and can pool together resources from different batch systems.
The primary purpose is to coordinate and administer many large sets of
jobs at once, keeping a history of the entire job lifecycle.

See also: Aartsen, Mark G., et al. ""The IceProd framework: Distributed data processing for the IceCube neutrino observatory."" Journal of parallel and distributed computing 75 (2015): 198-211.

**Note:**

For IceCube users with CVMFS access, IceProd is already installed.
To load the environment execute::

    /cvmfs/icecube.wisc.edu/iceprod/latest/env-shell.sh

or::

    eval `/cvmfs/icecube.wisc.edu/iceprod/latest/setup.sh`

depending on whether you want to get a new shell or load the variables
into the current shell.

Installation
------------

**Platforms**:

IceProd should run on any Unix-like platform, although only
Linux has been extensively tested and can be recommented for production
deployment.

**Prerequisites**:

Listed here are any packages outside pip:

* Python 3.7+
* MongoDB 3.6+    (for the REST API)
* nginx           (for ssl offloading and better security)
* globus          (for data transfer)

**Installation**:

From the latest release:

Get the tarball link from https://github.com/WIPACrepo/iceprod/releases/latest

Then install like::

    pip install https://github.com/WIPACrepo/iceprod/archive/v2.0.0.tar.gz

**Installing from master**:

If you must install the dev version from master, do::

    pip install --upgrade git+git://github.com/WIPACrepo/iceprod.git#egg=iceprod

",2023-07-07 15:59:31+00:00
icolos,Icolos,MolecularAI/Icolos,Icolos: A workflow manager for structure based post-processing of de novo generated small molecules,,True,49,2023-05-03 13:04:18+00:00,2022-01-28 15:32:19+00:00,11,5,5,6,v1.10,2022-09-05 14:36:09+00:00,Apache License 2.0,581,v1.10,9,2022-09-05 14:33:33+00:00,2023-05-03 13:04:18+00:00,2023-03-16 16:16:25+00:00,"**Please note: this repository is no longer being maintained.**

[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![docs](https://github.com/MolecularAI/Icolos/actions/workflows/pages/pages-build-deployment/badge.svg?branch=development)](https://github.com/MolecularAI/Icolos/actions/workflows/pages/pages-build-deployment)
[![PyPI version](https://badge.fury.io/py/icolos.svg)](https://badge.fury.io/py/icolos)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black) 
[![GitHub contributors](https://badgen.net/github/contributors/MolecularAI/Icolos)](https://GitHub.com/MolecularAI/Icolos/graphs/contributors/)
[![Latest tag](https://badgen.net/github/tag/MolecularAI/Icolos)](https://github.com/MolecularAI/Icolos/tag)

# `Icolos`: Workflow manager

The `Icolos` tool is a workflow manager for structure-based workflows in computational chemistry, that abstracts execution logic from implementation as much as possible. Icolos was designed to interface with [REINVENT](https://github.com/MolecularAI/Reinvent), and workflows can be called as a component of the scoring function, or to postprocess results with more expensive methods. Workflows are specified in `JSON` format (see folder `examples`). Currently wrapped are a diverse set of tools and internal steps, including docking, QM and MD capabilities. The pre-print is available [here](https://doi.org/10.26434/chemrxiv-2022-sjcp3).


## Introduction
`Icolos` provides a unified interface to a host of software for common computational chemistry calculations, with built in parallelization,
and straight-forward extensibiltiy to add additional functionality. It was principally developed to handle structural calculations for `REINVENT` jobs, however, workflows can also be run independently.

Workflows are constructed from elementary 'steps', individual blocks which are combined to specify a sequential list of operations, with control of the command-line options provided through step settings, and options to control other aspects of the step's behaviour included in the `additional` block.

For many use cases, one of the template workflows might suit your needs, or need a few tweaks to do what you want. Demonstration notebooks for common workflows are available [here](https://github.com/MolecularAI/IcolosCommunity).

## Initial configuration
You are welcome to clone the repository and use a local version, and in particular if you would like to experiment with the code base and/or contribute features, please get 
in contact with us.

## Installation
After cloning, first install and activate the `icolosprod` `conda` environment. To ensure the right installation directory is used, you can add the `--prefix` parameter to the `create` call, specifying the location of the `conda` environments.
```
conda env create -f environment_min.yml
conda activate icolosprod
```
Then install the package:
```
pip install -e .
```
This will give you access to the `icolos` entrypoint.

### `ESPsim` installation
The following will install the `ESPsim` package into the environment - this is only required if ligand-based matching using this package is desired.

```
cd ..
git clone https://github.com/hesther/espsim.git
cd espsim
conda activate icolosprod
pip install -e .
```
## Unit testing
Icolos is extensively unit tested, and relies on an external data repo located [here](https://github.com/MolecularAI/IcolosData).  The full test suite takes ~60 mins on a workstation, therefore it is recommended that you execute a subset of unit tests relevant to the workflow you are running.  To execute the full test suite, run something like:
```
pytest -n 8 tests/
```

## Execution
Once a `JSON` is specified, the workflow can be executed like so:

```
conda activate icolosprod
icolos -conf workflow.json
```

We usually advise to check the validity of your configuration file before you try to execute it. There is a bespoke `validator` entry point to facilitate this:

```
validator -conf workflow.json
```

## `SLURM` Execution
Once specified, a workflow can be called like this in a `bash` script:

```
#!/bin/bash -l
#SBATCH -N 1
#SBATCH -t 0-02:59:00
#SBATCH -p core
#SBATCH --ntasks-per-node=5
#SBATCH --mem-per-cpu=2G

source /<conda_path>/miniconda3/bin/activate /<conda_path>/minconda3/envs/icolosprod
icolos -conf workflow.json
```
For GROMACS workflows requiring the GPU partition, you will need to adapt the header accordingly, e.g. like so:

```
#!/bin/bash
#SBATCH -J gmx_cco1_fold_microsecond
#SBATCH -o MygpuJob_out_%j.txt
#SBATCH -e MygpuJob_err_%j.txt
#SBATCH -c 8
#SBATCH --gres=gpu:1
#SBATCH --mem-per-cpu=4g
#SBATCH -p gpu
#SBATCH --time=12:00:00

```
## Documentation
The API reference is available [here](https://molecularai.github.io/Icolos/index.html).


## Developers
- Christian Margreitter [@cmargreitter](https://github.com/CMargreitter)
- J. Harry Moore [@jharrymoore](https://github.com/jharrymoore)
- Matthias R. Bauer <mattias.r.b@gmail.com>
",2023-07-07 15:59:35+00:00
janis,janis,PMCC-BioinformaticsCore/janis,[Alpha] Janis: an open source tool to machine generate type-safe CWL and WDL workflows,https://janis.readthedocs.io/,False,39,2023-06-07 04:28:24+00:00,2019-01-24 05:09:26+00:00,13,13,9,20,,,GNU General Public License v3.0,954,v0.12.0,43,2023-06-14 05:39:39+00:00,2023-06-14 05:42:00+00:00,2023-06-14 05:39:39+00:00,"# Janis (janis-pipelines) (Alpha)


[![GitHub stars](https://img.shields.io/github/stars/PMCC-BioinformaticsCore/janis.svg?style=social)](https://github.com/PMCC-BioinformaticsCore/janis) [![Build Status](https://travis-ci.org/PMCC-BioinformaticsCore/janis.svg?branch=master)](https://travis-ci.org/PMCC-BioinformaticsCore/janis)  [![Documentation Status](https://readthedocs.org/projects/janis/badge/?version=latest)](https://janis.readthedocs.io/en/latest/?badge=latest)  [![PyPI version](https://badge.fury.io/py/janis-pipelines.svg)](https://badge.fury.io/py/janis-pipelines)  [![codecov](https://codecov.io/gh/PMCC-BioinformaticsCore/janis/branch/master/graph/badge.svg)](https://codecov.io/gh/PMCC-BioinformaticsCore/janis) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black) [![Gitter chat](https://badges.gitter.im/janis-pipelines.png)](https://gitter.im/janis-pipelines/community)
  
_Janis is a framework creating specialised, simple workflow definitions that are then transpiled to   
Common Workflow Language or Workflow Definition Language._  
  
Documentation is available here: https://janis.readthedocs.io/

## Introduction  

>| WARNING: this project is work-in-progress and is provided as-is without warranty of any kind. There may be breaking changes committed to this repository without notice. |
>|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|


Janis gives you an API to build computational workflows and will generate
a workflow description in CWL (v1.2) and WDL (version development). By using Janis, 
you get type-safety, portability and reproducibility across all of your execution environments.


Janis requires a Python installation > 3.6, and can be installed through PIP 
([project page](https://pypi.org/project/janis-pipelines/)):  
  
```bash
# Install janis and the toolkits
pip3 install janis-pipelines 
```

There are two ways to use Janis:

- Build workflows (and translate to CWL / WDL)
- Run tools or workflows with CWLTool or Cromwell

### Latest versions

Check the table below for the latest released versions of Janis (`janis -v`):

| Repository 	|   Version  	|
|------------	|:----------:	|
| [Janis](https://github.com/PMCC-BioinformaticsCore/janis) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.svg)](https://badge.fury.io/py/janis-pipelines) |
| [janis-assistant](https://github.com/PMCC-BioinformaticsCore/janis-assistant) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.runner.svg)](https://badge.fury.io/py/janis-pipelines.runner) |
| [janis-bioinformatics](https://github.com/PMCC-BioinformaticsCore/janis-bioinformatics) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.bioinformatics.svg)](https://badge.fury.io/py/janis-pipelines.bioinformatics) |
| [janis-core](https://github.com/PMCC-BioinformaticsCore/janis-core) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.core.svg)](https://badge.fury.io/py/janis-pipelines.core) |
| [janis-pipelines](https://github.com/PMCC-BioinformaticsCore/janis-pipelines) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.pipelines.svg)](https://badge.fury.io/py/janis-pipelines.pipelines) |
| [janis-templates](https://github.com/PMCC-BioinformaticsCore/janis-templates) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.templates.svg)](https://badge.fury.io/py/janis-pipelines.templates) |
| [janis-unix](https://github.com/PMCC-BioinformaticsCore/janis-unix) | [![PyPI version](https://badge.fury.io/py/janis-pipelines.unix.svg)](https://badge.fury.io/py/janis-pipelines.unix) |


### Example workflow

  
Let's construct a simple example that takes a string input, calls the 
[echo](https://janis.readthedocs.io/en/latest/tools/unix/echo.html) tool and exposes the 
Echo tool's output as a workflow output. 


  
```bash
# write the workflow to `helloworld.py`
cat <<EOT >> helloworld.py
import janis as j
from janis_unix.tools import Echo

w = j.WorkflowBuilder(""hello_world"")

w.input(""input_to_print"", j.String)
w.step(""echo"", Echo(inp=w.input_to_print))
w.output(""echo_out"", source=w.echo.out)
EOT


# Translate workflow to WDL
janis translate helloworld.py wdl

# Run the workflow
janis run -o helloworld-tutorial helloworld.py --input_to_print ""Hello, World!""

# See your output
cat helloworld-tutorial/echo_out
# Hello, World!
```

  
### How to use Janis

- [Tutorial 0 - Introduction to Janis](https://janis.readthedocs.io/en/latest/tutorials/tutorial0.html)
- [Tutorial 1 - Building a workflow](https://janis.readthedocs.io/en/latest/tutorials/tutorial1.html)
- [Tutorial 2 - Wrapping a new tool](https://janis.readthedocs.io/en/latest/tutorials/tutorial2.html)


#### Workshops

In addition, there are fully self-guided workshops that more broadly go through the functionality of Janis:

- [Workshop 1](https://github.com/PMCC-BioinformaticsCore/janis-workshops/tree/master/workshop1)
- [Workshop 2](https://github.com/PMCC-BioinformaticsCore/janis-workshops/tree/master/workshop2)

### Examples

Sometimes it's easier to learn by examples, here are a few hand picked examples:

- [Samtools View](https://github.com/PMCC-BioinformaticsCore/janis-bioinformatics/blob/master/janis_bioinformatics/tools/samtools/view/base.py) ([Docs](https://janis.readthedocs.io/en/latest/tools/bioinformatics/samtools/samtoolsview.html))

- [WGS Germline pipeline (GATK Only)](https://github.com/PMCC-BioinformaticsCore/janis-pipelines/blob/master/janis_pipelines/wgs_germline_gatk/wgsgermlinegatk.py) ([Docs](https://janis.readthedocs.io/en/latest/pipelines/wgsgermlinegatk.html))


### Toolbox

There are two toolboxes currently available on Janis:

- [Unix](https://github.com/PMCC-BioinformaticsCore/janis-unix) ([list of tools](https://janis.readthedocs.io/en/latest/tools/bioinformatics/index.html))
- [Bioinformatics](https://github.com/PMCC-BioinformaticsCore/janis-bioinformatics) ([list of tools](https://janis.readthedocs.io/en/latest/tools/unix/index.html))



## About  
  
> _Further information_: [About](https://janis.readthedocs.io/en/latest/about.html)   
  
This project was produced as part of the Portable Pipelines Project in partnership with:    
- [Melbourne Bioinformatics (University of Melbourne) ](https://www.melbournebioinformatics.org.au/)    
- [Peter MacCallum Cancer Centre](https://www.petermac.org/)    
- [Walter and Eliza Hall Institute of Medical Research (WEHI) ](https://www.wehi.edu.au/)    

### References:

Through conference or talks, this project has been referenced by the following titles:

- Walter and Eliza Hall Institute Talk (WEHI) 2019: _Portable Pipelines Project: Developing reproducible bioinformatics pipelines with standardised workflow languages_
- Bioinformatics Open Source Conference (BOSC) 2019: _Janis: an open source tool to machine generate type-safe CWL and WDL workflows_
- Victorian Cancer Bioinformatics Symposium (VCBS) 2019: _Developing portable variant calling pipelines with Janis_
- GIW / ABACBS 2019: _Janis: A Python framework for Portable Pipelines_
- Australian BioCommons, December 2019: _Portable pipelines: build once and run everywhere with Janis_
  
  
## Support  

### v0.9.0 Backwards Compatability

**NOTE: Version 0.9.0 brings changes to output directories and camel case changes**

- Janis watch will be incompatible with previously run workflows
- Your configs might break, as previous versions of janis were not cautious about camel case.
- Your templates might not work with unrecognised keys (try changing them to camel case instead)
- Changes to BamBai indexes, format is now `.bam.bai`

See the [CHANGELOG](https://github.com/PMCC-BioinformaticsCore/janis/blob/master/CHANGELOG.md)
for more information.


### Contributions
  
> _Further information_: [Development](https://janis.readthedocs.io/en/latest/development/)  
  
To get help with Janis, please ask a question on [Gitter](ttps://gitter.im/janis-pipelines/community) or 
[raise an issue](https://github.com/PMCC-BioinformaticsCore/janis/issues) on GitHub.

If you find an issue with the tool definitions, please see the relevant issue page:

- [Pipeline-bioinformatics](https://github.com/PMCC-BioinformaticsCore/janis-bioinformatics/issues)

This project is work-in-progress and is still in developments. Although we welcome contributions,
due to the immature state of this project we recommend raising issues through the
[Github issues page](https://github.com/PMCC-BioinformaticsCore/janis/issues) for Pipeline related issues.

Information about the project structure and more on contributing can be found within the documentation.
",2023-07-07 15:59:39+00:00
jms,JMS,RUBi-ZA/JMS,A workflow management system and web-based cluster front-end for high performance computing,,False,17,2022-11-10 19:27:35+00:00,2014-11-08 12:12:45+00:00,14,8,5,0,,,GNU General Public License v2.0,113,,0,,,2022-06-23 07:57:22+00:00,"JMS
===
JMS is a workflow management system and web-based cluster front-end for High Performance Computing (HPC) environments. It provides an interface to Torque (or similar resource managers) that allows users to submit and manage jobs as well as manage, configure and monitor the status of their cluster.

In addition to interfacing with a resource manager, JMS provides a fully-functional workflow management system that allows users to create complex computational pipelines via an easy-to-use, web interface. Users can upload their scripts, interface with installed programs on their cluster, or both, to build their workflows.

JMS was originally developed for use in the field of bioinformatics. It is, however, applicable to any scientific field that requires computationally intensive analysis to be performed over a cluster. It can also be used to integrate workflows into 3rd party websites via it's RESTful web API. JMS is is also a useful tool for system administrators who simply want to monitor and manage their cluster.

JMS is a Django project. We will welcome any and all help in developing it further.

JMS has been published [here](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0134273).

Installation
---
*Note: The following instructions are for Ubuntu 14.04, but can be used as a guideline for other Linux flavours.*

### Prerequisites
- [MySQL server](https://github.com/RUBi-ZA/JMS/wiki/Set-up-a-database-for-the-JMS)
- [NFS (or similar) mounted on all nodes of the cluster](https://github.com/RUBi-ZA/JMS/wiki/Set-up-NFS)
- [Torque Resource Manager](https://github.com/RUBi-ZA/JMS/wiki/Set-up-Torque)

### 1. Download and setup the JMS project

First of all, you will need to download the project from github. We recommend you download the project to the `/srv` directory so you will not need to change paths in the settings file later:
``` bash
cd /srv
sudo mkdir JMS
sudo chown user:user JMS
git clone https://github.com/RUBi-ZA/JMS.git
sudo chown user:user JMS -R
```

Navigate to the project src directory and setup a virtual environment:
``` bash
cd /srv/JMS/src
sudo apt-get install -y libmemcache-dev zlib1g-dev libssl-dev python-dev build-essential
virtualenv venv
source venv/bin/activate
pip install -r requirements.txt
```

Edit the `/srv/JMS/src/JMS/settings.py` file to include your database login credentials (you should have set these when [creating your database](https://github.com/RUBi-ZA/JMS/wiki/Set-up-a-database-for-the-JMS)) and the path to the [NFS share](https://github.com/RUBi-ZA/JMS/wiki/Set-up-NFS):

``` python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': 'JMS',                      # Or path to database file if using sqlite3.
        # The following settings are not used with sqlite3:
        'USER': 'username',
        'PASSWORD': 'password',
        'HOST': 'localhost', 
        'PORT': '',                      # Set to empty string for default.
    }
}


JMS_SETTINGS = {
    ""JMS_shared_directory"": ""/NFS/JMS/"",
    ""resource_manager"": {
        ""name"": ""torque"",
        ""log_file"": os.path.join(BASE, ""logs/torque.log"")
    }
}

IMPERSONATOR_SETTINGS = {
    ""key"": os.path.join(BASE_DIR, ""impersonator/pub.key""),
    ""url"": ""127.0.0.1:8123""
}

```

With the settings.py file set up with your database details and the path to your shared directory, run the following commands:
``` bash
cd /srv/JMS/src
source venv/bin/activate
python manage.py migrate
python manage.py setup
```

### 2. Start the queue daemon

The queue daemon is responsible for updating the JMS job history with details from the resource manager. If you don't start the queue_daemon, your job history will not be updated after the a job has been submitted i.e. no changes in state will be tracked during the job. To start the queue daemon, run the following command:
``` bash
sudo venv/bin/python manage.py queue_daemon start
```

To restart or stop the queue daemon, run the following commands respectively:
```
sudo venv/bin/python manage.py queue_daemon restart
sudo venv/bin/python manage.py queue_daemon stop
```

### 3. Start the impersonator server

The impersonator allows JMS to submit jobs as you. It is also used for a number of other reasons. If you are unable to login, chances are the impersonator is not running. To start it, run the following commands:
``` bash
sudo nohup venv/bin/python impersonator/server.py 8123 >/dev/null 2>&1 &
```

You can set which port JMS communicates with the impersonator on in the settings.py file:

``` python
IMPERSONATOR_SETTINGS = {
    ""key"": os.path.join(BASE_DIR, ""impersonator/pub.key""),
    ""url"": ""127.0.0.1:8123""
}
```

### 4. Test the JMS

To test that your installation is working, run the Django development web server:
``` bash
python manage.py runserver ip.address:8000
```

For improved performance, [host the JMS with Apache](https://github.com/RUBi-ZA/JMS/wiki/Hosting-with-Apache) or some other production web server.
",2023-07-07 15:59:44+00:00
jobcenter,jobcenter,yeastrc/jobcenter,"Jobcenter, a client-server application and framework for job management and distributed job execution",,False,11,2023-05-27 03:08:28+00:00,2015-04-23 17:45:55+00:00,11,2,2,1,1.7.2,2016-11-08 00:08:58+00:00,,106,1.7.2,17,2016-11-08 00:08:58+00:00,2023-05-27 03:08:27+00:00,2019-03-14 23:25:11+00:00,"# jobcenter
### A client-server application and framework for job management and distributed job execution

See the paper at (http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3494518/).

JobCenter is a client-server application and framework for job management and distributed job execution. The client and server components are both written in Java and are cross-platform and relatively easy to install. All communication with the server is client-driven, which allows worker nodes to run anywhere (even behind external firewalls or “in the cloud”) and provides inherent load balancing. Adding a worker node to the worker pool is as simple as dropping the JobCenter client files onto any computer and performing basic configuration, which provides tremendous ease-of-use, flexibility, and limitless horizontal scalability. Each worker installation may be independently configured, including the types of jobs it is able to run. Executed jobs may be written in any language and may include multistep workflows.

Please see the file '`docs/Limitations_on_Values_passed_through_Jobcenter_Framework.txt`' for information specifically about passing values through the Jobcenter framework.  The limitations on the values applies since the first version of Jobcenter. 

Breaking change  version 1.7 and current trunk:

In the Jobcenter client, calls the module makes on the `ModuleInterfaceResponse` response object to add run messages and run output parameters will throw checked exceptions 
if the Strings passed cannot be marshaled and unmarshaled as UTF-8 XML.

see '`docs/Release_History.txt`' for more info
",2023-07-07 15:59:47+00:00
jobflow,jobflow,materialsproject/jobflow,jobflow is a library for writing computational workflows.,https://materialsproject.github.io/jobflow,False,52,2023-06-25 02:29:33+00:00,2021-02-22 22:38:17+00:00,15,7,13,14,v0.1.11,2023-03-20 16:26:31+00:00,Other,957,v0.1.11,14,2023-03-20 16:26:31+00:00,2023-07-05 14:02:02+00:00,2023-07-03 23:23:46+00:00,"# jobflow

<a href=""https://github.com/materialsproject/jobflow/actions?query=workflow%3Atesting""><img alt=""code coverage"" src=""https://img.shields.io/github/actions/workflow/status/materialsproject/jobflow/testing.yml?branch=main&label=tests""></a>
<a href=""https://codecov.io/gh/materialsproject/jobflow/""><img alt=""code coverage"" src=""https://img.shields.io/codecov/c/gh/materialsproject/jobflow/main""></a>
<a href=""https://pypi.org/project/jobflow""><img alt=""pypi version"" src=""https://img.shields.io/pypi/v/jobflow?color=blue""></a>
<img alt=""supported python versions"" src=""https://img.shields.io/pypi/pyversions/jobflow"">

[Documentation](https://materialsproject.github.io/jobflow/) | [PyPI](https://pypi.org/project/jobflow/) | [GitHub](https://github.com/materialsproject/jobflow)

Jobflow is a free, open-source library for writing and executing workflows. Complex
workflows can be defined using simple python functions and executed locally or on
arbitrary computing resources using the [FireWorks][fireworks] workflow manager.

Some features that distinguish jobflow are dynamic workflows, easy compositing and
connecting of workflows, and the ability to store workflow outputs across multiple
databases.

## Is jobflow for me

jobflow is intended to be a friendly workflow software that is easy to get started with,
but flexible enough to handle complicated use cases.

Some of its features include:

- A clean and flexible Python API.
- A powerful approach to compositing and connecting workflows — information passing
  between jobs is a key goal of jobflow. Workflows can be nested allowing for a natural
  way to build complex workflows.
- Integration with multiple databases (MongoDB, S3, GridFS, and more) through the
  [Maggma][maggma] package.
- Support for the [FireWorks][fireworks] workflow management system, allowing workflow
  execution on multicore machines or through a queue, on a single machine or multiple
  machines.
- Support for dynamic workflows — workflows that modify themselves or create new ones
  based on what happens during execution.

## Workflow model

Workflows in jobflows are made up of two main components:

- A `Job` is an atomic computing job. Essentially any python function can be `Job`,
  provided its input and return values can be serialized to json. Anything returned by the job is
  considered an ""output"" and is stored in the jobflow database.
- A `Flow` is a collection of `Job` objects or other `Flow` objects. The connectivity
  between jobs is determined automatically from the job inputs. The execution order
  of jobs is automatically determined based on their connectivity.

Python functions can be easily converted in to `Job` objects using the `@job` decorator.
In the example below, we define a job to add two numbers.

```python
from jobflow import job, Flow

@job
def add(a, b):
    return a + b

add_first = add(1, 5)
add_second = add(add_first.output, 5)

flow = Flow([add_first, add_second])
flow.draw_graph().show()
```

The output of the job is accessed using the `output` attribute. As the job has not
yet been run, `output` contains be a reference to a future output. Outputs can be used
as inputs to other jobs and will be automatically ""resolved"" before the job is
executed.

Finally, we created a flow using the two `Job` objects. The connectivity between
the jobs is determined automatically and can be visualised using the flow graph.

<p align=""center"">
<img alt=""simple flow graph"" src=""https://raw.githubusercontent.com/materialsproject/jobflow/main/docs/_static/img/simple_flow.png"" width=""30%"" height=""30%"">
</p>

## Installation

The jobflow is a Python 3.8+ library and can be installed using pip.

```bash
pip install jobflow
```

## Quickstart and tutorials

To get a first glimpse of jobflow, we suggest that you follow our quickstart tutorial.
Later tutorials delve into the advanced features of jobflow.

- [Five-minute quickstart tutorial][quickstart]
- [Introduction to jobflow][introduction]
- [Defining Jobs using jobflow][defining-jobs]

## Need help?

Ask questions about jobflow on the [jobflow support forum][help-forum].
If you've found an issue with jobflow, please submit a bug report on [GitHub Issues][issues].

## What’s new?

Track changes to jobflow through the [changelog][changelog].

## Contributing

We greatly appreciate any contributions in the form of a pull request.
Additional information on contributing to jobflow can be found [here][contributing].
We maintain a list of all contributors [here][contributors].

## License

jobflow is released under a modified BSD license; the full text can be found [here][license].

## Acknowledgements

Jobflow was designed by Alex Ganose, Anubhav Jain, Gian-Marco Rignanese, David Waroquiers, and Guido Petretto. Alex Ganose implemented the first version of the package. Later versions have benefited from the contributions of several research groups. A full list of contributors is available [here](https://materialsproject.github.io/jobflow/contributors.html).

[maggma]: https://materialsproject.github.io/maggma/
[fireworks]: https://materialsproject.github.io/fireworks/
[help-forum]: https://matsci.org/c/fireworks
[issues]: https://github.com/materialsproject/jobflow/issues
[changelog]: https://materialsproject.github.io/jobflow/changelog.html
[contributing]: https://materialsproject.github.io/jobflow/contributing.html
[contributors]: https://materialsproject.github.io/jobflow/contributors.html
[license]: https://raw.githubusercontent.com/materialsproject/jobflow/main/LICENSE
[quickstart]: https://materialsproject.github.io/jobflow/tutorials/1-quickstart.html
[introduction]: https://materialsproject.github.io/jobflow/tutorials/2-introduction.html
[defining-jobs]: https://materialsproject.github.io/jobflow/tutorials/3-defining-jobs.html
[creating-flows]: https://materialsproject.github.io/jobflow/tutorials/4-creating-flows.html
[dynamic-flows]: https://materialsproject.github.io/jobflow/tutorials/5-dynamic-flows.html
[makers]: https://materialsproject.github.io/jobflow/tutorials/6-makers.html
[generalized-makers]: https://materialsproject.github.io/jobflow/tutorials/7-generalized-makers.html
[jobflow-fireworks]: https://materialsproject.github.io/jobflow/tutorials/8-fireworks.html
",2023-07-07 15:59:51+00:00
joblib,joblib,joblib/joblib,Computing with Python functions.,http://joblib.readthedocs.org,False,3319,2023-07-07 03:35:14+00:00,2010-05-07 06:48:26+00:00,380,60,105,0,,,"BSD 3-Clause ""New"" or ""Revised"" License",1475,debian/0.6.4-1,80,2012-05-08 02:35:53+00:00,2023-07-07 14:44:00+00:00,2023-07-06 06:38:23+00:00,"|PyPi| |Azure| |ReadTheDocs| |Codecov| 

.. |PyPi| image:: https://badge.fury.io/py/joblib.svg
   :target: https://badge.fury.io/py/joblib
   :alt: Joblib version

.. |Azure| image:: https://dev.azure.com/joblib/joblib/_apis/build/status/joblib.joblib?branchName=master
   :target: https://dev.azure.com/joblib/joblib/_build?definitionId=3&_a=summary&branchFilter=40
   :alt: Azure CI status

.. |ReadTheDocs| image:: https://readthedocs.org/projects/joblib/badge/?version=latest
    :target: https://joblib.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. |Codecov| image:: https://codecov.io/gh/joblib/joblib/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/joblib/joblib
   :alt: Codecov coverage


The homepage of joblib with user documentation is located on:

https://joblib.readthedocs.io

Getting the latest code
=======================

To get the latest code using git, simply type::

    git clone git://github.com/joblib/joblib.git

If you don't have git installed, you can download a zip or tarball
of the latest code: http://github.com/joblib/joblib/archives/master

Installing
==========

You can use `pip` to install joblib::

    pip install joblib

from any directory or::

    python setup.py install

from the source directory.

Dependencies
============

- Joblib has no mandatory dependencies besides Python (supported versions are
  3.7+).
- Joblib has an optional dependency on Numpy (at least version 1.6.1) for array
  manipulation.
- Joblib includes its own vendored copy of
  `loky <https://github.com/tomMoral/loky>`_ for process management.
- Joblib can efficiently dump and load numpy arrays but does not require numpy
  to be installed.
- Joblib has an optional dependency on
  `python-lz4 <https://pypi.python.org/pypi/lz4>`_ as a faster alternative to
  zlib and gzip for compressed serialization.
- Joblib has an optional dependency on psutil to mitigate memory leaks in
  parallel worker processes.
- Some examples require external dependencies such as pandas. See the
  instructions in the `Building the docs`_ section for details.

Workflow to contribute
======================

To contribute to joblib, first create an account on `github
<http://github.com/>`_. Once this is done, fork the `joblib repository
<http://github.com/joblib/joblib>`_ to have your own repository,
clone it using 'git clone' on the computers where you want to work. Make
your changes in your clone, push them to your github account, test them
on several computers, and when you are happy with them, send a pull
request to the main repository.

Running the test suite
======================

To run the test suite, you need the pytest (version >= 3) and coverage modules.
Run the test suite using::

    pytest joblib

from the root of the project.

Building the docs
=================

To build the docs you need to have sphinx (>=1.4) and some dependencies
installed::

    pip install -U -r .readthedocs-requirements.txt

The docs can then be built with the following command::

    make doc

The html docs are located in the ``doc/_build/html`` directory.


Making a source tarball
=======================

To create a source tarball, eg for packaging or distributing, run the
following command::

    python setup.py sdist

The tarball will be created in the `dist` directory. This command will
compile the docs, and the resulting tarball can be installed with
no extra dependencies than the Python standard library. You will need
setuptool and sphinx.

Making a release and uploading it to PyPI
=========================================

This command is only run by project manager, to make a release, and
upload in to PyPI::

    python setup.py sdist bdist_wheel
    twine upload dist/*


Note that the documentation should automatically get updated at each git
push. If that is not the case, try building th doc locally and resolve
any doc build error (in particular when running the examples).

Updating the changelog
======================

Changes are listed in the CHANGES.rst file. They must be manually updated
but, the following git command may be used to generate the lines::

    git log --abbrev-commit --date=short --no-merges --sparse

",2023-07-07 15:59:56+00:00
jtracker,jt-cli,jtracker-io/jt-cli,JTracker Command Line Interface,,False,2,2020-02-04 13:31:34+00:00,2018-02-09 04:39:12+00:00,1,4,1,25,,,Apache License 2.0,250,0.2.0a33,33,2019-03-26 21:55:28+00:00,,2019-04-17 03:24:41+00:00,"# JTracker CLI

JTracker is a scientific workflow management system. It provides workflow authoring, 
sharing and execution with full provenance tracking. JTracker system is designed as client-server architecture for distributed
compute environments. All jobs are centrally managed by a JTracker server, JTracker executors (the clients)
request jobs/tasks from the server and execute them on compute nodes the executors reside.

## Installation

JTracker client needs to be installed on a workflow task execution host. It may be a VM in a cloud environment, an
HPC node, or may be just your laptop, or all of them at the same time.

```
# JTracker cli requires Python3
# install pip3 if not installed already, for Debian or Ubuntu platform do this:
sudo apt-get update
sudo apt-get install python3-pip

# install jtracker
sudo pip3 install jtracker   # pip3 install --upgrade jtracker  # use this to upgrade jtracker to latest version

# if you see usage information with the follow command, you are ready to go
jt --help
```

## Quick test run with demo JTracker workflow

JTracker is in early phase of development, features and behaviours may change as we advance forward. However this quick
test run should give you a clear picture how JTracker is designed and how it may fit in your workflow use cases.

**Before proceeding further, please make sure you have installed (or upgraded to) the latest version of JT-CLI tool.**

This test run uses a demo JT server running at https://jtracker.io.

Note: please **do not** upload sensitive data when following along the steps.

### Register a user account

Please change `your_account_name` to your own in the following command.

```
jt user signup -u your_account_name
```

### Log in as the new user

```
# logging in has not been fully implemented, no password needed for now
jt user login -u your_account_name
```

### Register a JT workflow under your account

The workflow we use for this demo is available here:
 https://github.com/jtracker-io/demo-workflows/tree/master/webpage-word-count-2.

The workflow git release tag is 'webpage-word-count-2.0.0.1':
 https://github.com/jtracker-io/demo-workflows/releases/tag/webpage-word-count-2.0.0.1

```
jt wf register --git-server https://github.com \
               --git-account jtracker-io \
               --git-repo demo-workflows \
               --git-path webpage-word-count-2 \
               --git-tag webpage-word-count-2.0.0.1 \
               --wf-name webpage-word-count-2 \
               --wf-version 0.0.1 \
               --wf-type JTracker
```

### Create a Job Queue for the workflow you would like to run from

The following command creates a job queue for
workflow: `webpage-word-count-2` with version: `0.0.1`.

```
jt queue add --wf-name webpage-word-count-2 \
             --wf-version 0.0.1
```

Upon successful creation, you will get a UUID for the new job queue, record it for the next step. In
my test, I got `00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3`.

It's possible to create a job queue based off a workflow registered under another user
given that the workflow is accessible to you. In this case, you provide workflow fullname,
eg, `user1.webpage-word-count` for `webpage-word-count` workflow owned by `user1`.

### Enqueue some jobs

Now you are ready to add some jobs to the new queue.

```
# remember to replace '00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3' with your own queue ID
jt job add -q 00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3 -j '{
  ""webpage"": ""[webpage.html]https://dzone.com/cloud-computing-tutorials-tools-news"",
  ""words"": [ ""Cloud"", ""Docker"", ""Kubernetes"", ""OpenStack"" ]
}'
```

You can enqueue a couple of more jobs, simply replace `webpage_url` and `words` with your favorite values and
repeat the above command. New jobs can be added to the queue at any time.

### Launch JT executor

Finally, let's launch a JT executor to run those jobs.

```
# again, replace '00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3' with your own queue ID
jt -V debug exec run -q 00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3 -t 0  # -t 0 disables auto retry on task failure
```

This will launch an executor that will pull and run jobs from queue `00e2b2e4-f2dc-420a-bb2d-3df6a7984cc3`. Current
running jobs/tasks will be displayed in stdout (this can be turned off later).

There are some useful options give you control over how jobs/tasks are to be run. For example,
`-k` and `-p` allow you control how many parallel tasks and jobs the executor can run respectively.
Option `-c` tells executor to run continuously even after it finises all the jobs in the queue. This is useful
when you know there will be more jobs to be queued and you don't want to start the executor again.
Try `jt exec run --help` to get more information.

To increase job processing throughput, you can run many JT executors on multiple compute nodes
(in any environment cloud or HPC) at the same time.

It's possible to implement auto-scaling on your own, for example, using Kubernetes to increase or
decrease worker nodes on which JT executor runs.

### Check job status and output

If the executor is still running, you can perform the following commands in a different terminal. To stop an running executor, simply cancel it with `Control-C`.

Get job status in queue `09360ea8-748a-4a8d-9b55-16b5b7278069`.
```
jt job ls -q 09360ea8-748a-4a8d-9b55-16b5b7278069
```

Get detail for a particular job `c36f6ed7-7639-4ffc-984e-f83e00936d4d` in queue `09360ea8-748a-4a8d-9b55-16b5b7278069`.
```
jt job get -j c36f6ed7-7639-4ffc-984e-f83e00936d4d -q 09360ea8-748a-4a8d-9b55-16b5b7278069
```

In the response JSON you will be able to find the word count result.
",2023-07-07 16:00:00+00:00
judi,JUDI,ncbi/JUDI,"This repository contains the source code of JUDI, a workflow management system for developing complex bioinformatics software with many parameter settings. Bioinformatics pipeline: Just Do It!",,False,31,2023-02-10 14:13:14+00:00,2019-04-18 16:13:15+00:00,11,17,1,0,,,MIT License,61,,0,,,2021-12-23 21:05:49+00:00,"# JUDI - Bioinformatics Pipeline: *Just Do It*

*judi* comes from the idea of bringing the power and efficiency of *doit* to
execute any kind of task under many combinations of parameter settings.


## Sample Code

Snippet from [tutorial](example2), save it as ``dodo.py``. Also download [a.txt](example2/a.txt) and [b.txt](example2/b.txt).

```python
from judi import File, Task, add_param, show_param_db, combine_csvs

add_param([1, 2], 'n')

class GetCounts(Task):
  """"""Count lines, words and characters in file""""""
  inputs = {'inp': File('text', path=['a.txt', 'b.txt'])}
  targets = {'out': File('counts.csv')}
  actions = [(""(echo line word char file; wc {}) | sed 's/^ \+//;s/ \+/,/g' > {}"", [""$inp"", ""$out""])]

class CombineCounts(Task):
  """"""Combine counts""""""
  mask = ['n']
  inputs = {'inp': GetCounts.targets['out']}
  targets = {'out': File('result.csv', mask=mask, root='.')}
  actions = [(combine_csvs, [""#inp"", ""#out""])]
```

Run from terminal:

```console
$ doit list
CombineCounts
GetCounts
Task
$ doit
. GetCounts:n~2
. GetCounts:n~1
. CombineCounts:
```

## Project Details

 - Website & docs - [https://pyjudi.readthedocs.io](https://pyjudi.readthedocs.io)
 - Project management on github - [https://github.com/ncbi/JUDI](https://github.com/ncbi/JUDI)

## License

The MIT License
Copyright (c) 2019-2020 Soumitra Pal

see LICENSE file


## Install

*judi* is tested on python 3.6.

```console
$ pip install judi
```

## Dependencies

- doit

## Documentation

``docs`` folder contains ReST documentation based on Sphinx.

```console
$ make html
```

## Contributing

On github create pull requests using a named feature branch.
",2023-07-07 16:00:05+00:00
jug,jug,luispedro/jug,Parallel programming with Python,https://jug.readthedocs.io,False,397,2023-06-29 01:25:50+00:00,2009-05-19 21:30:53+00:00,61,20,15,18,v2.3.0,2023-06-26 21:17:36+00:00,MIT License,962,v2.3.0,66,2023-06-24 22:18:25+00:00,2023-06-29 01:25:51+00:00,2023-06-24 22:18:25+00:00,"===========================================
Jug: A Task-Based Parallelization Framework
===========================================

Jug allows you to write code that is broken up into
tasks and run different tasks on different processors.

.. image:: https://github.com/luispedro/jug/actions/workflows/python-package.yml/badge.svg
       :target: https://github.com/luispedro/jug/actions/workflows/python-package.yml

.. image:: https://zenodo.org/badge/205237.svg
   :target: https://zenodo.org/badge/latestdoi/205237

.. image:: https://anaconda.org/conda-forge/jug/badges/installer/conda.svg
    :target: https://anaconda.org/conda-forge/jug

.. image:: https://static.pepy.tech/personalized-badge/jug?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads
   :target: https://pepy.tech/project/jug

.. image:: https://img.shields.io/badge/CITATION-doi.org%2F10.5334%2Fjors.161-green.svg
   :target: https://doi.org/10.5334/jors.161


It uses the filesystem to communicate between processes and
works correctly over NFS, so you can coordinate processes on
different machines.

Jug is a pure Python implementation and should work on any platform.

Python versions 3.5 and above are supported.

*Website*: `http://luispedro.org/software/jug <http://luispedro.org/software/jug>`__

*Documentation*: `https://jug.readthedocs.org/ <https://jug.readthedocs.org/>`__

*Video*: On `vimeo <http://vimeo.com/8972696>`__ or `showmedo
<http://showmedo.com/videotutorials/video?name=9750000;fromSeriesID=975>`__

*Mailing List*: `https://groups.google.com/group/jug-users
<https://groups.google.com/group/jug-users>`__

Testimonials
------------

""I've been using jug with great success to distribute the running of a
reasonably large set of parameter combinations"" - Andreas Longva


Install
-------

You can install Jug with pip::

    pip install Jug

or use, if you are using `conda <https://anaconda.org/>`__, you can install jug
from `conda-forge <https://conda-forge.github.io/>`__ using the following
commands::

    conda config --add channels conda-forge
    conda install jug

Citation
--------

If you use Jug to generate results for a scientific publication, please cite

    Coelho, L.P., (2017). Jug: Software for Parallel Reproducible Computation in
    Python. Journal of Open Research Software. 5(1), p.30.

    https://doi.org/10.5334/jors.161


Short Example
-------------

Here is a one minute example. Save the following to a file called ``primes.py``
(if you have installed jug, you can obtain a slightly longer version of this
example by running ``jug demo`` on the command line)::

    from jug import TaskGenerator
    from time import sleep

    @TaskGenerator
    def is_prime(n):
        sleep(1.)
        for j in range(2,n-1):
            if (n % j) == 0:
                return False
        return True

    primes100 = [is_prime(n) for n in range(2,101)]

This is a brute-force way to find all the prime numbers up to 100. Of course,
this is only for didactical purposes, normally you would use a better method.
Similarly, the ``sleep`` function is so that it does not run too fast. Still,
it illustrates the basic functionality of Jug for embarassingly parallel
problems.

Type ``jug status primes.py`` to get::

    Task name                  Waiting       Ready    Finished     Running
    ----------------------------------------------------------------------
    primes.is_prime                  0          99           0           0
    ......................................................................
    Total:                           0          99           0           0


This tells you that you have 99 tasks called ``primes.is_prime`` ready to run.
So run ``jug execute primes.py &``. You can even run multiple instances in the
background (if you have multiple cores, for example). After starting 4
instances and waiting a few seconds, you can check the status again (with ``jug
status primes.py``)::

    Task name                  Waiting       Ready    Finished     Running
    ----------------------------------------------------------------------
    primes.is_prime                  0          63          32           4
    ......................................................................
    Total:                           0          63          32           4


Now you have 32 tasks finished, 4 running, and 63 still ready. Eventually, they
will all finish and you can inspect the results with ``jug shell primes.py``.
This will give you an ``ipython`` shell. The `primes100` variable is available,
but it is an ugly list of `jug.Task` objects. To get the actual value, you call
the `value` function::

    In [1]: primes100 = value(primes100)

    In [2]: primes100[:10]
    Out[2]: [True, True, False, True, False, True, False, False, False, True]

What's New
----------

Version 2.3.0 (*25 June 2023*)

- jug shell: Add ``get_filtered_tasks()``
- jug: Fix ``jug --version`` (which had been broken in the refactoring to use subcommands)
- jug shell: Fix message in jug shell when there are no dependencies (it would repeatedly print the message stating *this will only be run once*)
- jug pack: Make it much faster to invalidate elements
- file_store: ensure that the temporary directory exists

Version 2.2.3 (*26 May 2023*)
- Fix ``jug shell`` for newer versions of IPython

Version 2.2.2 (*19 July 2022*)
- Fix ``jug cleanup`` when packs are used (``jug pack``)

Version 2.2.1 (*19 May 2022*)
- Fix bug with ``jug cleanup`` and the redis backend (`#86 <https://github.com/luispedro/jug/issues/86>`__)

Version 2.2.0 (*3 May 2022*)

- Add ``jug pack`` subcommand
- Make ``get_tasks()`` return a copy of the tasks inside ``jug shell``
- Remove ``six`` dependency

Version 2.1.1 (*18 March 2021*)

- Include requirements files in distribution

Version 2.1.0 (*18 March 2021*)

- Improvements to webstatus (by Robert Denham)
- Removed Python 2.7 support
- Fix output encoding for Python 3.8
- Fix bug mixing ``mapreduce()`` & ``status --cache``
- Make block_access (used in ``mapreduce()``) much faster (20x)
- Fix important redis bug
- More precise output in ``cleanup`` command

Version 2.0.2 (Thu Jun 11 2020)

- Fix command line argument parsing

Version 2.0.1 (Thu Jun 11 2020)

- Fix handling of ``JUG_EXIT_IF_FILE_EXISTS`` environmental variable
- Fix passing an argument to ``jug.main()`` function
- Extend ``--pdb`` to exceptions raised while importing the jugfile (issue #79)

version **2.0.0** (Fri Feb 21 2020)

- jug.backend.base_store has 1 new method 'listlocks'
- jug.backend.base_lock has 2 new methods 'fail' and 'is_failed'
- Add 'jug execute --keep-failed' to preserve locks on failing tasks.
- Add 'jug cleanup --failed-only' to remove locks from failed tasks
- 'jug status' and 'jug graph' now display failed tasks
- Check environmental exit variables by default (suggested by Renato Alves, issue #66)
- Fix 'jug sleep-until' in the presence of barrier() (issue #71)

For older version see ``ChangeLog`` file or the `full history
<https://jug.readthedocs.io/en/latest/history.html>`__.




",2023-07-07 16:00:09+00:00
jupyterworkflow,jupyter-workflow,alpha-unito/jupyter-workflow,Distributed workflows design and execution with Jupyter Notebooks.,https://jupyter-workflow.di.unito.it,False,7,2023-06-19 20:51:48+00:00,2021-01-03 10:26:34+00:00,0,6,2,4,0.1.0.dev3,2022-04-27 09:34:09+00:00,GNU Lesser General Public License v3.0,110,0.1.0.dev3,4,2022-04-27 09:34:09+00:00,2023-07-06 13:01:55+00:00,2023-06-27 12:59:32+00:00,"# Jupyter Workflow

The Jupyter Workflow framework enables Jupyter Notebooks to describe complex workflows and to execute them in a distributed fashion on hybrid HPC-Cloud infrastructures. Jupyter Workflow relies on the [StreamFlow](https://github.com/alpha-unito/streamflow) WMS as its underlying runtime support.

## Install Jupyter Workflow

The Jupyter Workflow IPython kernel is available on [PyPI](https://pypi.org/project/jupyter-workflow/), so you can install it using pip.

```bash
pip install jupyter-workflow
```

Then, you can install it on a Jupyter Notebook server by running the following command.

```bash
python -m jupyter_workflow.ipython.install
```

Please note that Jupyter Workflow requires `python >= 3.8`. Then you can associate your Jupyter Notebooks with the newly installed kernel. Some examples can be found under the `examples` folder in the [GitHub repository](https://github.com/alpha-unito/jupyter-workflow).

## Jupyter Workflow Team

Iacopo Colonnelli <iacopo.colonnelli@unito.it> (creator and maintainer)  
Sergio Rabellino <sergio.rabellino@unito.it> (maintainer)  
Barbara Cantalupo <barbara.cantalupo@unito.it> (maintainer)  
Marco Aldinucci <aldinuc@di.unito.it> (maintainer)  
",2023-07-07 16:00:13+00:00
kapacitor,kapacitor,influxdata/kapacitor,"Open source framework for processing, monitoring, and alerting on time series data",,False,2231,2023-07-05 15:55:35+00:00,2015-08-31 14:54:42+00:00,497,125,109,37,v1.6.6,2023-04-28 19:19:18+00:00,MIT License,1978,v1.7.0-rc6,143,2023-05-17 16:08:36+00:00,2023-07-06 05:00:18+00:00,2023-06-20 15:48:32+00:00,"# Kapacitor [![Circle CI](https://circleci.com/gh/influxdata/kapacitor/tree/master.svg?style=svg&circle-token=78c97422cf89526309e502a290c230e8a463229f)](https://circleci.com/gh/influxdata/kapacitor/tree/master) [![Docker pulls](https://img.shields.io/docker/pulls/library/kapacitor.svg)](https://hub.docker.com/_/kapacitor/)
Open source framework for processing, monitoring, and alerting on time series data

# Installation

Kapacitor has two binaries:

* kapacitor – a CLI program for calling the Kapacitor API.
* kapacitord – the Kapacitor server daemon.

You can either download the binaries directly from the [downloads](https://influxdata.com/downloads/#kapacitor) page or go get them:

```sh
go get github.com/influxdata/kapacitor/cmd/kapacitor
go get github.com/influxdata/kapacitor/cmd/kapacitord
```

# Configuration
An example configuration file can be found [here](https://github.com/influxdata/kapacitor/blob/master/etc/kapacitor/kapacitor.conf)

Kapacitor can also provide an example config for you using this command:

```sh
kapacitord config
```


# Getting Started

This README gives you a high level overview of what Kapacitor is and what its like to use it. As well as some details of how it works.
To get started using Kapacitor see [this guide](https://docs.influxdata.com/kapacitor/latest/introduction/getting-started/). After you finish the getting started exercise you can check out the [TICKscripts](https://github.com/influxdata/kapacitor/tree/master/examples/telegraf) for different Telegraf plugins.

# Basic Example

Kapacitor uses a DSL named [TICKscript](https://docs.influxdata.com/kapacitor/latest/tick/) to define tasks.

A simple TICKscript that alerts on high cpu usage looks like this:

```javascript
stream
    |from()
        .measurement('cpu_usage_idle')
        .groupBy('host')
    |window()
        .period(1m)
        .every(1m)
    |mean('value')
    |eval(lambda: 100.0 - ""mean"")
        .as('used')
    |alert()
        .message('{{ .Level}}: {{ .Name }}/{{ index .Tags ""host"" }} has high cpu usage: {{ index .Fields ""used"" }}')
        .warn(lambda: ""used"" > 70.0)
        .crit(lambda: ""used"" > 85.0)

        // Send alert to hander of choice.

        // Slack
        .slack()
        .channel('#alerts')

        // VictorOps
        .victorOps()
        .routingKey('team_rocket')

        // PagerDuty
        .pagerDuty()
```

Place the above script into a file `cpu_alert.tick` then run these commands to start the task:

```sh
# Define the task (assumes cpu data is in db 'telegraf')
kapacitor define \
    cpu_alert \
    -type stream \
    -dbrp telegraf.default \
    -tick ./cpu_alert.tick
# Start the task
kapacitor enable cpu_alert
```
",2023-07-07 16:00:18+00:00
kedro,kedro,kedro-org/kedro,"Kedro is a toolbox for production-ready data science. It uses software engineering best practices to help you create data engineering and data science pipelines that are reproducible, maintainable, and modular.",https://kedro.org,False,8502,2023-07-07 15:30:29+00:00,2019-04-18 10:29:56+00:00,803,104,180,41,0.18.11,2023-07-03 12:19:53+00:00,Apache License 2.0,2308,0.18.11,41,2023-07-03 12:19:53+00:00,2023-07-07 15:35:25+00:00,2023-07-07 15:34:45+00:00,"![Kedro Logo Banner - Light](.github/demo-dark.png#gh-dark-mode-only)
![Kedro Logo Banner - Dark](.github/demo-light.png#gh-light-mode-only)
[![Python version](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10-blue.svg)](https://pypi.org/project/kedro/)
[![PyPI version](https://badge.fury.io/py/kedro.svg)](https://pypi.org/project/kedro/)
[![Conda version](https://img.shields.io/conda/vn/conda-forge/kedro.svg)](https://anaconda.org/conda-forge/kedro)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/kedro-org/kedro/blob/main/LICENSE.md)
[![Slack Organisation](https://img.shields.io/badge/slack-chat-blueviolet.svg?label=Kedro%20Slack&logo=slack)](https://slack.kedro.org)
![CircleCI - Main Branch](https://img.shields.io/circleci/build/github/kedro-org/kedro/main?label=main)
![Develop Branch Build](https://img.shields.io/circleci/build/github/kedro-org/kedro/develop?label=develop)
[![Documentation](https://readthedocs.org/projects/kedro/badge/?version=stable)](https://docs.kedro.org/)
[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6711/badge)](https://bestpractices.coreinfrastructure.org/projects/6711)

## What is Kedro?

Kedro is a toolbox for production-ready data science. It uses software engineering best practices to help you create data engineering and data science pipelines that are reproducible, maintainable, and modular.

Kedro is an open-source Python framework hosted by the [LF AI & Data Foundation](https://lfaidata.foundation/).

## How do I install Kedro?

To install Kedro from the Python Package Index (PyPI) run:

```
pip install kedro
```

It is also possible to install Kedro using `conda`:

```
conda install -c conda-forge kedro
```

Our [Get Started guide](https://docs.kedro.org/en/stable/get_started/install.html) contains full installation instructions, and includes how to set up Python virtual environments.

## What are the main features of Kedro?

![Kedro-Viz Pipeline Visualisation](https://github.com/kedro-org/kedro-viz/blob/main/.github/img/banner.png)
_A pipeline visualisation generated using [Kedro-Viz](https://github.com/kedro-org/kedro-viz)_

| Feature              | What is this?                                                                                                                                                                                                                                                                                                                                                                                  |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Project Template     | A standard, modifiable and easy-to-use project template based on [Cookiecutter Data Science](https://github.com/drivendata/cookiecutter-data-science/).                                                                                                                                                                                                                                        |
| Data Catalog         | A series of lightweight data connectors used to save and load data across many different file formats and file systems, including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems.                                                                                                              |
| Pipeline Abstraction | Automatic resolution of dependencies between pure Python functions and data pipeline visualisation using [Kedro-Viz](https://github.com/kedro-org/kedro-viz).                                                                                                                                                                                                                                  |
| Coding Standards     | Test-driven development using [`pytest`](https://github.com/pytest-dev/pytest), produce well-documented code using [Sphinx](http://www.sphinx-doc.org/en/master/), create linted code with support for [`flake8`](https://github.com/PyCQA/flake8), [`isort`](https://github.com/PyCQA/isort) and [`black`](https://github.com/psf/black) and make use of the standard Python logging library. |
| Flexible Deployment  | Deployment strategies that include single or distributed-machine deployment as well as additional support for deploying on Argo, Prefect, Kubeflow, AWS Batch and Databricks.                                                                                                                                                                                                                  |

## How do I use Kedro?

The [Kedro documentation](https://docs.kedro.org/en/stable/) first explains [how to install Kedro](https://docs.kedro.org/en/stable/get_started/install.html) and then introduces [key Kedro concepts](https://docs.kedro.org/en/stable/get_started/kedro_concepts.html).

- The first example illustrates the [basics of a Kedro project](https://docs.kedro.org/en/stable/get_started/new_project.html) using the Iris dataset
- You can then review the [spaceflights tutorial](https://docs.kedro.org/en/stable/tutorial/tutorial_template.html) to build a Kedro project for hands-on experience

For new and intermediate Kedro users, there's a comprehensive section on [how to visualise Kedro projects using Kedro-Viz](https://docs.kedro.org/en/stable/visualisation/kedro-viz_visualisation.html) and [how to work with Kedro and Jupyter notebooks](https://docs.kedro.org/en/stable/notebooks_and_ipython/kedro_and_notebooks).

Further documentation is available for more advanced Kedro usage and deployment. We also recommend the [glossary](https://docs.kedro.org/en/stable/resources/glossary.html) and the [API reference documentation](/kedro) for additional information.

## Why does Kedro exist?

Kedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:

- To address the main shortcomings of Jupyter notebooks, one-off scripts, and glue-code because there is a focus on
  creating **maintainable data science code**
- To enhance **team collaboration** when different team members have varied exposure to software engineering concepts
- To increase efficiency, because applied concepts like modularity and separation of concerns inspire the creation of
  **reusable analytics code**

## The humans behind Kedro

The [Kedro product team](https://docs.kedro.org/en/stable/contribution/technical_steering_committee.html#kedro-maintainers) and a number of [open source contributors from across the world](https://github.com/kedro-org/kedro/releases) maintain Kedro.

## Can I contribute?

Yes! Want to help build Kedro? Check out our [guide to contributing to Kedro](https://github.com/kedro-org/kedro/blob/main/CONTRIBUTING.md).

## Where can I learn more?

There is a growing community around Kedro. Have a look at the [Kedro FAQs](https://docs.kedro.org/en/stable/faq/faq.html#how-can-i-find-out-more-about-kedro) to find projects using Kedro and links to articles, podcasts and talks.

## Who likes Kedro?

There are Kedro users across the world, who work at start-ups, major enterprises and academic institutions like [Absa](https://www.absa.co.za/),
[Acensi](https://acensi.eu/page/home),
[Advanced Programming Solutions SL](https://www.linkedin.com/feed/update/urn:li:activity:6863494681372721152/),
[AI Singapore](https://makerspace.aisingapore.org/2020/08/leveraging-kedro-in-100e/),
[AMAI GmbH](https://www.am.ai/),
[Augment Partners](https://www.linkedin.com/posts/augment-partners_kedro-cheat-sheet-by-augment-activity-6858927624631283712-Ivqk),
[AXA UK](https://www.axa.co.uk/),
[Belfius](https://www.linkedin.com/posts/vangansen_mlops-machinelearning-kedro-activity-6772379995953238016-JUmo),
[Beamery](https://medium.com/hacking-talent/production-code-for-data-science-and-our-experience-with-kedro-60bb69934d1f),
[Caterpillar](https://www.caterpillar.com/),
[CRIM](https://www.crim.ca/en/),
[Dendra Systems](https://www.dendra.io/),
[Element AI](https://www.elementai.com/),
[GetInData](https://getindata.com/blog/running-machine-learning-pipelines-kedro-kubeflow-airflow),
[GMO](https://recruit.gmo.jp/engineer/jisedai/engineer/jisedai/engineer/jisedai/engineer/jisedai/engineer/jisedai/blog/kedro_and_mlflow_tracking/),
[Indicium](https://medium.com/indiciumtech/how-to-build-models-as-products-using-mlops-part-2-machine-learning-pipelines-with-kedro-10337c48de92),
[Imperial College London](https://github.com/dssg/barefoot-winnie-public),
[ING](https://www.ing.com),
[Jungle Scout](https://junglescouteng.medium.com/jungle-scout-case-study-kedro-airflow-and-mlflow-use-on-production-code-150d7231d42e),
[Helvetas](https://www.linkedin.com/posts/lionel-trebuchon_mlflow-kedro-ml-ugcPost-6747074322164154368-umKw),
[Leapfrog](https://www.lftechnology.com/blog/ai-pipeline-kedro/),
[McKinsey & Company](https://www.mckinsey.com/alumni/news-and-insights/global-news/firm-news/kedro-from-proprietary-to-open-source),
[Mercado Libre Argentina](https://www.mercadolibre.com.ar),
[Modec](https://www.modec.com/),
[Mosaic Data Science](https://www.youtube.com/watch?v=fCWGevB366g),
[NaranjaX](https://www.youtube.com/watch?v=_0kMmRfltEQ),
[NASA](https://github.com/nasa/ML-airport-taxi-out),
[NHS AI Lab](https://nhsx.github.io/skunkworks/synthetic-data-pipeline),
[Open Data Science LatAm](https://www.odesla.org/),
[Prediqt](https://prediqt.co/),
[QuantumBlack](https://medium.com/quantumblack/introducing-kedro-the-open-source-library-for-production-ready-machine-learning-code-d1c6d26ce2cf),
[ReSpo.Vision](https://neptune.ai/customers/respo-vision),
[Retrieva](https://tech.retrieva.jp/entry/2020/07/28/181414),
[Roche](https://www.roche.com/),
[Sber](https://www.linkedin.com/posts/seleznev-artem_welcome-to-kedros-documentation-kedro-activity-6767523561109385216-woTt),
[Société Générale](https://www.societegenerale.com/en),
[Telkomsel](https://medium.com/life-at-telkomsel/how-we-build-a-production-grade-data-pipeline-7004e56c8c98),
[Universidad Rey Juan Carlos](https://github.com/vchaparro/MasterThesis-wind-power-forecasting/blob/master/thesis.pdf),
[UrbanLogiq](https://urbanlogiq.com/),
[Wildlife Studios](https://wildlifestudios.com),
[WovenLight](https://www.wovenlight.com/) and
[XP](https://youtu.be/wgnGOVNkXqU?t=2210).

Kedro won [Best Technical Tool or Framework for AI](https://awards.ai/the-awards/previous-awards/the-4th-ai-award-winners/) in the 2019 Awards AI competition and a merit award for the 2020 [UK Technical Communication Awards](https://uktcawards.com/announcing-the-award-winners-for-2020/). It is listed on the 2020 [ThoughtWorks Technology Radar](https://www.thoughtworks.com/radar/languages-and-frameworks/kedro) and the 2020 [Data & AI Landscape](https://mattturck.com/data2020/). Kedro has received an [honorable mention in the User Experience category in Fast Company’s 2022 Innovation by Design Awards](https://www.fastcompany.com/90772252/user-experience-innovation-by-design-2022).

## How can I cite Kedro?

If you're an academic, Kedro can also help you, for example, as a tool to solve the problem of reproducible research. Use the ""Cite this repository"" button on [our repository](https://github.com/kedro-org/kedro) to generate a citation from the [CITATION.cff file](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files).
",2023-07-07 16:00:22+00:00
kestra,kestra,kestra-io/kestra,"Kestra is an infinitely scalable orchestration and scheduling platform, creating, running, scheduling, and monitoring millions of complex pipelines.",https://kestra.io,False,3585,2023-07-07 04:29:18+00:00,2019-08-24 13:56:15+00:00,207,39,27,24,v0.10.0,2023-07-06 12:42:44+00:00,Apache License 2.0,1802,v0.10.0,74,2023-07-06 12:42:42+00:00,2023-07-07 14:53:21+00:00,2023-07-06 12:42:54+00:00,"<p align=""center"">
  <a href=""https://www.kestra.io"">
    <img width=""460"" src=""https://kestra.io/logo.svg""  alt=""Kestra workflow orchestrator"" />
  </a>
</p>

<h1 align=""center"" style=""border-bottom: none"">
    Event-driven declarative orchestrator to simplify data operations <br>
</h1>

<div align=""center"">
  <a href=""https://github.com/kestra-io/kestra/blob/develop/LICENSE""><img src=""https://img.shields.io/github/license/kestra-io/kestra?style=flat-square"" alt=""License"" /></a>
  <a href=""https://github.com/kestra-io/kestra/pulse""><img src=""https://img.shields.io/github/commit-activity/m/kestra-io/kestra?style=flat-square"" alt=""Commits-per-month""></a>
  <a href=""https://github.com/kestra-io/kestra/stargazers""><img src=""https://img.shields.io/github/stars/kestra-io/kestra.svg?style=flat-square"" alt=""Github star"" /></a>
  <a href=""https://github.com/kestra-io/kestra/releases""><img src=""https://img.shields.io/github/tag-pre/kestra-io/kestra.svg?style=flat-square"" alt=""Last Version"" /></a>
  <a href=""https://hub.docker.com/r/kestra/kestra""><img src=""https://img.shields.io/docker/pulls/kestra/kestra.svg?style=flat-square"" alt=""Docker pull"" /></a>
  <a href=""https://artifacthub.io/packages/helm/kestra/kestra""><img src=""https://img.shields.io/badge/Artifact%20Hub-kestra-417598?style=flat-square&logo=artifacthub"" alt=""Artifact Hub"" /></a>
  <a href=""https://kestra.io""><img src=""https://img.shields.io/badge/Website-kestra.io-192A4E?style=flat-square"" alt=""Kestra infinitely scalable orchestration and scheduling platform""></a>
  <a href=""https://kestra.io/slack""><img src=""https://img.shields.io/badge/Slack-chat-400d40?style=flat-square&logo=slack"" alt=""Slack""></a>
  <a href=""https://github.com/kestra-io/kestra/discussions""><img src=""https://img.shields.io/github/discussions/kestra-io/kestra?style=flat-square"" alt=""Github discussions""></a>
  <a href=""https://twitter.com/kestra_io""><img src=""https://img.shields.io/twitter/follow/kestra_io?style=flat-square"" alt=""Twitter"" /></a>
  <a href=""https://app.codecov.io/gh/kestra-io/kestra""><img src=""https://img.shields.io/codecov/c/github/kestra-io/kestra?style=flat-square&token=It6L7BTaWK"" alt=""Code Cov"" /></a>
  <a href=""https://github.com/kestra-io/kestra/actions""><img src=""https://img.shields.io/github/actions/workflow/status/kestra-io/kestra/main.yml?branch=develop&style=flat-square"" alt=""Github Actions"" /></a>
</div>

<br />

<p align=""center"">
    <a href=""https://kestra.io/""><b>Website</b></a> •
    <a href=""https://twitter.com/kestra_io""><b>Twitter</b></a> •
    <a href=""https://www.linkedin.com/company/kestra/""><b>Linked In</b></a> •
    <a href=""https://kestra.io/slack""><b>Slack</b></a> •
    <a href=""https://kestra.io/docs/""><b>Documentation</b></a>
</p>

<br />

<p align=""center""><img src=""https://kestra.io/video.gif"" alt=""modern data orchestration and scheduling platform "" width=""640px"" /></p>


## Live Demo

Try Kestra using our [live demo](https://demo.kestra.io).

## What is Kestra

Kestra is an open-source, **event-driven** orchestrator that simplifies data operations and improves collaboration between engineers and business users. By bringing **Infrastructure as Code** best practices to data pipelines, Kestra allows you to build reliable workflows and manage them with confidence.

Thanks to the **declarative YAML interface** for defining orchestration logic, everyone who benefits from analytics can participate in the data pipeline creation process. The UI automatically adjusts the YAML definition any time you make changes to a workflow from the UI or via an API call. Therefore, the orchestration logic is defined declaratively in code, even if some workflow components are modified in other ways.

![Adding new tasks in the UI](https://kestra.io/adding-tasks.gif)


## Key concepts

1. `Flow` is the main component in Kestra. It's a container for your tasks and orchestration logic.
2. `Namespace` is used to provide logical isolation, e.g., to separate development and production environments. Namespaces are like folders on your file system — they organize flows into logical categories and can be nested to provide a hierarchical structure.
3. `Tasks` are atomic actions in a flow. By default, all tasks in the list will be executed sequentially, with additional customization options, a.o. to run tasks in parallel or allow a failure of specific tasks when needed.
4. `Triggers` define when a flow should run. In Kestra, flows are triggered based on events. Examples of such events include:
    - a regular time-based **schedule**
    - an **API** call (*webhook trigger*)
    - ad-hoc execution from the **UI**
    - a **flow trigger** - flows can be triggered from other flows using a [flow trigger](https://kestra.io/docs/developer-guide/triggers/flow) or a [subflow](https://kestra.io/docs/flow-examples/subflow), enabling highly modular workflows.
    - **custom events**, including a new file arrival (*file detection event*), a new message in a message bus, query completion, and more.
5. `Inputs` allow you to pass runtime-specific variables to a flow. They are strongly typed, and allow additional [validation rules](https://kestra.io/docs/developer-guide/inputs#input-validation).


## Extensible platform via plugins

Most tasks in Kestra are available as [plugins](https://kestra.io/plugins), but many type of tasks are available in the core library, including a.o. script tasks supporting various programming languages (e.g., Python, Node, Bash) and the ability to orchestrate your business logic packaged into Docker container images.

To create your own plugins, check the [plugin developer guide](https://kestra.io/docs/plugin-developer-guide).

## Rich orchestration capabilities

Kestra provides a variety of tasks to handle both simple and complex business logic, including:

- retries
- timeout
- error handling
- conditional branching
- dynamic tasks
- sequential and parallel tasks
- skipping tasks or triggers when needed by setting the flag `disabled` to `true`.
- configuring dependencies between tasks, flows and triggers
- advanced scheduling and trigger conditions
- backfills
- documenting your flows, tasks and triggers by adding a markdown description to any component
- adding labels to add additional metadata to your flows such as the flow owner or team:

```yaml
id: hello
namespace: prod
description: Hi from `Kestra` and a **markdown** description.
labels:
  owner: john-doe
  team: data-engineering
tasks:
  - id: hello
    type: io.kestra.core.tasks.log.Log
    message: Hello world!
    description: a *very* important task
    disabled: false
    timeout: 10M
    retry:
      type: constant # type: string
      interval: PT15M # type: Duration
      maxDuration: PT1H # type: Duration
      maxAttempt: 5 # type: int
      warningOnRetry: true # type: boolean, default is false
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    concurrent: 3
    tasks:
      - id: task1
        type: io.kestra.core.tasks.scripts.Bash
        commands:
          - 'echo ""running {{task.id}}""'
          - 'sleep 10'
      - id: task2
        type: io.kestra.core.tasks.scripts.Bash
        commands:
          - 'echo ""running {{task.id}}""'
          - 'sleep 10'
      - id: task3
        type: io.kestra.core.tasks.scripts.Bash
        commands:
          - 'echo ""running {{task.id}}""'
          - 'sleep 10'
triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/15 * * * *""
    backfill:
      start: 2023-06-25T14:00:00Z
```


## Built-in code editor

You can write workflows directly from the UI. When writing your workflows, the UI provides:
- autocompletion
- syntax validation
- embedded plugin documentation
- topology view (view of your dependencies in a Directed Acyclic Graph) that get updated live as you modify and add new tasks.


## Getting Started

To get a local copy up and running, follow the steps below.

### Prerequisites

Make sure that Docker is installed and running on your system. The default installation requires the following:
- [Docker](https://docs.docker.com/engine/install/)
- [Docker Compose](https://docs.docker.com/compose/install/)


### Launch Kestra

Download the Docker Compose file:

```sh
curl -o docker-compose.yml https://raw.githubusercontent.com/kestra-io/kestra/develop/docker-compose.yml
```

Alternatively, you can use `wget https://raw.githubusercontent.com/kestra-io/kestra/develop/docker-compose.yml`.


Start Kestra:

```sh
docker-compose up
```


Open `http://localhost:8080` in your browser and create your first flow.


### Hello-World flow

Here is a simple example logging hello world message to the terminal:

```yaml
id: hello
namespace: prod
tasks:
  - id: hello-world
    type: io.kestra.core.tasks.log.Log
    message: Hello world!
```

For more information:

- Follow the [getting started tutorial](https://kestra.io/docs/getting-started/).
- Read the [documentation](https://kestra.io/docs/) to understand how to:
  - [Develop your flows](https://kestra.io/docs/developer-guide/)
  - [Deploy Kestra](https://kestra.io/docs/administrator-guide/)
  - Use our [Terraform provider](https://kestra.io/docs/terraform/) to deploy your flows
  - Develop your [own plugins](https://kestra.io/docs/plugin-developer-guide/).




## Plugins
Kestra is built on a [plugin system](https://kestra.io/plugins/). You can find your plugin to interact with your provider; alternatively, you can follow [these steps](https://kestra.io/docs/plugin-developer-guide/) to develop your own plugin.


For a full list of plugins, check the [plugins page](https://kestra.io/plugins/).

Here are some examples of the available plugins:

<table>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-airbyte"">Airbyte</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-aws#s3"">Amazon S3</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-serdes#avro"">Avro</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-azure/#storage-blob"">Azure Blob Storage</a></td>
        <td><a href=""https://kestra.io/plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.bash"">Bash</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-gcp#bigquery"">Big Query</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-serdes#csv"">CSV</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-cassandra"">Cassandra</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-clickhouse"">ClickHouse</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-dbt"">DBT</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-debezium-mysql"">Debezium MYSQL</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-debezium-postgres"">Debezium Postgres</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-debezium-sqlserver"">Debezium Microsoft SQL Server</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-duckdb"">DuckDb</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-elasticsearch"">ElasticSearch</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-fivetran"">Fivetran</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-notifications#mail"">Email</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-fs#ftp"">FTP</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-fs#ftps"">FTPS</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-gcp#gcs"">Google Cloud Storage</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-googleworkspace#drive"">Google Drive</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-googleworkspace#sheets"">Google Sheets</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-script-groovy"">Groovy</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-fs#http"">Http</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-serdes#json"">JSON</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-script-jython"">Jython</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-kafka"">Kafka</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-kubernetes"">Kubernetes</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-mqtt"">MQTT</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-sqlserver"">Microsoft SQL Server</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-mongodb"">MongoDb</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-mysql"">MySQL</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-script-nashorn"">Nashorn</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.node"">Node</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-crypto#openpgp"">Open PGP</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-oracle"">Oracle</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-serdes#parquet"">Parquet</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-pinot"">Apache Pinot</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-postgres"">Postgres</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-powerbi"">Power BI</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-pulsar"">Apache Pulsar</a></td>
        <td><a href=""https://kestra.io/plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.python"">Python</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-redshift"">Redshift</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-rockset"">Rockset</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-fs#sftp"">SFTP</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-servicenow"">ServiceNow</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-singer"">Singer</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-notifications#slack"">Slack</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-snowflake"">Snowflake</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-soda"">Soda</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-spark"">Spark</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-tika"">Tika</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-trino"">Trino</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-vectorwise"">Vectorwise</a></td>
    </tr>
    <tr>
        <td><a href=""https://kestra.io/plugins/plugin-serdes#xml"">XML</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-gcp#vertexai/"">Vertex AI</a></td>
        <td><a href=""https://kestra.io/plugins/plugin-jdbc-vertica"">Vertica</a></td>
    </tr>
</table>



This list is growing quickly and we welcome contributions.

## Community Support

If you need help or have any questions, reach out using one of the following channels:

- [GitHub discussions](https://github.com/kestra-io/kestra/discussions) - useful to start a conversation that is not a bug or feature request.
- [Slack](https://kestra.io/slack) - join the community and get the latest updates.
- [Twitter](https://twitter.com/kestra_io) - to follow up with the latest updates.


## Roadmap

See the [open issues](https://github.com/kestra-io/kestra/issues) for a list of proposed features (and known issues) or look at the [project board](https://github.com/orgs/kestra-io/projects/2).


## Contributing

We love contributions, big or small. Check out [our contributor guide](https://github.com/kestra-io/kestra/blob/develop/.github/CONTRIBUTING.md) for details on how to contribute to Kestra.

See our [Plugin Developer Guide](https://kestra.io/docs/plugin-developer-guide/) for details on developing and publishing Kestra plugins.


## License
Apache 2.0 © [Kestra Technologies](https://kestra.io)
",2023-07-07 16:00:27+00:00
ketrew,ketrew,hammerlab/ketrew,Keep Track of Experimental Workflows,http://www.hammerlab.org/docs/ketrew/master/index.html,False,74,2023-03-15 16:09:19+00:00,2014-05-28 04:18:29+00:00,10,18,8,10,ketrew.3.2.1,2017-08-29 19:40:58+00:00,Apache License 2.0,1790,old-lwt,12,2017-05-04 15:14:16+00:00,,2018-02-05 00:28:15+00:00,"Ketrew: Keep Track of Experimental Workflows
============================================

**Ketrew** is:

1. An OCaml library providing an EDSL API to define complex and convoluted
   workflows (interdependent steps/programs using a lot of data, with many
   parameter variations, running on different hosts with various schedulers).
2. A client-server application to interact with these workflows.  The engine at
   heart of the server takes care of orchestrating workflows, and keeps track of
   everything that succeeds, fails, or gets lost.

See also the
[documentation](http://www.hammerlab.org/docs/ketrew/master/index.html)
for various releases.

If you have any questions, you may submit an
[issue](https://github.com/hammerlab/ketrew/issues), or join
the authors on the public “Slack” channel of the Hammer Lab:
[![Slack Status](http://publicslack.hammerlab.org/badge.svg)](http://publicslack.hammerlab.org)

Build & Install
---------------

See the specific documentation
on [building and installing](src/doc/Build_and_Install.md).
*TL;DR for OCaml hackers:*
 
    opam switch 4.03.0
    opam install [postgresql] [tls] ketrew

Getting Started
---------------

Ketrew is very flexible and hence may seem difficult to understand and setup at
first.
Let's get a minimal setup ready and a workflow running on it.

### Server-side Setup

This *tutorial* requires
[`docker`](https://en.wikipedia.org/wiki/Docker_%28software%29)
and [`docker-compose`](https://docs.docker.com/compose/):

    alias kdc='./tools/docker-compose/kdc.sh'
    kdc config --services

Let's get a Ketrew server with a PostgreSQL database and
a [Coclobas](https://github.com/hammerlab/coclobas) scheduler talking to each
other:

    kdc up -d
    
Check that everything is running:

    kdc ps

Check the Ketrew server status:

    curl http://127.0.0.1:8123/hello || echo 'Not ready'

After a minute or two you can check that everything is setup by visiting the
Ketrew UI: <http://127.0.0.1:8123/gui?token=nekot>:

<div>
<img  width=""100%""
src=""https://cloud.githubusercontent.com/assets/617111/23189040/047945d4-f85f-11e6-9453-feb3515fb7ca.png""
>
</div>

At any moment you can take everything down with:

    kdc down

Or use other inspection commands:

    kdc --help

### Client

We can now create a Ketrew client configuration, please choose a directory:

    export KETREW_ROOT=$HOME/tmp/kclient-config/

and initialize Ketrew there:

    ketrew init --configuration-path $KETREW_ROOT \
        --just-client http://127.0.0.1:8123/gui?token=nekot

The `ketrew submit` sub-command can create one-command workflows (uses the
`$KETREW_ROOT` path):

    ketrew submit \
         --wet-run --tag 1st-workflow --tag command-line \
         --daemonize /tmp/KT,'du -sh $HOME'

The job will appear on the WebUI and you can inspect/restart/kill it.

<div>
<img width=""100%""
  src=""https://cloud.githubusercontent.com/assets/617111/9421006/17bceb36-483a-11e5-8845-bb2234697a14.gif"">
</div>


If you don't like Web UI's you can use the text-based UI:

    $ ketrew interact
    [ketrew]
        Main menu
        Press a single key:
        * [q]: Quit
        * [v]: Toggle verbose
        * [s]: Display current status
        * [l]: Loop displaying the status
        * [k]: Kill targets
        * [e]: The Target Explorer™

As you can see, just from the command line, you can use `ketrew submit` to
launch *daemonized* tasks. To go further we need to use Ketrew's EDSL.

The EDSL: Defining Workflows
----------------------------

### Overview

The EDSL is an OCaml library where functions are used to build a workflow
data-structure. `Ketrew.Client.submit_workflow` is used to submit that
datastructure to the engine.

A workflow is a graph of “workflow-nodes” (sometimes called “targets”).

There are three kinds of links (edges) between nodes:

- `depends_on`: nodes that need to be ensured or satisfied before a node
  can start,
- `on_failure_activate`: nodes that will be activated if the node fails, and
- `on_success_activate`: nodes that will be activated only *after* a node
  succeeds.

See the `Ketrew.EDSL.workflow_node` function documentation for details. Any
OCaml program can use the EDSL (script, compiled, or even inside the
toplevel). See the documentation of the EDSL API (`Ketrew.EDSL`).

### A Quick Taste

You can run these commands for example in `utop` (`opam install utop`).

Load the Ketrew library to build and submit workflows:

```ocaml
#use ""topfind"";;
#thread;;
#require ""ketrew"";;
```

Globally tell the Ketrew client to get its configuration from the file created
[above](#Client):

```ocaml
let () =
  Unix.putenv
    ""KETREW_CONFIGURATION""
    (Sys.getenv ""HOME"" ^ ""/tmp/kclient-config/configuration.ml"");;
```

Submit an “empty” workflow-node to Ketrew (i.e. a node that does not do nor
*ensure* anything):

```ocaml
let () =
  let open Ketrew.EDSL in
  workflow_node without_product
    ~name:""Mostly Empty Node""
  |> Ketrew.Client.submit_workflow;;
```

Submit a node mostly equivalent to the one submitted from the command-line
(`ketrew submit --daemonize ...`):

```ocaml
let () =
  let open Ketrew.EDSL in
  workflow_node without_product
    ~name:""Equivalent to the Command Line one""
    ~make:(
      daemonize
        ~using:`Python_daemon
        ~host:Host.(parse ""/tmp/KT"")
        Program.(sh ""du -sh $HOME"")
    )
    ~tags:[""not-1st-workflow""; ""not-command-line""]
  |> Ketrew.Client.submit_workflow;;
```

Run a command, in a docker container scheduler by the Coclobas server (also
setup by `docker-compose` above):

```ocaml
#require ""coclobas.ketrew_backend"";;

let () =
  let open Ketrew.EDSL in
  workflow_node without_product
    ~name:""Uses a docker image to run some commands""
    ~make:(
      (* We use a different “backend”: *)
      Coclobas_ketrew_backend.Plugin.local_docker_program
        ~tmp_dir:""/tmp/secotrec-local-shared-temp""
        ~base_url:""http://coclo:8082""
        ~image:""ubuntu""
        Program.(
          (* sh ""sudo mkdir -m 777 -p /cloco-kube/playground"" && *)
          sh ""echo User"" && sh ""whoami"" &&
          sh ""echo Host"" && sh ""hostname"" &&
          sh ""echo Machine"" && sh ""uname -a"" &&
          exec [""sleep""; ""42""]
        )
    )
    ~tags:[""using-coclobas""; ""from-utop""]
  |> Ketrew.Client.submit_workflow;;
```

The Ketrew WebUI should look like this:

<div>
<img width=""100%""
src=""https://cloud.githubusercontent.com/assets/617111/23229136/259f6528-f90d-11e6-9215-ed15662364fc.png""
>
</div>

### Bigger Example

The following script extends the previous examples with the capability to send
emails upon the success or failure of your command.

```ocaml
#use ""topfind""
#thread
#require ""ketrew""

let run_command_with_daemonize ~cmd ~email =
  let module KEDSL = Ketrew.EDSL in
  (* Where to run stuff: *)
  let host = KEDSL.Host.tmp_on_localhost in
  (* A node that Ketrew will activate after cmd completes,
     on its success or failure. *)
  let email_target ~success =
    let msg_string = if success then ""succeeded"" else ""failed"" in
    let e_program =
      KEDSL.Program.shf ""echo \""'%s' %s\"" | mail -s \""Status update\"" %s""
        cmd msg_string
        email
    in
    let e_process =
      KEDSL.daemonize ~using:`Python_daemon ~host e_program in
    KEDSL.workflow_node KEDSL.without_product
      ~name:(""email result "" ^ msg_string)
      ~make:e_process
  in
  (* The function `KEDSL.workflow_node` creates a node in the workflow graph.
     The value `KEDSL.without_product` means this node does not
     “produce” anything, it is like a `.PHONY` target in `make`. *)
  KEDSL.workflow_node KEDSL.without_product
    ~name:""daemonized command with email notification""
    ~make:(
      (* A “program” is a datastructure representing an “extended shell script”. *)
      let program = KEDSL.Program.sh cmd in
      KEDSL.daemonize ~host ~using:`Python_daemon program
    )
    ~edges:[
      KEDSL.on_success_activate (email_target true);
      KEDSL.on_failure_activate (email_target false);
    ]

let () =
  (* Grab the command line arguments. *)
  let cmd   = Sys.argv.(1) in
  let email = Sys.argv.(2) in
  (* Create the  workflow with the first argument of the command line: *)
  let workflow = run_command_with_daemonize ~cmd ~email in
  (* Then, `Client.submit_workflow` is the only function that “does”
     something, it submits the constructed workflow to the engine: *)
  Ketrew.Client.submit_workflow workflow
```

You can run this [script](src/example_scripts/daemonize_workflow.ml) from the
shell with

    export KETREW_CONFIGURATION=$HOME/kclient-config/configuration.ml
    ocaml daemonize_workflow.ml 'du -sh $HOME' myaddress@email.com

Checking in with the GUI, we'll have a few new nodes, here is an example of
execution where the daemonized program does not know about the `mail` command:

<div>
<a
href=""https://cloud.githubusercontent.com/assets/617111/23230735/1ad232e6-f913-11e6-9503-03c8aa3cb60c.png""
><img width=""100%""
src=""https://cloud.githubusercontent.com/assets/617111/23230735/1ad232e6-f913-11e6-9503-03c8aa3cb60c.png""
></a>
</div>


Where to Go Next
----------------

From here:

- To write workflows for Ketrew see:
    - examples of more and more complicated
      workflows: [`src/test/Workflow_Examples.ml`](src/test/Workflow_Examples.ml) for
      examples
    - the documentation of the `Ketrew.EDSL` API.
- To configure Ketrew use the configuration
  file [documentation](src/doc/The_Configuration_File.md).
- You may want to “extend” Ketrew with new ways of running “long-running""
  computations: see the documentation
  on [plugins](src/doc/Long-Running_Plugins.md), and the examples in the
  library: like [`Ketrew.Lsf`](src/lib/lsf.mli) or in the
  tests: [`src/test/dummy_plugin.ml`](src/test/dummy_plugin.ml).
- You may want to extend Ketrew, or preconfigure it, *without* configuration
  files or dynamically loaded libraries:
  just [create](src/doc/Alternative_CLI_Application.md) your own comand-line
  app.
- If you are using Ketrew in server mode, you may want to know about
  the [commands](src/doc/Server_Commands.md) that the server can understand as
  it listens on a Unix-pipe.
- You may want to call out directly to the [HTTP API](src/doc/The_HTTP_API.md)
  (i.e. without `ketrew` as a client).
- If you want to help or simply to understand Ketrew see
  the [development](src/doc/Developer_Documentation.md) documentation, and have
  a look at the modules like [`Ketrew.Engine`](src/lib/engine.mli).

License
-------

It's [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0).

Badges
------

[![master Branch Build Status](https://travis-ci.org/hammerlab/ketrew.svg?branch=master)](https://travis-ci.org/hammerlab/ketrew)
[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.45295.svg)](http://dx.doi.org/10.5281/zenodo.45295)


",2023-07-07 16:00:31+00:00
kibaetl,kiba,thbar/kiba,Data processing & ETL framework for Ruby,https://www.kiba-etl.org,False,1703,2023-07-01 21:27:03+00:00,2013-07-18 22:21:34+00:00,84,46,5,7,v4.0.0,2021-03-24 12:34:34+00:00,Other,228,v4.0.0,10,2021-03-24 12:34:34+00:00,2023-06-22 09:09:10+00:00,2022-12-27 10:19:01+00:00,"# Kiba ETL

[![Gem Version](https://badge.fury.io/rb/kiba.svg)](http://badge.fury.io/rb/kiba)
[![Build Status](https://github.com/thbar/kiba/actions/workflows/ci.yml/badge.svg)](https://github.com/thbar/kiba/actions) [![Code Climate](https://codeclimate.com/github/thbar/kiba/badges/gpa.svg)](https://codeclimate.com/github/thbar/kiba)

Writing reliable, concise, well-tested & maintainable data-processing code is tricky.

Kiba lets you define and run such high-quality ETL ([Extract-Transform-Load](http://en.wikipedia.org/wiki/Extract,_transform,_load)) jobs using Ruby.

## Getting Started

Head over to the [Wiki](https://github.com/thbar/kiba/wiki) for up-to-date documentation.

**If you need help**, please [ask your question with tag kiba-etl on StackOverflow](http://stackoverflow.com/questions/ask?tags=kiba-etl) so that other can benefit from your contribution! I monitor this specific tag and will reply to you.

[Kiba Pro](https://www.kiba-etl.org/kiba-pro) customers get priority private email support for any unforeseen issues and simple matters such as installation troubles. Our consulting services will also be prioritized to Kiba Pro subscribers. If you need any coaching on ETL & data pipeline implementation, please [reach out via email](mailto:info@logeek.fr) so we can discuss how to help you out.

You can also check out the [author blog](https://thibautbarrere.com) and [StackOverflow answers](http://stackoverflow.com/questions/tagged/kiba-etl).

## Supported Ruby versions

Kiba currently supports Ruby 2.5+, JRuby 9.2+ and TruffleRuby. See [test matrix](https://github.com/thbar/kiba/actions).

## ETL consulting & commercial version

**Consulting services**: if your organization needs guidance on Kiba / ETL implementations, we provide consulting services. Contact at [https://www.logeek.fr](https://www.logeek.fr).

**Kiba Pro**: for vendor-backed ETL extensions, check out [Kiba Pro](https://www.kiba-etl.org/kiba-pro).

## License

Copyright (c) LoGeek SARL. Kiba is an Open Source project licensed under the terms of
the LGPLv3 license.  Please see <http://www.gnu.org/licenses/lgpl-3.0.html> for license text.

## Contributing & Legal

(agreement below borrowed from [Sidekiq Legal](https://github.com/mperham/sidekiq/blob/master/Contributing.md))

By submitting a Pull Request, you disavow any rights or claims to any changes submitted to the Kiba project and assign the copyright of those changes to LoGeek SARL.

If you cannot or do not want to reassign those rights (your employment contract for your employer may not allow this), you should not submit a PR. Open an issue and someone else can do the work.

This is a legal way of saying ""If you submit a PR to us, that code becomes ours"". 99.9% of the time that's what you intend anyways; we hope it doesn't scare you away from contributing.
",2023-07-07 16:00:35+00:00
kliko,kliko,gijzelaerr/kliko,Scientific Compute Container Specification and Support Library,http://kliko.readthedocs.org,False,11,2019-03-27 06:53:30+00:00,2016-02-05 11:33:34+00:00,1,5,1,11,0.8,2017-01-11 10:03:18+00:00,Other,124,0.8,11,2017-01-11 10:03:18+00:00,,2017-01-12 12:40:51+00:00,"=============================================================
KLIKO - Scientific Compute Container Specification and Parser
=============================================================

About
-----

KLIKO is a specification, validator and parser for the KLIKO Scientific Compute Container
specification. It enables a developer of scientific software to structure the input,
output and parameters of a dockerized compute task. KLIKO is written in Python.


Installation
------------

You can install Kliko inside a docker container or just on your system::

    $ python setup.py install


or from pypi::

    $ pip install kliko


Usage
-----

from a Python script inside a container::

    from kliko.validate import validate
    parameters = validate()

or to check if a kliko file has a valid syntax::

    $ kliko-validate /kliko.yml

or try to run the docker image from the examples folder directly::

    $ kliko-run kliko/minimal --help

        usage: kliko-run [-h] [--target_folder TARGET_FOLDER] --choice {second,first}
                         --char CHAR [--float FLOAT] --file FILE --int INT
                         image_name

        positional arguments:
          image_name

        optional arguments:
          -h, --help            show this help message and exit
          --target_folder TARGET_FOLDER
          --choice {second,first}
                                choice field (default: second)
          --char CHAR           char field, maximum of 10 chars (default: empty)
          --float FLOAT         float field (default: 0.0)
          --file FILE           file field, this file will be put in /input in case of
                                split io, /work in case of join io
          --int INT             int field


Documentation
-------------

The documentation can be found on http://kliko.readthedocs.org


Example
-------

There are examples of a kliko and parameters file in the ``examples`` folder.



Travis build status
-------------------

.. image:: https://img.shields.io/travis/gijzelaerr/kliko.svg
    :target: https://travis-ci.org/gijzelaerr/kliko

.. image:: https://img.shields.io/coveralls/gijzelaerr/kliko.svg
    :target: https://coveralls.io/github/gijzelaerr/kliko?branch=master

.. image:: https://img.shields.io/pypi/v/kliko.svg
     :target: https://pypi.python.org/pypi/kliko

.. image:: https://img.shields.io/pypi/pyversions/kliko.svg
     :target: https://pypi.python.org/pypi/kliko
",2023-07-07 16:00:39+00:00
knime,knime-core,knime/knime-core,KNIME Analytics Platform,http://www.knime.org,False,456,2023-07-07 09:02:38+00:00,2017-03-07 22:43:59+00:00,120,35,44,0,,,Other,18662,bigdata/4.0.1,144,2019-08-13 12:54:22+00:00,2023-07-07 11:52:20+00:00,2023-07-05 12:22:19+00:00,"# ![Image](https://www.knime.com/files/knime_logo_github_40x40_4layers.png) KNIME® Analytics Platform

### Content
This repository contains the source code of [KNIME Analytics Platform](http://www.knime.org). The code is organized as follows:

* _org.knime.core.*_: API definitions and framework

### Development
Instructions for how to develop extensions for KNIME Analytics Platform can be found in the _knime-sdk-setup_ repository on [BitBucket](https://bitbucket.org/KNIME/knime-sdk-setup) or [GitHub](http://github.com/knime/knime-sdk-setup).

### Join the Community!
* [KNIME Forum](https://tech.knime.org/forum)
",2023-07-07 16:00:43+00:00
kronos,kronos,jtaghiyar/kronos, A workflow assembler for cancer genome analytics and informatics,,False,19,2021-01-30 09:36:19+00:00,2015-12-08 02:31:30+00:00,2,4,1,4,v2.3.0,2016-10-06 18:11:42+00:00,Other,59,v2.3.0,4,2016-10-06 18:04:54+00:00,,2016-10-06 18:04:54+00:00,"# Kronos 

## About

A workflow assembler for genome analytics and informatics.

## Documentation

Please refer to kronos documentation here: <http://kronos.readthedocs.org/en/latest/>

## Installation

Using pip:

```
pip install kronos_pipeliner
```

## Dependencies

* [Python == 2.7.5](http://www.python.org)

### Python libraries

* [ruffus == 2.4.1](http://www.ruffus.org.uk/)

* [PyYaml == 3.11 ](http://pyyaml.org/)

### Optional Python Libraries

For running on a cluster using `-b/--job_scheduler drmaa` you will need to install:

* [drmaa-python == 0.7.6](http://drmaa-python.github.io)

## Questions and feedback

Please use our [kronos google group](https://groups.google.com/forum/#!forum/kronos_pipeliner).

## Contact

Jafar Taghiyar <jafar.taghiyar@gmail.com>.

## Change log

###2.3.0
* added a mechanism to update the requirements of the implicit merge via GENERAL section. **Note:** you can switch off the implicit merge and use an explicit merge if you want to specify a particular requirement for only that merge task.
* previously, multiple identical implicit merge tasks could exist in a workflow. In the new version, they are combined into a single merge task that happens only once. 
* added a switch called ```merge``` that defaults to ```True``` in the _run_ subsection of each task. If it is set to ```False```, the implicit merging mechanism is switched off for that task and the following warning message is shown when initializing the workflow:

```
UserWarning: Implicit merge is off for task <the_task_name>. You may have to use an explicit merge task.
```

* added support for tags in the interval file, _i.e._ an optional tag can be added for each chunk in each line of the interval file that will be used as the suffix for the name of the task corresponding to that chunk. The tags should be added to each line using tab as the separation character, e.g.

```
chunk1	tag1
chunk2	tag2
chunk3
chunk4 tag4
```

* bug fixes.

###2.2.0
* each task in the configuration file now has its own _requirements_ entry in the _run_ subsection which takes precedence over the requirements listed in the _GENERAL_ section. This enables users to have different versions of the same requirements for different tasks.
* interval file now takes precedence over the synchronization, i.e. if a task has an interval file, then it will not be synchronized with its predecessors. 
* added support for floating point memory requests.
* made all the merged files to store in a directory called _merge_.
* username and version are automatically added to the config files when using _make_config_ command.
* added a check to make sure that the input of the implicit merge node is always a list.
* bug fixes.

###2.1.0
* Kronos now uses multithreading instead of multiprocessing.

### 2.0.4
* removed the limitation on the number of simultaneous jobs/pipelines.
* added ```--no_prefix``` back to the input options of ```run``` command. 
* minor bug fixes.

### 2.0.3
First version released!
",2023-07-07 16:00:48+00:00
kubeflow,kubeflow,kubeflow/kubeflow,Machine Learning Toolkit for Kubernetes,,False,12718,2023-07-07 08:27:31+00:00,2017-11-30 18:44:19+00:00,2175,364,266,81,v1.7.0,2023-03-28 15:53:36+00:00,Apache License 2.0,2459,v1.7.0,101,2023-03-28 14:37:41+00:00,2023-07-07 13:31:57+00:00,2023-06-28 14:00:16+00:00,"<img src=""https://www.kubeflow.org/images/logo.svg"" width=""100"">
Kubeflow the cloud-native platform for machine learning operations - pipelines, training and deployment.

---

## Documentation
Please refer to the official docs at [kubeflow.org](http://kubeflow.org).

## Working Groups
The Kubeflow community is organized into working groups (WGs) with associated repositories, that focus on specific pieces of the ML platform. 

* [AutoML](https://github.com/kubeflow/community/tree/master/wg-automl)
* [Deployment](https://github.com/kubeflow/community/tree/master/wg-deployment)
* [Manifests](https://github.com/kubeflow/community/tree/master/wg-manifests)
* [Notebooks](https://github.com/kubeflow/community/tree/master/wg-notebooks)
* [Pipelines](https://github.com/kubeflow/community/tree/master/wg-pipelines)
* [Serving](https://github.com/kubeflow/community/tree/master/wg-serving)
* [Training](https://github.com/kubeflow/community/tree/master/wg-training)

## Quick Links
* [PR Dashboard](https://k8s-gubernator.appspot.com/pr)

## Get Involved
Please refer to the [Community](https://www.kubeflow.org/docs/about/community/) page.

",2023-07-07 16:00:51+00:00
kuiper,ekuiper,lf-edge/ekuiper,Lightweight data stream processing engine for IoT edge,https://ekuiper.org,False,1062,2023-07-07 13:44:38+00:00,2019-07-03 07:37:12+00:00,294,38,60,72,1.10.2,2023-07-07 07:45:14+00:00,Apache License 2.0,2223,1.11.0-alpha.1,72,2023-06-29 09:12:02+00:00,2023-07-07 13:44:38+00:00,2023-07-07 09:57:09+00:00,"# LF Edge eKuiper - An edge lightweight IoT data analytics software

[![GitHub Release](https://img.shields.io/github/release/lf-edge/ekuiper?color=brightgreen)](https://github.com/lf-edge/ekuiper/releases)
[![Docker Pulls](https://img.shields.io/docker/pulls/emqx/kuiper)](https://hub.docker.com/r/lfedge/ekuiper)
[![codecov](https://codecov.io/gh/lf-edge/ekuiper/branch/master/graph/badge.svg?token=24E9Q3C0M0)](https://codecov.io/gh/lf-edge/ekuiper)
[![Go Report Card](https://goreportcard.com/badge/github.com/lf-edge/ekuiper)](https://goreportcard.com/report/github.com/lf-edge/ekuiper)
[![Slack](https://img.shields.io/badge/Slack-LF%20Edge-39AE85?logo=slack)](https://slack.lfedge.org/)
[![Twitter](https://img.shields.io/badge/Follow-EMQ-1DA1F2?logo=twitter)](https://twitter.com/EMQTech)
[![Community](https://img.shields.io/badge/Community-Kuiper-yellow?logo=github)](https://github.com/lf-edge/ekuiper/discussions)
[![YouTube](https://img.shields.io/badge/Subscribe-EMQ-FF0000?logo=youtube)](https://www.youtube.com/channel/UC5FjR77ErAxvZENEWzQaO5Q)

[English](README.md) | [简体中文](README-CN.md)

## Overview

LF Edge eKuiper is a lightweight IoT data analytics and stream processing engine running on resource-constraint edge devices. The major goal for eKuiper is to provide a streaming software framework (similar to [Apache Flink](https://flink.apache.org)) in edge side.  eKuiper's  **rule engine** allows user to provide either SQL based or graph based (similar to Node-RED) rules to create IoT edge analytics applications within few minutes.

![arch](./docs/en_US/resources/arch.png)

**User scenarios**

It can be run at various IoT edge user scenarios, such as,
- Real-time processing of production line data in the IIoT 
- Gateway of connected vehicle analyze the data from CAN in IoV
- Real-time analysis of wind turbines and smart bulk energy storage data in smart energy

eKuiper processing at the edge can greatly reduce system response latency, save network bandwidth and storage costs and improve system security.

## Features

- Lightweight

  - Core server package is only about 4.5M, memory footprint is about 10MB

- Cross-platform

  - CPU Arch：X86 AMD * 32/64; ARM * 32/64; PPC
  - Popular Linux distributions, OpenWrt Linux, MacOS and Docker
  - Industrial PC, Raspberry Pi, industrial gateway, home gateway, MEC edge cloud server

- Data analysis support

  - Support data ETL
  - Data order, group, aggregation and join with different data sources (the data from databases and files)
  - 60+ functions, includes mathematical, string, aggregate and hash etc
  - 4 time windows & count window

- Highly extensible 

  It supports to extend at `Source`, `Functions` and `Sink` with Golang or Python.

  - Source: allows users to add more data source for analytics. 
  - Sink: allows users to send analysis result to different customized systems.
  - UDF functions: allow users to add customized functions for data analysis (for example, AI/ML function invocation) 

- Management

  - [A free web based management dashboard](https://hub.docker.com/r/emqx/ekuiper-manager) for visualized management
  - Plugins, streams and rules management through CLI, REST API and config maps(Kubernetes)
  - Easily be integrated with Kubernetes
    frameworks [KubeEdge](https://github.com/kubeedge/kubeedge), [OpenYurt](https://openyurt.io/), [K3s](https://github.com/rancher/k3s) [Baetyl](https://github.com/baetyl/baetyl)

- Integration with EMQX products

  Seamless integration with [EMQX](https://www.emqx.io/), [Neuron](https://neugates.io/) & [NanoMQ](https://nanomq.io/), and provided an end-to-end solution from IIoT, IoV 

## Quick start

- [5 minutes quick start](docs/en_US/getting_started/quick_start_docker.md)
- [Getting started](docs/en_US/getting_started/getting_started.md)
- [EdgeX rule engine tutorial](docs/en_US/edgex/edgex_rule_engine_tutorial.md)

## Community

Join our [Slack](https://slack.lfedge.org/), and then join [ekuiper](https://lfedge.slack.com/archives/C024F4P7KCK) or [ekuiper-user](https://lfedge.slack.com/archives/C024F4SMEMR) channel.

### Meeting

Subscribe to community events [calendar](https://lists.lfedge.org/g/ekuiper-tsc/calendar?calstart=2021-08-06).

Weekly community meeting at Friday 10:30AM GMT+8:
- [Zoom meeting link](https://zoom.us/j/95097577087?pwd=azZaOXpXWmFoOXpqK293RFp0N1pydz09 )
- [Meeting minutes](https://wiki.lfedge.org/display/EKUIPER/Weekly+Development+Meeting)

### Contributing
Thank you for your contribution! Please refer to the [CONTRIBUTING.md](./docs/en_US/CONTRIBUTING.md) for more information.

## Performance test result

### MQTT throughput test

- Using JMeter MQTT plugin to send IoT data to [EMQX Broker](https://www.emqx.io/), such as: `{""temperature"": 10, ""humidity"" : 90}`, the value of temperature and humidity are random integer between 0 - 100.
- eKuiper subscribe from EMQX Broker, and analyze data with SQL: `SELECT * FROM demo WHERE temperature > 50 `
- The analysis result are wrote to local file by using [file sink plugin](docs/en_US/guide/sinks/plugin/file.md).

| Devices                                        | Message # per second | CPU usage     | Memory usage |
|------------------------------------------------|----------------------|---------------|--------------|
| Raspberry Pi 3B+                               | 12k                  | sys+user: 70% | 20M          |
| AWS t2.micro( 1 Core * 1 GB) <br />Ubuntu18.04 | 10k                  | sys+user: 25% | 20M          |

### EdgeX throughput test

- A [Go application](test/edgex/benchmark/pub.go) is written to send data to ZeroMQ message bus, the data is as
  following.

  ```
  {
    ""Device"": ""demo"", ""Created"": 000, …
    ""readings"": 
    [
       {""Name"": ""Temperature"", value: ""30"", ""Created"":123 …},
       {""Name"": ""Humidity"", value: ""20"", ""Created"":456 …}
    ]
  }
  ```

- eKuiper subscribe from EdgeX ZeroMQ message bus, and analyze data with
  SQL: ``SELECT * FROM demo WHERE temperature > 50``. 90% of data will be filtered by the rule.

- The analysis result are sent to [nop sink](docs/en_US/guide/sinks/builtin/nop.md), so all the result data will be
  ignored.

|                                                | Message # per second | CPU usage     | Memory usage |
|------------------------------------------------|----------------------|---------------|--------------|
| AWS t2.micro( 1 Core * 1 GB) <br />Ubuntu18.04 | 11.4 k               | sys+user: 75% | 32M          |

### Max number of rules support

- 8000 rules with 800 message/second in total
- Configurations
  - 2 core * 4GB memory in AWS
  - Ubuntu
- Resource usage
  - Memory: 89% ~ 72%
  - CPU: 25%
  - 400KB - 500KB / rule
- Rule
  - Source: MQTT
  - SQL: `SELECT temperature FROM source WHERE temperature > 20` (90% data are filtered) 
  - Sink: Log

### Multiple rules with shared source instance

- 300 rules with a shared MQTT stream instance.
  - 500 messages/second in the MQTT source
  - 150,000 message processing per second in total
- Configurations:
  - 2 Core * 2GB memory in AWS
  - Ubuntu
- Resource usage
  - Memory: 95MB
  - CPU: 50%
- Rule
  - Source: MQTT
  - SQL: `SELECT temperature FROM source WHERE temperature > 20`, (90% data are filtered)
  - Sink: 90% nop and 10% MQTT

To run the benchmark by yourself, please check [the instruction](./test/benchmark/multiple_rules/readme.md).

## Documents

Check out the [latest document](https://ekuiper.org/docs/en/latest/) in official website.


## Build from source

#### Preparation

- Go version >= 1.18

#### Compile

+ Binary: 

  - Binary: `$ make`

  - Binary files that support EdgeX: `$ make build_with_edgex`

  - Minimal binary file with core runtime only: `$ make build_core`

+ Packages: `$ make pkg`

  - Packages: `$ make pkg`

  - Package files that support EdgeX: `$ make pkg_with_edgex`

+ Docker images: `$ make docker`

  > Docker images support EdgeX by default

Prebuilt binaries are provided in the release assets. If using os or arch which does not have prebuilt binaries, please
use cross-compilation, refer to [this doc](docs/en_US/operation/compile/cross-compile.md).

During compilation, features can be selected through go build tags so that users can build a customized product with
only the desired feature set to reduce binary size. This is critical when the target deployment environment has resource
constraint. Please refer to [features](docs/en_US/operation/compile/features.md) for more detail.

## Open source license

[Apache 2.0](LICENSE)
",2023-07-07 16:00:56+00:00
kurator-akka,kurator-akka,kurator-org/kurator-akka,Provenance-enabled workflow platform and toolkit to curate biodiversity data  https://doi.org/10.5281/zenodo.1068311,http://wiki.datakurator.org/wiki/Kurator,False,7,2022-07-16 09:30:02+00:00,2015-01-26 16:53:27+00:00,2,10,7,4,v1.0.1,2017-11-28 18:43:45+00:00,,463,v1.0.1,7,2017-11-28 18:42:51+00:00,,2022-07-06 20:44:44+00:00,"Kurator-Akka Framework
======================

The [kurator-akka](https://github.com/kurator-org/kurator-akka) repository hosts source code for the **Kurator-Akka** workflow engine component of the Kurator workflow automation toolkit. This software toolkit is being developed as part of the [Kurator project](http://wiki.datakurator.net/wiki/Kurator) and is designed to make it easy to develop and run high-performance data cleaning workflows.

**Kurator-Akka** is based on the [Akka actor toolkit and runtime](http://akka.io).  It aims to accelerate development of new data cleaning actors and to facilitate development of new workflows assembled from these actors.  **Kurator-Akka** supports actors implemented either in Java or Python, and the framework shields actor developers from the complexities of using the Akka API directly.  Workflows are specified using a YAML-based language that defines how data flows between the actors at run time.

The remainder of this README provides simple examples of actors and workflows that employ Kurator-Akka, describes how to run example workflows at the command line, and outlines the setup required to develop with or extend the **Kurator-Akka** source code.  Detailed user documentation will be provided elsewhere.

The TDWG 2015 presentation [Data cleaning with the Kurator toolkit](http://www.slideshare.net/TimothyMcPhillips/data-cleaning-with-the-kurator-toolkit-bridging-the-gap-between-conventional-scripting-and-highperformance-workflow-automation) provides an overview of the Kurator project and tools described in this README.  Also see the list of publications on the [Kurator project wiki](https://wiki.datakurator.net/wiki/Kurator#Publications).

Citing Kurator-Akka
-------------------

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1068311.svg)](https://doi.org/10.5281/zenodo.1068311)

Obtaining Kurator-Akka
----------------------

An executable jar is available in the [current release](https://github.com/kurator-org/kurator-akka/releases/latest)

Also see the [Kurator-Web](http://wiki.datakurator.org/wiki/Kurator-Web_User_Documentation) application for running Kurator-Akka workflows through a web interface.

Kurator-Akka is available as a dependency in maven central:

    <dependency>
       <groupId>org.datakurator</groupId>
       <artifactId>kurator-akka</artifactId>
       <version>1.0.1</version>
    </dependency>

Example actor and workflow
--------------------------

A *workflow* is a collection of actors configured to carry out some set of tasks.  An *actor* is a software component that receives data either from outside the workflow, or from other actors within the same workflow; and either communicates the results of its computations to other actors, or saves the results outside the running workflow.  Actors in a **Kurator-Akka** workflow run concurrently in different threads.

**Kurator-Akka** simplifies use of the underlying Akka actor framework by providing the `KuratorActor` class as an alternative to Akka's `UntypedActor` class.  A sub-class of `UntypedActor`, `KuratorActor` makes scientific-workflow specific capabilities available to actor and workflow authors, most critically through the `onData()` event handling method.  When Akka triggers events handled by  the `UntypedActor.onRecieve()` method, the `KuratorActor` implementation of `onRecieve()` makes calls to the `onData()` methods defined by Kurator-Akka actors.  **Kurator-Akka** further provides automated means for orchestrating actors in ways compatible with computational pipelines typically used to automate scientific workflows.  For example, **Kurator-Akka** correctly times actor initialization and actor shutdown events so that workflow authors need not concern themselves with these otherwise complex--yet critical--workflow management details.

Actors in **Kurator-Akka** may be implemented either in Java or in Python.


##### Java implementation of a Multiplier actor

The Java class below defines a simple actor for multiplying an integer by a configurable constant:

    import org.kurator.akka.KuratorActor;

    public class Multiplier extends KuratorActor {
        public double factor = 1;
        @Override public void onData(Object i) {
        	broadcast((double)i * factor);
        }
    }

As shown above, a new Java actor can be implemented by declaring a new Java class that overrides the `onData()` method of the `org.kurator.akka.KuratorActor` base class.  This method will be called by the **Kurator-Akka** framework each time the actor receives any data.  The call to `broadcast()` within the `onData()` method sends data (usually the results of performing some computation on the data received by the actor) to any other actors in the workflow configured to listen to this one.

An alternative approach to defining a **Kurator-Akka** Java actor is to implement the actor as a Plain Old Java Object (POJO) and declare to the Kurator-Akka framework which public method is to be called for each received data object. By default the method named `onData()` will be used. This method must be a function that returns the value to be broadcast to other actors.  A POJO implementation of the above Multiplier actor is as follows:

    public class Multiplier {
        public double factor = 1;
        public void multiply(Object i) {
        	return((double)i * factor);
        }
    }

No class inheritance is required when using using the POJO approach.  However, POJO actors have the limitation that only one output data item may be returned and communicated to other actors for each input data item the actor receives.  In contrast, Java actors derived from `org.kurator.akka.KuratorActor` may call the `broadcast()` method multiple times within the `onData()` method and so produce multiple outputs per input.

Both approaches for Java actors support event handlers in addition to `onData()`. These include `onInitialize()`, called before any actor produces output; `onStart()`, which allows actors to produce output data before any input data is received; and `onEnd()`, which is called after an actor has handled the last data item it will receive during a workflow run.

##### Python implementations of the Multiplier actor

An implementation of the same actor in Python is analogous to the POJO approach above, with the difference that the input data handler can, but does not need to be, defined as a method in a class. Any name may be used for the input data handing function; `on_data()` is assumed by default.

Thus, the following Python snippet can serve as a valid actor implementaion:

    factor = 1
    def multiply(i):
        return factor * i

And methods defined in Python classes work as well:

    class MultiplierActor(object):
        def __init__(self):
            self.factor = 1
        def multiply(i):
            return self.factor * i


As for POJOs, the **Kurator-Akka** framework can be configured to call the `multiply()` function on each data item received by either of the above implementations of the actor.  The value returned from the function is automatically broadcast to listeners.  Unlike POJO actors, actors implemented as Python functions alternatively may produce a sequence of output data items for each input data item received by using the `yield` keyword instead of `return`.

Similar to Java actors, Python actors may provide `on_initialize()`, `on_start()`, and `on_end()` event handlers (and may name these functions arbitrarily).

##### YAML declaration of a Python implementation of the Multiplier actor

In addition to the Java or Python definition of an actor, an *actor type declaration* authored in YAML is needed to make the actor available for use in workflows.  The following declares that actors of type `Multiplier`, a subtype of `PythonActor`, invoke the `multiply()` function defined in the file `multiplier.py` to handle incoming data:

    types:
    - id: Multiplier
      type: PythonActor
      properties:
        script: multiplier.py
        onData: multiply

The argument to the `script:` property is an absolute path to the Python file defining the function, or a path relative to the directory in which the workflow is invoked.  A  less fragile way of defining a Python actor is to make the Python script part of a Python package, and use the `module` property to refer to the script.  For example, if `multiplier.py` is in the `math.operators` package, then the following declaration will work:

    types:
    - id: Multiplier
      type: PythonActor
      properties:
        module: math.operators.multiplier
        onData: multiply

A declaration corresponding to a Python class implementation of the actor defined in a `math.actors` package would be as follows:

    types:
    - id: Multiplier
      type: PythonClassActor
      properties:
        pythonClass: math.actors.MultiplierActor
        onData: multiply


Analogous declarations must be provided for Java actors as well.


##### Defining a workflow that uses the Multiplier actor

With either of the above YAML snippets saved to a file named `actors.yaml`, the Python-based `Multiplier` actor can be used in a workflow also defined in YAML. The workflow below accepts an input value from the user, multiplies it by 3, and outputs the result:

    imports:

    - file:actors.yaml

    components:

    - id: MultiplyByThreeWorkflow
      type: Workflow
      properties:
        actors:
          - !ref ReadOneNumber
          - !ref MultiplyByThree
          - !ref PrintProduct

    - id: ReadOneNumber
      type: NumReader

    - id: MultiplyByThree
      type: Multiplier
      properties:
        listensTo:
          - !ref ReadOneNumber
        parameters:
          factor: 3

    - id: PrintProduct
      type: NumPrinter
      properties:
        listensTo:
          - !ref MultiplyByThree

The above declaration states the following: `MultiplyByThreeWorkflow` is a workflow comprising three actors, `ReadOneNumber`, `MultiplyByThree`, and `PrintProduct`. `MultiplyByThree` listens to (receives its input from) `ReadOneNumber`, and `PrintProduct` receives its input in turn from `MultiplyByThree`.  `MultiplyByThree` is declared to be an instance of the `Multiplier` actor type defined previously; this instance of `Multiplier` is configured to multiply each value it receives by a factor of 3.  The YAML declarations for the underlying actor types `NumReader`, `Multiplier`, and `NumPrinter` are all imported from `actors.yaml`.

The YAML definition of a workflow using Java implementations of each actor looks identical to a workflow using Python actors.  Java actors and Python actors can be used together in the same workflow.

##### Inlining Python actors

**Kurator-Akka** allows the code for Python actors to be provided *inline*, i.e. within the workflow definition itself. No additional Python script file or YAML actor type declaration is needed in this case (the type of the actor is simply `PythonActor`).  For example, the block of YAML defining the `MultiplyByThree` actor in the workflow definition above depends on an additional YAML declaration for the `Multiplier` actor defined in the `actors.yaml` file, which in turn depends on a Python script file named `multiplier.py`.  Because the code for this actor is only a few lines long, it may be reasonable to define the actor entirely inline.  In other words, this block of YAML in the workflow:

    - id: MultiplyByThree
      type: Multiplier
      properties:
        listensTo:
          - !ref ReadOneNumber
        parameters:
          factor: 3

can be replaced with:

    - id: MultiplyByThree
      type: PythonActor
      properties:
        listensTo:
          - !ref ReadOneNumber
        onData: triple
        code: |
          def triple(n):
            return 3 * n

The Python code defining the `multiply()` function is now defined within the same YAML file that declares the workflow as a whole. Inlined Python actors are useful for implementing simple actors needed for specific workflows.


Preparing to run Kurator-Akka
-----------------------------

This section describes how to set up an environment for writing your own actors and workflows, and executing them using **Kurator-Akka**. Instructions for building and extending the **Kurator-Akka** framework itself are provided following this section.

#### Check installed version of Java
**Kurator-Akka** requires Java version 1.8.0 or higher. To determine the version of java installed on your computer use the `-version` option to the `java` command. For example,

    $ java -version
    java version ""1.8.0_65""
    Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
    Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)


Instructions for installing Java may be found at [http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html](http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html).  If you plan to develop new actors in Java (not just in Python) be sure to install the JDK.  Otherwise a JRE is sufficient.

#### Download the Kurator-Akka jar

**Kurator-Akka** is distributed as an executable jar (Java archive) file that can be invoked using the `java -jar` command. To download the most recent build of the latest **Kurator-Akka** code, navigate to the [Latest Release](https://github.com/kurator-org/kurator-akka/releases/latest) and click on the `kurator-akka-1.0.1-jar-with-dependencies.jar` link under the *Assets* tab to download the *executable jar* artifact for the *kurator-akka* job.

#### Install Kurator-Akka on your system

Once you have downloaded the **Kurator-Akka** jar, save the file in a convenient location and rename it to something like `kurator-akka.jar`.   **Kurator-Akka** can now be run using the `java -jar` command. The jar file includes several sample workflow scripts which can be accessed from the command line.

Test your installation by running the `hello.yaml` workflow. Assuming `kurator-akka.jar` is in your current working directory type:

    java -jar kurator-akka.jar -f classpath:/org/kurator/akka/samples/hello.yaml

If the Kurator-Akka jar is stored elsewhere, qualify `kurator-akka.jar` with the path to that file. For example, if you stored the jar file in the bin subdirectory of your home directory, running Kurator-Akka would look something like this on Linux or OS X:

    $ java -jar ~/bin/kurator-akka.jar -f classpath:/org/kurator/akka/samples/hello.yaml
    Hello World!

On Windows platforms the command is similar:

    $ java -jar %USERPROFILE%\bin\kurator-akka.jar -f classpath:/org/kurator/akka/samples/hello.yaml
    Hello World!

#### Define a short command for running Kurator-Akka at the prompt

If you are running **Kurator-Akka** on an Apple OSX or Linux system (or use Git Bash or Cygwin on Windows), you may define an *alias* to simplify running **Kurator-Akka** at the command line. On Windows platforms you similarly may define a *doskey macro* for running **Kurator-Akka** at the prompt.

For example, if you have saved `kurator-akka.jar` to the bin subdirectory of your home directory, the following command will create a `bash` alias for running **Kurator-Akka** simply by typing `ka`:

    alias ka='java -jar ~/bin/kurator-akka.jar'

If you use `csh` or `tcsh` the command is:

    alias kurator-akka java -jar ~/bin/kurator-akka.jar

On Windows the command to create the `ka` doskey macro is:

     doskey ka=java -jar %USERPROFILE%\bin\kurator-akka.jar $*

On all platforms the `hello.yaml` demo now can be run using the short command:

    $ ka -f classpath:/org/kurator/akka/samples/hello.yaml
    Hello World!

#### Extract and run sample workflows (optional)

If you would like to browse and edit the sample workflows included in the **Kurator-Akka** jar, type the following (qualifying the path to `kurator-akka.jar` as necessary) to extract its contents to your filesystem:

    jar xf kurator-akka.jar org/kurator/akka/samples

To run a script residing on the filesystem, you can use the `file:` qualifier instead of `classpath:`:

    $ java -jar kurator-akka.jar -f file:org/kurator/akka/samples/hello.yaml
    Hello World!

The `file:` qualifier is optional, however. By default **Kurator-Akka** looks for workflows on your filesystem. So this will work, too:


    $ java -jar kurator-akka.jar -f org/kurator/akka/samples/hello.yaml
    Hello World!

Note that the path to `hello.yaml` above is relative (it does not start with a `/`).


Developing new Java actors
-------------------------------
New Java actors can be implemented using standard Java development tools.  Simply make sure that the `kurator-akka.jar` file is on the CLASSPATH when compiling your actors, and that the compiled `.class` files are on the CLASSPATH when running **Kurator-Akka**.

For example, if your new actor is defined in the file `MyActor.java`, and `kurator-akka.jar` is stored in `~/bin`, the following command will compile your actor to produce `MyActor.class`:

    javac -classpath ~/bin/kurator-akka.jar MyActor.java

Note that including `kurator-akka.jar` on the classpath during compilation is unnecessary if your actor is a POJO, i.e. not a subclass of `org.kurator.akka.KuratorActor`.

Developing new Python actors
--------------------------------------
New Python actors can be used in **Kurator-Akka** workflows without any manual compilation step. Python actors that depend only on packages provided as part of a standard Python distribution can be developed without any preparation other than downloading the **Kurator-Akka** jar file.  The `kurator-akka.jar` also includes within it all of the 3rd-party Python packages requierd by the basic actors provided in the jar.


However, if you develop new Python actors that depend on 3rd-party packages not included in the  **Kurator-Akka** jar file, you will need to install these packages to a local **Jython** installation.  Jython is a Java implementation of the Python language and runtime, and comes with the same package management tools as Python, including `pip`.

#### Install the Jython 2.7.1b3 distribution to support 3rd party Python packages

* Download the [Jython 2.7.1.b3 installer jar](http://search.maven.org/remotecontent?filepath=org/python/jython-installer/2.7.1b3/jython-installer-2.7.1b3.jar). The downloaded jar file will be named `jython_installer-2.7.1b3.jar`.

* The Jython installer can be started either by double-clicking the downloaded jar file (on Windows or OS X) or executing the downloaded jar at the command prompt using the `java -jar` command:

        java -jar jython_installer-2.7.1b3.jar

* Note the location of the Jython installation directory created by the installer.

#### Install dependencies for native actor support

As an alternative to the actors that used the Java based Python interpreter Jython, native actor support is provided for embedded Python and R.

On Linux, install the native dependencies for R and Python via:

    sudo apt-get install r-base python python-dev
    
On MacOS, install the native dependencies via:

    brew install python r

#### Make installed Python packages available to Kurator

Define the environment variable `JYTHONHOME` to indicate the path to the newly installed Jython 2.7.0 distribution. **Kurator-Akka** uses this variable to locate 3rd-party Python packages that specific actors depend upon.

In a bash shell the environment variable can be assigned with the following command (assuming, for example, that Jython was installed to the `jython2.7.1b3` directory within your home directory):

    export JYTHONHOME=$HOME/jython2.7.1b3/

On Windows it is easiest to define the variable using the Advanced system settings Environment Variables dialog:

    Control Panel -> System -> Advanced system settings -> Advanced -> Environment Variables

**Kurator-Akka** now will have access to all Python packages installed to your Jython installation.  Jython 2.7 includes the `pip` tool (in the `bin` subdirectory of the Jython installation) which makes it easy to install 3rd-party Python packages and to install their dependencies automatically.  For example, this following command installs the `suds-jurko` package which subsequently can be imported by Python actors:

    $JYTHON_HOME/bin/pip install suds-jurko

#### Make your own Python code available to **Kurator-Akka**

To make your own Python code available to **Kurator-Akka** specify the directory (or directories) containing your Python packages using the `JYTHONPATH` variable.  For example, if you have defined a Python function `add_two_numbers()` in a file named `adders.py`, and placed this Python module (file) in the `$HOME/packages/math/operators` directory, you can make your function available for use in an actor via the `math.operators.adders` module by defining `JYTHONPATH` to include the `$HOME/packages/` directory and then declaring the `module` and `onData` properties of the actor as follows:  

    types:
    - id: Adder
      type: PythonClassActor
      properties:
        module: math.operators.adders
        onData: add_two_numbers

Note that Python packages always require that a file named `__init__.py` be present in each directory comprising the package.  In the above example, the directories `$HOME/packages/math` and `$HOME/packages/math/operators` must each contain a file named `__init__.py`. These files may be empty.

For native actor support set the `PYTHONPATH` environment variable to the same value.

Instructions for Kurator-Akka workflow engine developers
---------------------------
If you would like to build, modify, or extend the **Kurator-Akka** workflow engine you may set up your development environment as follows.

#### JDK and Maven configuration

The **Kurator-Akka** framework is built using Maven 3. Before building **Kurator-Akka** confirm that the `mvn` command is in your path, that your version of Maven is at least 3.0.5, and that a JDK version 1.8.0 (or higher) is found by Maven:

    $ mvn --version
    Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T04:57:37-07:00)
    Maven home: /Users/tmcphill/DropBox/Library/Java/apache-maven-3.3.3
    Java version: 1.8.0_65, vendor: Oracle Corporation
    Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre
    Default locale: en_US, platform encoding: UTF-8
    OS name: ""mac os x"", version: ""10.11.3"", arch: ""x86_64"", family: ""mac""
    $

JDK 7 and Maven 3 downloads and detailed installation instructions can be found at the following links:

- [Instructions for installing and configuring JDK 1.8](lhttps://docs.oracle.com/javase/8/docs/technotes/guides/install/) (Oracle Java Documentation)
- [Instructions for installing and configuring Maven 3](http://maven.apache.org/download.cgi) (Apache Maven Project)


#### Project directory layout

**Kurator-Akka** adopts the default organization of source code, resources, and tests as defined by Maven.  See [maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html](http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html) for more information.  The most important directories are listed below:

Directory            | Description
---------------------|-----------
src/main/java        | Source code to be built and packaged for distribution.
src/main/resources   | Resource files to be packaged with production code.
src/test/java        | Source code for unit tests. Not included in packaged distributions.
src/test/resources   | Resource files available to tests. Not included in packaged distributions.
target               | Destination directory for packaged distributions (jar files) built by maven.
target/classes       | Compiled java classes for source code found under src/main/java.
target/test-classes  | Compiled java classes for test code found under src/test/java.
target/dependency    | Automatically resolved and downloaded dependencies (jars) that will be included in the standalone distribution.
target/apidocs/      | Local build of Javadoc documentation.
packages             | Custom Python packages to be included in the distribution.

Note that the final directory, `packages` contains all of the Python source files developed and delivered with the **Kurator-Akka** engine.  Any new packages added to this directory are automatically made available to **Kurator-Akka**.


#### Building and testing with maven

Note: The yaml-spring-loader dependency currently needs to be built from source from the kurator-org/yaml-spring-loader fork.  See that project in github, and install in your local maven repository before building Kurator-Akka.

**Kurator-Akka** can be built and tested from the command line using the following commands:

Maven command | Description
--------------|------------
mvn clean     | Delete the target directory including all compiled classes.
mvn compile   | Download required dependencies and compile source code in src/main/java.  Only those source files changes since the last compilation or clean are built.
mvn test      | Compile the classes in src/test/java and run all tests found therein. Peforms *mvn compile* first.
mvn package   | Package the compiled classes in target/classes and files found in src/main/resources in two jar files, **kurator-akka-1.0.1-SNAPSHOT.jar** and **kurator-akka-1.0.1-SNAPSHOT-jar-with-dependencies.jar**.  The latter also contains all jar dependencies. Performs *mvn compile* and *mvn test* first, and will not perform packaging step if any tests fail. Use the `-DskipTests` option to bypass tests.
mvn javadoc:javadoc | Build Javadoc documentation. The `mvn package` command also builds Javadoc.

#### Continuous integration with Bamboo

All code is built and tests run automatically on a build server at NCSA whenever changes are committed to directories used by maven.  Please confirm that the automated build and tests succeed after committing changes to code or resource files (it may take up to two minutes for a commit-triggered build to start).  Functional tests depend on the scripts in src/main/resources and are likely to fail if not updated following changes to these scripts.

Site                  | Url
----------------------| ---
Build history         | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-AKKA
Last build            | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-AKKA/latest
Last successful build | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-AKKA/latestSuccessful

The link to the latest successful build is useful for obtaining the most recently built jar file without building it yourself.  Follow the link to the [last successful build](https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-AKKA ""last successful build""), click the Artifacts tab, then download the executable jar.

#### Maintainer deployment: 

To deploy a snapshot to the snapshotRepository:

    mvn clean deploy

To deploy a new release to maven central, set the version in pom.xml to a non-snapshot version, then deploy with the release profile (which adds package signing and deployment to release staging:

    mvn clean deploy -P release

After this, you will need to login to the sonatype oss repository hosting nexus instance, find the staged release in the staging repositories, and perform the release.  It should be possible (haven't verified this yet) to perform the release from the command line instead by running:

    mvn nexus-staging:release -P release
    
<!---

Development for Kurator-Akka
----------------------------

TODO: Documentation of development for framework, and for included Java actors.

TODO: Including Java libraries of actors

To make a Java class file available as a core Kurator-Akka actor that can be invoked
from Kurator-Akka using a yaml workflow declaration, you will need to:  

(1) Create a java class (which either contains the desired functionality or wraps it),
which extends KuratorActor.

(2) Add a type definition block to src/main/resources/org/kurator/akka/types.yaml
giving an ID for the actor, and the classpath for the actor.

(3) Invoke the actor by ID in a workflow defined in a yaml file.

--->
",2023-07-07 16:01:01+00:00
lakefs,lakeFS,treeverse/lakeFS,lakeFS - Data version control for your data lake | Git for data,https://docs.lakefs.io,False,3484,2023-07-07 10:08:24+00:00,2019-09-12 11:46:28+00:00,288,41,75,126,v0.104.0,2023-06-18 11:47:04+00:00,Apache License 2.0,4374,v0.104.0,171,2023-06-18 11:47:04+00:00,2023-07-07 15:27:57+00:00,2023-07-07 13:51:02+00:00,"<p align=""center"">
  <img src=""docs/assets/img/logo_large.png""/>
</p>
<p align=""center"">
	<a href=""https://raw.githubusercontent.com/treeverse/lakeFS/master/LICENSE"" >
		<img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg"" alt=""Apache License"" /></a>
	<a href=""https://github.com/treeverse/lakeFS/actions?query=workflow%3AGo+branch%3Amaster++"">
		<img src=""https://github.com/treeverse/lakeFS/workflows/Go/badge.svg?branch=master"" alt=""Go tests status"" /></a>
	<a href=""https://github.com/treeverse/lakeFS/actions?query=workflow%3ANode+branch%3Amaster++"" >
		<img src=""https://github.com/treeverse/lakeFS/workflows/Node/badge.svg?branch=master"" alt=""Node tests status"" /></a>
	<a href=""https://github.com/treeverse/lakeFS/actions?query=workflow%3AEsti"">
		<img src=""https://github.com/treeverse/lakeFS/workflows/Esti/badge.svg"" alt=""Integration tests status"" /></a>
	<a href=""https://github.com/treeverse/lakeFS/actions/workflows/docs-pr.yaml"">
		<img src=""https://github.com/treeverse/lakeFS/actions/workflows/docs-pr.yaml/badge.svg"" alt=""Docs Preview & Link Check status"" /></a>
	<a href=""https://artifacthub.io/packages/search?repo=lakefs"">
		<img src=""https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/lakefs"" alt=""Artifact HUB"" /></a>
	<a href=""CODE_OF_CONDUCT.md"">
		<img src=""https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"" alt=""code of conduct""></a>
</p>


## lakeFS is Data Version Control (Git for Data)

lakeFS is an open-source tool that transforms your object storage into a Git-like repository. It enables you to manage your data lake the way you manage your code.

With lakeFS you can build repeatable, atomic, and versioned data lake operations - from complex ETL jobs to data science and analytics.

lakeFS supports AWS S3, Azure Blob Storage, and Google Cloud Storage as its underlying storage service. It is API compatible with S3 and works seamlessly with all modern data frameworks such as Spark, Hive, AWS Athena, DuckDB, and Presto.

For more information, see the [documentation](https://docs.lakefs.io).

## Getting Started

You can spin up a standalone sandbox instance of lakeFS using Docker:

```bash
docker run --pull always \
		   --name lakefs \
		   -p 8000:8000 \
		   treeverse/lakefs:latest \
		   run --local-settings
```

Once you've got lakeFS running, open [http://127.0.0.1:8000/](http://127.0.0.1:8000/) in your web browser.

### Quickstart

**👉🏻 For a hands-on walk through of the core functionality in lakeFS head over to [the quickstart](https://docs.lakefs.io/quickstart/) to jump right in!**

Make sure to also have a look at the [lakeFS samples](https://github.com/treeverse/lakeFS-samples). These are a rich resource of examples of end-to-end applications that you can build with lakeFS.

## Why Do I Need lakeFS?

### ETL Testing with Isolated Dev/Test Environment

When working with a data lake, it’s useful to have replicas of your production environment. These replicas allow you to test these ETLs and understand changes to your data without impacting downstream data consumers.

Running ETL and transformation jobs directly in production without proper ETL Testing is a guaranteed way to have data issues flow into dashboards, ML models, and other consumers sooner or later. The most common approach to avoid making changes directly in production is to create and maintain multiple data environments and perform ETL testing on them. Dev environment to develop the data pipelines and test environment where pipeline changes are tested before pushing it to production. With lakeFS you can create branches, and get a copy of the full production data, without copying anything. This enables a faster and easier process of ETL testing.

### Reproducibility

Data changes frequently. This makes the task of keeping track of its exact state over time difficult. Oftentimes, people maintain only one state of their data––its current state.

This has a negative impact on the work, as it becomes hard to:
* Debug a data issue.
* Validate machine learning training accuracy (re-running a model over different data gives different results).
Comply with data audits.

In comparison, lakeFS exposes a Git-like interface to data that allows keeping track of more than just the current state of data. This makes reproducing its state at any point in time straightforward.

### CI/CD for Data

Data pipelines feed processed data from data lakes to downstream consumers like business dashboards and machine learning models. As more and more organizations rely on data to enable business critical decisions, data reliability and trust are of paramount concern. Thus, it’s important to ensure that production data adheres to the data governance policies of businesses. These data governance requirements can be as simple as a file format validation, schema check, or an exhaustive PII(Personally Identifiable Information) data removal from all of organization’s data.

Thus, to ensure the quality and reliability at each stage of the data lifecycle, data quality gates need to be implemented. That is, we need to run Continuous Integration(CI) tests on the data, and only if data governance requirements are met can the data can be promoted to production for business use.

Everytime there is an update to production data, the best practice would be to run CI tests and then promote(deploy) the data to production. With lakeFS you can create hooks that make sure that only data that passed these tests will become part of production.

### Rollback
A rollback operation is used to to fix critical data errors immediately.

What is a critical data error? Think of a situation where erroneous or misformatted data causes a signficant issue with an important service or function. In such situations, the first thing to do is stop the bleeding.

Rolling back returns data to a state in the past, before the error was present. You might not be showing all the latest data after a rollback, but at least you aren’t showing incorrect data or raising errors. Since lakeFS provides versions of the data without making copies of the data, you can time travel between versions and roll back to the version of the data before the error was presented.

## Community

Stay up to date and get lakeFS support via:

- Share your lakeFS experience and get support on [our Slack](https://go.lakefs.io/JoinSlack).
- Follow us and join the conversation on [Twitter](https://twitter.com/lakeFS) and [Mastodon](https://data-folks.masto.host/@lakeFS).
- Learn from video tutorials on [our YouTube channel](https://lakefs.io/youtube).
- Read more on data versioning and other data lake best practices in [our blog](https://lakefs.io/blog/data-version-control/).
- Feel free to [contact us](https://lakefs.io/contact-us/) about anything else.

## More information

- Read the [documentation](https://docs.lakefs.io).
- See the [contributing guide](https://docs.lakefs.io/contributing).
- Take a look at our [roadmap](https://docs.lakefs.io/understand/roadmap.html) to peek into the future of lakeFS.

## Licensing

lakeFS is completely free and open-source and licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0).

## Who Uses lakeFS?

lakeFS is used by numerous companies, including those below. _If you use lakeFS and would like to be included here please open a PR._

* AirAsia
* APEX Global
* AppsFlyer
* Auburn University
* BAE Systems
* Bureau of Labor Statistics
* Cambridge Consultants
* Connor, Clark & Lunn Financial Group
* Context Labs Bv
* Daimler Truck
* Enigma
* EPCOR
* Ford Motor Company
* Generali
* Giesecke+Devrient
* greehill
* Karius
* Lockheed Martin
* Luxonis
* Mixpeek
* Netflix
* Paige
* PETRONAS
* Pollinate
* Proton Technologies AG
* ProtonMail
* Renaissance Computing Institute
* RHEA Group 
* RMS
* Sensum
* Similarweb
* State Street Global Advisors
* Terramera
* Tredence
* Volvo Cars
* Webiks
* Windward
* Woven by Toyota
",2023-07-07 16:01:05+00:00
law,law,riga/law,Build large-scale task workflows: luigi + job submission + remote targets + environment sandboxing using Docker/Singularity,http://law.readthedocs.io,False,74,2023-05-29 19:45:58+00:00,2016-12-03 15:34:35+00:00,31,3,18,40,v0.1.12,2023-01-09 18:14:03+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",1798,v0.1.12,40,2023-01-09 18:14:03+00:00,2023-07-03 07:19:09+00:00,2023-07-03 07:18:59+00:00,"<center>
  <a href=""https://github.com/riga/law"">
    <img src=""https://media.githubusercontent.com/media/riga/law/master/assets/logo.png"" />
  </a>
</center>


<!-- marker-after-logo -->


[![Build status](https://github.com/riga/law/workflows/Lint%20and%20test/badge.svg)](https://github.com/riga/law/actions?query=workflow%3A%22Lint+and+test%22)
[![Docker images](https://github.com/riga/law/workflows/Deploy%20images/badge.svg)](https://github.com/riga/law/actions?query=workflow%3A%22Deploy+images%22)
[![Documentation status](https://readthedocs.org/projects/law/badge/?version=latest)](http://law.readthedocs.io/en/latest)
[![Package version](https://img.shields.io/pypi/v/law.svg?style=flat)](https://pypi.python.org/pypi/law)
[![License](https://img.shields.io/github/license/riga/law.svg)](https://github.com/riga/law/blob/master/LICENSE)
[![DOI](https://zenodo.org/badge/75482295.svg)](https://zenodo.org/badge/latestdoi/75482295)


**Note**: This project is currently under development.
Version 1.0.0 will be the first, fully documented beta release, targetted for mid 2023.

Use law to build complex and large-scale task workflows.
It is build on top of [luigi](https://github.com/spotify/luigi) and adds abstractions for **run locations**, **storage locations** and **software environments**.
Law strictly disentangles these building blocks and ensures they remain interchangeable and resource-opportunistic.

Key features:

- CLI with auto-completion and interactive status and dependency inspection.
- Remote targets with automatic retries and local caching
  - WebDAV, HTTP, Dropbox, SFTP, all WLCG protocols (srm, xrootd, dcap, gsiftp, webdav, ...)
- Automatic submission to batch systems from within tasks
  - HTCondor, LSF, gLite, ARC, Slurm
- Environment sandboxing, configurable on task level
  - Docker, Singularity, Sub-Shells, Virutal envs


<!-- marker-after-header -->


## Contents

<!-- marker-after-contents-heading -->

- [First steps](#first-steps)
   - [Installation and dependencies](#installation-and-dependencies)
   - [Usage at CERN](#usage-at-cern)
   - [Overcomplete example config](#overcomplete-example-config)
- [Projects using law](#projects-using-law)
- [Examples](#examples)
- [Further topics](#further-topics)
   - [Auto completion on the command-line](#auto-completion-on-the-command-line)
   - [Tests](#tests)
   - [Contributors](#contributors)
   - [Development](#development)


<!-- marker-before-body -->


## First steps

### Installation and dependencies

Install via [pip](https://pypi.python.org/pypi/law):

```shell
pip install law
```

This command also installs [luigi](https://pypi.python.org/pypi/luigi) and [six](https://pypi.python.org/pypi/six) (Python 2 support will be dropped soon).

If you plan to use remote targets, the (default) implementation also requires [gfal2](https://dmc-docs.web.cern.ch/dmc-docs/gfal2/gfal2.html) and [gfal2-python](https://pypi.python.org/pypi/gfal2-python) (optional) to be installed, either via pip or conda.

```shell
conda install -c conda-forge gfal2 gfal2-util
```


### Usage at CERN

See the [wiki](https://github.com/riga/law/wiki/Usage-at-CERN).


### Overcomplete example config

See [law.cfg.example](https://github.com/riga/law/tree/master/law.cfg.example).


## Projects using law

- CMS Di-Higgs Inference Tools:
  - Basis for statistical analysis for all Di-Higgs searches in CMS, starting at datacard-level
  - [repo](https://gitlab.cern.ch/hh/tools/inference), [docs](https://cms-hh.web.cern.ch/cms-hh/tools/inference/index.html)
- CMS B-Tag SF Measurement:
  - Automated workflow for deriving shape-calibrating b-tag scale factors, starting at MiniAOD-level
  - [repo](https://github.com/cms-btv-pog/jet-tagging-sf)
- CMS Tau POG ML Tools:
  - Preprocessing pipeline for ML trainings in the TAU group
  - [repo](https://github.com/cms-tau-pog/TauMLTools)
- CMS HLT Config Parser:
  - Collects information from various databases (HLT, bril, etc.) and shows menus, triggers paths, filter names for configurable MC datasets or data runs
  - [repo](https://github.com/riga/cms-hlt-parser)
- UHH-CMS Analysis Framework:
  - Python based, fully automated, columnar framework, including job submission, resolution of systematics and ML pipelines, starting at NanoAOD-level with an optimized multi-threaded column reader
  - [repo](https://github.com/uhh-cms/analysis_playground), [docs](http://analysis_playground.readthedocs.io), [task structure](https://github.com/uhh-cms/analysis_playground/issues/25)
- RWTH-CMS Analysis Framework:
  - Basis for multiple CMS analyses ranging from Di-Higgs, to single Higgs and b-tag SF measurements, starting at NanoAOD-level and based on coffea processors
  - [repo](https://git.rwth-aachen.de/3pia/cms_analyses/common/-/tree/master/)
- CIEMAT-CMS Analysis Framework:
  - Python and RDataFrame based framework starting from NanoAOD and targetting multiple CMS analyses
  - [repo](https://gitlab.cern.ch/cms-phys-ciemat/nanoaod_base_analysis/)
- CMS 3D Z+jet 13TeV analysis
  - Analysis workflow management from NTuple production to final plots and fits
  - [repo](https://gitlab.etp.kit.edu/cverstege/zjet-analysis)
- NP-correction derivation tool
  - MC generation with Herwig and analysis of generated events with Rivet
  - [repo](https://github.com/HerrHorizontal/herwig-run)
- CMS SUSY Searches at DESY
  - Analysis framework for CMS SUSY searches going from custom NanoAODs -> NTuple production -> DNN-based inference -> final plots and fits
  - [repo](https://github.com/frengelk/Susy1LeptonAnalysis)
- Kingmaker (CMS Ntuple Production with CROWN)
  - Ntuple conversion from CMS nanoAOD to analysis Ntuples using the CROWN framework. Also includes the training of an event classifier on those ntuples.
  - [repo](https://github.com/KIT-CMS/KingMaker), [CROWN](https://github.com/KIT-CMS/CROWN)

If your project uses law but is not yet listed here, feel free to open a pull request or mention your project details in a new [issue](https://github.com/riga/law/issues/new?assignees=riga&labels=docs&template=register-project.md) and it will be added.


## Examples

All examples can be run either in a Jupyter notebook or a dedicated docker container.
For the latter, do

```shell
docker run -ti riga/law:example <example_name>
```

- [loremipsum](https://github.com/riga/law/tree/master/examples/loremipsum): The *hello world* example of law.
- [workflows](https://github.com/riga/law/tree/master/examples/workflows): Law workflows.
- [notebooks](https://github.com/riga/law/tree/master/examples/notebooks): Examples showing how to use and work with law in notebooks.
- [dropbox_targets](https://github.com/riga/law/tree/master/examples/dropbox_targets): Working with targets that are stored on Dropbox.
- [wlcg_targets](https://github.com/riga/law/tree/master/examples/wlcg_targets): Working with targets that are stored on WLCG storage elements (dCache, EOS, ...). TODO.
- [htcondor_at_vispa](https://github.com/riga/law/tree/master/examples/htcondor_at_vispa): HTCondor workflows at the [VISPA service](https://vispa.physik.rwth-aachen.de).
- [htcondor_at_cern](https://github.com/riga/law/tree/master/examples/htcondor_at_cern): HTCondor workflows at the CERN batch infrastructure.
- [sequential_htcondor_at_cern](https://github.com/riga/law/tree/master/examples/sequential_htcondor_at_cern): Continuation of the [htcondor_at_cern](https://github.com/riga/law/tree/master/examples/htcondor_at_cern) example, showing sequential jobs that eagerly start once jobs running previous requirements succeeded.
- [htcondor_at_naf](https://github.com/riga/law/tree/master/examples/htcondor_at_naf): HTCondor workflows at German [National Analysis Facility (NAF)](https://confluence.desy.de/display/IS/NAF+-+National+Analysis+Facility).
- [slurm_at_maxwell](https://github.com/riga/law/tree/master/examples/slurm_at_maxwell): Slurm workflows at the [Desy Maxwell cluster](https://confluence.desy.de/display/MXW/Maxwell+Cluster).
- [grid_at_cern](https://github.com/riga/law_example_WLCG): Workflows that run jobs and store data on the WLCG.
- [lsf_at_cern](https://github.com/riga/law/tree/master/examples/lsf_at_cern): LSF workflows at the CERN batch infrastructure.
- [docker_sandboxes](https://github.com/riga/law/tree/master/examples/docker_sandboxes): Environment sandboxing using Docker. TODO.
- [singularity_sandboxes](https://github.com/riga/law/tree/master/examples/singularity_sandboxes): Environment sandboxing using Singularity. TODO.
- [subshell_sandboxes](https://github.com/riga/law/tree/master/examples/subshell_sandboxes): Environment sandboxing using Subshells. TODO.
- [parallel_optimization](https://github.com/riga/law/tree/master/examples/parallel_optimization): Parallel optimization using [scikit optimize](https://scikit-optimize.github.io).
- [notifications](https://github.com/riga/law/tree/master/examples/notifications): Demonstration of slack and telegram task status notifications..
- [CMS Single Top Analysis](https://github.com/riga/law_example_CMSSingleTopAnalysis): Simple physics analysis using law.


## Further topics

### Auto completion on the command-line

**bash**

```shell
source ""$( law completion )""
```

**zsh**

zsh is able to load and evaluate bash completion scripts via `bashcompinit`.
In order for `bashcompinit` to work, you should run `compinstall` to enable completion scripts:

```shell
autoload -Uz compinstall && compinstall
```

After following the instructions, these lines should be present in your `~/.zshrc`:

```shell
# The following lines were added by compinstall
zstyle :compinstall filename '~/.zshrc'

autoload -Uz +X compinit && compinit
autoload -Uz +X bashcompinit && bashcompinit
# End of lines added by compinstall
```

If this is the case, just source the law completion script (which internally enables `bashcompinit`) and you're good to go:

```shell
source ""$( law completion )""
```


### Tests

To run and test law, there are various docker `riga/law` images available on the [DockerHub](https://cloud.docker.com/u/riga/repository/docker/riga/law), corresponding to different OS and Python versions (based on [micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)).
Start them via

```shell
docker run -ti riga/law:<the_tag>
```

|    OS    | Python |                   Tags                    |
| -------- | ------ | ----------------------------------------- |
| Centos 8 |   3.11 | c8-py311, py311                           |
| Centos 8 |   3.10 | c8-py310, c8-py3, c8, py310, py3, latest  |
| Centos 8 |    3.9 | c8-py39, py39                             |
| Centos 8 |    3.8 | c8-py38, py38                             |
| Centos 8 |    3.7 | c8-py37, py37                             |
| Centos 7 |   3.10 | c7-py310                                  |
| Centos 7 |    3.9 | c7-py39, c7-py3, c7                       |
| Centos 7 |    3.8 | c7-py38                                   |
| Centos 7 |    3.7 | c7-py37                                   |
| Centos 7 |    3.6 | c7-py36, py36 (removed soon)              |
| Centos 7 |    2.7 | c7-py27, c7-py2, py27, py2 (removed soon) |


### Contributors

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://github.com/riga""><img src=""https://avatars.githubusercontent.com/u/1908734?v=4?s=100"" width=""100px;"" alt=""Marcel Rieger""/><br /><sub><b>Marcel Rieger</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=riga"" title=""Code"">💻</a> <a href=""https://github.com/riga/law/pulls?q=is%3Apr+reviewed-by%3Ariga"" title=""Reviewed Pull Requests"">👀</a> <a href=""#maintenance-riga"" title=""Maintenance"">🚧</a> <a href=""https://github.com/riga/law/commits?author=riga"" title=""Documentation"">📖</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/pfackeldey""><img src=""https://avatars.githubusercontent.com/u/18463582?v=4?s=100"" width=""100px;"" alt=""Peter Fackeldey""/><br /><sub><b>Peter Fackeldey</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=pfackeldey"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/yrath""><img src=""https://avatars.githubusercontent.com/u/20044510?v=4?s=100"" width=""100px;"" alt=""Yannik Rath""/><br /><sub><b>Yannik Rath</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=yrath"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jaimeleonh""><img src=""https://avatars.githubusercontent.com/u/47629805?v=4?s=100"" width=""100px;"" alt=""Jaime Leon Holgado""/><br /><sub><b>Jaime Leon Holgado</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=jaimeleonh"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/lmoureaux""><img src=""https://avatars.githubusercontent.com/u/22327575?v=4?s=100"" width=""100px;"" alt=""Louis Moureaux""/><br /><sub><b>Louis Moureaux</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=lmoureaux"" title=""Code"">💻</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.linkedin.com/in/lgeiger""><img src=""https://avatars.githubusercontent.com/u/13285808?v=4?s=100"" width=""100px;"" alt=""Lukas Geiger""/><br /><sub><b>Lukas Geiger</b></sub></a><br /><a href=""https://github.com/riga/law/commits?author=lgeiger"" title=""Code"">💻</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.


### Development

- Source hosted at [GitHub](https://github.com/riga/law)
- Report issues, questions, feature requests on [GitHub Issues](https://github.com/riga/law/issues)


<!-- marker-after-body -->
",2023-07-07 16:01:09+00:00
linkedpipesetl,etl,linkedpipes/etl,"LinkedPipes ETL is an RDF based, lightweight ETL tool",https://etl.linkedpipes.com,False,130,2023-06-08 22:03:31+00:00,2016-01-15 17:23:57+00:00,29,16,7,1,2022-12-16,2022-12-16 12:31:46+00:00,Other,2593,2023-04-03,5,2023-03-30 09:54:46+00:00,2023-06-29 07:54:41+00:00,2023-06-23 19:22:06+00:00,"# LinkedPipes ETL
[![Travis Status](https://travis-ci.com/linkedpipes/etl.svg?branch=develop)](https://travis-ci.com/linkedpipes/etl)

LinkedPipes ETL is an RDF based, lightweight ETL tool.
- [Library of components](https://etl.linkedpipes.com/components) to get you started faster
- [Sharing of configuration](https://etl.linkedpipes.com/templates/) among individual pipelines using templates
- RDF configuration of transformation pipelines

## Requirements
- Linux, Windows, iOS
- [Docker], [Docker Compose]

### For building locally
- [Java] 17, 18
- [Git]
- Optionally [Maven]
- [Node.js] 18 & npm

## Installation and startup
You can run LP-ETL in Docker, or build it from the source.

### Docker
To start LP-ETL you can use:
```
git clone https://github.com/linkedpipes/etl.git
cd etl
docker-compose up
```
This would use pre-build images stored at [DockerHub].
The images are build from the main branch.

Alternatively you can use one liner.
For example to run LP-ETL from ```develop``` branch on ```http://localhost:9080``` use can use following command:
```
curl https://raw.githubusercontent.com/linkedpipes/etl/develop/docker-compose.yml | LP_ETL_PORT=9080 LP_VERSION=develop docker-compose -f - up
```

You may need to run the ```docker-compose``` command as ```sudo``` or be in the ```docker``` group.

#### Building Docker images
You can build LP-ETL images your self.
Note that on Windows, there is an [issue with buildkit](https://github.com/moby/buildkit/issues/1684).
See the [temporary workaround](https://github.com/linkedpipes/etl/issues/851#issuecomment-814058925).

#### Configuration
Environment variables:
- ```LP_VERSION``` - default value ```main```, determine the version of Docker images.
- ```LP_ETL_DOMAIN``` - The URL of the instance, this is used instead of the ```domain.uri``` from the configuration. 
- ```LP_ETL_PORT``` - Specify port mapping for frontend, this is where you can connect to your instance.
  This does NOT have to be the same as port in ```LP_ETL_DOMAIN``` in case of reverse-proxying.

```docker-compose``` utilizes several volumes that can be used to access/provide data.
See ```docker-compose.yml``` comments for examples and configuration.
You may want to create your own ```docker-compose.yml``` for custom configuration.

### From source on Linux

#### Installation
```sh
$ git clone https://github.com/linkedpipes/etl.git
$ cd etl
$ mvn install
```

#### Configuration
The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories. 

#### Startup
```sh
$ cd deploy
$ ./executor.sh >> executor.log &
$ ./executor-monitor.sh >> executor-monitor.log &
$ ./storage.sh >> storage.log &
$ ./frontend.sh >> frontend.log &
```

#### Running LP-ETL as a systemd service
See example service files in the ```deploy/systemd``` folder.

### From source on Windows
Note that it is also possible to use [Bash on Ubuntu on Windows] or [Cygwin] and proceed as with Linux.

#### Installation
```sh
git clone https://github.com/linkedpipes/etl.git
cd etl
mvn install
```

#### Configuration
The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories. 

#### Startup
In the ```deploy``` folder, run
- ```executor.bat```
- ```executor-monitor.bat```
- ```storage.bat```
- ```frontend.bat```

## Data import
You can copy pipelines and templates data from one instance to another directly.

Assume that you have copy of a data directory ```./data-source``` with ```pipelines``` and ```templates``` subdirectories. 
You can obtain the directory from any running instance, you can even merge content of multiple of those directories together.
In the next step you would like to import the data into a new instance. 
You can just copy the files to respective directories under ```./data-target```.
Keep in mind that this would preserve the IRIs.

Should you need to change the IRIs, you should employ import and export functionality available in the frontend.

## Plugins - Components
The components live in the ```jars``` directory.
If you need to create your own component, you can copy an existing component and change it.
 
## Update notes
> Update note 5: 2019-09-03 breaking changes in the configuration file. Remove ```/api/v1``` from the ```executor-monitor.webserver.uri```, so it looks like: ```executor-monitor.webserver.uri = http://localhost:8081```. You can also remove ```executor.execution.uriPrefix``` as the value is derived from ```domain.uri```.

> Update note 4: 2019-07-03 we changed the way frontend is run. If you do not use our script to run it, you need to update yours. 

> Update note 3: When upgrading from develop prior to 2017-02-14, you need to delete ```{deploy}/jars``` and ```{deploy}/osgi```. 

> Update note 2: When upgrading from master prior to 2016-11-04, you need to move your pipelines folder from e.g., ```/data/lp/etl/pipelines``` to ```/data/lp/etl/storage/pipelines```, update the configuration.properites file and possibly the update/restart scripts as there is a new component, ```storage```.

> Update note 1: When upgrading from master prior to 2016-04-07, you need to delete your old execution data (e.g., in /data/lp/etl/working/data)

[Java]: <http://www.oracle.com/technetwork/java/javase/downloads/index.html>
[Git]: <https://git-scm.com/>
[Maven]: <https://maven.apache.org/>
[Node.js]: <https://nodejs.org>
[Cygwin]: <https://www.cygwin.com/>
[Bash on Ubuntu on Windows]: <https://msdn.microsoft.com/en-us/commandline/wsl/about>
[Docker]: <https://www.docker.com/>
[Docker Compose]: <https://docs.docker.com/compose/>
[DockerHub]: <https://hub.docker.com/>
",2023-07-07 16:01:14+00:00
livy,incubator-livy,apache/incubator-livy,Apache Livy is an open source REST interface for interacting with Apache Spark from anywhere.,https://livy.apache.org/,False,784,2023-07-07 04:54:14+00:00,2017-06-25 07:00:09+00:00,570,58,67,0,,,Apache License 2.0,502,v0.7.1-incubating-rc1,13,2021-02-04 02:31:32+00:00,2023-06-28 06:31:48+00:00,2023-06-17 00:03:57+00:00,"# Apache Livy

[![Build Status](https://travis-ci.org/apache/incubator-livy.svg?branch=master)](https://travis-ci.org/apache/incubator-livy)

Apache Livy is an open source REST interface for interacting with
[Apache Spark](http://spark.apache.org) from anywhere. It supports executing snippets of code or
programs in a Spark context that runs locally or in
[Apache Hadoop YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).

* Interactive Scala, Python and R shells
* Batch submissions in Scala, Java, Python
* Multiple users can share the same server (impersonation support)
* Can be used for submitting jobs from anywhere with REST
* Does not require any code change to your programs

[Pull requests](https://github.com/apache/incubator-livy/pulls) are welcomed! But before you begin,
please check out the [Contributing](http://livy.incubator.apache.org/community#Contributing)
section on the [Community](http://livy.incubator.apache.org/community) page of our website.

## Online Documentation

Guides and documentation on getting started using Livy, example code snippets, and Livy API
documentation can be found at [livy.incubator.apache.org](http://livy.incubator.apache.org).

## Before Building Livy

To build Livy, you will need:

Debian/Ubuntu:
  * mvn (from ``maven`` package or maven3 tarball)
  * openjdk-8-jdk (or Oracle JDK 8)
  * Python 2.7+
  * R 3.x

Redhat/CentOS:
  * mvn (from ``maven`` package or maven3 tarball)
  * java-1.8.0-openjdk (or Oracle JDK 8)
  * Python 2.7+
  * R 3.x

MacOS:
  * Xcode command line tools
  * Oracle's JDK 1.8
  * Maven (Homebrew)
  * Python 2.7+
  * R 3.x

Required python packages for building Livy:
  * cloudpickle
  * requests
  * requests-kerberos
  * flake8
  * flaky
  * pytest


To run Livy, you will also need a Spark installation. You can get Spark releases at
https://spark.apache.org/downloads.html.

Livy requires Spark 2.4+. You can switch to a different version of Spark by setting the
``SPARK_HOME`` environment variable in the Livy server process, without needing to rebuild Livy.


## Building Livy

Livy is built using [Apache Maven](http://maven.apache.org). To check out and build Livy, run:

```
git clone https://github.com/apache/incubator-livy.git
cd incubator-livy
mvn package
```

You can also use the provided [Dockerfile](./dev/docker/livy-dev-base/Dockerfile):

```
git clone https://github.com/apache/incubator-livy.git
cd incubator-livy
docker build -t livy-ci dev/docker/livy-dev-base/
docker run --rm -it -v $(pwd):/workspace -v $HOME/.m2:/root/.m2 livy-ci mvn package
```

> **Note**: The `docker run` command maps the maven repository to your host machine's maven cache so subsequent runs will not need to download dependencies.

By default Livy is built against Apache Spark 2.4.5, but the version of Spark used when running
Livy does not need to match the version used to build Livy. Livy internally handles the differences
between different Spark versions.

The Livy package itself does not contain a Spark distribution. It will work with any supported
version of Spark without needing to rebuild.

### Build Profiles

| Flag         | Purpose                                                            |
|--------------|--------------------------------------------------------------------|
| -Phadoop2    | Choose Hadoop2 based build dependencies (default configuration)    |
| -Pspark2     | Choose Spark 2.x based build dependencies (default configuration)  |
| -Pspark3     | Choose Spark 3.x based build dependencies                          |
| -Pscala-2.11 | Choose Scala 2.11 based build dependencies (default configuration) |        
| -Pscala-2.12 | Choose scala 2.12 based build dependencies                         |
",2023-07-07 16:01:18+00:00
longbow,Longbow,HECBioSim/Longbow,Longbow is a tool for automating simulations on a remote HPC machine. Longbow is designed to mimic the normal way an application is run locally but allows simulations to be sent to powerful machines. ,http://www.hecbiosim.ac.uk,False,16,2021-12-23 17:56:47+00:00,2016-09-07 15:08:40+00:00,2,6,3,14,v1.5.2,2018-09-28 14:38:50+00:00,Other,860,v1.5.2,25,2018-09-28 14:38:50+00:00,,2020-01-13 14:25:00+00:00,".. image:: https://img.shields.io/pypi/v/Longbow.svg
  :target: https://pypi.python.org/pypi/Longbow/
.. image:: https://img.shields.io/pypi/pyversions/Longbow.svg
  :target: https://pypi.python.org/pypi/Longbow
.. image:: https://img.shields.io/pypi/status/Longbow.svg
  :target: https://pypi.python.org/pypi/Longbow
.. image:: https://travis-ci.org/HECBioSim/Longbow.svg?branch=master
  :target: https://travis-ci.org/HECBioSim/Longbow
.. image:: https://coveralls.io/repos/github/HECBioSim/Longbow/badge.svg?branch=master
  :target: https://coveralls.io/github/HECBioSim/Longbow?branch=master
.. image:: https://api.codacy.com/project/badge/Grade/78370da5b0284083b64e7c1ef030ee8e
  :target: https://www.codacy.com/gh/HECBioSim/Longbow?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=HECBioSim/Longbow&amp;utm_campaign=Badge_Grade
.. image:: https://readthedocs.org/projects/longbow/badge/?version=latest
  :target: https://longbow.readthedocs.io/en/latest/?badge=latest
  :alt: Documentation Status


*******
Longbow
*******

Longbow is an automated simulation submission and monitoring tool. Longbow
is designed to reproduce the look and feel of using software on the users
local computer with the difference that the heavy lifting is done by a
supercomputer.

Longbow will automatically generate the necessary submit files and handle all
initial file transfer, monitor jobs, transfer files at configurable
intervals and perform final file transfer and cleanup.

Longbow can be used to launch one-off jobs, generate ensembles of similar jobs
or even run many different jobs over many different supercomputers.

Out of the box, Longbow is currently supporting the PBS/Torque, LSF, SGE,
Slurm, SoGE schedulers and ships with application plugins for commonly used
bio-molecular simulation softwares AMBER, CHARMM, GROMACS, LAMMPS, NAMD.
Longbow is however highly configurable and will function normally with generic
software without plugins, however plugins can easily be made to extend Longbow
to fully support applications and schedulers that do not ship out of the box.

Using Longbow can be as simple as the following example:

local: executable -a arg1 -b arg2 -c arg3

remote: longbow executable -a arg1 -b arg2 -c arg3

Longbow is also available to developers of applications which require support
for automating job submission. Longbow is available as a convenient and
light-weight python API that can be integrated in a number of different way.


Licensing
=========

Longbow is released under the BSD 3-clause license. A copy of this license is
provided when Longbow is downloaded and installed.


Citing
======

If you make use of Longbow in your own code or in production simulations that
result in publishable output, then please reference our paper:

Gebbie-Rayet, J, Shannon, G, Loeffler, H H and Laughton, C A 2016 Longbow: 
A Lightweight Remote Job Submission Tool. Journal of Open Research Software, 
4: e1, DOI: http://dx.doi.org/10.5334/jors.95


Installation
============

Releases can be installed either via pip or can be installed manually, to
install via pip:

pip install longbow

or to install manually (see docs) Longbow can be downloaded here:

http://www.hecbiosim.ac.uk/longbow

and then extract and run the setup.py script to install.


Documentation
=============

Documentation for Longbow users can be found here:

http://www.hecbiosim.ac.uk/longbow-docs


Examples
========

Example files can be installed either through the Longbow command-line or by
downloading from the HECBioSim website manually:

longbow --examples

http://www.hecbiosim.ac.uk/longbow-examples


Support
=======

Support for any issues arising from using Longbow, whether these are questions,
to report a bug or to suggest new ideas. You should use the Longbow forums
here:

https://github.com/HECBioSim/Longbow/issues


Developers
==========

Developers that wish to contribute to Longbow are welcome. We do ask that if
you wish to contribute to the Longbow base code that you contact us first.

The following resources are available to developers:

Code repository: https://github.com/hecbiosim/longbow

Unit testing: https://travis-ci.org/HECBioSim/Longbow

Code coverage: https://coveralls.io/github/HECBioSim/Longbow
",2023-07-07 16:01:23+00:00
loom,loom,StanfordBioinformatics/loom,A tool for running bioinformatics workflows locally or in the cloud.,,False,29,2022-08-23 23:42:59+00:00,2014-11-21 21:44:27+00:00,8,20,3,0,,,GNU Affero General Public License v3.0,1518,0.7.4rc1,76,2019-12-13 23:51:59+00:00,2023-05-30 20:04:39+00:00,2019-08-21 16:09:49+00:00,"What is loom?
=============

Loom is a platform-independent tool to create, execute, track, and share workflows.

Why use Loom?
=============

Ease of use
-----------

Loom runs out-of-the-box locally or in the cloud.

Repeatable analysis
-------------------

Loom makes sure you can repeat your analysis months and years down the road after you've lost your notebook, your data analyst has found a new job, and your server has had a major OS version upgrade.

Loom uses Docker to reproduce your runtime environment, records file hashes to verify analysis inputs, and keeps fully reproducible records of your work.

Traceable results
-----------------

Loom remembers anything you ever run and can tell you exactly how each result was produced.

Portability between platforms
-----------------------------

Exactly the same workflow can be run on your laptop or on a public cloud service.

Open architecture
-----------------

Not only is Loom open source and free to use, it uses an inside-out architecture that minimizes lock-in and lets you easily share your work with other people.

- Write your results to a traditional filesystem or object store and browse them outside of Loom
- Publish your tools as Docker images
- Publish your workflows as simple, human-readable documents
- Collaborate by sharing your workflows and results between Loom servers
- Connect Loom to multiple file stores without creating redundant copies
- Efficient re-use of results for redundant analysis steps

How many times do you really need to run the same analysis on the same inputs? Loom knows which steps in your workflow have already been run and seamlessly integrates previous results with the current run, while still maintaining data provenance and traceability.

Graphical user interface
------------------------

While you may want to automate your analysis from the command line, a graphical user interface is useful for interactively browsing workflows and results.

Security and compliance
-----------------------

Loom is designed with clinical compliance in mind.

Who needs Loom?
===============

Loom is built for the kind of workflows that bioinformaticians run -- multi-step analyses with large data files passed between steps. But nothing about Loom is specific to bioinformatics.

Loom is scalable and supports individual analysts or large institutions.

Get started
===========

Check out our Getting Started Guide and give Loom a try.

http://loom.readthedocs.io/en/latest/installing.html

What is the current status?
===========================

Loom is under active development. To get involved, contact info@loomengine.org

Contributors
============

- Nathan Hammond
- Isaac Liao
",2023-07-07 16:01:27+00:00
looper,looper,pepkit/looper,A job submitter for Portable Encapsulated Projects,http://looper.databio.org,False,15,2023-06-17 23:22:34+00:00,2017-11-16 16:39:21+00:00,5,3,15,26,v1.4.0,2023-04-24 20:40:00+00:00,"BSD 2-Clause ""Simplified"" License",2688,v1.4.0,34,2023-04-24 20:40:00+00:00,2023-06-27 22:03:22+00:00,2023-06-27 22:03:19+00:00,"# <img src=""docs/img/looper_logo.svg"" alt=""looper logo"" height=""70"">

![Run pytests](https://github.com/pepkit/looper/workflows/Run%20pytests/badge.svg)
[![PEP compatible](http://pepkit.github.io/img/PEP-compatible-green.svg)](http://pepkit.github.io)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


`Looper` is a pipeline submission engine. The typical use case is to run a bioinformatics pipeline across many different input samples. Instructions are in the [documentation](http://looper.databio.org/).
",2023-07-07 16:01:31+00:00
lsstdatamanagement,pipe_base,lsst/pipe_base,LSST Data Management: base classes for data processing tasks,https://pipelines.lsst.io/v/daily/modules/lsst.pipe.base/index.html,False,7,2023-07-07 11:19:21+00:00,2014-08-16 07:49:52+00:00,12,46,36,0,,,GNU General Public License v3.0,1613,w.2023.27,494,2023-07-06 01:27:11+00:00,2023-07-07 14:26:11+00:00,2023-07-06 01:27:11+00:00,"# lsst-pipe-base

[![pypi](https://img.shields.io/pypi/v/lsst-pipe-base.svg)](https://pypi.org/project/lsst-pipe-base/)
[![codecov](https://codecov.io/gh/lsst/pipe_base/branch/main/graph/badge.svg?token=nTEA5ZAYiD)](https://codecov.io/gh/lsst/pipe_base)

Pipeline infrastructure code for the [Rubin Science Pipelines](https://pipelines.lsst.io).

* SPIE Paper from 2022: [The Vera C. Rubin Observatory Data Butler and Pipeline Execution System](https://arxiv.org/abs/2206.14941)

PyPI: [lsst-pipe-base](https://pypi.org/project/lsst-pipe-base/)
",2023-07-07 16:01:36+00:00
luigi,luigi,spotify/luigi,"Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. ",,False,16625,2023-07-07 14:27:47+00:00,2012-09-20 15:06:38+00:00,2377,481,354,56,3.3.0,2023-05-04 09:47:52+00:00,Apache License 2.0,4078,v1.2.1,65,2015-05-26 09:57:19+00:00,2023-07-07 14:27:47+00:00,2023-05-04 09:47:52+00:00,".. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png
   :alt: Luigi Logo
   :align: center

.. image:: https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fspotify%2Fluigi%2Fbadge&label=build&logo=none&%3Fref%3Dmaster&style=flat
    :target: https://actions-badge.atrox.dev/spotify/luigi/goto?ref=master

.. image:: https://img.shields.io/codecov/c/github/spotify/luigi/master.svg?style=flat
    :target: https://codecov.io/gh/spotify/luigi?branch=master

.. image:: https://img.shields.io/pypi/v/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

.. image:: https://img.shields.io/pypi/l/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

Luigi is a Python (3.6, 3.7, 3.8, 3.9, 3.10 tested) package that helps you build complex
pipelines of batch jobs. It handles dependency resolution, workflow management,
visualization, handling failures, command line integration, and much more.

Getting Started
---------------

Run ``pip install luigi`` to install the latest stable version from `PyPI
<https://pypi.python.org/pypi/luigi>`_. `Documentation for the latest release
<https://luigi.readthedocs.io/en/stable/>`__ is hosted on readthedocs.

Run ``pip install luigi[toml]`` to install Luigi with `TOML-based configs
<https://luigi.readthedocs.io/en/stable/configuration.html>`__ support.

For the bleeding edge code, ``pip install
git+https://github.com/spotify/luigi.git``. `Bleeding edge documentation
<https://luigi.readthedocs.io/en/latest/>`__ is also available.

Background
----------

The purpose of Luigi is to address all the plumbing typically associated
with long-running batch processes. You want to chain many tasks,
automate them, and failures *will* happen. These tasks can be anything,
but are typically long running things like
`Hadoop <http://hadoop.apache.org/>`_ jobs, dumping data to/from
databases, running machine learning algorithms, or anything else.

There are other software packages that focus on lower level aspects of
data processing, like `Hive <http://hive.apache.org/>`__,
`Pig <http://pig.apache.org/>`_, or
`Cascading <http://www.cascading.org/>`_. Luigi is not a framework to
replace these. Instead it helps you stitch many tasks together, where
each task can be a `Hive query <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hive.html>`__,
a `Hadoop job in Java <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hadoop_jar.html>`_,
a  `Spark job in Scala or Python <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.spark.html>`_,
a Python snippet,
`dumping a table <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.sqla.html>`_
from a database, or anything else. It's easy to build up
long-running pipelines that comprise thousands of tasks and take days or
weeks to complete. Luigi takes care of a lot of the workflow management
so that you can focus on the tasks themselves and their dependencies.

You can build pretty much any task you want, but Luigi also comes with a
*toolbox* of several common task templates that you use. It includes
support for running
`Python mapreduce jobs <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hadoop.html>`_
in Hadoop, as well as
`Hive <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hive.html>`__,
and `Pig <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.pig.html>`__,
jobs. It also comes with
`file system abstractions for HDFS <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hdfs.html>`_,
and local files that ensures all file system operations are atomic. This
is important because it means your data pipeline will not crash in a
state containing partial data.

Visualiser page
---------------

The Luigi server comes with a web interface too, so you can search and filter
among all your tasks.

.. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/visualiser_front_page.png
   :alt: Visualiser page

Dependency graph example
------------------------

Just to give you an idea of what Luigi does, this is a screen shot from
something we are running in production. Using Luigi's visualiser, we get
a nice visual overview of the dependency graph of the workflow. Each
node represents a task which has to be run. Green tasks are already
completed whereas yellow tasks are yet to be run. Most of these tasks
are Hadoop jobs, but there are also some things that run locally and
build up data files.

.. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/user_recs.png
   :alt: Dependency graph

Philosophy
----------

Conceptually, Luigi is similar to `GNU
Make <http://www.gnu.org/software/make/>`_ where you have certain tasks
and these tasks in turn may have dependencies on other tasks. There are
also some similarities to `Oozie <http://oozie.apache.org/>`_
and `Azkaban <https://azkaban.github.io/>`_. One major
difference is that Luigi is not just built specifically for Hadoop, and
it's easy to extend it with other kinds of tasks.

Everything in Luigi is in Python. Instead of XML configuration or
similar external data files, the dependency graph is specified *within
Python*. This makes it easy to build up complex dependency graphs of
tasks, where the dependencies can involve date algebra or recursive
references to other versions of the same task. However, the workflow can
trigger things not in Python, such as running
`Pig scripts <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.pig.html>`_
or `scp'ing files <https://luigi.readthedocs.io/en/latest/api/luigi.contrib.ssh.html>`_.

Who uses Luigi?
---------------

We use Luigi internally at `Spotify <https://www.spotify.com>`_ to run
thousands of tasks every day, organized in complex dependency graphs.
Most of these tasks are Hadoop jobs. Luigi provides an infrastructure
that powers all kinds of stuff including recommendations, toplists, A/B
test analysis, external reports, internal dashboards, etc.

Since Luigi is open source and without any registration walls, the exact number
of Luigi users is unknown. But based on the number of unique contributors, we
expect hundreds of enterprises to use it. Some users have written blog posts
or held presentations about Luigi:

* `Spotify <https://www.spotify.com>`_ `(presentation, 2014) <http://www.slideshare.net/erikbern/luigi-presentation-nyc-data-science>`__
* `Foursquare <https://foursquare.com/>`_ `(presentation, 2013) <http://www.slideshare.net/OpenAnayticsMeetup/luigi-presentation-17-23199897>`__
* `Mortar Data (Datadog) <https://www.datadoghq.com/>`_ `(documentation / tutorial) <http://help.mortardata.com/technologies/luigi>`__
* `Stripe <https://stripe.com/>`_ `(presentation, 2014) <http://www.slideshare.net/PyData/python-as-part-of-a-production-machine-learning-stack-by-michael-manapat-pydata-sv-2014>`__
* `Buffer <https://buffer.com/>`_ `(blog, 2014) <https://overflow.bufferapp.com/2014/10/31/buffers-new-data-architecture/>`__
* `SeatGeek <https://seatgeek.com/>`_ `(blog, 2015) <http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/>`__
* `Treasure Data <https://www.treasuredata.com/>`_ `(blog, 2015) <http://blog.treasuredata.com/blog/2015/02/25/managing-the-data-pipeline-with-git-luigi/>`__
* `Growth Intelligence <http://growthintel.com/>`_ `(presentation, 2015) <http://www.slideshare.net/growthintel/a-beginners-guide-to-building-data-pipelines-with-luigi>`__
* `AdRoll <https://www.adroll.com/>`_ `(blog, 2015) <http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html>`__
* 17zuoye `(presentation, 2015) <https://speakerdeck.com/mvj3/luiti-an-offline-task-management-framework>`__
* `Custobar <https://www.custobar.com/>`_ `(presentation, 2016) <http://www.slideshare.net/teemukurppa/managing-data-workflows-with-luigi>`__
* `Blendle <https://launch.blendle.com/>`_ `(presentation) <http://www.anneschuth.nl/wp-content/uploads/sea-anneschuth-streamingblendle.pdf#page=126>`__
* `TrustYou <http://www.trustyou.com/>`_ `(presentation, 2015) <https://speakerdeck.com/mfcabrera/pydata-berlin-2015-processing-hotel-reviews-with-python>`__
* `Groupon <https://www.groupon.com/>`_ / `OrderUp <https://orderup.com>`_ `(alternative implementation) <https://github.com/groupon/luigi-warehouse>`__
* `Red Hat - Marketing Operations <https://www.redhat.com>`_ `(blog, 2017) <https://github.com/rh-marketingops/rh-mo-scc-luigi>`__
* `GetNinjas <https://www.getninjas.com.br/>`_ `(blog, 2017) <https://labs.getninjas.com.br/using-luigi-to-create-and-monitor-pipelines-of-batch-jobs-eb8b3cd2a574>`__
* `voyages-sncf.com <https://www.voyages-sncf.com/>`_ `(presentation, 2017) <https://github.com/voyages-sncf-technologies/meetup-afpy-nantes-luigi>`__
* `Open Targets <https://www.opentargets.org/>`_ `(blog, 2017) <https://blog.opentargets.org/using-containers-with-luigi>`__
* `Leipzig University Library <https://ub.uni-leipzig.de>`_ `(presentation, 2016) <https://de.slideshare.net/MartinCzygan/build-your-own-discovery-index-of-scholary-eresources>`__ / `(project) <https://finc.info/de/datenquellen>`__
* `Synetiq <https://synetiq.net/>`_ `(presentation, 2017) <https://www.youtube.com/watch?v=M4xUQXogSfo>`__
* `Glossier <https://www.glossier.com/>`_ `(blog, 2018) <https://medium.com/glossier/how-to-build-a-data-warehouse-what-weve-learned-so-far-at-glossier-6ff1e1783e31>`__
* `Data Revenue <https://www.datarevenue.com/>`_ `(blog, 2018) <https://www.datarevenue.com/en/blog/how-to-scale-your-machine-learning-pipeline>`_
* `Uppsala University <http://pharmb.io>`_ `(tutorial) <http://uppnex.se/twiki/do/view/Courses/EinfraMPS2015/Luigi.html>`_   / `(presentation, 2015) <https://www.youtube.com/watch?v=f26PqSXZdWM>`_ / `(slides, 2015) <https://www.slideshare.net/SamuelLampa/building-workflows-with-spotifys-luigi>`_ / `(poster, 2015) <https://pharmb.io/poster/2015-sciluigi/>`_ / `(paper, 2016) <https://doi.org/10.1186/s13321-016-0179-6>`_ / `(project) <https://github.com/pharmbio/sciluigi>`_
* `GIPHY <https://giphy.com/>`_ `(blog, 2019) <https://engineering.giphy.com/luigi-the-10x-plumber-containerizing-scaling-luigi-in-kubernetes/>`__
* `xtream <https://xtreamers.io/>`__ `(blog, 2019) <https://towardsdatascience.com/lessons-from-a-real-machine-learning-project-part-1-from-jupyter-to-luigi-bdfd0b050ca5>`__
* `CIAN <https://cian.ru/>`__ `(presentation, 2019) <https://www.highload.ru/moscow/2019/abstracts/6030>`__

Some more companies are using Luigi but haven't had a chance yet to write about it:

* `Schibsted <http://www.schibsted.com/>`_
* `enbrite.ly <http://enbrite.ly/>`_
* `Dow Jones / The Wall Street Journal <http://wsj.com>`_
* `Hotels.com <https://hotels.com>`_
* `Newsela <https://newsela.com>`_
* `Squarespace <https://www.squarespace.com/>`_
* `OAO <https://adops.com/>`_
* `Grovo <https://grovo.com/>`_
* `Weebly <https://www.weebly.com/>`_
* `Deloitte <https://www.Deloitte.co.uk/>`_
* `Stacktome <https://stacktome.com/>`_
* `LINX+Neemu+Chaordic <https://www.chaordic.com.br/>`_
* `Foxberry <https://www.foxberry.com/>`_
* `Okko <https://okko.tv/>`_
* `ISVWorld <http://isvworld.com/>`_
* `Big Data <https://bigdata.com.br/>`_
* `Movio <https://movio.co.nz/>`_
* `Bonnier News <https://www.bonniernews.se/>`_
* `Starsky Robotics <https://www.starsky.io/>`_
* `BaseTIS <https://www.basetis.com/>`_
* `Hopper <https://www.hopper.com/>`_
* `VOYAGE GROUP/Zucks <https://zucks.co.jp/en/>`_
* `Textpert <https://www.textpert.ai/>`_
* `Tracktics <https://www.tracktics.com/>`_
* `Whizar <https://www.whizar.com/>`_
* `xtream <https://www.xtreamers.io/>`__
* `Skyscanner <https://www.skyscanner.net/>`_
* `Jodel <https://www.jodel.com/>`_
* `Mekar <https://mekar.id/en/>`_
* `M3 <https://corporate.m3.com/en/>`_
* `Assist Digital <https://www.assistdigital.com/>`_
* `Meltwater <https://www.meltwater.com/>`_
* `DevSamurai <https://www.devsamurai.com/>`_

We're more than happy to have your company added here. Just send a PR on GitHub.

External links
--------------

* `Mailing List <https://groups.google.com/d/forum/luigi-user/>`_ for discussions and asking questions. (Google Groups)
* `Releases <https://pypi.python.org/pypi/luigi>`_ (PyPI)
* `Source code <https://github.com/spotify/luigi>`_ (GitHub)
* `Hubot Integration <https://github.com/houzz/hubot-luigi>`_ plugin for Slack, Hipchat, etc (GitHub)

Authors
-------

Luigi was built at `Spotify <https://www.spotify.com>`_, mainly by
`Erik Bernhardsson <https://github.com/erikbern>`_ and
`Elias Freider <https://github.com/freider>`_.
`Many other people <https://github.com/spotify/luigi/graphs/contributors>`_
have contributed since open sourcing in late 2012.
`Arash Rouhani <https://github.com/tarrasch>`_ was the chief maintainer from 2015 to 2019, and now
Spotify's Data Team maintains Luigi.
",2023-07-07 16:01:41+00:00
madats,madats,dghoshal-lbl/madats,,,False,1,2020-11-09 18:30:03+00:00,2018-05-03 19:46:22+00:00,0,3,2,1,v1.1.2,2018-08-10 17:22:40+00:00,Other,101,v1.1.2,4,2018-08-10 17:21:56+00:00,,2020-09-19 07:01:16+00:00,"**************************************************************************
MaDaTS: Managing Data on Tiered Storage for Scientific Workflows

* Author: Devarshi Ghoshal
* v1.1.3
* Created: Oct 25, 2016
* Updated: Sep 16, 2020
**************************************************************************

MaDaTS provides an integrated data management and workflow execution
framework on multi-tiered storage systems. Users of MaDaTS can execute
a workflow by either specifying the workflow stages in a YAML description
file, or use the API to manage workflows and associated data. Some examples
of specifying the workflow description and using the API are provided in
the examples/ directory.

The MaDaTS API provides simple data abstractions for managing workflow and
data on multi-tiered storage. It takes a data-driven approach to executing
workflows, where a workflow is mapped to a Virtual Data Space (VDS) consisting
of virtual data objects. A user simply creates a VDS and adds virtual data
objects to the VDS, and MaDaTS takes care of all the necessary data movements
and bindings to seamlessly manage a workflow and associated data across multiple
storage tiers.  

PRE-REQUISITES
--------------
* Python (>= 2.7)
* pip (>= 9.0)

BUILD
-----
To install MaDaTS, do:

        pip install -r requirements.txt
        python setup.py install


EXECUTION SETUP
---------------
The environment variable `MADATS_HOME` should be set prior to using MaDaTS.
The setup script creates a MADATS_HOME file that can be sourced prior to
using MaDaTS as:

       source MADATS_HOME

Alternatively, users can set `MADATS_HOME` as:

       # BASH / ZSH
       export MADATS_HOME=</path/to/madats/source/directory>

       # CSH / TCSH
       setenv MADATS_HOME </path/to/madats/source/directory>


TEST
-----
To test MaDaTS, do:

        source MADATS_HOME
        py.test tests/test_madats.py


Example
-------
In order to manage data and workflow, users need to create virtual data objects
and tasks, and add them to a Virtual Data Space (VDS). Data and tasks of the
workflow are managed through MaDaTS by simply calling the `manage` function.
An example listing the steps to manage a workflow and its data through MaDaTS
is given below.

	import madats
	
	# Create a Virtual Data Space (VDS)
	vds = madats.VirtualDataSpace()

	# Create Virtual Data Object
	vdo = madats.VirtualDataObject('/path/to/data')

	# Create a Task
	task = madats.Task(command='/application/program')
	task.params = ['arg1', 'arg2', vdo]

	# Associate tasks to virtual data objects
	vdo.producers = [task]

	# Add the virtual data object to the VDS
	vds.add(vdo)

	# Manage data and workflow execution through MaDaTS
	madats.manage(vds)

It is important to note how MaDaTS uses data as the first-class
citizen. Everything in MaDaTS is centered around virtual data objects.
Tasks are specified as *producers* and *consumers* of virtual data
objects. A VDS is a collection of several virtual data objects that
that represent the data of a workflow.

In addition to creating a VDS step-by-step as shown above, users
can also map a workflow into VDS. MaDaTS provides the `map` function
that takes as input a YAML description of a workflow, or a dict-like
object (similar to JSON). 

       import madats

       # Map a YAML workflow description to VDS
       vds = madats.map('workflow/description/yaml', language='yaml') 

$MADATS_HOME/examples/madats_workflow.yaml specifies a description file
for a sample workflow. The examples/ directory also contains examples that
describe different ways of specifying a workflow and data management properties
in MaDaTS.

Configuring Storage Tiers
--------------------------
MaDaTS is designed to manage data seamlessly across multiple storage tiers. The storage
configuration can be defined through `config/storage.yaml`. The configuration file contains
an identifier for each storage tier and its associated properties.

Batch Scheduler
---------------
MaDaTS currently supports PBS and SLURM batch schedulers for managing workflow tasks as
batch jobs. The various options for the batch schedulers are specified in their respective
configuration files `config/pbs.cfg` and `config/slurm.cfg`. Users can also add their own
batch schedulers by specifying the respective configuration files.

LICENSE
-------
3-Clause BSD License.
Copyright (c) 2018, The Regents of the University of California,
through Lawrence Berkeley National Laboratory (subject to receipt
of any required approvals from the U.S. Dept. of Energy). All rights
reserved.",2023-07-07 16:01:45+00:00
maestro,maestrowf,LLNL/maestrowf,A tool to easily orchestrate general computational workflows both locally and on supercomputers,,False,114,2023-06-21 16:33:09+00:00,2017-07-10 21:18:45+00:00,39,19,23,7,v1.1.9,2022-05-29 02:50:36+00:00,MIT License,285,v1.1.9,19,2022-05-29 02:50:36+00:00,2023-06-21 16:33:10+00:00,2023-05-10 17:54:08+00:00,"![](https://github.com/LLNL/maestrowf/raw/develop/assets/logo.png?raw=true ""Orchestrate your workflows with ease!"")

# Maestro Workflow Conductor ([maestrowf](https://pypi.org/project/maestrowf/))

[![Build Status](https://travis-ci.org/LLNL/maestrowf.svg?branch=develop)](https://travis-ci.org/LLNL/maestrowf)
[![PyPI](https://img.shields.io/pypi/v/maestrowf.svg)](https://pypi.python.org/pypi?name=maestrowf&version=1.0.0&:action=display)
![Spack](https://img.shields.io/spack/v/py-maestrowf)
[![Issues](https://img.shields.io/github/issues/LLNL/maestrowf.svg)](https://github.com/LLNL/maestrowf/issues)
[![Forks](https://img.shields.io/github/forks/LLNL/maestrowf.svg)](https://github.com/LLNL/maestrowf/network)
[![Stars](https://img.shields.io/github/stars/LLNL/maestrowf.svg)](https://github.com/LLNL/maestrowf/stargazers)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/LLNL/maestrowf/master/LICENSE)

[![Downloads](https://static.pepy.tech/personalized-badge/maestrowf?period=total&units=international_system&left_color=grey&right_color=blue&left_text=Downloads)](https://pepy.tech/project/maestrowf)
[![Downloads](https://static.pepy.tech/personalized-badge/maestrowf?period=month&units=international_system&left_color=grey&right_color=blue&left_text=Downloads/Month)](https://pepy.tech/project/maestrowf)

Maestro can be installed via [pip](https://pip.pypa.io/):

    pip install maestrowf

## Documentation

* [Maestro Documentation](https://maestrowf.readthedocs.io) - Official Maestro documentation.
* [Maestro Sheetmusic](https://github.com/LLNL/maestro_sheetmusic) - A collection of sample and user contributed Maestro study examples.
* [Maestro Samples](/samples) - Maestro sample studies.

## Getting Started is Quick and Easy

Create a `YAML` file named `study.yaml` and paste the following content into the file:

``` yaml
description:
    name: hello_world
    description: A simple 'Hello World' study.

study:
    - name: say-hello
      description: Say hello to the world!
      run:
          cmd: |
            echo ""Hello, World!"" > hello_world.txt
```

> *PHILOSOPHY*: Maestro believes in the principle of a clearly defined process, specified as a list of tasks, that are self-documenting and clear in their intent.

Running the `hello_world` study is as simple as...

    maestro run study.yaml

## Creating a Parameter Study is just as Easy

With the addition of the `global.parameters` block, and a few simple tweaks to your `study` block, the complete specification should look like this:

``` yaml
description:
    name: hello_planet
    description: A simple study to say hello to planets (and Pluto)

study:
    - name: say-hello
      description: Say hello to a planet!
      run:
          cmd: |
            echo ""Hello, $(PLANET)!"" > hello_$(PLANET).txt

global.parameters:
    PLANET:
        values: [Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto]
        label: PLANET.%%
```

> *PHILOSOPHY*: Maestro believes that a workflow should be easily parameterized with minimal modifications to the core process.

Maestro will automatically expand each parameter into its own isolated workspace, generate a script for each parameter, and automatically monitor execution of each task.

And, running the study is still as simple as:

``` bash
    maestro run study.yaml
```

## Scheduling Made Simple

But wait there's more! If you want to schedule a study, it's just as simple. With some minor modifications, you are able to run on an [HPC](https://en.wikipedia.org/wiki/Supercomputer) system.

``` yaml
description:
    name: hello_planet
    description: A simple study to say hello to planets (and Pluto)

batch:
    type:  slurm
    queue: pbatch
    host:  quartz
    bank:  science

study:
    - name: say-hello
      description: Say hello to a planet!
      run:
          cmd: |
            echo ""Hello, $(PLANET)!"" > hello_$(PLANET).txt
          nodes: 1
          procs: 1
          walltime: ""00:02:00""

global.parameters:
    PLANET:
        values: [Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto]
        label: PLANET.%%
```

> **NOTE**: This specification is configured to run on LLNL's quartz cluster. Under the `batch` header, you will need to make the necessary changes to schedule onto other HPC resources.
>
> *PHILOSOPHY*: Maestro believes that how a workflow is defined should be decoupled from how it's run. We achieve this capability by providing a seamless interface to multiple schedulers that allows Maestro to readily port workflows to multiple platforms.

For other samples, see the [samples](/samples) subfolder. To continue with our Hello World example, see the [Basics of Study Construction](https://maestrowf.readthedocs.io/en/latest/hello_world.html) in our [documentation](https://maestrowf.readthedocs.io/en/latest/index.html).

## An Example Study using LULESH

Maestro comes packed with a basic example using [LULESH](https://github.com/LLNL/LULESH), a proxy application provided by LLNL. You can find the example [here](https://maestrowf.readthedocs.io/en/latest/quick_start.html#).

## What is Maestro?

Maestro is an open-source HPC software tool that defines a YAML-based study specification for defining multistep workflows and automates execution of software flows on HPC resources. The core design tenants of Maestro focus on encouraging clear workflow communication and documentation, while making consistent execution easier to allow users to focus on science. Maestro's study specification helps users think about complex workflows in a step-wise, intent-oriented, manner that encourages modularity and tool reuse. These principles are becoming increasingly important as computational science is continuously more present in scientific fields and has started to require a similar rigor to physical experiment. Maestro is currently in use for multiple projects at Lawrence Livermore National Laboratory and has been used to run existing codes including MFEM, and other simulation codes. It has also been used in other areas including in the training of machine-learned models and more.

### Maestro's Foundation and Core Concepts

There are many definitions of workflow, so we try to keep it simple and define the term as follows:

``` text
A set of high level tasks to be executed in some order, with or without dependencies on each other.
```

We have designed Maestro around the core concept of what we call a ""study"". A study is defined as a set of steps that are executed (a workflow) over a set of parameters. A study in Maestro's context is analogous to an actual tangible scientific experiment, which has a set of clearly defined and repeatable steps which are repeated over multiple specimen.

Maestro's core tenets are defined as follows:

##### Repeatability

A study should be easily repeatable. Like any well-planned and implemented science experiment, the steps themselves should be executed the exact same way each time a study is run over each set of parameters or over different runs of the study itself.

##### Consistent

Studies should be consistently documented and able to be run in a consistent fashion. The removal of variation in the process means less mistakes when executing studies, ease of picking up studies created by others, and uniformity in defining new studies.

##### Self-documenting

Documentation is important in computational studies as much as it is in physical science. The YAML specification defined by Maestro provides a few required key encouraging human-readable documentation. Even further, the specification itself is a documentation of a complete workflow.

----------------

## Setting up your Python Environment

To get started, we recommend using virtual environments. If you do not have the
Python `virtualenv` package installed, take a look at their official [documentation](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) to get started.

To create a new virtual environment:

    python -m virtualenv maestro_venv
    source maestro_venv/bin/activate

### Getting Started for Contributors

If you plan to develop on Maestro, install the repository directly using:

    pip install poetry
    poetry install

Once set up, test the environment. The paths should point to a virtual environment folder.

    which python
    which pip

----------------

## Using Maestro Dockerfiles


Maestro comes packaged with a set of Docker files for testing things out. The two primary files
are:

- A standard `Dockerfile` in the root of the Maestro repository. This file is a standard install
  of Maestro meant to try out Maestro on the demo samples provided with this repository. In order
  to try Maestro locally, with [Docker](https://www.docker.com/) installed run:

  ```
  docker build -t maestrowf .
  docker run -ti maestrowf
  ```

  From within the container run the following:

  ```
  maestro run ./maestrowf/samples/lulesh/lulesh_sample1_unix.yaml
  ```

- In order to try out Flux 0.19.0 integration, from the root of the Maestro repository run the
  following:

  ```
  docker build -t flux_0190 -f ./docker/flux/0.19.0/Dockerfile .
  docker run -ti flux_0190
  ```

  From within the container run the following:

  ```
  maestro run ./maestrowf/samples/lulesh/lulesh_sample1_unix_flux.yaml
  ```

----------------

## Contributors

Many thanks go to MaestroWF's [contributors](https://github.com/LLNL/maestrowf/graphs/contributors).

If you have any questions or to submit feature requests please [open a ticket](https://github.com/llnl/maestrowf/issues).

----------------

## Release
MaestroWF is released under an MIT license.  For more details see the
NOTICE and LICENSE files.

``LLNL-CODE-734340``
",2023-07-07 16:01:49+00:00
mahout,mahout,apache/mahout,Mirror of Apache Mahout,,False,2083,2023-07-07 12:49:06+00:00,2014-05-23 07:00:07+00:00,953,237,42,0,,,Apache License 2.0,4553,mahout-collections-1.0,35,2010-05-12 16:07:37+00:00,2023-07-07 12:49:06+00:00,2023-03-29 21:57:39+00:00,"<!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the ""License""); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

Welcome to Apache Mahout!
===========
The goal of the Apache Mahout™ project is to build an environment for quickly creating scalable, performant machine learning applications.

For additional information about Mahout, visit the [Mahout Home Page](http://mahout.apache.org/)

#### Setting up your Environment
Whether you are using the Mahout- shell, running command line jobs, or using it as a library to build apps, you will need to set-up several environment variables. Edit your environment in `~/.bash_profile` for Mac or `~/.bashrc` for many Linux distributions. Add the following
```
export MAHOUT_HOME=/path/to/mahout
export MAHOUT_LOCAL=true # for running standalone on your dev machine, 
# unset MAHOUT_LOCAL for running on a cluster
```
You will need `$JAVA_HOME`, and if you are running on Spark, you will also need `$SPARK_HOME`.

#### Using Mahout as a Library
Running any application that uses Mahout will require installing a binary or source version and setting the environment.
To compile from source:
* `mvn -DskipTests clean install`
* To run tests do `mvn test`
* To set up your IDE, do `mvn eclipse:eclipse` or `mvn idea:idea`

To use Maven, add the appropriate setting to your pom.xml or build.sbt following the template below.


To use the Samsara environment you'll need to include both the engine neutral math-scala dependency:
```
<dependency>
    <groupId>org.apache.mahout</groupId>
    <artifactId>mahout-math-scala</artifactId>
    <version>${mahout.version}</version>
</dependency>
```
and a dependency for back end engine translation, e.g:
```
<dependency>
    <groupId>org.apache.mahout</groupId>
    <artifactId>mahout-spark</artifactId>
    <version>${mahout.version}</version>
</dependency>
```
#### Building From Source

###### Prerequisites:

Linux Environment (preferably Ubuntu 16.04.x) Note: Currently, only the JVM-only build will work on a Mac.
gcc > 4.x
NVIDIA Card (installed with OpenCL drivers alongside usual GPU drivers)

###### Downloads

Install java 1.7+ in an easily accessible directory (for this example,  ~/java/)
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
    
Create a directory ~/apache/.
    
Download apache Maven 3.3.9 and un-tar/gunzip to ~/apache/apache-maven-3.3.9/ .
https://maven.apache.org/download.cgi
        
Download and un-tar/gunzip Hadoop 2.4.1 to ~/apache/hadoop-2.4.1/ .
https://archive.apache.org/dist/hadoop/common/hadoop-2.4.1/    

Download and un-tar/gunzip spark-1.6.3-bin-hadoop2.4 to  ~/apache/ .
http://spark.apache.org/downloads.html
Choose release: Spark-1.6.3 (Nov 07 2016)
Choose a package type: Pre-Built for Hadoop 2.4

Install ViennaCL 1.7.0+
If running Ubuntu 16.04+

```
sudo apt-get install libviennacl-dev
```

Otherwise if your distribution’s package manager does not have a viennniacl-dev package >1.7.0, clone it directly into the directory which will be included in when  being compiled by Mahout:

```
mkdir ~/tmp
cd ~/tmp && git clone https://github.com/viennacl/viennacl-dev.git
cp -r viennacl/ /usr/local/
cp -r CL/ /usr/local/
```

Ensure that the OpenCL 1.2+ drivers are all installed (packed with most consumer-grade NVIDIA drivers).  Not sure about higher-end cards.

Clone mahout repository into `~/apache`.

```
git clone https://github.com/apache/mahout.git
```

###### Configuration

When building mahout for a spark backend, we need four System Environment variables set:
```
    export MAHOUT_HOME=/home/<user>/apache/mahout
    export HADOOP_HOME=/home/<user>/apache/hadoop-2.4.1
    export SPARK_HOME=/home/<user>/apache/spark-1.6.3-bin-hadoop2.4    
    export JAVA_HOME=/home/<user>/java/jdk-1.8.121
```

Mahout on Spark regularly uses one more env variable, the IP of the Spark clusters' master node (usually, the node hosting the session user).

To use four local cores (Spark master need not be running)
```
export MASTER=local[4]
```
To use all available local cores (again, Spark master need not be running)
```
export MASTER=local[*]
```
To point to a cluster with spark running: 
```
export MASTER=spark://master.ip.address:7077
```

We then add these to the path:

```
   PATH=$PATH$:MAHOUT_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$JAVA_HOME/bin
```

These get appended to the users' ~/.bashrc file.


###### Building Mahout with Apache Maven

Currently, Mahout has three builds.  From the  $MAHOUT_HOME directory, we may issue the commands to build each using mvn profiles.

JVM only:
```
mvn clean install -DskipTests
```

JVM with native OpenMP level 2 and level 3 matrix/vector Multiplication
```
mvn clean install -Pviennacl-omp -Phadoop2 -DskipTests
```
JVM with native OpenMP and OpenCL for Level 2 and level 3 matrix/vector Multiplication.  (GPU errors fall back to OpenMP, and currently, only a single GPU/node is supported).
```
mvn clean install -Pviennacl -Phadoop2 -DskipTests
```

#### Testing the Mahout Environment

Mahout provides an extension to the spark-shell that is good for getting to know the language, testing partition loads, prototyping algorithms, etc.

To launch the shell in local mode with two threads - simply do the following:
```
$ MASTER=local[2] mahout spark-shell
```

After a very verbose startup, a Mahout welcome screen will appear:

```
Loading /home/andy/sandbox/apache-mahout-distribution-0.13.0/bin/load-shell.scala...
import org.apache.mahout.math._
import org.apache.mahout.math.scalabindings._
import org.apache.mahout.math.drm._
import org.apache.mahout.math.scalabindings.RLikeOps._
import org.apache.mahout.math.drm.RLikeDrmOps._
import org.apache.mahout.sparkbindings._
sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = org.apache.mahout.sparkbindings.SparkDistributedContext@3ca1f0a4

                _                 _
_ __ ___   __ _| |__   ___  _   _| |_
 '_ ` _ \ / _` | '_ \ / _ \| | | | __|
 | | | | (_| | | | | (_) | |_| | |_
_| |_| |_|\__,_|_| |_|\___/ \__,_|\__|  version 0.13.0


That file does not exist


scala>
```
At the scala> prompt, enter: 
```   
scala> :load /home/<andy>/apache/mahout/examples
                               /bin/SparseSparseDrmTimer.mscala
```
Which will load a matrix multiplication timer function definition. To run the matrix timer: 
```
        scala> timeSparseDRMMMul(1000,1000,1000,1,.02,1234L)
            {...} res3: Long = 16321
```
Note the 14.1 release is missing a class required for this will be fixed in 14.2. We can see that the JVM only version is slow, thus our motive for GPU and Native Multithreading support.

To understand the processes getting performed under the hood of the timer, we may examine the .mscala (mahout scala) code that is both fully functional scala and the Mahout R-Like DSL for tensor algebra:    
```



def timeSparseDRMMMul(m: Int, n: Int, s: Int, para: Int, pctDense: Double = .20, seed: Long = 1234L): Long = {
  val drmA = drmParallelizeEmpty(m , s, para).mapBlock(){
       case (keys,block:Matrix) =>
           val R =  scala.util.Random
           R.setSeed(seed)
           val blockB = new SparseRowMatrix(block.nrow, block.ncol)
           blockB := {x => if (R.nextDouble < pctDense) R.nextDouble else x }
       (keys -> blockB)
  }

  val drmB = drmParallelizeEmpty(s , n, para).mapBlock(){
       case (keys,block:Matrix) =>
           val R =  scala.util.Random
           R.setSeed(seed + 1)
           val blockB = new SparseRowMatrix(block.nrow, block.ncol)
           blockB := {x => if (R.nextDouble < pctDense) R.nextDouble else x }
       (keys -> blockB)
  }

  var time = System.currentTimeMillis()

  val drmC = drmA %*% drmB
 
  // trigger computation
  drmC.numRows()

  time = System.currentTimeMillis() - time

  time  
 
}
```

For more information, please see the following references:

http://mahout.apache.org/users/environment/in-core-reference.html

http://mahout.apache.org/users/environment/out-of-core-reference.html

http://mahout.apache.org/users/sparkbindings/play-with-shell.html

http://mahout.apache.org/users/environment/classify-a-doc-from-the-shell.html

Note that due to an intermittent out-of-memory bug in a Flink-based test, we have disabled it from the binary releases. To use Flink, please uncomment the line in the root pom.xml in the `<modules>` block, so it reads `<module>flink</module>`.

#### Examples
For examples of how to use Mahout, see the examples directory located in `examples/bin`

For information on how to contribute, visit the [How to Contribute Page](https://mahout.apache.org/developers/how-to-contribute.html)

#### Legal
Please see the `NOTICE.txt` included in this directory for more information.

[![Build Status](https://api.travis-ci.org/apache/mahout.svg?branch=master)](https://travis-ci.org/apache/mahout)
<!--
[![Coverage Status](https://coveralls.io/repos/github/apache/mahout/badge.svg?branch=master)](https://coveralls.io/github/apache/mahout?branch=master)
-->
",2023-07-07 16:01:53+00:00
mara,mara-pipelines,mara/mara-pipelines,"A lightweight opinionated ETL framework, halfway between plain scripts and Apache Airflow",,False,2011,2023-07-06 01:10:27+00:00,2018-03-31 20:37:22+00:00,106,58,13,10,3.4.0,2023-05-01 19:51:54+00:00,MIT License,163,3.4.0,39,2023-05-01 19:51:54+00:00,2023-07-06 01:10:27+00:00,2023-05-01 19:51:54+00:00,"# Mara Pipelines

[![Build & Test](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml/badge.svg)](https://github.com/mara/mara-pipelines/actions/workflows/build.yaml)
[![PyPI - License](https://img.shields.io/pypi/l/mara-pipelines.svg)](https://github.com/mara/mara-pipelines/blob/main/LICENSE)
[![PyPI version](https://badge.fury.io/py/mara-pipelines.svg)](https://badge.fury.io/py/mara-pipelines)
[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://communityinviter.com/apps/mara-users/public-invite)



This package contains a lightweight data transformation framework with a focus on transparency and complexity reduction. It has a number of baked-in assumptions/ principles:

- Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code.

- PostgreSQL as a data processing engine.

- Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines.

- GNU make semantics. Nodes depend on the completion of upstream nodes. No data dependencies or data flows.

- No in-app data processing: command line tools as the main tool for interacting with databases and data.

- Single machine pipeline execution based on Python's [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html). No need for distributed task queues. Easy debugging and output logging.

- Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.

&nbsp;

## Installation

To use the library directly, use pip:

```
pip install mara-pipelines
```

or

```
pip install git+https://github.com/mara/mara-pipelines.git
```

For an example of an integration into a flask application, have a look at the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2).

Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux).

&nbsp;

## Example

Here is a pipeline ""demo"" consisting of three nodes that depend on each other: the task `ping_localhost`, the pipeline `sub_pipeline` and the task `sleep`:

```python
from mara_pipelines.commands.bash import RunBash
from mara_pipelines.pipelines import Pipeline, Task
from mara_pipelines.ui.cli import run_pipeline, run_interactively

pipeline = Pipeline(
    id='demo',
    description='A small pipeline that demonstrates the interplay between pipelines, tasks and commands')

pipeline.add(Task(id='ping_localhost', description='Pings localhost',
                  commands=[RunBash('ping -c 3 localhost')]))

sub_pipeline = Pipeline(id='sub_pipeline', description='Pings a number of hosts')

for host in ['google', 'amazon', 'facebook']:
    sub_pipeline.add(Task(id=f'ping_{host}', description=f'Pings {host}',
                          commands=[RunBash(f'ping -c 3 {host}.com')]))

sub_pipeline.add_dependency('ping_amazon', 'ping_facebook')
sub_pipeline.add(Task(id='ping_foo', description='Pings foo',
                      commands=[RunBash('ping foo')]), ['ping_amazon'])

pipeline.add(sub_pipeline, ['ping_localhost'])

pipeline.add(Task(id='sleep', description='Sleeps for 2 seconds',
                  commands=[RunBash('sleep 2')]), ['sub_pipeline'])
```

Tasks contain lists of commands, which do the actual work (in this case running bash commands that ping various hosts).

&nbsp;

In order to run the pipeline, a PostgreSQL database is recommended to be configured for storing run-time information, run output and status of incremental processing:

```python
import mara_db.auto_migration
import mara_db.config
import mara_db.dbs

mara_db.config.databases \
    = lambda: {'mara': mara_db.dbs.PostgreSQLDB(host='localhost', user='root', database='example_etl_mara')}

mara_db.auto_migration.auto_discover_models_and_migrate()
```

Given that PostgresSQL is running and the credentials work, the output looks like this (a database with a number of tables is created):

```
Created database ""postgresql+psycopg2://root@localhost/example_etl_mara""

CREATE TABLE data_integration_file_dependency (
    node_path TEXT[] NOT NULL,
    dependency_type VARCHAR NOT NULL,
    hash VARCHAR,
    timestamp TIMESTAMP WITHOUT TIME ZONE,
    PRIMARY KEY (node_path, dependency_type)
);

.. more tables
```

### CLI UI

This runs a pipeline with output to stdout:

```python
from mara_pipelines.ui.cli import run_pipeline

run_pipeline(pipeline)
```

![Example run cli 1](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-1.gif)

&nbsp;

And this runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:

```python
run_pipeline(sub_pipeline, nodes=[sub_pipeline.nodes['ping_amazon']], with_upstreams=True)
```

![Example run cli 2](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-2.gif)

&nbsp;


And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:

```python
from mara_pipelines.ui.cli import run_interactively

run_interactively()
```

![Example run cli 3](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-cli-3.gif)



### Web UI

More importantly, this package provides an extensive web interface. It can be easily integrated into any [Flask](https://flask.palletsprojects.com/) based app and the [mara example project](https://github.com/mara/mara-example-project) demonstrates how to do this using [mara-app](https://github.com/mara/mara-app).

For each pipeline, there is a page that shows

- a graph of all child nodes and the dependencies between them
- a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)
- a table of all the pipeline's nodes with their average run times and the resulting queuing priority
- output and timeline for the last runs of the pipeline


![Mara pipelines web ui 1](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/mara-pipelines-web-ui-1.png)

For each task, there is a page showing

- the upstreams and downstreams of the task in the pipeline
- the run times of the task in the last 30 days
- all commands of the task
- output of the last runs of the task

![Mara pipelines web ui 2](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/mara-pipelines-web-ui-2.png)


Pipelines and tasks can be run from the web ui directly, which is probably one of the main features of this package:

![Example run web ui](https://github.com/mara/mara-pipelines/raw/3.2.x/docs/_static/example-run-web-ui.gif)

&nbsp;

## Getting started

Documentation is currently work in progress. Please use the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2) as a reference for getting started.

## Links

* Documentation: https://mara-pipelines.readthedocs.io/
* Changes: https://mara-pipelines.readthedocs.io/en/latest/changes.html
* PyPI Releases: https://pypi.org/project/mara-pipelines/
* Source Code: https://github.com/mara/mara-pipelines
* Issue Tracker: https://github.com/mara/mara-pipelines/issues
",2023-07-07 16:01:58+00:00
mario,mario,intentmedia/mario,"Functional, Typesafe, Declarative Data Pipelines",,False,139,2023-02-19 16:14:28+00:00,2015-06-05 19:13:14+00:00,18,90,2,0,,,MIT License,5,,0,,,2015-08-14 18:09:36+00:00,"# Mario

![image](http://rocketdock.com/images/screenshots/thumbnails/Mario_pipe_trash_f.png)

Mario is a Scala library focused on defining complex data pipelines in a functional, typesafe, and efficient way.  See the [launch blog post](http://intentmedia.com/one-up-building-machine-learning-pipelines-with-mario/) for more details on the motivation behind the library.

## ![image](http://icons.iconarchive.com/icons/ph03nyx/super-mario/32/Retro-Block-Question-icon.png) Defining  pipelines

Pipelines are very easy to build, using only the `pipe` function. You can construct pipelines with and without depedencies.  Pipelines can be non-linear, but must be acyclic.  The lack of cycles is enforced by the library, so it is impossible to define a cyclic dependency in Mario.

Execution of pipelines is done concurrently, guaranteeing that each step is executed just once.

## ![image](http://icons.iconarchive.com/icons/ph03nyx/super-mario/32/Retro-Flower-Fire-icon.png) Usage

### Import

```scala
import com.intentmedia.mario.Pipeline._
```

### Example

Here is a simple 3 step pipeline:

```scala
// independent step
val step1 = pipe(0 until 100)
// unary dependent step
val step2 = pipe((a: Range) => a.size until 200, step1)
// binary dependent step
val step3 = pipe((a: Range, b: Range) => a ++ b, step1, step2)

step3.run().size
// 200 (step1 will be only executed once)
```

Independent pipelines can be executed using `runWith`:

```scala
val result = step3.runWith(pipe(println(""foo"")), pipe(println(""bar"")))
result.run().size
// 200 (will also print ""foo"" and ""bar"")
```

Pipelines can be composed using for comprehensions:

```scala
for {
  step1 <- pipe(0 until 100)
  step2 <- pipe((a: Range) => a.size until 200, step1)
  step3 <- pipe((a: Range, b: Range) => a ++ b, step1, step2)
} yield step3.run().size
```

## ![image](http://icons.iconarchive.com/icons/ph03nyx/super-mario/32/Retro-Mushroom-1UP-icon.png) Installation
Add the following to your sbt build:

```scala
libraryDependencies += ""com.intentmedia.mario"" %% ""mario"" % ""0.1.0""
```

## ![image](http://icons.iconarchive.com/icons/ph03nyx/super-mario/32/Retro-Fire-Ball-icon.png) Roadmap
* Fault tolerance
* Implicit caching (in Spark)
* Web UI
",2023-07-07 16:02:02+00:00
marquez,marquez,MarquezProject/marquez,"Collect, aggregate, and visualize a data ecosystem's metadata",https://marquezproject.ai,False,1376,2023-07-05 18:36:48+00:00,2018-07-05 22:43:20+00:00,236,46,73,40,0.36.0,2023-06-27 13:46:11+00:00,Apache License 2.0,2507,marquez-airflow-0.3.9,76,2020-12-17 07:37:43+00:00,2023-07-07 14:00:30+00:00,2023-07-07 13:59:09+00:00,"<div align=""center"">
  <img src=""./docs/assets/images/marquez-logo.png"" width=""500px"" />
  <a href=""https://lfaidata.foundation/projects"">
    <img src=""./docs/assets/images/lfaidata-project-badge-incubation-black.png"" width=""125px"" />
  </a>
</div>

Marquez is an open source **metadata service** for the **collection**, **aggregation**, and **visualization** of a data ecosystem's metadata. It maintains the provenance of how datasets are consumed and produced, provides global visibility into job runtime and frequency of dataset access, centralization of dataset lifecycle management, and much more. Marquez was released and open sourced by [WeWork](https://www.wework.com).

## Badges

[![CircleCI](https://circleci.com/gh/MarquezProject/marquez/tree/main.svg?style=shield)](https://circleci.com/gh/MarquezProject/marquez/tree/main)
[![codecov](https://codecov.io/gh/MarquezProject/marquez/branch/main/graph/badge.svg)](https://codecov.io/gh/MarquezProject/marquez/branch/main)
[![status](https://img.shields.io/badge/status-active-brightgreen.svg)](#status)
[![Slack](https://img.shields.io/badge/slack-chat-blue.svg)](http://bit.ly/MqzSlack)
[![license](https://img.shields.io/badge/license-Apache_2.0-blue.svg)](https://raw.githubusercontent.com/MarquezProject/marquez/main/LICENSE)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)
[![maven](https://img.shields.io/maven-central/v/io.github.marquezproject/marquez-api.svg)](https://search.maven.org/search?q=g:io.github.marquezproject)
[![docker](https://img.shields.io/badge/docker-hub-blue.svg?style=flat)](https://hub.docker.com/r/marquezproject/marquez)
[![Known Vulnerabilities](https://snyk.io/test/github/MarquezProject/marquez/badge.svg)](https://snyk.io/test/github/MarquezProject/marquez)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5160/badge)](https://bestpractices.coreinfrastructure.org/projects/5160)

## Status

Marquez is an [LF AI & Data Foundation](https://lfaidata.foundation/projects/marquez) incubation project under active development, and we'd love your help!

## Adopters

Want to be added? Send a pull request our way!

* [Astronomer](https://astronomer.io)
* [Datakin](https://datakin.com)
* [Northwestern Mutual](https://www.northwesternmutual.com)

## Try it!

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/MarquezProject/marquez)

## Quickstart

Marquez provides a simple way to collect and view _dataset_, _job_, and _run_ metadata using [OpenLineage](https://openlineage.io). The easiest way to get up and running is with Docker. From the base of the Marquez repository, run:

```
$ ./docker/up.sh
```

> **Tip:** Use the `--build` flag to build images from source, and/or `--seed` to start Marquez with sample lineage metadata. For a more complete example using the sample metadata, please follow our [quickstart](https://marquezproject.github.io/marquez/quickstart.html) guide.

> **Note:** Port 5000 is now reserved for MacOS. If running locally on MacOS, you can run `./docker/up.sh --api-port 9000` to configure the API to listen on port 9000 instead. Keep in mind that you will need to update the URLs below with the appropriate port number.

**`WEB UI`**

You can open [http://localhost:3000](http://localhost:3000) to begin exploring the Marquez Web UI. The UI enables you to discover dependencies between jobs and the datasets they produce and consume via the lineage graph, view run metadata of current and previous job runs, and much more!

<p align=""center"">
  <img src=""./web/docs/demo.gif"">
</p>

**`HTTP API`**

The Marquez [HTTP API](https://marquezproject.github.io/marquez/openapi.html) listens on port `5000` for all calls and port `5001` for the admin interface. The admin interface exposes helpful endpoints like `/healthcheck` and `/metrics`. To verify the HTTP API server is running and listening on `localhost`, browse to [http://localhost:5001](http://localhost:5001). To begin collecting lineage metadata as OpenLineage events, use the [LineageAPI](https://marquezproject.github.io/marquez/openapi.html#tag/Lineage/paths/~1lineage/post) or an OpenLineage [integration](https://openlineage.io/docs/integrations/about).

> **Note:** By default, the HTTP API does not require any form of authentication or authorization.

**`GRAPHQL`**

To explore metadata via graphql, browse to [http://localhost:5000/graphql-playground](http://localhost:5000/graphql-playground). The graphql endpoint is currently in _beta_ and is located at [http://localhost:5000/api/v1-beta/graphql](http://localhost:5000/api/v1-beta/graphql).

## Documentation

We invite everyone to help us improve and keep documentation up to date. Documentation is maintained in this repository and can be found under [`docs/`](https://github.com/MarquezProject/marquez/tree/main/docs).

> **Note:** To begin collecting metadata with Marquez, follow our [quickstart](https://marquezproject.github.io/marquez/quickstart.html) guide. Below you will find the steps to get up and running from source.

## Versions and OpenLineage Compatibility

Versions of Marquez are compatible with OpenLineage unless noted otherwise. We ensure backward compatibility with a newer version of Marquez by recording events with an older OpenLineage specification version. **We strongly recommend understanding how the OpenLineage specification is** [versioned](https://github.com/OpenLineage/OpenLineage/blob/main/spec/Versioning.md) **and published**.

| **Marquez**                                                                                      | **OpenLineage**                                               | **Status**    |
|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------|---------------|
| [`UNRELEASED`](https://github.com/MarquezProject/marquez/blob/main/CHANGELOG.md#unreleased)      | [`1-0-5`](https://openlineage.io/spec/1-0-5/OpenLineage.json) | `CURRENT`     |
| [`0.36.0`](https://github.com/MarquezProject/marquez/blob/0.36.0/CHANGELOG.md#0350---2023-06-27) | [`1-0-5`](https://openlineage.io/spec/1-0-5/OpenLineage.json) | `RECOMMENDED` |
| [`0.35.0`](https://github.com/MarquezProject/marquez/blob/0.35.0/CHANGELOG.md#0340---2023-06-13) | [`1-0-5`](https://openlineage.io/spec/1-0-0/OpenLineage.json) | `MAINTENANCE` |

> **Note:** The [`openlineage-python`](https://pypi.org/project/openlineage-python) and [`openlineage-java`](https://central.sonatype.com/artifact/io.openlineage/openlineage-java) libraries will a higher version than the OpenLineage [specification](https://github.com/OpenLineage/OpenLineage/tree/main/spec) as they have different version requirements.

We currently maintain three categories of compatibility: `CURRENT`, `RECOMMENDED`, and `MAINTENANCE`. When a new version of Marquez is released, it's marked as `RECOMMENDED`, while the previous version enters `MAINTENANCE` mode (which gets bug fixes whenever possible). The unreleased version of Marquez is marked `CURRENT` and does not come with any guarantees, but is assumed to remain compatible with OpenLineage, although surprises happen and there maybe rare exceptions.

## Modules

Marquez uses a _multi_-project structure and contains the following modules:

* [`api`](https://github.com/MarquezProject/marquez/tree/main/api): core API used to collect metadata
* [`web`](https://github.com/MarquezProject/marquez/tree/main/web): web UI used to view metadata
* [`clients`](https://github.com/MarquezProject/marquez/tree/main/clients): clients that implement the HTTP [API](https://marquezproject.github.io/marquez/openapi.html)
* [`chart`](https://github.com/MarquezProject/marquez/tree/main/chart): helm chart

> **Note:** The `integrations` module was removed in [`0.21.0`](https://github.com/MarquezProject/marquez/blob/main/CHANGELOG.md#removed), so please use an OpenLineage [integration](https://openlineage.io/integration) to collect lineage events easily.

## Requirements

* [Java 17](https://adoptium.net)
* [PostgreSQL 12.1](https://www.postgresql.org/download)

> **Note:** To connect to your running PostgreSQL instance, you will need the standard [`psql`](https://www.postgresql.org/docs/9.6/app-psql.html) tool.

## Building

To build the entire project run:

```bash
./gradlew build
```

The executable can be found under `api/build/libs/`

## Configuration

To run Marquez, you will have to define `marquez.yml`. The configuration file is passed to the application and used to specify your database connection. The configuration file creation steps are outlined below.

### Step 1: Create Database

When creating your database using [`createdb`](https://www.postgresql.org/docs/12/app-createdb.html), we recommend calling it `marquez`:

```bash
$ createdb marquez
```

### Step 2: Create `marquez.yml`

With your database created, you can now copy [`marquez.example.yml`](https://github.com/MarquezProject/marquez/blob/main/marquez.example.yml):

```
$ cp marquez.example.yml marquez.yml
```

You will then need to set the following environment variables (we recommend adding them to your `.bashrc`): `POSTGRES_DB`, `POSTGRES_USER`, and `POSTGRES_PASSWORD`. The environment variables override the equivalent option in the configuration file.

By default, Marquez uses the following ports:

* TCP port `8080` is available for the HTTP API server.
* TCP port `8081` is available for the admin interface.

> **Note:** All of the configuration settings in `marquez.yml` can be specified either in the configuration file or in an environment variable.

## Running the [HTTP API](https://github.com/MarquezProject/marquez/blob/main/src/main/java/marquez/MarquezApp.java) Server

```bash
$ ./gradlew :api:runShadow
```
Marquez listens on port `8080` for all API calls and port `8081` for the admin interface. To verify the HTTP API server is running and listening on `localhost`, browse to [http://localhost:8081](http://localhost:8081). We encourage you to familiarize yourself with the [data model](https://marquezproject.github.io/marquez/#data-model) and [APIs](https://marquezproject.github.io/marquez/openapi.html) of Marquez. To run the web UI, please follow the steps outlined [here](https://github.com/MarquezProject/marquez/tree/main/web#development).

> **Note:** By default, the HTTP API does not require any form of authentication or authorization.

## Related Projects

* [`OpenLineage`](https://github.com/OpenLineage/OpenLineage): an open standard for metadata and lineage collection

## Getting Involved

* Website: https://marquezproject.ai
* Source: https://github.com/MarquezProject/marquez
* Chat: [MarquezProject Slack](https://bit.ly/MqzSlackInvite)
* Twitter: [@MarquezProject](https://twitter.com/MarquezProject)

## Contributing

See [CONTRIBUTING.md](https://github.com/MarquezProject/marquez/blob/main/CONTRIBUTING.md) for more details about how to contribute.

## Reporting a Vulnerability

If you discover a vulnerability in the project, please open an issue and attach the ""security"" label.

----
SPDX-License-Identifier: Apache-2.0
Copyright 2018-2023 contributors to the Marquez project.
",2023-07-07 16:02:05+00:00
martian,martian,martian-lang/martian,Language and framework for high performance computational pipelines.,http://martian-lang.org,False,134,2023-06-13 03:57:28+00:00,2017-06-20 22:14:53+00:00,18,12,23,34,v4.0.10,2022-11-15 23:29:03+00:00,MIT License,1935,v4.0.10,40,2022-11-15 23:27:22+00:00,2023-07-04 19:42:57+00:00,2023-05-23 19:09:06+00:00,"<p align=""center"">
  <a href=""http://martian-lang.org"">
    <img src=""https://avatars0.githubusercontent.com/u/16513506?v=4&s=200"">
  </a>
  <p align=""center"">
    Language and framework for developing high performance computational pipelines.
  </p>
</p>

[![GoDoc](https://godoc.org/github.com/martian-lang/martian?status.svg)](https://godoc.org/github.com/martian-lang/martian)
[![Build Status](https://github.com/martian-lang/martian/actions/workflows/test.yml/badge.svg)](https://github.com/martian-lang/martian/actions/workflows/test.yml)

## Getting Started

Please see the [Martian Documentation](http://martian-lang.org).

The easiest way to get started is

```sh
$ git clone https://github.com/martian-lang/martian.git
$ cd martian
$ make
```

Alternatively, build with [bazel](https://bazel.build):

```sh
$ bazel build //:mrp
```

Note that while `go get` will not work very well for this repository, because it
will skip fetching the web UI and runtime configuration files, and because this
repository had already tagged version 3 by the time go modules came around, and
is thus not following the expected go conventions for how code is organized for
non-v1 versions.

### Note on semantic versioning

Semantic versioning for martian is based on pipeline compatibility, not the Go
API.  That is, a major version change indicates that pipelines (defined in
`.mro` files) may no longer function correctly.  This unfortunately poses problems
with go modules, (which didn't exist yet at the time v3 was first tagged) which
exepect version tags to be referring to the semver compatibility of the Go API.
We hope to rectify this in v5, but ironically that will force an
backwards-incompatible change to the Go API.  In the mean time, if you wish to
depend on martian as a go module, use the git commit rather than a version tag.

## Copyright and License

Code and documentation copyright 2014-2017 the [Martian Authors](https://github.com/martian-lang/martian/graphs/contributors) and [10x Genomics, Inc.](https://10xgenomics.com) Code released under the [MIT License](https://github.com/martian-lang/martian/blob/master/LICENSE). Documentation released under [Creative Commons](https://github.com/martian-lang/martian-docs/blob/master/LICENSE).
",2023-07-07 16:02:10+00:00
mathesar,mathesar,centerofci/mathesar,Web application providing an intuitive user experience to databases.,https://mathesar.org/,False,1760,2023-07-07 12:45:58+00:00,2021-03-17 17:04:53+00:00,233,25,80,3,0.1.2,2023-06-13 17:00:48+00:00,GNU General Public License v3.0,10914,0.1.2,3,2023-06-13 17:00:48+00:00,2023-07-07 15:04:58+00:00,2023-07-07 13:18:17+00:00,"<p align=""center"">
    <img src=""https://user-images.githubusercontent.com/845767/218793207-a84a8c9e-d147-40a8-839b-f2b5d8b1ccba.png"" width=450px alt=""Mathesar logo""/>
</p>
<p align=""center""><b>An intuitive UI for managing data, for users of all technical skill levels. Built on Postgres.</b></p>
<p align=""center"">
    <img alt=""License"" src=""https://img.shields.io/github/license/centerofci/mathesar"">
    <img alt=""GitHub closed issues"" src=""https://img.shields.io/github/issues-closed/centerofci/mathesar"">
    <img alt=""GitHub commit activity"" src=""https://img.shields.io/github/commit-activity/w/centerofci/mathesar"">
    <img alt=""Codecov"" src=""https://img.shields.io/codecov/c/github/centerofci/mathesar"">
</p>

<p align=""center"">
  <a href=""https://mathesar.org?ref=github-readme"" target=""_blank"">Website</a> • <a href=""https://docs.mathesar.org?ref=github-readme"" target=""_blank"">Docs</a> • <a href=""https://demo.mathesar.org?ref=github-readme"" target=""_blank"">Live Demo</a> • <a href=""https://wiki.mathesar.org/en/community/matrix"" target=""_blank"">Matrix (chat)</a> • <a href=""https://discord.gg/enaKqGn5xx"" target=""_blank"">Discord</a> • <a href=""https://wiki.mathesar.org/"" target=""_blank"">Wiki</a>
</p>


# Mathesar

Mathesar is a straightforward open source tool that provides a **spreadsheet-like interface** to a PostgreSQL **database**. Our web-based interface helps you and your collaborators work with data more independently and comfortably – **no technical skills needed**.

You can use Mathesar to build **data models**, **enter data**, and even **build reports**. You host your own Mathesar installation, which gives you ownership, privacy, and control of your data.

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**

- [Sponsors](#sponsors)
- [Status](#status)
- [Join our community!](#join-our-community)
- [Screenshots](#screenshots)
- [Live Demo](#live-demo)
- [Features](#features)
- [Self-hosting](#self-hosting)
- [Our motivation](#our-motivation)
- [Contributing](#contributing)
- [License](#license)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## Sponsors
Our top sponsors! Become a sponsor on [GitHub](https://github.com/sponsors/centerofci) or [Open Collective](https://opencollective.com/mathesar).

<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%"">
          <a href=""https://www.thingylabs.io/"">
              <img src=""https://user-images.githubusercontent.com/287034/226116547-cd28e16a-4c89-4a01-bc98-5a19b02ab1b2.png"" width=""100px;"" alt=""Thingylabs GmbH""/>
              <br />
              <sub><b>Thingylabs GmbH</b></sub>
          </a>
          <br />
      </td>
    </tr>
  </tbody>
</table>

## Status
- [x] **Public Alpha**: You can install and deploy Mathesar on your server. Go easy on us!
- [ ] **Public Beta**: Stable and feature-rich enough to implement in production
- [ ] **Public**: Production-ready

We are currently in the **public alpha** stage.

## Join our community!
The Mathesar team is on [Matrix](https://wiki.mathesar.org/en/community/matrix) (chat service). We also have [mailing lists](https://wiki.mathesar.org/en/community/mailing-lists) and the core team discusses day-to-day work on our developer mailing list. 

## Screenshots

![crm-table](https://user-images.githubusercontent.com/287034/220773466-1790a826-923e-47a8-8f7e-1edb67970a16.png)

![authors-filter](https://user-images.githubusercontent.com/287034/220773378-78e05984-5f0f-4ed2-9682-b75ca0f6867c.png)

![talks-with-topics](https://user-images.githubusercontent.com/287034/220773633-0a4ff810-a1e1-476f-b5b0-2667ba97f07a.png)

![author-record](https://user-images.githubusercontent.com/287034/220773738-a3fd0dda-cf16-45ed-a8ef-4e40647bb074.png)

![arxiv-schema](https://user-images.githubusercontent.com/287034/220773323-bd6ffb31-835b-4df5-981e-dae6341d42bb.png)

![db-page](https://user-images.githubusercontent.com/287034/220773522-8c1c1483-2389-4f5e-83b2-e54836983035.png)  

## Live Demo
Check out a [live demo of Mathesar here](https://demo.mathesar.org)!

## Features
- **Built on Postgres**: Connect to an existing Postgres database or set one up from scratch.
- **Set up your data models**: Easily create and update Postgres schemas and tables.
- **Data entry**: Use our spreadsheet-like interface to view, create, update, and delete table records.
- **Filter, sort, and group**: Quickly slice your data in different ways.
- **Query builder**: Use our Data Explorer to build queries without knowing anything about SQL or joins.
- **Schema migrations**: Transfer columns between tables in two clicks.
- **Uses Postgres features**: Mathesar uses and manipulates Postgres schemas, primary keys, foreign keys, constraints and data types. e.g. ""Links"" in the UI are foreign keys in the database.
- **Custom data types**: Custom data types for emails and URLs (more coming soon), validated at the database level.
- **Basic access control**: Users can have Viewer (read-only), Editor (can only edit data, but not data structure), or Manager (can edit both data and its structure) roles.

## Self-hosting
Please see [our documentation](https://docs.mathesar.org/) for instructions on installing Mathesar on your own server.

## Our motivation
Mathesar is a non-profit project. Our goal is to make understanding and working with data easy for everyone.

Databases have been around for a long time and solve common data problems really well. But working with databases often requires custom software. Or complex tooling that people struggle to get their heads around.

We want to make existing database functionality more accessible, for users of all technical skill levels.

## Contributing
We actively encourage contribution! Get started by reading our [Contributor Guide](./CONTRIBUTING.md).

## License
Mathesar is open source under the GPLv3 license - see [LICENSE](LICENSE). It also contains derivatives of third-party open source modules licensed under the MIT license. See the list and respective licenses in [THIRDPARTY](THIRDPARTY).
",2023-07-07 16:02:14+00:00
mdstudio,MDStudio,MD-Studio/MDStudio,A general framework for microservice based distributed applications,,False,12,2023-01-27 11:09:41+00:00,2016-05-09 06:34:57+00:00,0,5,5,2,,,Apache License 2.0,1610,1.0.0-alpha1,2,2018-09-26 12:33:09+00:00,,2019-12-20 09:30:55+00:00,"# MDStudio
[![Build Status](https://travis-ci.org/MD-Studio/MDStudio.svg?branch=master)](https://travis-ci.org/MD-Studio/MDStudio)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/aff6245274f44a7991a3a25976ad6472)](https://www.codacy.com/app/tifonzafel/MDStudio?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=MD-Studio/MDStudio&amp;utm_campaign=Badge_Grade)
[![codecov](https://codecov.io/gh/MD-Studio/MDStudio/branch/master/graph/badge.svg)](https://codecov.io/gh/MD-Studio/MDStudio)

![Configuration settings](docs/img/mdstudio-logo.png)

MDStudio is a stand-alone framework to build distributed applications using a broker-based ([Crossbar](https://crossbar.io)) microservice concept.
The only requirement for any piece of software or script to become a microservice is a formal definition of the functional endpoints that expose
the functionality the software has to offer. In MDStudio, this definition is a very thin layer available in nearly 12 different [languages](https://crossbar.io/about/Supported-Languages/)
in which communication is network based using JSON and the input/output of the endpoints is fully described using JSON schemas.



## Installation

To make development and deployment easier we have setup a docker environment hosting the microservice broker and support microservices in a self
consistent environment.

> **NOTE**: some of the Python dependencies of MDStudio are version fixed. To prevent accidental updates that may prevent MDStudio
  from working we advise to install MDStudio and it's dependencies in a virtual environment such as `virtual env` or Anaconda/miniconda
  virtual environment.

### Quick-start docker based installation

The only requirement MDStudio has is [Docker](https://www.docker.com/).
Next, run the builder and start the microservice docker environment under bash as:

	Clone the repository using the devel branch
	>> git clone -b devel git@github.com:MD-Studio/MDStudio.git
	Install the dependencies from the MDStudio folder
	>> pip install -r requirements-dev.txt
	Install mdstudio
	>> pip install -e mdstudio
    >> ./build.sh
    >> ./standalone.sh

The docker containers only need to be build once and for subsequent usage running
standalone.sh is sufficient.

### Manual (non-docker) based installation

The docker based environment is the most convenient way for developing microservices and deploying distributed applications.
If you are unable to use Docker or want to keep track of the whole environment for yourself, you should follow these installation
instructions.

The MDStudio application is written in Python and mostly self contained thanks to the use of an in-application Python virtual environment.
The application has currently been successfully tested with Python versions: 2.7

The only external dependencies are:

 * [MongoDB](https://www.mongodb.com) - A NoSQL database.
 * [Pipenv](https://github.com/kennethreitz/pipenv) - A python virtual environment manager.
 * [Redis](https://redis.io/) - A fast caching layer.

Install the virtual environment with:

    >> pipenv install --skip-lock --dev

The application is started on the command line as:

    >> pipenv shell
    >> python .

#### PyCharm IDE Integration
Go to `File > Project Settings > Project Interpreter`, and add a remote interpreter,
and make sure it matches this screen.

![Configuration settings](docs/img/pycharm-config.png)

Note specifically:

|                      |                                                            |
|----------------------|------------------------------------------------------------|
| Interpreter path     | `/root/mdstudio/app-4PlAip0Q/bin/python`   |


## Creating microservices

Now that you have successfully installed the MDStudio framework you can start having fun by writing some
microservices and using their endpoints in any number of different ways as briefly described in the intro.

For guidance and inspiration on how to write and use microservices, please have a look at the [MDStudio examples](https://github.com/MD-Studio/MDStudio_examples)
repository on GitHub.
",2023-07-07 16:02:19+00:00
merlin,merlin,LLNL/merlin,Machine Learning for HPC Workflows,,False,94,2023-06-05 13:20:51+00:00,2019-11-19 23:31:13+00:00,20,13,11,30,1.10.1,2023-05-04 21:49:23+00:00,MIT License,876,1.10.1,30,2023-05-04 21:49:23+00:00,2023-06-29 22:46:49+00:00,2023-06-28 17:18:25+00:00,"![Python versions](https://img.shields.io/pypi/pyversions/merlin)
[![License](https://img.shields.io/pypi/l/merlin)](https://pypi.org/project/merlin/)
![Activity](https://img.shields.io/github/commit-activity/m/LLNL/merlin)
[![Issues](https://img.shields.io/github/issues/LLNL/merlin)](https://github.com/LLNL/merlin/issues)
[![Pull requests](https://img.shields.io/github/issues-pr/LLNL/merlin)](https://github.com/LLNL/merlin/pulls)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/LLNL/merlin.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/LLNL/merlin/context:python)

![Merlin](https://raw.githubusercontent.com/LLNL/merlin/main/docs/images/merlin.png)

## A brief introduction to Merlin
Merlin is a tool for running machine learning based workflows. The goal of
Merlin is to make it easy to build, run, and process the kinds of large
scale HPC workflows needed for cognitive simulation.

At its heart, Merlin is a distributed task queuing system, designed to allow complex
HPC workflows to scale to large numbers of simulations 
(we've done 100 Million on the Sierra Supercomputer).

Why would you want to run that many simulations?
To become your own Big Data generator.

Data sets of this size can be large enough to train deep neural networks
that can mimic your HPC application, to be used for such
things as design optimization, uncertainty quantification and statistical
experimental inference. Merlin's been used to study inertial confinement
fusion, extreme ultraviolet light generation, structural mechanics and
atomic physics, to name a few.

How does it work?

In essence, Merlin coordinates complex workflows through a persistent
external queue server that lives outside of your HPC systems, but that
can talk to nodes on your cluster(s). As jobs spin up across your ecosystem,
workers on those allocations pull work from a central server, which
coordinates the task dependencies for your workflow. Since this coordination
is done via direct connections to the workers (i.e. not through a file
system), your workflow can scale to very large numbers of workers,
which means a very large number of simulations with very little overhead.

Furthermore, since the workers pull their instructions from the central
server, you can do a lot of other neat things, like having multiple
batch allocations contribute to the same work (think surge computing), or
specialize workers to different machines (think CPU workers for your
application and GPU workers that train your neural network). Another
neat feature is that these workers can add more work back to central
server, which enables a variety of dynamic workflows, such as may be
necessary for the intelligent sampling of design spaces or reinforcement
learning tasks.

Merlin does all of this by leveraging some key HPC and cloud computing
technologies, building off open source components. It uses
[maestro]( https://github.com/LLNL/maestrowf) to
provide an interface for describing workflows, as well as for defining
workflow task dependencies. It translates those dependencies into concrete
tasks via [celery](https://docs.celeryproject.org/), 
which can be configured for a variety of backend
technologies ([rabbitmq](https://www.rabbitmq.com) and
[redis](https://redis.io) are currently supported). Although not
a hard dependency, we encourage the use of
[flux](http://flux-framework.org) for interfacing with
HPC batch systems, since it can scale to a very large number of jobs.

The integrated system looks a little something like this:

<img src=""docs/images/merlin_arch.png"" alt=""a typical Merlin workflow"">

In this example, here's how it all works:

1. The scientist describes her HPC workflow as a maestro DAG (directed acyclic graph)
""spec"" file `workflow.yaml`
2. She then sends it to the persistent server with  `merlin run workflow.yaml` .
Merlin translates the file into tasks.
3. The scientist submits a job request to her HPC center. These jobs ask for workers via
the command `merlin run-workers workflow.yaml`.
4. Coffee break.
5. As jobs stand up, they pull work from the queue, making calls to flux to get the 
necessary HPC resources.
5. Later, workers on a different allocation, with GPU resources connect to the 
server and contribute to processing the workload.

The central queue server deals with task dependencies and keeps the workers fed.

For more details, check out the rest of the [documentation](https://merlin.readthedocs.io/).

Need help? <merlin@llnl.gov>

## Quick Start

Note: Merlin supports Python 3.6+.

To install Merlin and its dependencies, run:

    $ pip3 install merlin
    
Create your application config file:

    $ merlin config

That's it.

To run something a little more like what you're interested in,
namely a demo workflow that has simulation and machine learning,
first generate an example workflow:

    $ merlin example feature_demo

Then install the workflow's dependencies:

    $ pip install -r feature_demo/requirements.txt

Then process the workflow and create tasks on the server:

    $ merlin run feature_demo/feature_demo.yaml

And finally, launch workers that can process those tasks:

    $ merlin run-workers feature_demo/feature_demo.yaml


## Documentation
[**Full documentation**](http://merlin.readthedocs.io/) is available, or
run:

    $ merlin --help

(or add `--help` to the end of any sub-command you
want to learn more about.)


## Code of Conduct
Please note that Merlin has a
[**Code of Conduct**](.github/CODE_OF_CONDUCT.md). By participating in
the Merlin community, you agree to abide by its rules.


## License
Merlin is distributed under the terms of the [MIT LICENSE](https://github.com/LLNL/merlin/blob/main/LICENSE).

LLNL-CODE-797170
",2023-07-07 16:02:23+00:00
meshroom,Meshroom,alicevision/Meshroom,3D Reconstruction Software,http://alicevision.org,False,9490,2023-07-06 21:48:07+00:00,2015-04-22 17:33:16+00:00,992,281,48,7,v2023.2.0,2023-06-26 16:25:42+00:00,Other,3254,v2023.2.0,8,2023-06-26 16:25:42+00:00,2023-07-07 15:43:06+00:00,2023-07-07 15:43:03+00:00,"# ![Meshroom - 3D Reconstruction Software](/docs/logo/banner-meshroom.png)

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2997/badge)](https://bestpractices.coreinfrastructure.org/projects/2997)

Meshroom is a free, open-source 3D Reconstruction Software based on the [AliceVision](https://github.com/alicevision/AliceVision) Photogrammetric Computer Vision framework.

Learn more details about the pipeline on [AliceVision website](http://alicevision.github.io).

See [results of the pipeline on sketchfab](http://sketchfab.com/AliceVision).

Continuous integration:
* Windows: [![Build status](https://ci.appveyor.com/api/projects/status/25sd7lfr3v0rnvni/branch/develop?svg=true)](https://ci.appveyor.com/project/AliceVision/meshroom/branch/develop)
* Linux: [![Build Status](https://travis-ci.org/alicevision/meshroom.svg?branch=develop)](https://travis-ci.org/alicevision/meshroom)


## Photogrammetry

Photogrammetry is the science of making measurements from photographs.
It infers the geometry of a scene from a set of unordered photographs or videos.
Photography is the projection of a 3D scene onto a 2D plane, losing depth information.
The goal of photogrammetry is to reverse this process.

See the [presentation of the pipeline steps](http://alicevision.github.io/#photogrammetry).


## Manual

https://meshroom-manual.readthedocs.io


## Tutorials

* [Meshroom: Open Source 3D Reconstruction Software](https://www.youtube.com/watch?v=v_O6tYKQEBA) by [Mikros Image](http://www.mikrosimage.com)

Overall presentation of the Meshroom software.

* [Meshroom Tutorial on Sketchfab](https://sketchfab.com/blogs/community/tutorial-meshroom-for-beginners) by [Mikros Image](http://www.mikrosimage.com)

Detailed tutorial with a focus on the features of the 2019.1 release.

* [Photogrammetry 2 – 3D scanning with just PHONE/CAMERA simpler, better than ever!](https://www.youtube.com/watch?v=1D0EhSi-vvc) by [Prusa 3D Printer](https://blog.prusaprinters.org)

Overall presentation of the photogrammetry practice with Meshroom.

* [How to 3D Photoscan Easy and Free! by ](https://www.youtube.com/watch?v=k4NTf0hMjtY) by [CG Geek](https://www.youtube.com/channel/UCG8AxMVa6eutIGxrdnDxWpQ)

Overall presentation of the protogrammetry practice with Meshroom and detailed presentation how to do the retolopogy in Blender.

* [Meshroom Survival Guide](https://www.youtube.com/watch?v=eiEaHLNJJ94) by [Moviola](https://moviola.com)

Presentation of the Meshroom software with a focus on using it for Match Moving.


## Customization

### Custom Pipelines

You can create custom pipelines in the user interface and save it as template: `File > Advanced > Save As Template`.
You can define the `MESHROOM_PIPELINE_TEMPLATES_PATH` environment variable to specific folders to make these pipelines available in Meshroom.
In a standard precompiled version of Meshroom, you can also directly add custom pipelines in `lib/meshroom/pipelines`.

### Custom Nodes

You can create custom nodes in python and make them available in Meshroom using the `MESHROOM_NODES_PATH` environment variable.
[Here is an example](meshroom/nodes/blender/ScenePreview.py) to launch a Blender rendering from Meshroom.
In a standard precompiled version of Meshroom, you can also directly add custom nodes in `lib/meshroom/nodes`.
To be recognized by Meshroom, a custom folder with nodes should be a Python module (an `__init__.py` file is needed).


## License

The project is released under MPLv2, see [**COPYING.md**](COPYING.md).


## Citation

If you use this project for a publication, please cite the [paper](https://hal.archives-ouvertes.fr/hal-03351139):
  ```
  @inproceedings{alicevision2021,
    title={{A}liceVision {M}eshroom: An open-source {3D} reconstruction pipeline},
    author={Carsten Griwodz and Simone Gasparini and Lilian Calvet and Pierre Gurdjos and Fabien Castan and Benoit Maujean and Gregoire De Lillo and Yann Lanthony},
    booktitle={Proceedings of the 12th ACM Multimedia Systems Conference - {MMSys '21}},
    doi = {10.1145/3458305.3478443},
    publisher = {ACM Press},
    year = {2021}
  }
  ```

## Get the project

You can [download pre-compiled binaries for the latest release](https://github.com/alicevision/meshroom/releases).  

If you want to build it yourself, see [**INSTALL.md**](INSTALL.md) to setup the project and pre-requisites.

Get the source code and install runtime requirements:
```bash
git clone --recursive git://github.com/alicevision/meshroom
cd meshroom
pip install -r requirements.txt
```


## Start Meshroom

You need to have [AliceVision](https://github.com/alicevision/AliceVision) installation in your PATH (and LD_LIBRARY_PATH on Linux/macOS).

 - __Launch the User Interface__

```bash
# Windows
set PYTHONPATH=%CD% && python meshroom/ui
# Linux/macOS
PYTHONPATH=$PWD python meshroom/ui
```

On Ubuntu, you may have conflicts between native drivers and mesa drivers. In that case, you need to force usage of native drivers by adding them to the LD_LIBRARY_PATH:
`LD_LIBRARY_PATH=/usr/lib/nvidia-340 PYTHONPATH=$PWD python meshroom/ui`
You may need to adjust the folder `/usr/lib/nvidia-340` with the correct driver version.

 - __Launch a 3D reconstruction in command line__

```bash
# Windows: set PYTHONPATH=%CD% &&
# Linux/macOS: PYTHONPATH=$PWD
python bin/meshroom_batch --input INPUT_IMAGES_FOLDER --output OUTPUT_FOLDER
```

## Start Meshroom without building AliceVision

To use Meshroom (ui) without building AliceVision
*   Download a [release](https://github.com/alicevision/meshroom/releases)
*   Checkout corresponding Meshroom (ui) version/tag to avoid versions incompatibilities
*   `LD_LIBRARY_PATH=~/foo/Meshroom-2023.2.0/aliceVision/lib/ PATH=$PATH:~/foo/Meshroom-2023.2.0/aliceVision/bin/ PYTHONPATH=$PWD python3 meshroom/ui`

## Start and Debug Meshroom in an IDE

PyCharm Community is free IDE which can be used. To start and debug a project with that IDE,
right-click on `Meshroom/ui/__main__.py` > `Debug`, then `Edit Configuration`, in `Environment variables` : 
*   If you want to use aliceVision built by yourself add: `PATH=$PATH:/foo/build/Linux-x86_64/`
*   If you want to use aliceVision release add: `LD_LIBRARY_PATH=/foo/Meshroom-2023.2.0/aliceVision/lib/;PATH=$PATH:/foo/Meshroom-2023.2.0/aliceVision/bin/` (Make sure that you are on the branch matching the right version)

![image](https://user-images.githubusercontent.com/937836/127321375-3bf78e73-569d-414a-8649-de0307adf794.png)


## FAQ

See the [Meshroom wiki](https://github.com/alicevision/meshroom/wiki) for more information.


## Contact

Use the public mailing-list to ask questions or request features. It is also a good place for informal discussions like sharing results, interesting related technologies or publications:
> [alicevision@googlegroups.com](mailto:alicevision@googlegroups.com)
> [http://groups.google.com/group/alicevision](http://groups.google.com/group/alicevision)

You can also contact the core team privately on: [alicevision-team@googlegroups.com](mailto:alicevision-team@googlegroups.com).
",2023-07-07 16:02:27+00:00
metaflow,metaflow,Netflix/metaflow,:rocket: Build and manage real-life data science projects with ease!,https://metaflow.org,False,6803,2023-07-07 07:55:07+00:00,2019-09-17 17:48:25+00:00,649,265,70,87,2.9.7,2023-06-27 09:04:40+00:00,Apache License 2.0,733,rec_0.0.2,100,2021-05-02 09:06:53+00:00,2023-07-07 15:10:37+00:00,2023-07-07 15:10:34+00:00,"![Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)

# Metaflow

Metaflow is a human-friendly library that helps scientists and engineers build and manage real-life data science projects. Metaflow was [originally developed at Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) to boost productivity of data scientists who work on a wide variety of projects from classical statistics to state-of-the-art deep learning.

For more information, see [Metaflow's website](https://metaflow.org) and [documentation](https://docs.metaflow.org).

## From prototype to production (and back)

Metaflow provides a simple, friendly API that covers foundational needs of ML, AI, and data science projects:
<img src=""./docs/prototype-to-prod.png"" width=""800px"">

1. [Rapid local prototyping](https://docs.metaflow.org/metaflow/basics), [support for notebooks](https://docs.metaflow.org/metaflow/visualizing-results), and [built-in experiment tracking and versioning](https://docs.metaflow.org/metaflow/client).
2. [Horizontal and vertical scalability to the cloud](https://docs.metaflow.org/scaling/remote-tasks/introduction), utilizing both CPUs and GPUs, and [fast data access](https://docs.metaflow.org/scaling/data).
3. [Managing dependencies](https://docs.metaflow.org/scaling/dependencies) and [one-click deployments to highly available production orchestrators](https://docs.metaflow.org/production/introduction).


## Getting started

Getting up and running is easy. If you don't know where to start, [Metaflow sandbox](https://outerbounds.com/sandbox) will have you running and exploring Metaflow in seconds.

### Installing Metaflow in your Python environment

To install Metaflow in your local environment, you can install from [PyPi](https://pypi.org/project/metaflow/):

```sh
pip install metaflow
```
Alternatively, you can also install from [conda-forge](https://anaconda.org/conda-forge/metaflow):

```sh
conda install -c conda-forge metaflow
```
If you are eager to try out Metaflow in practice, you can start with the [tutorial](https://docs.metaflow.org/getting-started/tutorials). After the tutorial, you can learn more about how Metaflow works [here](https://docs.metaflow.org/metaflow/basics).

### Deploying infrastructure for Metaflow in your cloud
<img src=""./docs/multicloud.png"" width=""800px"">


While you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to [scale out to external compute clusters](https://docs.metaflow.org/scaling/remote-tasks/introduction) 
and to [deploy to production-grade workflow orchestrators](https://docs.metaflow.org/production/introduction). To benefit from these features, follow this [guide](https://outerbounds.com/engineering/welcome/) to 
configure Metaflow and the infrastructure behind it appropriately.

## [Resources](https://docs.metaflow.org/introduction/metaflow-resources)

### [Slack Community](http://slack.outerbounds.co/)
An active [community](http://slack.outerbounds.co/) of thousands of data scientists and ML engineers discussing the ins-and-outs of applied machine learning.

### [Tutorials](https://outerbounds.com/docs/tutorials-index/)
- [Introduction to Metaflow](https://outerbounds.com/docs/intro-tutorial-overview/)
- [Natural Language Processing with Metaflow](https://outerbounds.com/docs/nlp-tutorial-overview/)
- [Computer Vision with Metaflow](https://outerbounds.com/docs/cv-tutorial-overview/)
- [Recommender Systems with Metaflow](https://outerbounds.com/docs/recsys-tutorial-overview/)
- And more advanced content [here](https://outerbounds.com/docs/tutorials-index/)

### [Generative AI and LLM use cases](https://outerbounds.com/blog/?category=Foundation%20Models)
- [Infrastructure Stack for Large Language Models](https://outerbounds.com/blog/llm-infrastructure-stack/)
- [Parallelizing Stable Diffusion for Production Use Cases](https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/)
- [Whisper with Metaflow on Kubernetes](https://outerbounds.com/blog/whisper-kubernetes/)
- [Training a Large Language Model With Metaflow, Featuring Dolly](https://outerbounds.com/blog/train-dolly-metaflow/)

## Get in touch
There are several ways to get in touch with us:
- [Slack Community](http://slack.outerbounds.co/)
- [Github Issues](https://github.com/Netflix/metaflow/issues)

## Contributing
We welcome contributions to Metaflow. Please see our [contribution guide](https://docs.metaflow.org/introduction/contributing-to-metaflow) for more details.
",2023-07-07 16:02:32+00:00
metapipe,metapipe,TorkamaniLab/metapipe,A pipeline generator and runtime system,,False,12,2023-05-12 08:50:11+00:00,2015-10-30 19:21:06+00:00,4,8,3,4,1.3,2017-09-21 18:09:26+00:00,MIT License,259,v1.2,4,2016-03-29 22:34:55+00:00,2023-05-12 08:50:11+00:00,2017-09-21 18:18:39+00:00,"# Metapipe

*A pipeline generator and runtime system*

[![Build Status](https://travis-ci.org/TorkamaniLab/metapipe.svg)](https://travis-ci.org/TorkamaniLab/metapipe)
[![Coverage Status](https://coveralls.io/repos/github/TorkamaniLab/metapipe/badge.svg?branch=master)](https://coveralls.io/github/TorkamaniLab/metapipe?branch=master)
[![Python 2.7 Status](https://img.shields.io/badge/Python-2.7-brightgreen.svg)](https://img.shields.io/badge/Python-2.7-blue.svg)
[![Python 3.4 Status](https://img.shields.io/badge/Python-3.4-brightgreen.svg)](https://img.shields.io/badge/Python-3.4-blue.svg)
[![Python 3.5 Status](https://img.shields.io/badge/Python-3.5-brightgreen.svg)](https://img.shields.io/badge/Python-3.5-blue.svg)
[![Packagist](https://img.shields.io/packagist/l/doctrine/orm.svg)](https://github.com/TorkamaniLab/metapipe/blob/master/LICENSE)

Metapipe is a simple command line tool for building and running complex analysis pipelines. If you use a PBS/Torque queue for cluster computing, or if you have complex batch processing that you want simplified, metapipe is the tool for you.

<img src=""docs/pipeline.gif"" width=""350px"" align=""right"" />

Metapipe's goal is to improve **readability**, and **maintainability** when building complex pipelines.

In addition to helping you generate and maintain complex pipelines, **metapipe also helps you debug them**! How? Well metapipe watches your jobs execute and keeps tabs on them. This means, unlike conventional batch queue systems like PBS/Torque alone, metapipe can give you accurate error information, and even resubmit failing jobs! Metapipe enhances the power of any PBS/Torque queue!

- What if I [don't use PBS/Torque](#other-queue-systems), or [a queue system at all?](#no-queue-no-problem)


## How do I get it?

It's super simple!

`pip install metapipe`

To make it easy, metapipe runs on Python 2.7, 3.4, and 3.5!


## What does it do?

In the bad old days (before metapipe), if you wanted to make an analysis pipeline, you needed to know how to code. **Not anymore!** Metapipe makes it easy to build and run your analysis pipelines! **No more code, just commands!** This makes your pipelines easy to understand and change!


## Documentation & Help

[Check out the full documentation at ReadTheDocs &#8594;](http://metapipe.readthedocs.org/en/latest/index.html)

If you need help with Metapipe, or you'd like to chat about new features, get in touch by filing an issue, or at `#metapipe` on freenode!


### Here's a sample!

Let's say you have a few command-line tools that you want to string together into a pipeline. You used to have to know Python, Perl, Bash, or some other scripting language; now you can use Metapipe!

```bash
[COMMANDS]
# Let's get the first and third columns from each of
# our files, and put the output in seperate files.
cut -f 1,3 {1||2||3} > {o}

# Once that's done, we'll need to take the output and 
# run each through our custom processing script individually.
# Here we can give a custom extension to the default output file.
python3 my_script.py --output {o.processed.csv} -i {1.*||}

# Finally, we want to collect each sample and analyze 
# them all together. We also need to use a custom version 
# of Python for this.
custom_python anaylysis.py -o {o.results.txt} {2.*}

[FILES]
1. controls.1.csv
2. controls.2.csv
3. controls.3.csv

[PATHS]
custom_python ~/path/to/my/custom/python/version
```

Excluding the comments, this entire analysis pipeline is 13 lines long, and extremely readable! What's even better? If you want to change any steps, its super easy! That's the power of Metapipe!


## No Queue? No Problem!

Lots of people don't use a PBS/Torque queue system, or a queue system at all, and metapipe can help them as well! Metapipe runs locally and will give you all the same benefits of a batch queue system! It runs jobs in parallel, and provide detailed feedback when jobs go wrong, and automatic job re-running if they fail.

To run metapipe locally, see the app's help menu!

`metapipe --help`


## Other Queue Systems

Metapipe is a very modular tool, and is designed to support any execution backend. Right now we only support PBS, but if you know just a little bit of Python, you can add support for any queue easily! *More information coming soon!*
",2023-07-07 16:02:36+00:00
mistral,mistral,openstack/mistral,Workflow Service for OpenStack. Mirror of code maintained at opendev.org.,https://opendev.org/openstack/mistral,False,246,2023-06-08 10:13:17+00:00,2013-10-29 20:46:49+00:00,112,42,160,0,,,Apache License 2.0,4359,xena-em,129,2021-07-19 02:32:27+00:00,2023-07-05 15:40:26+00:00,2023-07-05 15:39:49+00:00,"========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/mistral.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

Mistral
=======

Workflow Service integrated with OpenStack. This project aims to provide a
mechanism to define tasks and workflows in a simple YAML-based language, manage
and execute them in a distributed environment.

Project Resources
-----------------

* `Mistral Official Documentation <https://docs.openstack.org/mistral/latest/>`_

    * `User Documentation <https://docs.openstack.org/mistral/latest/user/index.html>`_

    * `Administrator Documentation <https://docs.openstack.org/mistral/latest/admin/index.html>`_

    * `Developer Documentation <https://docs.openstack.org/mistral/latest/developer/index.html>`_

* Project status, bugs, and blueprints are tracked on
  `Launchpad <https://launchpad.net/mistral/>`_

* CloudFlow: visualization tool for workflow executions on https://github.com/nokia/CloudFlow

* Apache License Version 2.0 http://www.apache.org/licenses/LICENSE-2.0

* Release notes for the project can be found at:
  https://docs.openstack.org/releasenotes/mistral/

* Source for the project can be found at:
  https://opendev.org/openstack/mistral
",2023-07-07 16:02:40+00:00
mkite,mkite_core,mkite-group/mkite_core,Core functions for mkite,,False,1,2023-04-28 17:51:50+00:00,2023-04-28 05:30:43+00:00,0,1,1,1,v0.1.0,2023-04-28 16:49:56+00:00,Apache License 2.0,4,v0.1.0,1,2023-04-28 16:49:56+00:00,2023-04-28 17:51:50+00:00,2023-04-28 16:49:56+00:00,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/mkite-group/mkite_core/main/docs/_static/mkite-core.png"" width=""300""><br>
</div>

# mkite-core: essentials for mkite

mkite_core offers a set of tools for the mkite suite. For example, it provides essential classes for recipes, plugins, and midware for models. mkite_core also offers some I/O support for interfacing mkite with other packages, such as rdkit.

## Documentation

General tutorial for `mkite` and its plugins are available in the [main documentation](https://mkite.org).
Complete API documentation is pending.

## Installation

`mkite_core` is essential to run all other components of mkite. To install this package, use pip:

```bash
pip install mkite_core
```

Alternatively, for a development version, clone this repo and install it in editable form:

```bash
pip install -U git+https://github.com/mkite-group/mkite_core
```

## Contributions

Contributions to the entire mkite suite are welcomed.
You can send a pull request or open an issue for this plugin or either of the packages in mkite.
When doing so, please adhere to the [Code of Conduct](CODE_OF_CONDUCT.md) in the mkite suite.

The mkite package was created by Daniel Schwalbe-Koda <dskoda@llnl.gov>.

### Citing mkite

If you use mkite in a publication, please cite the following paper:

```bibtex
@article{mkite2023,
    title = {mkite: A distributed computing platform for high-throughput materials simulations},
    author = {Schwalbe-Koda, Daniel},
    year = {2023},
    journal = {arXiv:2301.08841},
    doi = {10.48550/arXiv.2301.08841},
    url = {https://doi.org/10.48550/arXiv.2301.08841},
    arxiv={2301.08841},
}
```

## License

The mkite suite is distributed under the following license: Apache 2.0 WITH LLVM exception.

All new contributions must be made under this license.

SPDX: Apache-2.0, LLVM-exception

LLNL-CODE-848161
",2023-07-07 16:02:44+00:00
mleap,mleap,combust/mleap,MLeap: Deploy ML Pipelines to Production,https://combust.github.io/mleap-docs/,False,1463,2023-07-01 16:16:26+00:00,2016-08-23 03:51:03+00:00,313,71,69,4,v0.23.0,2023-06-23 21:37:25+00:00,Apache License 2.0,1028,v0.23.0,35,2023-06-23 21:37:25+00:00,2023-07-01 16:16:26+00:00,2023-06-29 16:05:06+00:00,"<a href=""https://combust.github.io/mleap-docs/""><img src=""logo.png"" alt=""MLeap Logo"" width=""176"" height=""70"" /></a>

[![Gitter](https://badges.gitter.im/combust/mleap.svg)](https://gitter.im/combust/mleap?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![Build Status](https://travis-ci.org/combust/mleap.svg?branch=master)](https://travis-ci.org/combust/mleap)
[![Maven Central](https://maven-badges.herokuapp.com/maven-central/ml.combust.mleap/mleap-base_2.12/badge.svg)](https://maven-badges.herokuapp.com/maven-central/ml.combust.mleap/mleap-base_2.12)

Deploying machine learning data pipelines and algorithms should not be a time-consuming or difficult task. MLeap allows data scientists and engineers to deploy machine learning pipelines from Spark and Scikit-learn to a portable format and execution engine.

## Documentation

Documentation is available at [https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/).

Read [Serializing a Spark ML Pipeline and Scoring with MLeap](https://github.com/combust-ml/mleap/wiki/Serializing-a-Spark-ML-Pipeline-and-Scoring-with-MLeap) to gain a full sense of what is possible.

## Introduction

Using the MLeap execution engine and serialization format, we provide a performant, portable and easy-to-integrate production library for machine learning data pipelines and algorithms.

For portability, we build our software on the JVM and only use serialization formats that are widely-adopted.

We also provide a high level of integration with existing technologies.

Our goals for this project are:

1. Allow Researchers/Data Scientists and Engineers to continue to build data pipelines and train algorithms with Spark and Scikit-Learn
2. Extend Spark/Scikit/TensorFlow by providing ML Pipelines serialization/deserialization to/from a common framework (Bundle.ML)
3. Use MLeap Runtime to execute your pipeline and algorithm without dependenices on Spark or Scikit (numpy, pandas, etc)

## Overview

1. Core execution engine implemented in Scala
2. [Spark](http://spark.apache.org/), PySpark and Scikit-Learn support
3. Export a model with Scikit-learn or Spark and execute it using the MLeap Runtime (without dependencies on the Spark Context, or sklearn/numpy/pandas/etc)
4. Choose from 2 portable serialization formats (JSON, Protobuf)
5. Implement your own custom data types and transformers for use with MLeap data frames and transformer pipelines
6. Extensive test coverage with full parity tests for Spark and MLeap pipelines
7. Optional Spark transformer extension to extend Spark's default transformer offerings

<img src=""assets/images/single-runtime.jpg"" alt=""Unified Runtime""/>

## Dependency Compatibility Matrix

Other versions besides those listed below may also work (especially more recent Java versions for the JRE), 
but these are the configurations which are tested by mleap.

| MLeap Version | Spark Version | Scala Version    | Java Version | Python Version | XGBoost Version | Tensorflow Version |
|---------------|---------------|------------------|--------------|----------------|-----------------|--------------------|
| 0.23.0        | 3.4.0         | 2.12.13          | 11           | 3.7, 3.8       | 1.7.3           | 2.10.1             |
| 0.22.0        | 3.3.0         | 2.12.13          | 11           | 3.7, 3.8       | 1.6.1           | 2.7.0              |
| 0.21.1        | 3.2.0         | 2.12.13          | 11           | 3.7            | 1.6.1           | 2.7.0              |
| 0.21.0        | 3.2.0         | 2.12.13          | 11           | 3.6, 3.7       | 1.6.1           | 2.7.0              |
| 0.20.0        | 3.2.0         | 2.12.13          | 8            | 3.6, 3.7       | 1.5.2           | 2.7.0              |
| 0.19.0        | 3.0.2         | 2.12.13          | 8            | 3.6, 3.7       | 1.3.1           | 2.4.1              |
| 0.18.1        | 3.0.2         | 2.12.13          | 8            | 3.6, 3.7       | 1.0.0           | 2.4.1              |
| 0.18.0        | 3.0.2         | 2.12.13          | 8            | 3.6, 3.7       | 1.0.0           | 2.4.1              |
| 0.17.0        | 2.4.5         | 2.11.12, 2.12.10 | 8            | 3.6, 3.7       | 1.0.0           | 1.11.0             |

## Setup

### Link with Maven or SBT

#### SBT

```sbt
libraryDependencies += ""ml.combust.mleap"" %% ""mleap-runtime"" % ""0.23.0""
```

#### Maven

```pom
<dependency>
    <groupId>ml.combust.mleap</groupId>
    <artifactId>mleap-runtime_2.12</artifactId>
    <version>0.23.0</version>
</dependency>
```

### For Spark Integration

#### SBT

```sbt
libraryDependencies += ""ml.combust.mleap"" %% ""mleap-spark"" % ""0.23.0""
```

#### Maven

```pom
<dependency>
    <groupId>ml.combust.mleap</groupId>
    <artifactId>mleap-spark_2.12</artifactId>
    <version>0.23.0</version>
</dependency>
```

### PySpark Integration

Install MLeap from [PyPI](https://pypi.org/project/mleap/)
```bash
$ pip install mleap
```

## Using the Library

For more complete examples, see our other Git repository: [MLeap Demos](https://github.com/combust/mleap-demo)

### Create and Export a Spark Pipeline

The first step is to create our pipeline in Spark. For our example we will manually build a simple Spark ML pipeline.


```scala
import ml.combust.bundle.BundleFile
import ml.combust.mleap.spark.SparkSupport._
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.bundle.SparkBundleContext
import org.apache.spark.ml.feature.{Binarizer, StringIndexer}
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import resource._

  val datasetName = ""./examples/spark-demo.csv""

  val dataframe: DataFrame = spark.sqlContext.read.format(""csv"")
    .option(""header"", true)
    .load(datasetName)
    .withColumn(""test_double"", col(""test_double"").cast(""double""))

  // User out-of-the-box Spark transformers like you normally would
  val stringIndexer = new StringIndexer().
    setInputCol(""test_string"").
    setOutputCol(""test_index"")

  val binarizer = new Binarizer().
    setThreshold(0.5).
    setInputCol(""test_double"").
    setOutputCol(""test_bin"")

  val pipelineEstimator = new Pipeline()
    .setStages(Array(stringIndexer, binarizer))

  val pipeline = pipelineEstimator.fit(dataframe)

  // then serialize pipeline
  val sbc = SparkBundleContext().withDataset(pipeline.transform(dataframe))
  for(bf <- managed(BundleFile(""jar:file:/tmp/simple-spark-pipeline.zip""))) {
    pipeline.writeBundle.save(bf)(sbc).get
  }
```

The dataset used for training can be found [here](https://github.com/combust/mleap/tree/master/examples/spark-demo.csv)

Spark pipelines are not meant to be run outside of Spark. They require a DataFrame and therefore a SparkContext to run. These are expensive data structures and libraries to include in a project. With MLeap, there is no dependency on Spark to execute a pipeline. MLeap dependencies are lightweight and we use fast data structures to execute your ML pipelines.

### PySpark Integration

Import the MLeap library in your PySpark job

```python
import mleap.pyspark
from mleap.pyspark.spark_support import SimpleSparkSerializer
```

See [PySpark Integration of python/README.md](python/README.md#pyspark-integration) for more.

### Create and Export a Scikit-Learn Pipeline

```python
import pandas as pd

from mleap.sklearn.pipeline import Pipeline
from mleap.sklearn.preprocessing.data import FeatureExtractor, LabelEncoder, ReshapeArrayToN1
from sklearn.preprocessing import OneHotEncoder

data = pd.DataFrame(['a', 'b', 'c'], columns=['col_a'])

categorical_features = ['col_a']

feature_extractor_tf = FeatureExtractor(input_scalars=categorical_features, 
                                         output_vector='imputed_features', 
                                         output_vector_items=categorical_features)

# Label Encoder for x1 Label 
label_encoder_tf = LabelEncoder(input_features=feature_extractor_tf.output_vector_items,
                               output_features='{}_label_le'.format(categorical_features[0]))

# Reshape the output of the LabelEncoder to N-by-1 array
reshape_le_tf = ReshapeArrayToN1()

# Vector Assembler for x1 One Hot Encoder
one_hot_encoder_tf = OneHotEncoder(sparse=False)
one_hot_encoder_tf.mlinit(prior_tf = label_encoder_tf, 
                          output_features = '{}_label_one_hot_encoded'.format(categorical_features[0]))

one_hot_encoder_pipeline_x0 = Pipeline([
                                         (feature_extractor_tf.name, feature_extractor_tf),
                                         (label_encoder_tf.name, label_encoder_tf),
                                         (reshape_le_tf.name, reshape_le_tf),
                                         (one_hot_encoder_tf.name, one_hot_encoder_tf)
                                        ])

one_hot_encoder_pipeline_x0.mlinit()
one_hot_encoder_pipeline_x0.fit_transform(data)
one_hot_encoder_pipeline_x0.serialize_to_bundle('/tmp', 'mleap-scikit-test-pipeline', init=True)

# array([[ 1.,  0.,  0.],
#        [ 0.,  1.,  0.],
#        [ 0.,  0.,  1.]])
```

### Load and Transform Using MLeap

Because we export Spark and Scikit-learn pipelines to a standard format, we can use either our Spark-trained pipeline or our Scikit-learn pipeline from the previous steps to demonstrate usage of MLeap in this section. The choice is yours!

```scala
import ml.combust.bundle.BundleFile
import ml.combust.mleap.runtime.MleapSupport._
import resource._
// load the Spark pipeline we saved in the previous section
val bundle = (for(bundleFile <- managed(BundleFile(""jar:file:/tmp/simple-spark-pipeline.zip""))) yield {
  bundleFile.loadMleapBundle().get
}).opt.get

// create a simple LeapFrame to transform
import ml.combust.mleap.runtime.frame.{DefaultLeapFrame, Row}
import ml.combust.mleap.core.types._

// MLeap makes extensive use of monadic types like Try
val schema = StructType(StructField(""test_string"", ScalarType.String),
  StructField(""test_double"", ScalarType.Double)).get
val data = Seq(Row(""hello"", 0.6), Row(""MLeap"", 0.2))
val frame = DefaultLeapFrame(schema, data)

// transform the dataframe using our pipeline
val mleapPipeline = bundle.root
val frame2 = mleapPipeline.transform(frame).get
val data2 = frame2.dataset

// get data from the transformed rows and make some assertions
assert(data2(0).getDouble(2) == 1.0) // string indexer output
assert(data2(0).getDouble(3) == 1.0) // binarizer output

// the second row
assert(data2(1).getDouble(2) == 2.0)
assert(data2(1).getDouble(3) == 0.0)
```

## Documentation

For more documentation, please see our [documentation](https://combust.github.io/mleap-docs/), where you can learn to:

1. Implement custom transformers that will work with Spark, MLeap and Scikit-learn
2. Implement custom data types to transform with Spark and MLeap pipelines
3. Transform with blazing fast speeds using optimized row-based transformers
4. Serialize MLeap data frames to various formats like avro, json, and a custom binary format
5. Implement new serialization formats for MLeap data frames
6. Work through several demonstration pipelines which use real-world data to create predictive pipelines
7. Supported Spark transformers
8. Supported Scikit-learn transformers
9. Custom transformers provided by MLeap

## Contributing

* Write documentation.
* Write a tutorial/walkthrough for an interesting ML problem
* Contribute an Estimator/Transformer from Spark
* Use MLeap at your company and tell us what you think
* Make a feature request or report a bug in github
* Make a pull request for an existing feature request or bug report
* Join the discussion of how to get MLeap into Spark as a dependency. Talk with us on Gitter (see link at top of README.md)

## Building

Please ensure you have sbt 1.4.9, java 11, scala 2.12.13

1. Initialize the git submodules `git submodule update --init --recursive`
2. Run `sbt compile`

## Thank You

Thank you to [Swoop](https://www.swoop.com/) for supporting the XGboost
integration.

## Contributors Information

* Jason Sleight ([jsleight](https://github.com/jsleight))
* Talal Riaz ([talalryz](https://github.com/talalryz))
* Weichen Xu ([WeichenXu123](https://github.com/WeichenXu123))

## Past contributors
* Hollin Wilkins (hollin@combust.ml)
* Mikhail Semeniuk (mikhail@combust.ml)
* Anca Sarb (sarb.anca@gmail.com)
* Ryan Vogan (rvogan@yelp.com)


## License

See LICENSE and NOTICE file in this repository.

Copyright 2016 Combust, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",2023-07-07 16:02:48+00:00
mlflow,mlflow,mlflow/mlflow,Open source platform for the machine learning lifecycle,https://mlflow.org,False,14746,2023-07-07 15:14:32+00:00,2018-06-05 16:05:58+00:00,3442,291,458,70,v2.4.1,2023-06-10 01:20:38+00:00,Apache License 2.0,4304,v2.4.1,119,2023-06-10 01:20:38+00:00,2023-07-07 16:02:06+00:00,2023-07-07 15:18:16+00:00,"=============================================
MLflow: A Machine Learning Lifecycle Platform
=============================================

MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code
into reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be
used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you
currently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:

* `MLflow Tracking <https://mlflow.org/docs/latest/tracking.html>`_: An API to log parameters, code, and
  results in machine learning experiments and compare them using an interactive UI.
* `MLflow Projects <https://mlflow.org/docs/latest/projects.html>`_: A code packaging format for reproducible
  runs using Conda and Docker, so you can share your ML code with others.
* `MLflow Models <https://mlflow.org/docs/latest/models.html>`_: A model packaging format and tools that let
  you easily deploy the same model (from any ML library) to batch and real-time scoring on platforms such as
  Docker, Apache Spark, Azure ML and AWS SageMaker.
* `MLflow Model Registry <https://mlflow.org/docs/latest/model-registry.html>`_: A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.

|docs| |pypi| |conda-forge| |cran| |maven| |license| |downloads| |slack| |twitter|

.. |docs| image:: https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge
    :target: https://mlflow.org/docs/latest/index.html
    :alt: Latest Docs
.. |pypi| image:: https://img.shields.io/pypi/v/mlflow.svg?style=for-the-badge&logo=pypi&logoColor=white
    :target: https://pypi.org/project/mlflow/
    :alt: Latest Python Release
.. |conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/mlflow.svg?style=for-the-badge&logo=anaconda
    :target: https://anaconda.org/conda-forge/mlflow
    :alt: Latest Conda Release
.. |cran| image:: https://img.shields.io/cran/v/mlflow.svg?style=for-the-badge&logo=r
    :target: https://cran.r-project.org/package=mlflow
    :alt: Latest CRAN Release
.. |maven| image:: https://img.shields.io/maven-central/v/org.mlflow/mlflow-parent.svg?style=for-the-badge&logo=apache-maven
    :target: https://mvnrepository.com/artifact/org.mlflow
    :alt: Maven Central
.. |license| image:: https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache
    :target: https://github.com/mlflow/mlflow/blob/master/LICENSE.txt
    :alt: Apache 2 License
.. |downloads| image:: https://img.shields.io/pypi/dw/mlflow?style=for-the-badge&logo=pypi&logoColor=white
    :target: https://pepy.tech/project/mlflow
    :alt: Total Downloads
.. |slack| image:: https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge
    :target: `Slack`_
    :alt: Slack
.. |twitter| image:: https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white
    :target: https://twitter.com/MLflow
    :alt: Account Twitter

.. _Slack: https://join.slack.com/t/mlflow-users/shared_invite/zt-1iffrtbly-UNU8hV03aV8feUeGmqf_uA

Job Statuses

|examples| |cross-version-tests| |r-devel| |test-requirements| |stale| |push-images|

.. |examples| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/examples.yml?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github
    :target: https://github.com/mlflow/mlflow/actions?query=workflow%3AExamples+event%3Aschedule
    :alt: Examples Action Status
.. |cross-version-tests| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/cross-version-tests.yml?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github
    :target: https://github.com/mlflow/mlflow/actions?query=workflow%3ACross%2Bversion%2Btests+event%3Aschedule
.. |r-devel| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/r.yml?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github
    :target: https://github.com/mlflow/mlflow/actions?query=workflow%3AR+event%3Aschedule
.. |test-requirements| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/requirements.yml?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge
    :target: https://github.com/mlflow/mlflow/actions?query=workflow%3ATest%2Brequirements+event%3Aschedule
.. |stale| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/stale.yml?branch=master&event=schedule&label=stale&logo=github&style=for-the-badge
    :target: https://github.com/mlflow/mlflow/actions?query=workflow%3AStale+event%3Aschedule
.. |push-images| image:: https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml?event=release&label=push-images&logo=github&style=for-the-badge
    :target: https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease

Installing
----------
Install MLflow from PyPI via ``pip install mlflow``

MLflow requires ``conda`` to be on the ``PATH`` for the projects feature.

Nightly snapshots of MLflow master are also available `here <https://mlflow-snapshots.s3-us-west-2.amazonaws.com/>`_.

Install a lower dependency subset of MLflow from PyPI via ``pip install mlflow-skinny``
Extra dependencies can be added per desired scenario.
For example, ``pip install mlflow-skinny pandas numpy`` allows for mlflow.pyfunc.log_model support.

Documentation
-------------
Official documentation for MLflow can be found at https://mlflow.org/docs/latest/index.html.

Roadmap
-------
The current MLflow Roadmap is available at https://github.com/mlflow/mlflow/milestone/3. We are
seeking contributions to all of our roadmap items with the ``help wanted`` label. Please see the
`Contributing`_ section for more information.

Community
---------
For help or questions about MLflow usage (e.g. ""how do I do X?"") see the `docs <https://mlflow.org/docs/latest/index.html>`_
or `Stack Overflow <https://stackoverflow.com/questions/tagged/mlflow>`_.

To report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue.

For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
or join us on `Slack`_.

Running a Sample App With the Tracking API
------------------------------------------
The programs in ``examples`` use the MLflow Tracking API. For instance, run::

    python examples/quickstart/mlflow_tracking.py

This program will use `MLflow Tracking API <https://mlflow.org/docs/latest/tracking.html>`_,
which logs tracking data in ``./mlruns``. This can then be viewed with the Tracking UI.


Launching the Tracking UI
-------------------------
The MLflow Tracking UI will show runs logged in ``./mlruns`` at `<http://localhost:5000>`_.
Start it with::

    mlflow ui

**Note:** Running ``mlflow ui`` from within a clone of MLflow is not recommended - doing so will
run the dev UI from source. We recommend running the UI from a different working directory,
specifying a backend store via the ``--backend-store-uri`` option. Alternatively, see
instructions for running the dev UI in the `contributor guide <CONTRIBUTING.md>`_.


Running a Project from a URI
----------------------------
The ``mlflow run`` command lets you run a project packaged with a MLproject file from a local path
or a Git URI::

    mlflow run examples/sklearn_elasticnet_wine -P alpha=0.4

    mlflow run https://github.com/mlflow/mlflow-example.git -P alpha=0.4

See ``examples/sklearn_elasticnet_wine`` for a sample project with an MLproject file.


Saving and Serving Models
-------------------------
To illustrate managing models, the ``mlflow.sklearn`` package can log scikit-learn models as
MLflow artifacts and then load them again for serving. There is an example training application in
``examples/sklearn_logistic_regression/train.py`` that you can run as follows::

    $ python examples/sklearn_logistic_regression/train.py
    Score: 0.666
    Model saved in run <run-id>

    $ mlflow models serve --model-uri runs:/<run-id>/model

    $ curl -d '{""dataframe_split"": {""columns"":[0],""index"":[0,1],""data"":[[1],[-1]]}}' -H 'Content-Type: application/json'  localhost:5000/invocations

**Note:** If using MLflow skinny (``pip install mlflow-skinny``) for model serving, additional
required dependencies (namely, ``flask``) will need to be installed for the MLflow server to function.

Official MLflow Docker Image
----------------------------

The official MLflow Docker image is available on GitHub Container Registry at https://ghcr.io/mlflow/mlflow.

.. code-block:: shell

    export CR_PAT=YOUR_TOKEN
    echo $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin
    # Pull the latest version
    docker pull ghcr.io/mlflow/mlflow
    # Pull 2.2.1
    docker pull ghcr.io/mlflow/mlflow:v2.2.1

Contributing
------------
We happily welcome contributions to MLflow. We are also seeking contributions to items on the
`MLflow Roadmap <https://github.com/mlflow/mlflow/milestone/3>`_. Please see our
`contribution guide <CONTRIBUTING.md>`_ to learn more about contributing to MLflow.

Core Members
------------

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- `Harutaka Kawamura <https://github.com/harupy>`_
- `Weichen Xu <https://github.com/WeichenXu123>`_
- `Corey Zumar <https://github.com/dbczumar>`_
- `Ben Wilson <https://github.com/BenWilson2>`_
- `Serena Ruan <https://github.com/serena-ruan>`_
- `Gabriel Fu <https://github.com/gabrielfu>`_
",2023-07-07 16:02:52+00:00
mlr3pipelines,mlr3pipelines,mlr-org/mlr3pipelines,Dataflow Programming for Machine Learning in R,https://mlr3pipelines.mlr-org.com/,False,122,2023-05-29 00:39:50+00:00,2017-10-10 19:01:45+00:00,23,18,21,3,v0.1.2,2019-12-20 21:24:12+00:00,GNU Lesser General Public License v3.0,2754,v0.5.0,20,2023-05-22 05:37:35+00:00,2023-06-15 23:56:39+00:00,2023-05-22 10:15:57+00:00,"
# mlr3pipelines <img src=""man/figures/logo.png"" align=""right"" />

Package website: [release](https://mlr3pipelines.mlr-org.com/) |
[dev](https://mlr3pipelines.mlr-org.com/dev/)

Dataflow Programming for Machine Learning in R.

<!-- badges: start -->

[![tic](https://github.com/mlr-org/mlr3pipelines/workflows/r-cmd-check/badge.svg?branch=master)](https://github.com/mlr-org/mlr3pipelines/actions)
[![CRAN](https://www.r-pkg.org/badges/version/mlr3pipelines)](https://cran.r-project.org/package=mlr3pipelines)
[![StackOverflow](https://img.shields.io/badge/stackoverflow-mlr3-orange.svg)](https://stackoverflow.com/questions/tagged/mlr3)
[![Mattermost](https://img.shields.io/badge/chat-mattermost-orange.svg)](https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/)
<!-- badges: end -->

## What is `mlr3pipelines`?

Watch our “WhyR 2020” Webinar Presentation on Youtube for an
introduction\! Find the slides
[here](https://raw.githubusercontent.com/mlr-org/mlr-outreach/main/2020_whyr/slides.pdf).

[![WhyR 2020
mlr3pipelines](https://img.youtube.com/vi/4r8K3GO5wk4/0.jpg)](https://www.youtube.com/watch?v=4r8K3GO5wk4)

**`mlr3pipelines`** is a [dataflow
programming](https://en.wikipedia.org/wiki/Dataflow_programming) toolkit
for machine learning in R utilising the
**[mlr3](https://github.com/mlr-org/mlr3)** package. Machine learning
workflows can be written as directed “Graphs” that represent data flows
between preprocessing, model fitting, and ensemble learning units in an
expressive and intuitive language. Using methods from the
**[mlr3tuning](https://github.com/mlr-org/mlr3tuning)** package, it is
even possible to simultaneously optimize parameters of multiple
processing units.

In principle, *mlr3pipelines* is about defining singular data and model
manipulation steps as “PipeOps”:

``` r
pca        = po(""pca"")
filter     = po(""filter"", filter = mlr3filters::flt(""variance""), filter.frac = 0.5)
learner_po = po(""learner"", learner = lrn(""classif.rpart""))
```

These pipeops can then be combined together to define machine learning
pipelines. These can be wrapped in a `GraphLearner` that behave like any
other `Learner` in `mlr3`.

``` r
graph = pca %>>% filter %>>% learner_po
glrn = GraphLearner$new(graph)
```

This learner can be used for resampling, benchmarking, and even tuning.

``` r
resample(tsk(""iris""), glrn, rsmp(""cv""))
#> <ResampleResult> of 10 iterations
#> * Task: iris
#> * Learner: pca.variance.classif.rpart
#> * Warnings: 0 in 0 iterations
#> * Errors: 0 in 0 iterations
```

## Feature Overview

Single computational steps can be represented as so-called **PipeOps**,
which can then be connected with directed edges in a **Graph**. The
scope of *mlr3pipelines* is still growing; currently supported features
are:

  - Simple data manipulation and preprocessing operations, e.g. PCA,
    feature filtering
  - Task subsampling for speed and outcome class imbalance handling
  - *mlr3* *Learner* operations for prediction and stacking
  - Simultaneous path branching (data going both ways)
  - Alternative path branching (data going one specific way, controlled
    by hyperparameters)
  - Ensemble methods and aggregation of predictions

## Documentation

The easiest way to get started is reading some of the vignettes that are
shipped with the package, which can also be viewed online:

  - [Quick Introduction](https://mlr3book.mlr-org.com/pipelines.html),
    with short examples to get started

## Bugs, Questions, Feedback

*mlr3pipelines* is a free and open source software project that
encourages participation and feedback. If you have any issues,
questions, suggestions or feedback, please do not hesitate to open an
“issue” about it on the GitHub page\!

In case of problems / bugs, it is often helpful if you provide a
“minimum working example” that showcases the behaviour (but don’t
worry about this if the bug is obvious).

Please understand that the resources of the project are limited:
response may sometimes be delayed by a few days, and some feature
suggestions may be rejected if they are deemed too tangential to the
vision behind the project.

## Citing mlr3pipelines

If you use mlr3pipelines, please cite our [JMLR
article](https://jmlr.org/papers/v22/21-0281.html):

    @Article{mlr3pipelines,
      title = {{mlr3pipelines} - Flexible Machine Learning Pipelines in R},
      author = {Martin Binder and Florian Pfisterer and Michel Lang and Lennart Schneider and Lars Kotthoff and Bernd Bischl},
      journal = {Journal of Machine Learning Research},
      year = {2021},
      volume = {22},
      number = {184},
      pages = {1-7},
      url = {https://jmlr.org/papers/v22/21-0281.html},
    }

## Similar Projects

A predecessor to this package is the
[*mlrCPO*-package](https://github.com/mlr-org/mlrCPO), which works with
*mlr* 2.x. Other packages that provide, to varying degree, some
preprocessing functionality or machine learning domain specific
language, are the *[caret](https://github.com/topepo/caret)* package and
the related *[recipes](https://recipes.tidymodels.org/)* project, and
the *[dplyr](https://github.com/tidyverse/dplyr)* package.
",2023-07-07 16:02:57+00:00
moa,Moa,mfiers/Moa,Lightweight workflows in bioinformatics:,http://moa.readthedocs.org,False,22,2020-10-30 09:48:00+00:00,2009-07-31 00:22:10+00:00,10,6,5,0,,,GNU General Public License v3.0,2433,v0.11.24,40,2014-11-05 05:29:15+00:00,,2014-11-05 05:26:48+00:00,"For information on how to install and operate Moa, please read the
documentation at:

http://moa.readthedocs.org/en/latest/index.html

check the source code at:

https://github.com/mfiers/Moa

and run:

moa --help
",2023-07-07 16:03:01+00:00
mpipe,mpipe,vmlaker/mpipe,Python API for writing multiprocessing pipelines,http://vmlaker.github.io/mpipe,False,82,2023-06-12 06:19:52+00:00,2013-03-31 22:39:06+00:00,21,6,3,0,,,MIT License,94,,0,,2023-06-29 14:46:21+00:00,2020-09-29 13:25:18+00:00,".. image:: http://vmlaker.github.io/mpipe/_static/logo.png
  :alt: MPipe Logo
  :align: right
  :target: http://vmlaker.github.io/mpipe

.. image:: https://travis-ci.org/vmlaker/mpipe.png?branch=master
  :alt: Build Result Image
  :target: https://travis-ci.org/vmlaker/mpipe

A tiny Python module that lets you 
easily write multi-stage, multiprocess pipeline algorithms. 
For the full description, including user guide and examples, 
visit `the documentation page <http://vmlaker.github.io/mpipe>`_.
",2023-07-07 16:03:05+00:00
neatseq-flow,neatseq-flow,bioinfo-core-BGU/neatseq-flow,A Lightweight Software for Efficient Execution of High Throughput Sequencing Workflows,http://neatseq-flow.readthedocs.io/en/latest/,False,3,2023-01-15 14:40:11+00:00,2017-08-08 09:31:39+00:00,4,1,2,3,NeatSeq_Flow_v1.7.0,2019-06-27 08:03:12+00:00,GNU General Public License v3.0,502,v1.3.0,11,2018-02-15 09:20:30+00:00,2023-06-29 11:11:19+00:00,2023-06-29 11:11:12+00:00,"# **NeatSeq-Flow**: A Lightweight Software for Efficient Execution of High Throughput Sequencing Workflows.
![NeatSeq-Flow Logo](docs/source/figs/NeatSeq_Flow_logo.png ""NeatSeq-Flow"")


[![Documentation Status](https://readthedocs.org/projects/neatseq-flow/badge/?version=latest)](http://neatseq-flow.readthedocs.io/en/latest/?badge=latest)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
![GitHub release](https://img.shields.io/github/release-pre/sklarz-bgu/neatseq-flow.svg)
>>>>>>> devel
![GitHub repo size](https://img.shields.io/github/repo-size/sklarz-bgu/neatseq-flow.svg)
![GitHub top language](https://img.shields.io/github/languages/top/sklarz-bgu/neatseq-flow.svg)
![GitHub last commit](https://img.shields.io/github/last-commit/sklarz-bgu/neatseq-flow.svg)

<!--- [![Github All Releases](https://img.shields.io/github/downloads/sklarz-bgu/neatseq-flow/total.svg)]() -->


# Background

**[See full documentation on RTD.](http://NeatSeq-Flow.readthedocs.io/en/latest/)**

## Brief description

* NeatSeq-Flow is a platform for modular design and execution of bioinformatics workflows on a local computer or, preferably, computer cluster. 
* The platform has a command-line interface as well as a fully functional graphical user interface (GUI), both used locally without the need to connect to remote servers. 
* Analysis programs comprising a workflow can be anything executable from the Linux command-line, either publicly available or in-house programs. 
* Ready-to-use workflows are available for common Bioinformatics analyses such as assembly & annotation, RNA-Seq, ChIP-Seq, variant calling, metagenomics and genomic epidemiology. 
* Creation and sharing of new workflows is easy and intuitive, without need for programming knowledge. 
* NeatSeq-Flow is general-purpose and may easily be adjusted to work on different types of analyses other than high-throughput sequencing.


## The main benefits in using NeatSeq-Flow

* Simple, cross-platform installation.
* All workflow components and parameters, as well as their order of execution (a.k.a workflow design), are specified in a single file which may be prepared by the user either manually (YAML format) or through the GUI. This, together with the shell scripts produced by NeatSeq-Flow and additional NeatSeq-Flow summary files, comprise a complete documentation of the executed workflow and enable future execution of the exact same workflow or modifications thereof.
* The user is relieved from the need to know or manage the locations of intermediate or final files, or to transfer files between workflow steps. Workflow output file locations are determined by NeatSeq-Flow such that they are neatly organized in an intuitive directory structure.
* NeatSeq-Flow’s “Terminal Monitor” shows script execution in real time, and reports on execution errors immediately when they occur, thus facilitating user control on the workflow.
* The platform can accommodate workflows of any degree of complexity, and efficiently executes them in a parallelized manner on the user’s computer cluster.
* Through an intuitive GUI, NeatSeq-Flow is fully accessible to non-programmers, without compromising power, flexibility and efficiency.
* Users can easily create complex workflows from a variety of high-throughput sequencing applications made available by NeatSeq-Flow as independent modules. In addition, a generic module enables direct incorporation of applications without pre-built modules.
* Advanced users can run NeatSeq-Flow through the command-line, and create their own modules using a provided template and only basic Python commands.
* The modules and workflows are designed to be easily shared. In addition, the support for usage of CONDA environments enables easy portability and sharing of entire working environment for workflow execution.




# Installation & Execution

## Quick start

Check out the [Tutorial](http://neatseq-flow.readthedocs.io/en/latest/Example_WF.html) .

## Installing NeatSeq-Flow

The easiest way to install is using **Conda**, Conda will install NeatSeq-Flow and NeatSeq-Flow-GUI with all its dependencies **in one go**: 

  - First if you don't have **Conda**, [install it!](https://conda.io/miniconda.html) 
  - Then in the terminal:
    1. Create the **NeatSeq_Flow** conda environment:
    ```Bash
      conda env create levinl/neatseq_flow
    ```  


## Executing NeatSeq-Flow

1. Activate the environment

   1. Activate the **NeatSeq_Flow** conda environment:
    ```Bash
      bash
      source activate NeatSeq_Flow
      export CONDA_BASE=$(conda info --root)
    ```
    2. Run **NeatSeq_Flow_GUI**:
    ```Bash 
       NeatSeq_Flow_GUI.py
    ```
    3. For more option:
    ```Bash 
        NeatSeq_Flow_GUI.py -h
        usage: NeatSeq_Flow_GUI.py [-h] [--Server] [--PORT CHAR] [--HOST CHAR] [--SSL]
                           [--SSH_HOST CHAR] [--SSH_PORT CHAR] [--USER CHAR]
                           [--PASSW CHAR] [--USERSFILE CHAR]
                           [--UNLOCK_USER_DIR] [--WOKFLOW_DIR CHAR]
                           [--CONDA_BIN CHAR] [--LOG_DIR CHAR]

        NeatSeq-Flow GUI By Liron Levin

        optional arguments:
          -h, --help          show this help message and exit
          --Server            Run as Server
          --PORT CHAR         Use this port in which to run the app, If not set will
                              search for open port (Works only When --Server is set)
          --HOST CHAR         The host name/ip to serve the app, If not set, will try
                              to identify automatically (Works only When --Server is
                              set)
          --SSL               Use SSL (Only When --Server is set)
          --SSH_HOST CHAR     Connect using SSH to a remote host, NeatSeq-Flow needs
                              to be installed on the remote host (Works only When
                              --Server is set)
          --SSH_PORT CHAR     When --SSH_HOST is set use this ssh port to connect to a
                              remote host.
          --USER CHAR         User Name For This Serve (Works only When --Server is
                              set)
          --PASSW CHAR        Password For This Serve (Works only When --Server is
                              set)
          --USERSFILE CHAR    The location of a Users file in which a list of users,
                              E-mails addresses and Users directories are separated by
                              one space (as:USER user@example.com /USER/DIR). The
                              login password will be send to the user e-mail after
                              filling its user name and the password generated at the
                              beginning of the run (Works only When --Server is set).
                              You will need a Gmail account to send the password to
                              the users (you will be prompt to type in your Gmail
                              address and password)
          --UNLOCK_USER_DIR   Don't Lock Users to their Directory Or to the Current
                              Working Directory
          --WOKFLOW_DIR CHAR  A Path to a Directory containing work-flow files to
                              choose from at log-in. Works only When --Server is set.
                              If --SSH_HOST is set, the Path needs to be in the remote
                              host.
          --CONDA_BIN CHAR    A path to a the CONDA bin location. If --SSH_HOST is
                              set, the Path needs to be in the remote host.
          --LOG_DIR CHAR      A path to a directory to save log files about users
                              statistics. Only woks If --Server is set. In any way the
                              path needs to be at the local host.
    ```

1. Alternatively, run NeatSeq_Flow command-line version:

    ```Bash 
       neatseq_flow.py --help
    ``` 

### Using Docker

  Running the NeatSeq-Flow Platform on Windows/Mac Using a Docker Container
  Follow the instructions here:
    [NeatSeq-Flow Using Docker](https://github.com/bioinfo-core-BGU/NeatSeq-Flow-Using-Docker) 


# Additional repositories

The installation process described above installs the following three NeatSeq-Flow repositories:

1. [The main NeatSeq-Flow repository](https://github.com/bioinfo-core-BGU/neatseq-flow)
1. [NeatSeq-Flow's GUI repository](https://github.com/bioinfo-core-BGU/NeatSeq-Flow-GUI)
1. [NeatSeq-Flow's module repository](https://github.com/bioinfo-core-BGU/neatseq-flow-modules)


# Customers

[The National Knowledge Center for Rare / Orphan Diseases](http://in.bgu.ac.il/en/rod/Pages/default.aspx)

# Contact

Please contact 

Liron Levin at:     [levinl@bgu.ac.il](mailto:levinl@bgu.ac.il)

Menachem Sklarz at: [sklarz@bgu.ac.il](mailto:sklarz@bgu.ac.il)

# Citation

Sklarz, Menachem, et al. (2017) **NeatSeq-Flow: A Lightweight Software for Efficient Execution of High Throughput Sequencing Workflows**. [bioRxiv doi: 10.1101/173005](http://www.biorxiv.org/content/early/2017/08/08/173005)
",2023-07-07 16:03:08+00:00
nephele,nephele2,niaid/nephele2,,,False,3,2022-09-01 14:35:10+00:00,2020-04-23 16:40:44+00:00,2,9,1,0,,,Other,3,,0,,,2021-08-12 15:08:40+00:00,"# Nephele: Microbiome Analysis

Nephele: Microbiome Analysis is a web-based platform for microbiome data analysis. It is developed and maintained by a team from the Office of Cyber Infrastructure and Computational Biology ([OCICB](https://www.niaid.nih.gov/about/cyber-infrastructure-computational-biology-contacts)), which is part of the National Institute of Allergy and Infectious Diseases (NIAID). See more information at https://nephele.niaid.nih.gov/about/.


## About this repository

The purpose of this repository is to allow Nephele users and others interested in microbiome analysis to examine how we use various software tools within the Nephele v2.0 platform. This repository contains the code related to the website with user-facing interactions for the platform and the data analysis pipeline scripts that Nephele executes on the Amazon cloud to process sequence data inputs provided by users. The bioinformatics tools used by the pipelines need to be installed separately, and are listed in individual pipeline READMEs and reference sections. License information for code contained in this repository is noted in [LICENSE.md](LICENSE.md).
",2023-07-07 16:03:13+00:00
nesoni,nesoni,Victorian-Bioinformatics-Consortium/nesoni,High throughput sequencing analysis tools,,False,30,2022-10-20 23:19:22+00:00,2012-05-15 01:05:06+00:00,10,6,4,5,v0.134,2020-07-04 01:57:34+00:00,GNU General Public License v2.0,173,v0.134,5,2020-07-04 01:57:34+00:00,,2022-10-20 23:19:12+00:00,"Nesoni 
======

With the closure of the Victorian Bioinformatics Consortium, I anticipate little further development on Nesoni.

Nesoni still serves as the basis for ""Tail Tools"", which remains under development. The parallel processing code is also used by ""Demakein"".

- Paul Harrison, January 2015


What Nesoni does
----------------

Nesoni is a high-throughput sequencing data analysis toolset, which the Victorian Bioinformatics Consortium developed to cope with the flood of Illumina, 454, and SOLiD data being produced. 

The VBC's work was largely with bacterial genomes, and the design tradeoffs in Nesoni reflect this.

Nesoni focusses on analysing the alignment of reads to a reference genome. Use of the SHRiMP and Bowtie2 read aligners is automated by nesoni. We use SHRiMP as it is able to detect small insertions and deletions in addition to SNPs. Output from other aligners may be imported in SAM format.

Nesoni can call a consensus of read alignments, taking care to indicate ambiguity. This can then be used in various ways: to determine the protein level changes resulting from SNPs and indels, to find differences between multiple strains, or to produce n-way comparison data suitable for phylogenetic analysis in SplitsTree4.


Requirements
============

Python 2.7 or higher. Use of PyPy where possible is highly recommended 
for performance.

Python libraries
* Recommended:
  * BioPython [1]
* Optional (used by non-core nesoni tools):
  * numpy
  * matplotlib

External programs:
* Required:
  * SHRiMP or Bowtie2
  * samtools (0.1.19 or higher)
* Required for VCF based variant calling:
  * Picard [2]
  * Freebayes (9.9.2 or higher)
* Optional for VCF based variant calling:
  * SplitsTree4

R libraries required by R-based tools (mostly for RNA-seq):
* Required:
  * varistran from https://github.com/MonashBioinformaticsPlatform/varistran
  * limma, edgeR (3.2.4 or higher) from BioConductor
  * seriation from CRAN
* Optional:
  * goseq from BioConductor


[1] BioPython is used for reading GenBank files.

[2] There does not seem to be a standard way to install .jar files. 
    Nesoni will search for .jar files in directories listed in 
    environment variables $PATH and $JARPATH.


Installation
============

The easy way to install or upgrade:

    pip install -I nesoni

Then type ""nesoni"" and follow the command to install the R module.

See below for more ways to install nesoni.


Advanced Installation 
---------------------

From source, download and untar the source tarball, then:

    python setup.py install

Optional:

    R CMD INSTALL nesoni/nesoni-r


For PyPy it seems to be currently easiest to install nesoni in 
a virtualenv:

    virtualenv -p pypy my-pypy-env
    my-pypy-env/bin/pip install -I git+https://bitbucket.org/pypy/numpy.git
    my-pypy-env/bin/pip install -I biopython 
    my-pypy-env/bin/pip install -I nesoni

You can also set up a CPython virtualenv like this:

    virtualenv my-python-env
    my-python-env/bin/pip install -I numpy 
    my-python-env/bin/pip install -I matplotlib 
    my-python-env/bin/pip install -I biopython 
    my-python-env/bin/pip install -I nesoni


Using nesoni
============

Type

    nesoni

for usage instructions.

nesoni can also be used without installing, from the directory in
which you unpacked it:

    python -m nesoni




",2023-07-07 16:03:18+00:00
nessie,nessie,projectnessie/nessie,Nessie: Transactional Catalog for Data Lakes with Git-like semantics,https://projectnessie.org,False,657,2023-07-07 10:12:12+00:00,2020-04-09 18:39:03+00:00,91,28,42,72,nessie-0.64.0,2023-07-03 16:49:35+00:00,Apache License 2.0,6204,nessie-0.64.0,79,2023-07-03 16:49:35+00:00,2023-07-07 15:45:37+00:00,2023-07-07 15:08:47+00:00,"# Project Nessie

Project Nessie is a Transactional Catalog for Data Lakes with Git-like semantics.

[![Zulip](https://img.shields.io/badge/Zulip-Chat-blue?color=3d4db3&logo=zulip&style=for-the-badge&logoColor=white)](https://project-nessie.zulipchat.com/)
[![Group Discussion](https://img.shields.io/badge/Discussion-Groups-blue.svg?color=3d4db3&logo=google&style=for-the-badge&logoColor=white)](https://groups.google.com/g/projectnessie)
[![Twitter](https://img.shields.io/badge/Twitter-Follow_Us-blue?color=3d4db3&logo=twitter&style=for-the-badge&logoColor=white)](https://twitter.com/projectnessie)
[![Website](https://img.shields.io/badge/https-projectnessie.org-blue?color=3d4db3&logo=firefox&style=for-the-badge&logoColor=white)](https://projectnessie.org/)

[![Maven Central](https://img.shields.io/maven-central/v/org.projectnessie/nessie?label=Maven%20Central&logo=apachemaven&color=3f6ec6&style=for-the-badge&logoColor=white)](https://search.maven.org/artifact/org.projectnessie/nessie)
[![PyPI](https://img.shields.io/pypi/v/pynessie.svg?label=PyPI&logo=python&color=3f6ec6&style=for-the-badge&logoColor=white)](https://pypi.python.org/pypi/pynessie)
[![quay.io Docker](https://img.shields.io/maven-central/v/org.projectnessie/nessie?label=quay.io+Docker&logo=docker&color=3f6ec6&style=for-the-badge&logoColor=white)](https://quay.io/repository/projectnessie/nessie?tab=tags)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/nessie&color=3f6ec6&labelColor=&style=for-the-badge&logoColor=white)](https://artifacthub.io/packages/search?repo=nessie)
[![Swagger Hub](https://img.shields.io/badge/swagger%20hub-nessie-3f6ec6?style=for-the-badge&logo=swagger&link=https%3A%2F%2Fapp.swaggerhub.com%2Fapis%2Fprojectnessie%2Fnessie)](https://app.swaggerhub.com/apis/projectnessie/nessie)


[![Build Status](https://img.shields.io/github/actions/workflow/status/projectnessie/nessie/ci.yml?branch=main&label=Main%20CI&logo=Github&style=flat-square)](https://github.com/projectnessie/nessie/actions/workflows/ci.yml?query=branch%3Amain)
[![Query Engines CI](https://img.shields.io/github/actions/workflow/status/projectnessie/query-engine-integration-tests/main.yml?label=Nessie%2FIceberg%20in-dev&logo=Github&style=flat-square)](https://github.com/projectnessie/query-engine-integration-tests/actions/workflows/main.yml?query=branch%3Amain)
[![Java 17+18](https://img.shields.io/github/actions/workflow/status/projectnessie/nessie/newer-java.yml?label=Java%2017%2B&logo=Github&style=flat-square)](https://github.com/projectnessie/nessie/actions/workflows/newer-java.yml)
[![Windows Build](https://img.shields.io/github/actions/workflow/status/projectnessie/nessie/ci-win.yml?label=Windows&logo=windows&style=flat-square)](https://github.com/projectnessie/nessie/actions/workflows/ci-win.yml)
[![macOS Build](https://img.shields.io/github/actions/workflow/status/projectnessie/nessie/ci-mac.yml?label=macOS&logo=apple&style=flat-square)](https://github.com/projectnessie/nessie/actions/workflows/ci-mac.yml)

More information can be found at [projectnessie.org](https://projectnessie.org/).

Nessie supports Iceberg Tables/Views. Additionally, Nessie is focused on working with the widest range of tools possible, which can be seen in the [feature matrix](https://projectnessie.org/tools/#feature-matrix).

## Using Nessie

You can quickly get started with Nessie by using our small, fast docker image.

**IMPORTANT NOTE** Nessie moves away from `docker.io` to GitHub's container registry `ghcr.io`,
and also `quay.io`. Previous releases are already available on both ghcr.io and quay.io. Please
update references to `projectnessie/nessie` in your code to either `ghcr.io/projectnessie/nessie`
or `quay.io/projectnessie/nessie`.

```
docker pull ghcr.io/projectnessie/nessie
docker run -p 19120:19120 ghcr.io/projectnessie/nessie
```
_For trying Nessie image with different configuration options, refer to the templates under the [docker module](./docker#readme)._<br>

A local [Web UI](https://projectnessie.org/tools/ui/) will be available at this point.

Then install the Nessie CLI tool (to learn more about CLI tool and how to use it, check [Nessie CLI Documentation](https://projectnessie.org/tools/cli/)).

```
pip install pynessie
```

From there, you can use one of our technology integrations such those for 

* [Spark via Iceberg](https://projectnessie.org/tools/iceberg/spark/)
* [Hive via Iceberg](https://projectnessie.org/tools/iceberg/hive/)

To learn more about all supported integrations and tools, check [here](https://projectnessie.org/tools/) 

Have fun! We have a Google Group and a Slack channel we use for both developers and 
users. Check them out [here](https://projectnessie.org/community/).

### Authentication

By default, Nessie servers run with authentication disabled and all requests are processed under the ""anonymous""
user identity.

Nessie supports bearer tokens and uses [OpenID Connect](https://openid.net/connect/) for validating them.

Authentication can be enabled by setting the following Quarkus properties:
* `nessie.server.authentication.enabled=true`
* `quarkus.oidc.auth-server-url=<OpenID Server URL>`
* `quarkus.oidc.client-id=<Client ID>`

#### Experimenting with Nessie Authentication in Docker

One can start the `projectnessie/nessie` docker image in authenticated mode by setting
the properties mentioned above via docker environment variables. For example:

```shell
docker run -p 19120:19120 \
  -e QUARKUS_OIDC_CLIENT_ID=<Client ID> \
  -e QUARKUS_OIDC_AUTH_SERVER_URL=<OpenID Server URL> \
  -e NESSIE_SERVER_AUTHENTICATION_ENABLED=true \
  --network host \
  ghcr.io/projectnessie/nessie
```

## Building and Developing Nessie

### Requirements

- JDK 11 or higher: JDK11 or higher is needed to build Nessie (artifacts are built 
  for Java 8)

### Installation

Clone this repository:
```bash
git clone https://github.com/projectnessie/nessie
cd nessie
```

Then open the project in IntelliJ or Eclipse, or just use the IDEs to clone this github repository.

Refer to [CONTRIBUTING](./CONTRIBUTING.md) for build instructions.

### Compatibility

Nessie Iceberg's integration is compatible with Iceberg as in the following table:

| Nessie version | Iceberg version | Spark version                                                                                 | Hive version | Flink version          | Presto version                      | Trino version |
|----------------|-----------------|-----------------------------------------------------------------------------------------------|--------------|------------------------|-------------------------------------|---------------|
| 0.64.0         | 1.3.0           | 3.1.x (Scala 2.12), 3.2.x (Scala 2.12+2.13), 3.3.x (Scala 2.12+2.13), 3.4.x (Scala 2.12+2.13) | n/a          | 1.15.x, 1.16.x, 1.17.x | 0.277, 0.278.x, 0.279, 0.280, 0.281 | 419           |

### Distribution
To run:
1. configuration in `servers/quarkus-server/src/main/resources/application.properties`
2. execute `./gradlew quarkusDev`
3. go to `http://localhost:19120`

### UI 
To run the ui (from `ui` directory):
1. If you are running in test ensure that `setupProxy.js` points to the correct api instance. This ensures we avoid CORS
issues in testing
2. `npm install` will install dependencies
3. `npm run start` to start the ui in development mode via node

To deploy the ui (from `ui` directory):
1. `npm install` will install dependencies
2. `npm build` will minify and collect the package for deployment in `build`
3. the `build` directory can be deployed to any static hosting environment or run locally as `serve -s build`

### Docker image

Official Nessie images are built with support for [multiplatform builds](./tools/dockerbuild#readme). But to quickly
build a docker image for testing purposes, simply run the following command:

```shell
./gradlew :nessie-quarkus:clean :nessie-quarkus:quarkusBuild
docker build -f ./tools/dockerbuild/docker/Dockerfile-jvm -t nessie-unstable:latest ./servers/quarkus-server 
```

Check that your image is available locally:

```shell
docker images
```

You should see something like this:

```
REPOSITORY       TAG     IMAGE ID       CREATED          SIZE
nessie-unstable  latest  24bb4c7bd696   15 seconds ago   555MB
```

Once this is done you can run your image with `docker run -p 19120:19120 quay.io/nessie-unstable:latest`, passing the relevant
environment variables, if any. Environment variables names must follow MicroProfile Config's [mapping
rules](https://github.com/eclipse/microprofile-config/blob/master/spec/src/main/asciidoc/configsources.asciidoc#environment-variables-mapping-rules).

## Nessie related repositories

* [Nessie Demos](https://github.com/projectnessie/nessie-demos): Demos for Nessie
* [CEL Java](https://github.com/projectnessie/cel-java): Java port of the Common Expression Language
* [Nessie apprunner](https://github.com/projectnessie/nessie-apprunner): Maven and Gradle plugins to use Nessie in integration tests.

## Contributing

### Code Style

The Nessie project uses the Google Java Code Style, scalafmt and pep8.
See [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.

## Acknowledgements

See [ACKNOWLEDGEMENTS.md](ACKNOWLEDGEMENTS.md)
",2023-07-07 16:03:22+00:00
netflixconductor,conductor,Netflix/conductor,Conductor is a microservices orchestration engine.,https://conductor.netflix.com,False,9856,2023-07-07 11:48:36+00:00,2016-12-07 22:48:42+00:00,2172,499,214,316,v3.13.7,2023-06-08 14:21:56+00:00,Apache License 2.0,3221,v3.13.7,388,2023-06-08 14:21:56+00:00,2023-07-07 11:02:14+00:00,2023-07-07 08:02:39+00:00,"![Conductor](docs/docs/img/logo.png)

# Conductor
[![Github release](https://img.shields.io/github/v/release/Netflix/conductor.svg)](https://GitHub.com/Netflix/conductor/releases)
[![CI](https://github.com/Netflix/conductor/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/Netflix/conductor/actions/workflows/ci.yml)
[![License](https://img.shields.io/github/license/Netflix/conductor.svg)](http://www.apache.org/licenses/LICENSE-2.0)
[![NetflixOSS Lifecycle](https://img.shields.io/osslifecycle/Netflix/conductor.svg)]()

[![GitHub stars](https://img.shields.io/github/stars/Netflix/conductor.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/Netflix/conductor/stargazers/)
[![GitHub forks](https://img.shields.io/github/forks/Netflix/conductor.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/Netflix/conductor/network/)


Conductor is a platform created by Netflix to orchestrate workflows that span across microservices.
Conductor is maintained by Media Workflow Infrastructure team at Netflix.

For more information, see [Main Documentation Site](https://conductor.netflix.com/)


## Releases
The latest version is [![Github release](https://img.shields.io/github/v/release/Netflix/conductor.svg)](https://GitHub.com/Netflix/conductor/releases)

[2.31.8](https://github.com/Netflix/conductor/releases/tag/v2.31.8) is the **final** release of `2.31` branch. As of Feb 2022, `1.x` & `2.x` versions are no longer supported.

## Resources
#### [Slack Community](https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg)
We have an active [community](https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg) of Conductor users and contributors on the channel.
#### [Documentation Site](https://conductor.netflix.com/)
[Documentation](https://conductor.netflix.com/) and tutorial on how to use Conductor

## Workflow Creation in Code
Conductor supports creating workflows using JSON and Code.  
SDK support for creating workflows using code is available in multiple languages and can be found at https://github.com/conductor-sdk

## Community Contributions
The modules contributed by the community are housed at [conductor-community](https://github.com/Netflix/conductor-community). Compatible versions of the community modules are released simultaneously with releases of the main modules.

[Discussion Forum](https://github.com/Netflix/conductor/discussions): Please use the forum for questions and discussing ideas and join the community.

[List of Conductor community projects](/docs/docs/resources/related.md): Backup tool, Cron like workflow starter, Docker containers and more.

## Getting Started - Building & Running Conductor
###  Using Docker:
The easiest way to get started is with Docker containers. Please follow the instructions [here](https://conductor.netflix.com/gettingstarted/docker.html). 

###  From Source:
Conductor Server is a [Spring Boot](https://spring.io/projects/spring-boot) project and follows all applicable conventions. See instructions [here](http://conductor.netflix.com/gettingstarted/source.html).

## Published Artifacts
Binaries are available from [Netflix OSS Maven](https://artifacts.netflix.net/netflixoss/com/netflix/conductor/) repository, or the [Maven Central Repository](https://search.maven.org/search?q=g:com.netflix.conductor).

| Artifact                        | Description                                                                                     |
|---------------------------------|-------------------------------------------------------------------------------------------------|
| conductor-common                | Common models used by various conductor modules                                                 |
| conductor-core                  | Core Conductor module                                                                           |
| conductor-redis-persistence     | Persistence and queue using Redis/Dynomite                                                      |
| conductor-cassandra-persistence | Persistence using Cassandra                                                                     |
| conductor-es6-persistence       | Indexing using Elasticsearch 6.X                                                                |
| conductor-rest                  | Spring MVC resources for the core services                                                      |
| conductor-ui                    | node.js based UI for Conductor                                                                  |
| conductor-client                | Java client for Conductor that includes helpers for running worker tasks                        |
| conductor-client-spring         | Client starter kit for Spring                                                                   |
| conductor-java-sdk              | SDK for writing workflows in code                                                               |
| conductor-server                | Spring Boot Web Application                                                                     |
| conductor-redis-lock            | Workflow execution lock implementation using Redis                                              |
| conductor-awss3-storage         | External payload storage implementation using AWS S3                                            |
| conductor-awssqs-event-queue    | Event queue implementation using AWS SQS                                                        |
| conductor-http-task             | Workflow system task implementation to send make requests                                       |
| conductor-json-jq-task          | Workflow system task implementation to evaluate JSON using [jq](https://stedolan.github.io/jq/) |
| conductor-grpc                  | Protobuf models used by the server and client                                                   |
| conductor-grpc-client           | gRPC client to interact with the gRPC server                                                    |
| conductor-grpc-server           | gRPC server Application                                                                         |
| conductor-test-harness          | Integration and regression tests                                                                |

## Database Requirements

* The default persistence used is Redis
* The indexing backend is [Elasticsearch](https://www.elastic.co/) (6.x)

## Other Requirements
* JDK 11+
* UI requires Node 14 to build. Earlier Node versions may work but is untested.

## Get Support
There are several ways to get in touch with us:
* [Slack Community](https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg)
* [GitHub Discussion Forum](https://github.com/Netflix/conductor/discussions)

## Contributions
Whether it is a small documentation correction, bug fix or a new feature, contributions are highly appreciated. We just ask you to follow standard OSS guidelines. The [Discussion Forum](https://github.com/Netflix/conductor/discussions) is a good place to ask questions, discuss new features and explore ideas. Please check with us before spending too much time, only to find out later that someone else is already working on a similar feature.

`main` branch is the current working branch. Please send your PR's to `main` branch, making sure that it builds on your local system successfully. Also, please make sure all the conflicts are resolved.

## License
Copyright 2022 Netflix, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
",2023-07-07 16:03:27+00:00
nextflow,nextflow,nextflow-io/nextflow,A DSL for data-driven computational pipelines,http://nextflow.io,False,2082,2023-07-07 13:11:45+00:00,2013-03-27 11:17:33+00:00,547,86,133,239,v23.04.2,2023-06-08 10:01:14+00:00,Apache License 2.0,5940,v23.06.0-edge,270,2023-06-14 21:11:41+00:00,2023-07-07 15:58:48+00:00,2023-07-07 05:55:05+00:00,"![Nextflow logo](https://github.com/nextflow-io/trademark/blob/master/nextflow2014_no-bg.png)

*""Dataflow variables are spectacularly expressive in concurrent programming""*
<br>[Henri E. Bal , Jennifer G. Steiner , Andrew S. Tanenbaum](https://dl.acm.org/doi/abs/10.1145/72551.72552)


![Nextflow CI](https://github.com/nextflow-io/nextflow/workflows/Nextflow%20CI/badge.svg)
[![Nextflow version](https://img.shields.io/github/release/nextflow-io/nextflow.svg?colorB=26af64&style=popout)](https://github.com/nextflow-io/nextflow/releases/latest)
[![Nextflow Twitter](https://img.shields.io/twitter/url/https/nextflowio.svg?colorB=26af64&&label=%40nextflow&style=popout)](https://twitter.com/nextflowio)
[![Nextflow Publication](https://img.shields.io/badge/Published-Nature%20Biotechnology-26af64.svg?colorB=26af64&style=popout)](https://www.nature.com/articles/nbt.3820)
[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?colorB=26af64&style=popout)](http://bioconda.github.io/recipes/nextflow/README.html)
[![Nextflow license](https://img.shields.io/github/license/nextflow-io/nextflow.svg?colorB=26af64&style=popout)](https://github.com/nextflow-io/nextflow/blob/master/COPYING)

Quick overview
==============
Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows.
It supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch,
Google Cloud Life Sciences, and Kubernetes. Additionally, it provides support for manage your workflow dependencies
through built-in support for Conda, Spack, Docker, Podman, Singularity, Modules, and more.

## Contents
- [Rationale](#rationale)
- [Quick start](#quick-start)
- [Documentation](#documentation)
- [Tool Management](#tool-management)
  - [Conda environments](#conda-environments)
  - [Spack environments](#spack-environments)
  - [Docker, Podman and Singularity](#containers)
  - [Environment Modules](#environment-modules)
- [HPC Schedulers](#hpc-schedulers)
  - [SGE](#hpc-schedulers)
  - [Univa Grid Engine](#hpc-schedulers)
  - [LSF](#hpc-schedulers)
  - [SLURM](#hpc-schedulers)
  - [PBS/Torque](#hpc-schedulers)
  - [HyperQueue (experimental)](#hpc-schedulers)
  - [HTCondor (experimental)](#hpc-schedulers)
  - [Moab](#hpc-schedulers)
- [Cloud Support](#cloud-support)
  - [AWS Batch](#cloud-support)
  - [Google Cloud Batch](#cloud-support)
  - [Google Life Sciences](#cloud-support)
  - [Kubernetes](#cloud-support)
- [Community](#community)
- [Build from source](#build-from-source)
- [Contributing](#contributing)
- [License](#license)
- [Citations](#citations)
- [Credits](#credits)


Rationale
=========

With the rise of big data, techniques to analyse and run experiments on large datasets are increasingly necessary.

Parallelization and distributed computing are the best ways to tackle this problem, but the tools commonly available to the bioinformatics community often lack good support for these techniques, or provide a model that fits badly with the specific requirements in the bioinformatics domain and, most of the time, require the knowledge of complex tools or low-level APIs.

Nextflow framework is based on the dataflow programming model, which greatly simplifies writing parallel and distributed pipelines without adding unnecessary complexity and letting you concentrate on the flow of data, i.e. the functional logic of the application/algorithm.

It doesn't aim to be another pipeline scripting language yet, but it is built around the idea that the Linux platform is the *lingua franca* of data science, since it provides many simple command line and scripting tools, which by themselves are powerful, but when chained together facilitate complex data manipulations.

In practice, this means that a Nextflow script is defined by composing many different processes. Each process can execute a given bioinformatics tool or scripting language, to which is added the ability to coordinate and synchronize the processes execution by simply specifying their inputs and outputs.



Quick start
============

Download the package
--------------------

Nextflow does not require any installation procedure, just download the distribution package by copying and pasting
this command in your terminal:

```
curl -fsSL https://get.nextflow.io | bash
```

It creates the ``nextflow`` executable file in the current directory. You may want to move it to a folder accessible from your ``$PATH``.

Download from Conda
-------------------

Nextflow can also be installed from Bioconda

```
conda install -c bioconda nextflow 
```

Documentation
=============

Nextflow documentation is available at this link http://docs.nextflow.io


HPC Schedulers
==============

*Nextflow* supports common HPC schedulers, abstracting the submission of jobs from the user. 

Currently the following clusters are supported:

  + [SGE](https://www.nextflow.io/docs/latest/executor.html#sge)
  + [Univa Grid Engine](https://www.nextflow.io/docs/latest/executor.html#sge)
  + [LSF](https://www.nextflow.io/docs/latest/executor.html#lsf)
  + [SLURM](https://www.nextflow.io/docs/latest/executor.html#slurm)
  + [PBS/Torque](https://www.nextflow.io/docs/latest/executor.html#pbs-torque)
  + [Flux](https://www.nextflow.io/docs/latest/executor.html#flux)
  + [HyperQueue (beta)](https://www.nextflow.io/docs/latest/executor.html#hyperqueue)
  + [HTCondor (beta)](https://www.nextflow.io/docs/latest/executor.html#htcondor)
  + [Moab](https://www.nextflow.io/docs/latest/executor.html#moab)

For example to submit the execution to a SGE cluster create a file named `nextflow.config`, in the directory
where the pipeline is going to be launched, with the following content:

```nextflow
process {
  executor='sge'
  queue='<your execution queue>'
}
```

In doing that, processes will be executed by Nextflow as SGE jobs using the `qsub` command. Your 
pipeline will behave like any other SGE job script, with the benefit that *Nextflow* will 
automatically and transparently manage the processes synchronisation, file(s) staging/un-staging, etc.  


Cloud support
=============
*Nextflow* also supports running workflows across various clouds and cloud technologies. Managed solutions from major 
cloud providers are also supported through AWS Batch, Azure Batch and Google Cloud compute services. 
Additionally, *Nextflow* can run workflows on either on-prem or managed cloud Kubernetes clusters. 

Currently supported cloud platforms:
  + [AWS Batch](https://www.nextflow.io/docs/latest/awscloud.html#aws-batch)
  + [Azure Batch](https://azure.microsoft.com/en-us/services/batch/)
  + [Google Cloud Batch](https://cloud.google.com/batch)
  + [Google Cloud Life Sciences](https://cloud.google.com/life-sciences)
  + [Kubernetes](https://www.nextflow.io/docs/latest/kubernetes.html)



Tool management
================

Containers
----------------

*Nextflow* has first class support for containerization. It supports both [Docker](https://www.nextflow.io/docs/latest/docker.html) and [Singularity](https://www.nextflow.io/docs/latest/singularity.html) container engines. Additionally, *Nextflow* can easily switch between container engines enabling workflow portability. 

```nextflow
process samtools {
  container 'biocontainers/samtools:1.3.1'

  """"""
  samtools --version 
  """"""

}
```

Conda environments
------------------

[Conda environments](https://www.nextflow.io/docs/latest/conda.html) provide another option for managing software packages in your workflow. 


Spack environments
------------------

[Spack environments](https://www.nextflow.io/docs/latest/spack.html) provide an option to build software packages from source using Spack, a popular package manager within the HPC community.


Environment Modules
-------

[Environment modules](https://www.nextflow.io/docs/latest/process.html#module) commonly found in HPC environments can also be used to manage the tools used in a *Nextflow* workflow. 


Community
=========

You can post questions, or report problems by using the Nextflow [discussions](https://github.com/nextflow-io/nextflow/discussions)
or the Nextflow [Slack community chat](https://www.nextflow.io/slack-invite.html).

*Nextflow* also hosts a yearly workshop showcasing researcher's workflows and advancements in the langauge. Talks from the past workshops are available on the [Nextflow YouTube Channel](https://www.youtube.com/channel/UCB-5LCKLdTKVn2F4V4KlPbQ)

The [nf-core](https://nf-co.re/) project is a community effort aggregating high quality *Nextflow* workflows which can be used by the community. 


Build from source
=================

Required dependencies
---------------------

* Compiler Java 11 or later
* Runtime Java 11 or later

Build from source
-----------------

*Nextflow* is written in [Groovy](http://groovy-lang.org) (a scripting language for the JVM). A pre-compiled,
ready-to-run, package is available at the [Github releases page](https://github.com/nextflow-io/nextflow/releases),
thus it is not necessary to compile it in order to use it.

If you are interested in modifying the source code, or contributing to the project, it worth knowing that
the build process is based on the [Gradle](http://www.gradle.org/) build automation system.

You can compile *Nextflow* by typing the following command in the project home directory on your computer:

```bash
make compile
```

The very first time you run it, it will automatically download all the libraries required by the build process.
It may take some minutes to complete.

When complete, execute the program by using the `launch.sh` script in the project directory.

The self-contained runnable Nextflow packages can be created by using the following command:

```bash
make pack
```            

Once compiled use the script `./launch.sh` as a replacement for the usual `nextflow` command.

The compiled packages can be locally installed using the following command:

```bash
make install
```

A self-contained distribution can be created with the command: `make pack`.  To include support of GA4GH and its dependencies in the binary, use `make packGA4GH` instead.


IntelliJ IDEA
---------------

Nextflow development with [IntelliJ IDEA](https://www.jetbrains.com/idea/) requires a recent version of the IDE (2019.1.2 or later).

If you have it installed in your computer, follow the steps below in order to use it with Nextflow:

1. Clone the Nextflow repository to a directory in your computer.
2. Open IntelliJ IDEA and choose ""New > Project from Existing Sources..."" in the ""File"" menu bar.
3. Select the Nextflow project root directory in your computer and click ""OK"".
4. Then, choose the ""Gradle"" item in the ""Import project from external model"" list and click on ""Finish"" button to finalize the import.
5. When the import process completes, select the ""Project structure"" command in the ""File"" menu bar.
6. In the showed dialog click on the ""Project"" item in the list of the left, and make sure that
   the ""Project SDK"" choice on the right contains Java 11 (or later, up to 18).
7. Set the code formatting options with settings provided [here](https://github.com/nextflow-io/nextflow/blob/master/CONTRIBUTING.md#ide-settings).



Contributing
============

Project contribution are more than welcome. See the [CONTRIBUTING](CONTRIBUTING.md) file for details.


Build servers
=============

  * [Travis-CI](https://travis-ci.org/nextflow-io/nextflow)
  * [GitHub Actions](https://github.com/nextflow-io/nextflow/actions)

License
=======

The *Nextflow* framework is released under the Apache 2.0 license.

Citations
=========

If you use Nextflow in your research, please cite:

P. Di Tommaso, et al. Nextflow enables reproducible computational workflows. Nature Biotechnology 35, 316–319 (2017) doi:[10.1038/nbt.3820](http://www.nature.com/nbt/journal/v35/n4/full/nbt.3820.html)

Credits
=======

Nextflow is built on two great pieces of open source software, namely <a href='http://groovy-lang.org' target='_blank'>Groovy</a>
and <a href='http://www.gpars.org/' target='_blank'>Gpars</a>.

YourKit is kindly supporting this open source project with its full-featured Java Profiler.
Read more http://www.yourkit.com
",2023-07-07 16:03:31+00:00
ngless,ngless,ngless-toolkit/ngless,NGLess: NGS with less work,https://ngless.embl.de,False,140,2023-07-05 21:50:45+00:00,2013-01-08 15:56:55+00:00,24,11,8,21,v1.5.0,2022-09-14 10:03:23+00:00,Other,2444,v1.5.0,27,2022-09-13 23:15:43+00:00,2023-07-05 21:50:45+00:00,2023-03-30 06:56:45+00:00,"# NGLess: NGS Processing with Less Work

![NGLess logo](NGLess-logo-128x64.png) Ngless is a domain-specific language for
NGS (next-generation sequencing data) processing.

[![Build & test](https://github.com/ngless-toolkit/ngless/actions/workflows/build_w_nix.yml/badge.svg)](https://github.com/ngless-toolkit/ngless/actions/workflows/build_w_nix.yml)
[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/hyperium/hyper/master/LICENSE)
[![Install with Bioconda](https://anaconda.org/bioconda/ngless/badges/version.svg)](https://anaconda.org/bioconda/ngless)
[![Install with Bioconda](https://anaconda.org/bioconda/ngless/badges/downloads.svg)](https://anaconda.org/bioconda/ngless)
[![Citation for NGLess](https://img.shields.io/badge/CITATION-DOI%3A10.1186%252Fs40168--019--0684--8-brightgreen.svg)](https://doi.org/10.1186/s40168-019-0684-8)


For questions and discussions, please use the [NGLess mailing
list](https://groups.google.com/forum/#!forum/ngless).

If you are using NGLess, please cite:

> _NG-meta-profiler: fast processing of metagenomes using NGLess, a
> domain-specific language_ by Luis Pedro Coelho, Renato Alves, Paulo Monteiro,
> Jaime Huerta-Cepas, Ana Teresa Freitas, Peer Bork, Microbiome (2019)
> [https://doi.org/10.1186/s40168-019-0684-8](https://doi.org/10.1186/s40168-019-0684-8)

![NGLess cartoon](docs/NGLess-cartoon.svg)

## Example

    ngless ""1.5""
    input = fastq(['ctrl1.fq','ctrl2.fq','stim1.fq','stim2.fq'])
    input = preprocess(input) using |read|:
        read = read[5:]
        read = substrim(read, min_quality=26)
        if len(read) < 31:
            discard

    mapped = map(input,
                    reference='hg19')
    write(count(mapped, features=['gene']),
            ofile='gene_counts.csv',
            format={csv})


For more information, check [the docs](https://ngless.embl.de). We also have [a
YouTube
tutorial](https://www.youtube.com/playlist?list=PLn-ZqA9cHNdSsmVTojYL1lEcfh-J3Hdff)
on how to use NGLess and [SemiBin](https://semibin.rtfd.io/) together (but you
can learn to use NGLess independently of SemiBin).

## Installing

See the [install documentation](https://ngless.embl.de/install.html) for more
information.

### Bioconda

The recommended way to install NGLess is through
[bioconda](https://bioconda.github.io):

    conda install -c bioconda ngless 

### Docker

Alternatively, a docker container with NGLess is available at
[docker hub](https://hub.docker.com/r/nglesstoolkit/ngless):

    docker run -v $PWD:/workdir -w /workdir -it nglesstoolkit/ngless:1.5.0 ngless --version

Adapt the mount flags (``-v``) as needed.

### Linux

You can download a [statically linked version of NGless
1.5.0](https://github.com/ngless-toolkit/ngless/releases/download/v1.5.0/NGLess-v1.5.0-Linux-static-full)

This should work across a wide range of Linux versions (please
[report](https://github.com/ngless-toolkit/ngless/issues) any issues you encounter):

    curl -L -O https://github.com/ngless-toolkit/ngless/releases/download/v1.5.0/NGLess-v1.5.0-Linux-static-full
    chmod +x NGLess-v1.5.0-Linux-static-full
    ./NGLess-v1.5.0-Linux-static-full

This downloaded file bundles bwa, samtools and megahit (also statically linked).

### From Source

Installing/compiling from source is also possible. Clone
[https://github.com/ngless-toolkit/ngless](https://github.com/ngless-toolkit/ngless)

#### Dependencies

The simplest way to get an environment with all the dependencies is to use conda:

    conda create -n ngless
    conda activate ngless
    conda config --add channels conda-forge
    conda install stack cairo bzip2 gmp zlib perl wget xz pkg-config make

You should have `gcc` installed (or another C-compiler).

The following sequence of commands should download and build the software

    git clone https://github.com/ngless-toolkit/ngless
    cd ngless
    stack setup
    make

To install, you can use the following command (replace `<PREFIX>` with
the directory where you wish to install, default is `/usr/local`):

    make make

## Running Sample Test Scripts on Local Machine

For developers who have successfully compiled and installed NGless, running the
test scripts in the `tests` folder would be the next line of action to have the
output of sample test cases.

    cd tests

Once in the `tests` directory, select any of the test folders to run NGless.

For example, here we would run the `regression-fqgz` test:

    cd regression-fqgz
    ngless ungzip.ngl

After running this script open the newly generated folder `ungzip.ngl.output_ngless` and view the template in the **index.html** file.

For developers who have done this much more datasets for testing purposes can be referenced and used by reading these documentation links:
**[Human Gut Metagenomics Functional & Taxonomic Profiling](https://ngless.embl.de/tutorial-gut-metagenomics.html#)**
**[Ocean Metagenomics Functional Profiling](https://ngless.embl.de/tutorial-ocean-metagenomics.html)**
**[Ocean Metagenomics Assembly and Gene Prediction](https://ngless.embl.de/tutorial-assembly-gp.html)**


## More information

- [Full documentation](https://ngless.embl.de/)
- [Frequently Asked Questions (FAQ)](https://ngless.embl.de/faq.html)
- [ngless mailing list](https://groups.google.com/forum/#!forum/ngless)
- [What's new log](https://ngless.embl.de/whatsnew.html)
- [NGless 1.5.0 Release Documentation](https://ngless.embl.de/whatsnew.html#version-1-5-0)

## Authors

- [Luis Pedro Coelho](https://luispedro.org) (email: [luispedro@big-data-biology.org](mailto:luispedro@big-data-biology.org)) (on twitter: [@luispedrocoelho](https://twitter.com/luispedrocoelho))
- Paulo Monteiro
-  Renato Alves
- [Ana Teresa Freitas](https://web.tecnico.ulisboa.pt/ana.freitas/)
-  Peer Bork

",2023-07-07 16:03:36+00:00
ngsane,ngsane,BauerLab/ngsane,Analysis Framework for Biological Data from High Throughput Experiments,,False,34,2021-10-28 20:06:26+00:00,2013-04-15 23:31:55+00:00,22,16,6,9,v0.5.2.0,2015-08-10 11:51:56+00:00,Other,1869,v0.5.2.0,18,2015-08-10 11:51:56+00:00,,2016-08-24 02:36:54+00:00,"NGSANE is a framework for advanced production informatics of Next Generation 
Sequencing libraries.

Version: v0.5.2.0


################################################################################
# Associated publication

""NGSANE: A Lightweight Production Informatics Framework for High Throughput 
Data Analysis""
Buske FA, French HJ, Smith MA, Clark SJ, Bauer DC 
Bioinformatics 2014 May 15;30(10):1471-2. doi: 10.1093/bioinformatics/btu036. 
Epub 2014 Jan 26.

################################################################################
# Setup:

Define the environment variable NGSANE_BASE and point it to the directory that
contains the NGSANE framework. You may want to set up $NGSANE_BASE environment 
variable on the cluster or in your .bash_rc or .profile You can also specify it 
in the project-based config file.

For more information check out the wiki at
https://github.com/BauerLab/ngsane/wiki

################################################################################
# Structure of NGSANE:

ngsane
- bin/ 
	contains the trigger.sh script that is used to launch any and all 
	jobs by supplying an appropriate config file

- conf/ 
	contains the config file for the cluster environment at hand. 
	Rename the sample_header.sh template into header.sh and populate the 
	variables contained within with the values specific to your system

- core/ 
	contains the core scripts handling job submission, logging and report 
        generation etc.
	
- doc/
	contains documents/presentations/publication describing the framework
	
- mods/ 
	contains all modules currently available from within NGSANE
	
- sampleConfigs/
	contains a sample config file for various pipelines/modules than can be 
	triggered in the NGSANE framework
	copy the corresponding file(s) to your data and customise as appropriate
	
- tools/
	contains various helper scripts used within the modules, 
	mostly tapping into Python and R

################################################################################
# How to create a new project:

NGSANE requires a config file for each of your projects and a (set of) fastq 
files that need to be placed in a simple but predefined structure.

Make a new folder for your project and create the following folder structure:
<PROJECT>
- fastq
- - <EXPERIMENT1>
- - - <LIBRARY1_READ1>.fastq[.gz]
- - - <LIBRARY1_READ2>.fastq[.gz] (if paired library)
- - - <LIBRARY2_READ1>.fastq[.gz]
- - - <LIBRARY2_READ2>.fastq[.gz] (if paired library)
- config.txt

Here <PROJECT> is your project name. The fastq folder contains all raw data
grouped by EXPERIMENT (or any other meaningful way). Each EXPERIMENT folder
can contain multiple sequencing libraries. You probably want them gzipped to
save space. You can add additional EXPERIMENT folders if you like. However, 
all libraries should have the same ending as well as the same read-pair 
identification (e.g. have the suffix _R1.fastq.gz for read1 of a zipped fastq).
for examples of config files for different pipelines look in the 
ngsane/sampleConfigs/.

################################################################################
# How to run NGSANE jobs:

The main access point is trigger.sh, which can be invoked to run in different
modes
>trigger.sh <CONFIG> <TASK>

options for TASK:
  [empty]    start a dry-run: create folders,prints what will be done
  new        detect new data that has not been processed yet.
  fetchdata  get data from remote server (via smbclient)
  pushresult puts results to remote server (via smbclient)
  armed      submit tasks to the queue
  direct     run task directly (e.g. on node after qrsh)
  postonly   run only the post analysis steps of a task (if available)
  verify     check the pbs logfiles for errors
  recover    pick up unfinished business (interrupted jobs)
  html       check the pbs logfiles for errors and and make summary HTML page
  report     alias for html
  trackhubs      generate trackhubs
  clean          removes all dummy files

Following the folder structure example from above, you can submit jobs
using a properly configured config.txt form within the PROJECT folder as 
follows:

PROJECT>trigger.sh ./config.txt armed

################################################################################	
Contact:
	Denis.Bauer - at - csiro.au
	f.buske - at - garvan.org.au
",2023-07-07 16:03:40+00:00
niassa,niassa-docs,morgantaschuk/niassa-docs,Host the website for Niassa,https://morgantaschuk.com/niassa-docs/,False,0,2022-11-22 20:55:18+00:00,2018-09-19 21:52:38+00:00,0,1,1,0,,,GNU General Public License v3.0,14,,0,,,2018-10-23 20:57:13+00:00,"# Niassa Documentation

Hosts the website for [Niassa](https://www.github.com/oicr-gsi/niassa). Much of
the documentation was copied from the earlier 
[SeqWare](https://www.github.com/seqware/seqware) documentation and modified 
heavily to reflect the changes and ways we use Niassa at OICR.

The website is built using [Jekyll](https://jekyllrb.com/) and currently uses
a modification of the default Minima template.


## Build

```
bundle install
```

## Serve (for testing/development) 

We use --incremental build because of some of the Liquid tags we've added that 
take a while to render all pages (can be up to a minute).

```
bundle exec jekyll serve --incremental
```

## Developer documentation

### Creating documentation for a new version

Different versions of documentation should be hosted in a directory 
corresponding to the version number. To start the documentation for a new 
version, recursively copy the entire directory for the previous version into the 
version number of your choice. 

```bash
# to make docs for version 2.0.4
cp -r 2.0.2 2.0.4
```

You do *not* need to do a massive find-replace for version numbers: this should
already be handled by the Liquid tags described below.

### Liquid tags to automatically fill in version numbers

At the top of each page in a version directory, you will see an `include` tag 
for [functions.liquid](_includes/functions.liquid), 
which contains functions to automatically set the URL and version numbers in
versioned subdirectories:

```
{% include functions.liquid %}
```

* **version_url**: When linking within the version directory, use the variable 
  `{{version_url}}` to fill in the absolute path to the root of the version 
  directory. e.g. Rather than linking to the Pipeline page with 
  `[Pipeline]( ""/2.0.2/pipeline"" | absolute_url)`, use 
  `[Pipeline]({{version_url}}/pipeline)`.
* **version**: fills in the version according to the directory name sitting 
  in basedir. e.g. Rather than `seqware-distribution-2.0.2-full.jar`, use 
  `seqware-distribution-{{version}}-full.jar`.
  
### 'current' redirects

The `current` directory contains redirect pages that will automatically redirect
to subdirectory that matches the `current_version` in 
[_config.yml](_config.yml). The correct relative page needs to exist in 
[current](current) for it to redirect properly. 

### The nav bar

For items to appear in navigation, add `nav: true` to their front matter. Only
pages in the `current_version` directory in [_config.yml](_config.yml) will be
included.

## License

The Niassa website is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

The Niassa website is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the [GNU General Public License](LICENSE)
along with the Niassa website.  If not, see <http://www.gnu.org/licenses/>.

## Contact

The Niassa website is built and maintained by the
[Genome Sequence Informatics](https://gsi.oicr.on.ca) group at
[Ontario Institute for Cancer Research](https://oicr.on.ca). Get in touch by
submitting a question or issue to our
[Github issue tracker](https://github.com/oicr-gsi/niassa-docs/issues).
",2023-07-07 16:03:44+00:00
apachenifi,nifi,apache/nifi,Apache NiFi,https://nifi.apache.org/,False,3864,2023-07-07 09:24:36+00:00,2014-12-12 08:00:05+00:00,2487,188,319,0,,,Apache License 2.0,8627,support/nifi-1.11.1,151,2020-01-31 17:32:10+00:00,2023-07-07 15:26:02+00:00,2023-07-06 21:30:34+00:00,"<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the ""License""); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at
      http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
[<img src=""https://nifi.apache.org/assets/images/apache-nifi-logo.svg"" width=""300"" height=""126"" alt=""Apache NiFi""/>][nifi]

[![ci-workflow](https://github.com/apache/nifi/workflows/ci-workflow/badge.svg)](https://github.com/apache/nifi/actions/workflows/ci-workflow.yml)
[![system-tests](https://github.com/apache/nifi/workflows/system-tests/badge.svg)](https://github.com/apache/nifi/actions/workflows/system-tests.yml)
[![Docker pulls](https://img.shields.io/docker/pulls/apache/nifi.svg)](https://hub.docker.com/r/apache/nifi/)
[![Version](https://img.shields.io/maven-central/v/org.apache.nifi/nifi-utils.svg)](https://nifi.apache.org/download.html)
[![Slack](https://img.shields.io/badge/chat-on%20Slack-brightgreen.svg)](https://s.apache.org/nifi-community-slack)

[Apache NiFi](https://nifi.apache.org/) is an easy to use, powerful, and
reliable system to process and distribute data.

## Table of Contents

- [Features](#features)
- [Requirements](#requirements)
- [Getting Started](#getting-started)
- [MiNiFi subproject](#minifi-subproject)
- [Registry subproject](#registry-subproject)
- [Getting Help](#getting-help)
- [Documentation](#documentation)
- [License](#license)
- [Export Control](#export-control)

## Features

Apache NiFi was made for dataflow. It supports highly configurable directed graphs of data routing, transformation, and system mediation logic. Some of its key features include:

- Web-based user interface
  - Seamless experience for design, control, and monitoring
  - Multi-tenant user experience
- Highly configurable
  - Loss tolerant vs guaranteed delivery
  - Low latency vs high throughput
  - Dynamic prioritization
  - Flows can be modified at runtime
  - Back pressure
  - Scales up to leverage full machine capability
  - Scales out with zero-leader clustering model
- Data Provenance
  - Track dataflow from beginning to end
- Designed for extension
  - Build your own processors and more
  - Enables rapid development and effective testing
- Secure
  - SSL, SSH, HTTPS, encrypted content, etc...
  - Pluggable fine-grained role-based authentication/authorization
  - Multiple teams can manage and share specific portions of the flow

## Minimum Recommendations
* JDK 17.0.6
* Apache Maven 3.9.2

## Minimum Requirements
* JDK 17.0.6
* Apache Maven 3.9.2

## Getting Started

Read through the [quickstart guide for development](https://nifi.apache.org/quickstart.html).
It will include information on getting a local copy of the source, give pointers on issue
tracking, and provide some warnings about common problems with development environments.

For a more comprehensive guide to development and information about contributing to the project
read through the [NiFi Developer's Guide](https://nifi.apache.org/developer-guide.html).

### Building

Run the following Maven command to build standard project modules using parallel execution:

    ./mvnw clean install -T2C

Run the following Maven command to build project modules with static analysis to confirm compliance with code and licensing requirements:

    ./mvnw clean install -T2C -P contrib-check

Building on Microsoft Windows requires using `mvnw.cmd` instead of `mwnw` to run the Maven Wrapper.

### Deploying

Change directories to `nifi-assembly`. The `target` directory contains binary archives.

    laptop:nifi myuser$ cd nifi-assembly
    laptop:nifi-assembly myuser$ ls -lhd target/nifi*
    drwxr-xr-x  3 myuser  mygroup   102B Apr 30 00:29 target/nifi-1.0.0-SNAPSHOT-bin
    -rw-r--r--  1 myuser  mygroup   144M Apr 30 00:30 target/nifi-1.0.0-SNAPSHOT-bin.tar.gz
    -rw-r--r--  1 myuser  mygroup   144M Apr 30 00:30 target/nifi-1.0.0-SNAPSHOT-bin.zip

Copy the `nifi-VERSION-bin.tar.gz` or `nifi-VERSION-bin.zip` to a separate deployment directory.
Extracting the distribution will create a new directory named for the version.

    laptop:nifi-assembly myuser$ mkdir ~/example-nifi-deploy
    laptop:nifi-assembly myuser$ tar xzf target/nifi-*-bin.tar.gz -C ~/example-nifi-deploy
    laptop:nifi-assembly myuser$ ls -lh ~/example-nifi-deploy/
    total 0
    drwxr-xr-x  10 myuser  mygroup   340B Apr 30 01:06 nifi-1.0.0-SNAPSHOT

### Starting

Change directories to the deployment location and run the following command to start NiFi.

    laptop:~ myuser$ cd ~/example-nifi-deploy/nifi-*
    laptop:nifi-1.0.0-SNAPSHOT myuser$ ./bin/nifi.sh start
  
Running `bin/nifi.sh start` starts NiFi in the background and exits. Use `--wait-for-init` with an optional timeout in
seconds to wait for a complete startup before exiting.

    laptop:nifi-1.0.0-SNAPSHOT myuser$ ./bin/nifi.sh start --wait-for-init 120
  
### Authenticating

The default configuration generates a random username and password on startup. NiFi writes the generated credentials
to the application log located in `logs/nifi-app.log` under the NiFi installation directory.

The following command can be used to find the generated credentials on operating systems with `grep` installed:

    laptop:nifi-1.0.0-SNAPSHOT myuser$ grep Generated logs/nifi-app*log

NiFi logs the generated credentials as follows:

    Generated Username [USERNAME]
    Generated Password [PASSWORD]

The `USERNAME` will be a random UUID composed of 36 characters. The `PASSWORD` will be a random string composed of
32 characters. The generated credentials will be stored in `conf/login-identity-providers.xml` with the password stored
using bcrypt hashing. Record these credentials in a secure location for access to NiFi.

The random username and password can be replaced with custom credentials using the following command:

    ./bin/nifi.sh set-single-user-credentials <username> <password>
  
### Running

Open the following link in a web browser to access NiFi: https://localhost:8443/nifi

The web browser will display a warning message indicating a potential security risk due to the self-signed
certificate NiFi generated during initialization. Accepting the potential security risk and continuing to load
the interface is an option for initial development installations. Production deployments should provision a certificate
from a trusted certificate authority and update the NiFi keystore and truststore configuration.

Accessing NiFi after accepting the self-signed certificate will display the login screen.
![NiFi Login Screen](nifi-docs/src/main/asciidoc/images/nifi-login.png?raw=true)

Using the generated credentials, enter the generated username in the `User` field
and the generated password in the `Password` field, then select `LOG IN` to access the system.
![NiFi Flow Authenticated Screen](nifi-docs/src/main/asciidoc/images/nifi-flow-authenticated.png?raw=true)

### Configuring

The [NiFi User Guide](https://nifi.apache.org/docs/nifi-docs/html/user-guide.html) describes how to build a data flow.

### Stopping

Run the following command to stop NiFi:

    laptop:~ myuser$ cd ~/example-nifi-deploy/nifi-*
    laptop:nifi-1.0.0-SNAPSHOT myuser$ ./bin/nifi.sh stop

## MiNiFi subproject

MiNiFi is a child project effort of Apache NiFi. It is a complementary data collection approach that supplements the core tenets of [NiFi](https://nifi.apache.org/) in dataflow management, focusing on the collection of data at the source of its creation.

Specific goals for MiNiFi are comprised of:
- small and lightweight footprint
- central management of agents
- generation of data provenance
- integration with NiFi for follow-on dataflow management and full chain of custody of information

Perspectives of the role of MiNiFi should be from the perspective of the agent acting immediately at, or directly adjacent to, source sensors, systems, or servers.

To run:
- Change directory to 'minifi-assembly'. In the target directory, there should be a build of minifi.

        $ cd minifi-assembly
        $ ls -lhd target/minifi*
        drwxr-xr-x  3 user  staff   102B Jul  6 13:07 minifi-1.14.0-SNAPSHOT-bin
        -rw-r--r--  1 user  staff    39M Jul  6 13:07 minifi-1.14.0-SNAPSHOT-bin.tar.gz
        -rw-r--r--  1 user  staff    39M Jul  6 13:07 minifi-1.14.0-SNAPSHOT-bin.zip

- For testing ongoing development you could use the already unpacked build present in the directory
  named ""minifi-*version*-bin"", where *version* is the current project version. To deploy in another
  location make use of either the tarball or zipfile and unpack them wherever you like. The
  distribution will be within a common parent directory named for the version.

        $ mkdir ~/example-minifi-deploy
        $ tar xzf target/minifi-*-bin.tar.gz -C ~/example-minifi-deploy
        $ ls -lh ~/example-minifi-deploy/
        total 0
        drwxr-xr-x  10 user  staff   340B Jul 6 01:06 minifi-1.14.0-SNAPSHOT

To run MiNiFi:
- Change directory to the location where you installed MiNiFi and run it.

        $ cd ~/example-minifi-deploy/minifi-*
        $ ./bin/minifi.sh start

- View the logs located in the logs folder
  $ tail -F ~/example-minifi-deploy/logs/minifi-app.log

- For help building your first data flow and sending data to a NiFi instance see the System Admin Guide located in the docs folder or making use of the minifi-toolkit, which aids in adapting NiFi templates to MiNiFi YAML configuration file format.

- If you are testing ongoing development, you will likely want to stop your instance.

        $ cd ~/example-minifi-deploy/minifi-*
        $ ./bin/minifi.sh stop

### Docker Build

To build:
- Run a full NiFi build (see above for instructions). Then from the minifi/ subdirectory, execute `mvn -P docker clean install`.  This will run the full build, create a docker image based on it, and run docker-compose integration tests.  After it completes successfully, you should have an apacheminifi:${minifi.version} image that can be started with the following command (replacing ${minifi.version} with the current maven version of your branch):
```
docker run -d -v YOUR_CONFIG.YML:/opt/minifi/minifi-${minifi.version}/conf/config.yml apacheminifi:${minifi.version}
```

## Registry subproject

Registry—a subproject of Apache NiFi—is a complementary application that provides a central location for storage and management of shared resources across one or more instances of NiFi and/or MiNiFi.

### Getting Registry Started

1) Build NiFi (see [Getting Started for NiFi](#getting-started) )
    
or
    
Build only the Registry subproject:

    cd nifi/nifi-registry
    mvn clean install

    
If you wish to enable style and license checks, specify the contrib-check profile:

    mvn clean install -Pcontrib-check


2) Start Registry

    cd nifi-registry/nifi-registry-assembly/target/nifi-registry-<VERSION>-bin/nifi-registry-<VERSION>/
    ./bin/nifi-registry.sh start

Note that the application web server can take a while to load before it is accessible.   

3) Accessing the application web UI
 
With the default settings, the application UI will be available at [http://localhost:18080/nifi-registry](http://localhost:18080/nifi-registry) 
   
4) Accessing the application REST API

If you wish to test against the application REST API, you can access the REST API directly. With the default settings, the base URL of the REST API will be at `http://localhost:18080/nifi-registry-api`. A UI for testing the REST API will be available at [http://localhost:18080/nifi-registry-api/swagger/ui.html](http://localhost:18080/nifi-registry-api/swagger/ui.html) 

5) Accessing the application logs

Logs will be available in `logs/nifi-registry-app.log`

### Database Testing

In order to ensure that NiFi Registry works correctly against different relational databases, 
the existing integration tests can be run against different databases by leveraging the [Testcontainers framework](https://www.testcontainers.org/).

Spring profiles are used to control the DataSource factory that will be made available to the Spring application context. 
DataSource factories are provided that use the Testcontainers framework to start a Docker container for a given database and create a corresponding DataSource. 
If no profile is specified then an H2 DataSource will be used by default and no Docker containers are required.

Assuming Docker is running on the system where the build is running, then the following commands can be run:

| Target Database | Build Command                                                      | 
|-----------------|--------------------------------------------------------------------|
| All supported   | `mvn verify -Ptest-all-dbs`                                        |
| H2 (default)    | `mvn verify`                                                       |
| MariaDB 10.3    | `mvn verify -Pcontrib-check -Dspring.profiles.active=mariadb-10-3` |
| MySQL 8         | `mvn verify -Pcontrib-check -Dspring.profiles.active=mysql-8`      |
| PostgreSQL 10   | `mvn verify -Dspring.profiles.active=postgres-10`                  |

For a full list of the available DataSource factories, consult the `nifi-registry-test` module.

## Getting Help
If you have questions, you can reach out to our mailing list: dev@nifi.apache.org
([archive](https://lists.apache.org/list.html?dev@nifi.apache.org)). For more interactive discussions, community members can often be found in the following locations:

- Apache NiFi Slack Workspace: https://apachenifi.slack.com/

  New users can join the workspace using the following [invite link](https://join.slack.com/t/apachenifi/shared_invite/zt-11njbtkdx-ZRU8FKYSWoEHRJetidy0zA).
  
To submit a feature request or bug report, please file a Jira at [https://issues.apache.org/jira/projects/NIFI/issues](https://issues.apache.org/jira/projects/NIFI/issues). If this is a **security vulnerability report**, please email [security@nifi.apache.org](mailto:security@nifi.apache.org) directly and review the [Apache NiFi Security Vulnerability Disclosure](https://nifi.apache.org/security.html) and [Apache Software Foundation Security](https://www.apache.org/security/committers.html) processes first. 

## Documentation

See https://nifi.apache.org/ for the latest NiFi documentation.

See https://nifi.apache.org/minifi and https://cwiki.apache.org/confluence/display/MINIFI for the latest MiNiFi-specific documentation.

See https://nifi.apache.org/registry for the latest Registry-specific documentation.

## License

Except as otherwise noted this software is licensed under the
[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0.html)

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

  https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

## Export Control

This distribution includes cryptographic software. The country in which you
currently reside may have restrictions on the import, possession, use, and/or
re-export to another country, of encryption software. BEFORE using any
encryption software, please check your country's laws, regulations and
policies concerning the import, possession, or use, and re-export of encryption
software, to see if this is permitted. See <https://www.wassenaar.org/> for more
information.

The U.S. Government Department of Commerce, Bureau of Industry and Security
(BIS), has classified this software as Export Commodity Control Number (ECCN)
5D002.C.1, which includes information security software using or performing
cryptographic functions with asymmetric algorithms. The form and manner of this
Apache Software Foundation distribution makes it eligible for export under the
License Exception ENC Technology Software Unrestricted (TSU) exception (see the
BIS Export Administration Regulations, Section 740.13) for both object code and
source code.

The following provides more details on the included cryptographic software:

Apache NiFi uses BouncyCastle, JCraft Inc., and the built-in
Java cryptography libraries for SSL, SSH, and the protection
of sensitive configuration parameters. See

- https://bouncycastle.org/about.html
- http://www.jcraft.com/c-info.html
- https://www.oracle.com/corporate/security-practices/corporate/governance/global-trade-compliance.html

for more details on each of these libraries cryptography features.

[nifi]: https://nifi.apache.org/
[logo]: https://nifi.apache.org/assets/images/apache-nifi-logo.svg
",2023-07-07 18:43:08+00:00
nipype,nipype,nipy/nipype,Workflows and interfaces for neuroimaging packages,https://nipype.readthedocs.org/en/latest/,False,689,2023-06-30 06:10:16+00:00,2010-07-22 17:06:49+00:00,519,50,204,43,1.8.4,2022-09-01 20:44:06+00:00,Other,14762,v1.0.0,76,2018-01-25 20:31:13+00:00,2023-07-06 16:24:24+00:00,2023-07-06 16:14:17+00:00,"========================================================
NIPYPE: Neuroimaging in Python: Pipelines and Interfaces
========================================================

.. image:: https://travis-ci.org/nipy/nipype.svg?branch=master
  :target: https://travis-ci.org/nipy/nipype

.. image:: https://circleci.com/gh/nipy/nipype/tree/master.svg?style=svg
  :target: https://circleci.com/gh/nipy/nipype/tree/master

.. image:: https://codecov.io/gh/nipy/nipype/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/nipy/nipype

.. image:: https://api.codacy.com/project/badge/Grade/452bfc0d4de342c99b177d2c29abda7b
  :target: https://www.codacy.com/app/nipype/nipype?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nipy/nipype&amp;utm_campaign=Badge_Grade

.. image:: https://img.shields.io/pypi/v/nipype.svg
    :target: https://pypi.python.org/pypi/nipype/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/nipype.svg
    :target: https://pypi.python.org/pypi/nipype/
    :alt: Supported Python versions

.. image:: https://img.shields.io/pypi/status/nipype.svg
    :target: https://pypi.python.org/pypi/nipype/
    :alt: Development Status

.. image:: https://img.shields.io/pypi/l/nipype.svg
    :target: https://pypi.python.org/pypi/nipype/
    :alt: License

.. image:: https://img.shields.io/badge/gitter-join%20chat%20%E2%86%92-brightgreen.svg?style=flat
    :target: http://gitter.im/nipy/nipype
    :alt: Chat

.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.596855.svg
   :target: https://doi.org/10.5281/zenodo.596855
   :alt: Citable DOI

Current neuroimaging software offer users an incredible opportunity to
analyze data using a variety of different algorithms. However, this has
resulted in a heterogeneous collection of specialized applications
without transparent interoperability or a uniform operating interface.

*Nipype*, an open-source, community-developed initiative under the
umbrella of NiPy, is a Python project that provides a uniform interface
to existing neuroimaging software and facilitates interaction between
these packages within a single workflow. Nipype provides an environment
that encourages interactive exploration of algorithms from different
packages (e.g., SPM, FSL, FreeSurfer, AFNI, Slicer, ANTS), eases the
design of workflows within and between packages, and reduces the
learning curve necessary to use different packages. Nipype is creating a
collaborative platform for neuroimaging software development in a
high-level language and addressing limitations of existing pipeline
systems.

*Nipype* allows you to:

* easily interact with tools from different software packages
* combine processing steps from different software packages
* develop new workflows faster by reusing common steps from old ones
* process data faster by running it in parallel on many cores/machines
* make your research easily reproducible
* share your processing workflows with the community

Documentation
-------------

Please see the ``doc/README.txt`` document for information on our
documentation.

Website
-------

Information specific to Nipype is located here::

    http://nipy.org/nipype

Python 2 Statement
------------------

Python 2.7 reaches its end-of-life in January 2020, which means it will
*no longer be maintained* by Python developers. `Many projects
<https://python3statement.org/>`__ are removing support in advance of this
deadline, which will make it increasingly untenable to try to support
Python 2, even if we wanted to.

The final series with 2.7 support is 1.3.x. If you have a package using
Python 2 and are unable or unwilling to upgrade to Python 3, then you
should use the following `dependency
<https://www.python.org/dev/peps/pep-0440/#version-specifiers>`__ for
Nipype::

    nipype<1.4

Bug fixes will be accepted against the ``maint/1.3.x`` branch.

Support and Communication
-------------------------

If you have a problem or would like to ask a question about how to do something in Nipype please open an issue to
`NeuroStars.org <http://neurostars.org>`_ with a *nipype* tag. `NeuroStars.org <http://neurostars.org>`_  is a
platform similar to StackOverflow but dedicated to neuroinformatics.

To participate in the Nipype development related discussions please use the following mailing list::

       http://mail.python.org/mailman/listinfo/neuroimaging

Please add *[nipype]* to the subject line when posting on the mailing list.

You can even hangout with the Nipype developers in their
`Gitter <https://gitter.im/nipy/nipype>`_ channel or in the BrainHack `Slack <https://brainhack.slack.com/messages/C1FR76RAL>`_ channel. (Click `here <https://brainhack-slack-invite.herokuapp.com>`_ to join the Slack workspace.)


Contributing to the project
---------------------------

If you'd like to contribute to the project please read our `guidelines <https://github.com/nipy/nipype/blob/master/CONTRIBUTING.md>`_. Please also read through our `code of conduct <https://github.com/nipy/nipype/blob/master/CODE_OF_CONDUCT.md>`_.
",2023-07-07 18:43:13+00:00
nnodes,nnodes,icui/nnodes,,,False,2,2022-03-18 12:19:39+00:00,2022-02-18 16:22:50+00:00,3,1,4,0,,,MIT License,152,,0,,,2023-02-15 22:03:30+00:00,"# nnodes

Nnodes is a simple workflow manager for Python functions and command line tools. It makes your life easier when running complicated jobs either in your local computer or in a large-scale cluster.

[Documentation](https://icui.github.io/nnodes/index.html)

Intro slides ([PDF](https://raw.githubusercontent.com/icui/nnodes/main/doc/source/files/slides.pdf)), poster ([PDF](https://raw.githubusercontent.com/icui/nnodes/main/doc/source/files/poster.pdf))

## Quick start
Install
```sh
pip install nnodes
```

Run example workflow (Gaussian inversion, requires numpy)
```sh
git clone https://github.com/icui/nnodes
cd nnodes/examples/gaussian
nnrun
```

Check out [Get started](https://icui.github.io/nnodes/basics/index.html) for details.

## Features
- **Progress control**.
No job progress will be lost in nnodes. The progress management is adaptable so that an interrupted workflow can be stopped and resumed at any point, and it is possible to rewind to a previous state if any parameter does not turn out to perform well; running a partial workflow or merging multiple workflows are also supported.<br>
![Workflow](doc/source/images/readme/inversion.png)

- **MPI execution**.
Parallel execution of MPI tasks is easy and no manual configuration is required. An MPI task from any part of the workflow will be sent to an MPI executor and be executed whenever the cluster resources is available. This makes sure that the node hours are fully utilized.<br>
![Workflow](doc/source/images/readme/mpi.png)

- **Parameter management**.
Inspired by HTML document, nnodes introduces a hierarchical parameter system that simplifies the process of passing parameters to functions. A parameter in the parent node will by default be propagated to the child node, unless overwritten. This eliminates the need to pass the same parameter to different functions under the same parent node.<br>
![Workflow](doc/source/images/readme/inherit.png)

## Why nnodes?
Workflow manager is essential for many scientific applications and there is a large number of existing workflow managers available. Many of them are mature and well maintained (see [Workflows Community](https://workflows.community) for a curated list and [Existing Workflow Systems](https://s.apache.org/existing-workflow-systems) for a comprehensive list). However, we believe that nnodes still has unique advantages. In short, it is simpler than most general-purpose workflow managers and more flexible than most problem-specific workflow managers.

- **Simplicity**. Most professional workflow managers have very steep learning curves, and are sometimes deeply bound with specific computing architectures. Nodes, on the other hand, provides a unified interface for all operations and utilizes only high level APIs. Migrating existing workflows to nnodes is seamless in most cases.
- **Flexibility**. Nnodes is not tied to a specific scientific problem, and it is decided by the user how deeply they wish to integrate their projects with nnodes. Users can simply use nnodes as a progress controller, or MPI executor, which requires little code change, or they can go so far as to let nnodes manage their entire project.
- **Portability**. Nnodes currently supports Slurm and LSF systems but also has API for users to define their custom environment. The workflow is saved in a single pickle file that can be transferred to a new system and continue from where it was left off. The MPI executor adapts automatically so no manual configuration is needed to utilize the full cluster resources.

## Alternatives
If you are looking for more options, below are some projects worth checking out:

- [Ensemble Toolkit](https://radical-cybertools.github.io/entk/index.html)
- [FireWorks](https://materialsproject.github.io/fireworks/)
- [Workflows Community](https://workflows.community)

## Contact
If you have any questions, please submit a [GitHub issue](https://github.com/icui/nnodes/issues) or send an [email](mailto:ccui@princeton.edu).
",2023-07-07 18:43:18+00:00
node-red,node-red,node-red/node-red,Low-code programming for event-driven applications,http://nodered.org,False,16950,2023-07-07 18:18:23+00:00,2013-09-05 13:30:47+00:00,3137,536,179,128,3.0.2,2022-08-04 13:13:26+00:00,Apache License 2.0,8039,v0.2.0,153,2013-10-16 18:46:37+00:00,2023-07-07 18:18:23+00:00,2023-07-05 15:22:38+00:00,"# Node-RED

http://nodered.org

[![Build Status](https://github.com/node-red/node-red/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/node-red/node-red/actions?query=branch%3Amaster)

Low-code programming for event-driven applications.

![Node-RED: Low-code programming for event-driven applications](http://nodered.org/images/node-red-screenshot.png)

## Quick Start

Check out http://nodered.org/docs/getting-started/ for full instructions on getting
started.

1. `sudo npm install -g --unsafe-perm node-red`
2. `node-red`
3. Open <http://localhost:1880>

## Getting Help

More documentation can be found [here](http://nodered.org/docs).

For further help, or general discussion, please use the [Node-RED Forum](https://discourse.nodered.org) or [slack team](https://nodered.org/slack).

## Developers

If you want to run the latest code from git, here's how to get started:

1. Clone the code:

        git clone https://github.com/node-red/node-red.git
        cd node-red

2. Install the node-red dependencies

        npm install

3. Build the code

        npm run build

4. Run

        npm start

## Contributing

Before raising a pull-request, please read our
[contributing guide](https://github.com/node-red/node-red/blob/master/CONTRIBUTING.md).

This project adheres to the [Contributor Covenant 1.4](http://contributor-covenant.org/version/1/4/).
 By participating, you are expected to uphold this code. Please report unacceptable
 behavior to any of the project's core team at team@nodered.org.

## Authors

Node-RED is a project of the [OpenJS Foundation](http://openjsf.org).

It is maintained by:

 * Nick O'Leary [@knolleary](http://twitter.com/knolleary)
 * Dave Conway-Jones [@ceejay](http://twitter.com/ceejay)
 * And many others...


## Copyright and license

Copyright OpenJS Foundation and other contributors, https://openjsf.org under [the Apache 2.0 license](LICENSE).
",2023-07-07 18:43:21+00:00
noflo,noflo,noflo/noflo,Flow-based programming for JavaScript,https://noflojs.org/,False,3398,2023-07-06 08:39:42+00:00,2011-06-06 04:33:02+00:00,269,137,25,28,1.3.0,2020-11-23 14:43:48+00:00,MIT License,2705,1.4.3,80,2020-12-10 16:16:55+00:00,2023-07-06 08:39:42+00:00,2021-03-08 05:55:19+00:00,"NoFlo: Flow-based programming for JavaScript
============================================

NoFlo is an implementation of [flow-based programming](http://en.wikipedia.org/wiki/Flow-based_programming) for JavaScript running on both Node.js and the browser. From WikiPedia:

> In computer science, flow-based programming (FBP) is a programming paradigm that defines applications as networks of ""black box"" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.

Developers used to the [Unix philosophy](http://en.wikipedia.org/wiki/Unix_philosophy) should be immediately familiar with FBP:

> This is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.

It also fits well in Alan Kay's [original idea of object-oriented programming](http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en):

> I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning -- it took a while to see how to do messaging in a programming language efficiently enough to be useful).

NoFlo components can be written in any language that transpiles down to JavaScript, including ES6. The system is heavily inspired by [J. Paul Morrison's](http://www.jpaulmorrison.com/) book [Flow-Based Programming](http://www.jpaulmorrison.com/fbp/#More).

Read more at <https://noflojs.org/>.

## Suitability

NoFlo is not a web framework or a UI toolkit. It is a way to coordinate and reorganize data flow in any JavaScript application. As such, it can be used for whatever purpose JavaScript can be used for. We know of NoFlo being used for anything from building [web servers](https://thegrid.io) and build tools, to coordinating events inside [GUI applications](https://flowhub.io), [driving](http://meemoo.org/blog/2015-01-14-turtle-power-to-the-people) [robots](http://bergie.iki.fi/blog/noflo-ardrone/), or building Internet-connected [art installations](http://bergie.iki.fi/blog/ingress-table/).

## Tools and ecosystem

NoFlo itself is just a library for implementing flow-based programs in JavaScript. There is an ecosystem of tools around NoFlo and the [fbp protocol](https://flowbased.github.io/fbp-protocol/) that make it more powerful. Here are some of them:

* [Flowhub](https://app.flowhub.io) -- browser-based visual programming **IDE** for NoFlo and other flow-based systems
* [noflo-nodejs](https://github.com/noflo/noflo-nodejs) -- command-line interface for running NoFlo programs on **Node.js**
* [noflo-browser-app](https://github.com/noflo/noflo-browser-app) -- template for building NoFlo programs for **the web**
* [noflo-assembly](https://github.com/noflo/noflo-assembly) -- **industrial approach** for designing NoFlo programs
* [fbp-spec](https://github.com/flowbased/fbp-spec) -- **data-driven tests** for NoFlo and other FBP environments
* [flowtrace](https://github.com/flowbased/flowtrace) -- tool for **retroactive debugging** of NoFlo programs. Supports visual replay with Flowhub

See also the [list of reusable NoFlo modules on NPM](https://www.npmjs.com/browse/keyword/noflo).

## Requirements and installing

NoFlo is available for Node.js [via NPM](https://npmjs.org/package/noflo), so you can install it with:

```bash
$ npm install noflo --save
```

You can make a browser build of NoFlo using webpack. For webpack builds, you need configure the component loader statically with [noflo-component-loader](https://github.com/noflo/noflo-component-loader). For projects using Grunt, [grunt-noflo-browser](https://github.com/noflo/grunt-noflo-browser) plugin makes this easy.

### Installing from Git

NoFlo requires a reasonably recent version of [Node.js](http://nodejs.org/), and some [npm](http://npmjs.org/) packages. Ensure that you have NoFlo checked out from Git, and all NPM dependencies installed. Build NoFlo with:

```bash
$ npm run build
```

Then you can install everything needed by a simple:

```bash
$ npm link
```

NoFlo is available from [GitHub](https://github.com/noflo/noflo) under the MIT license.

## Changes

Please refer to the [Release Notes](https://github.com/noflo/noflo/releases) and the [CHANGES.md document](https://github.com/noflo/noflo/blob/master/CHANGES.md).

## Usage

Please refer to <http://noflojs.org/documentation/>. For visual programming with NoFlo, see <https://docs.flowhub.io/>.

## Development

NoFlo development happens on GitHub. Just fork the [main repository](https://github.com/noflo/noflo), make modifications and send a pull request.

We have an extensive suite of tests available for NoFlo. Run them with:

```bash
$ npm run build
$ npm test
```

### Platform-specific tests

By default, the tests are run for both Node.js and the browser. You can also run only the tests for a particular target platform:

```bash
$ npm run test:node
```

or:

```bash
$ npm run test:browser
```

## Discussion

There is a `#noflo` channel on the [Flow-Based Programming Slack](https://join.slack.com/t/fbphq/shared_invite/enQtOTM4ODkzMTYyODE3LTJiMmNlZjhiMWY1MDY1ODA4Y2YzNDBlNDZlMTBkMDNlMjcwNzg2MGZhZjA2NjJjYTliYTM0OTIyYmM0Yzk0MDQ), and questions can be posted with the [`noflo` tag on Stack Overflow](http://stackoverflow.com/questions/tagged/noflo). See <http://noflojs.org/support/> for other ways to get in touch.
",2023-07-07 18:43:25+00:00
noodles,noodles,NLeSC/noodles,"Computational workflow engine, making distributed computing in Python easy!",http://nlesc.github.io/noodles,False,21,2023-01-02 11:32:13+00:00,2015-11-02 11:42:22+00:00,7,8,10,9,v0.3.4,2022-02-08 13:19:35+00:00,Apache License 2.0,582,v0.3.4,9,2022-02-08 13:19:35+00:00,,2022-02-08 13:19:35+00:00,"---
title: Noodles - parallel programming in Python
---

[![Travis](https://travis-ci.org/NLeSC/noodles.svg?branch=master)](https://travis-ci.org/NLeSC/noodles)
[![Zenodo DOI](https://zenodo.org/badge/45391130.svg)](https://zenodo.org/badge/latestdoi/45391130)
[![Code coverage](https://codecov.io/gh/NLeSC/noodles/branch/master/graph/badge.svg)](https://codecov.io/gh/NLeSC/noodles)
[![Documentation](https://readthedocs.org/projects/noodles/badge/?version=latest)](https://noodles.readthedocs.io/en/latest/?badge=latest)

::: {.splash}
* Write readable code
* Parallelise with a dash of Noodle sauce!
* Scale your applications from laptop to HPC using Xenon
    + [Learn more about Xenon](https://xenon-middleware.github.io/xenon)
* Read our [documentation](https://noodles.rtfd.io/), including tutorials on:
    + [Creating parallel programs](https://noodles.readthedocs.io/en/latest/poetry_tutorial.html)
    + [Circumventing the global interpreter lock](https://noodles.readthedocs.io/en/latest/prime_numbers.html)
    + [Handling errors in a meaningful way](https://noodles.readthedocs.io/en/latest/errors.html)
    + [Serialising your data](https://noodles.readthedocs.io/en/latest/serialisation.html)
    + [Functional programming and flow control](https://noodles.readthedocs.io/en/latest/control_your_flow.html)
:::

# What is Noodles?

Noodles is a task-based parallel programming model in Python that offers the same intuitive interface when running complex workflows on your laptop or on large computer clusters.

# Installation
To install the latest version from PyPI:

```
pip install noodles
```

To enable the Xenon backend for remote job execution,

```
pip install noodles[xenon]
```

This requires a Java Runtime to be installed, you may check this by running

```
java --version
```

which should print the version of the currently installed JRE.


# Documentation
All the latest documentation is available on [Read the Docs](https://noodles.rtfd.io/).

",2023-07-07 18:43:30+00:00
omega|mldataopsplatform,omegaml,omegaml/omegaml,MLOps simplified. From ML Pipeline ⇨ Data Product without the hassle,https://omegaml.github.io/omegaml/,False,88,2023-05-31 15:43:49+00:00,2018-12-10 13:20:15+00:00,12,6,5,21,release/0.15.5,2023-04-13 15:29:37+00:00,Apache License 2.0,320,status-before-update,49,2021-01-14 20:06:18+00:00,2023-06-28 15:45:23+00:00,2023-06-17 14:06:04+00:00,"omega|ml - MLOps for humans
===========================

with just a single line of code you can

- deploy machine learning models straight from Jupyter Notebook (or any other code)
- implement data pipelines quickly, without memory limitation, all from a Pandas-like API
- serve models and data from an easy to use REST API

Further, omega|ml is the fastest way to

- scale model training on the included scalable pure-Python compute cluster, on Spark or any other cloud
- collaborate on data science projects easily, sharing Jupyter Notebooks
- deploy beautiful dashboards right from your Jupyter Notebook, using dashserve

Quick start
-----------

Start the omega|ml server right from your laptop or virtual machine

.. code::

    $ wget https://raw.githubusercontent.com/omegaml/omegaml/master/docker-compose.yml
    $ docker-compose up -d

Jupyter Notebook is immediately available at http://localhost:8899 (`omegamlisfun` to login).
Any notebook you create will automatically be stored in the integrated omega|ml database, making collaboration a breeze.
The REST API is available at http://localhost:5000.

Already have a Python environment (e.g. Jupyter Notebook)?
Leverage the power of omega|ml by installing as follows:

.. code::

    # assuming you have started the server as per above
    $ pip install omegaml

Further information
-------------------

* Documentation: https://omegaml.github.io/omegaml/
* Contributions: http://bit.ly/omegaml-contribute

Examples
--------

.. code::

    # transparently store Pandas Series and DataFrames or any Python object
    om.datasets.put(df, 'stats')
    om.datasets.get('stats', sales__gte=100)

    # transparently store and get models
    clf = LogisticRegression()
    om.models.put(clf, 'forecast')
    clf = om.models.get('forecast')

    # run and scale models directly on the integrated Python or Spark compute cluster
    om.runtime.model('forecast').fit('stats[^sales]', 'stats[sales]')
    om.runtime.model('forecast').predict('stats')
    om.runtime.model('forecast').gridsearch(X, Y)

    # use the REST API to store and retrieve data, run predictions
    requests.put('/v1/dataset/stats', json={...})
    requests.get('/v1/dataset/stats?sales__gte=100')
    requests.put('/v1/model/forecast', json={...})


Use Cases
=========

omega|ml currently supports scikit-learn, Keras and Tensorflow out of the box.
Need to deploy a model from another framework? Open an issue at
https://github.com/omegaml/omegaml/issues or drop us a line at support@omegaml.io


Machine Learning Deployment
---------------------------

- deploy models to production with a single line of code
- serve and use models or datasets from a REST API


Data Science Collaboration
--------------------------

- get a fully integrated data science workplace within minutes
- easily share models, data, jupyter notebooks and reports with your collaborators

Centralized Data & Compute cluster
----------------------------------

- perform out-of-core computations on a pure-python or Apache Spark compute cluster
- have a shared NoSQL database (MongoDB), out of the box, working like a Pandas dataframe
- use a compute cluster to train your models with no additional setup

Scalability and Extensibility
-----------------------------

- scale your data science work from your laptop to team to production with no code changes
- integrate any machine learning framework or third party data science platform with a common API

Towards Data Science recently published an article on omega|ml:
https://towardsdatascience.com/omega-ml-deploying-data-machine-learning-pipelines-the-easy-way-a3d281569666

In addition omega|ml provides an easy-to-use extensions API to support any kind of models,
compute cluster, database and data source.

*Commercial Edition & Support*

https://omegaml.io

omega|ml Commercial Edition provides security on every level and is ready made for Kubernetes
deployment. It is licensed separately for on-premise, private or hybrid cloud.
Sign up at https://omegaml.io
",2023-07-07 18:43:33+00:00
apacheoodt,oodt,apache/oodt,Mirror of Apache OODT,,True,62,2023-07-03 18:39:01+00:00,2013-01-21 08:00:15+00:00,64,20,24,0,,,Apache License 2.0,2251,oodt-0.7,33,2014-08-19 02:12:03+00:00,2023-05-18 01:48:55+00:00,2022-01-07 06:11:46+00:00,"# Welcome to Apache OODT  <http://oodt.apache.org/>

Apache Object Oriented Data Technology (OODT) is the smart way to integrate and archive your processes, your data, and its metadata. OODT allows you to:

- Generate Data
- Process Data
- Manage Your Data
- Distribute Your Data
- Analyze Your Data
Allowing for the integration of data, computation, visualization and other components.

OODT also allows for remote execution of jobs on scalable computational infrastructures so that computational and data-
intensive processing can be integrated into OODT’s data processing pipelines using cloud computing and high-performance 
computing environments.

![Overview of OODT Main Components](http://oodt.apache.org/img/oodt-diag.png ""OODT Component Overview"")


## Why OODT?
Traditional processing pipelines are commonly made up of custom UNIX shell scripts and fragile custom written glue code.
Apache OODT uses structured XML-based capturing of the processing pipeline that can be understood and modified by 
non-programmers to create, edit, manage and provision workflow and task execution.

It is being used on a number of successful projects at NASA's Jet Propulsion Laboratory/California 
Institute of Technology, and many other research institutions and universities, 
specifically those part of the:

- **National Cancer Institute's (NCI's) Early Detection Research Network (EDRN)
  project** - over 40+ institutions all performing research into discovering
  biomarkers which are early indicators of disease.
- **NASA's Planetary Data System (PDS)** - NASA's planetary data archive, a
  repository and registry for all planetary data collected over the past 30+
  years.
- Various Earth Science data processing missions, including
  **Seawinds/QuickSCAT**, the **Orbiting Carbon Observatory**, the **NPP Sounder PEATE
  project**, and the **Soil Moisture Active Passive (SMAP) mission**.
- [**Apache DRAT**](https://github.com/apache/drat) - A distributed release audit tool written on top of OODT's 
capabilities.

OODT is a Top Level project of the Apache Software Foundation
<http://www.apache.org/>.

***

# Getting Started

## Useful Resources
- [*OODT Wiki*](https://cwiki.apache.org/confluence/display/OODT/Home)
- [*Apache OODT platform: Use metadata as a first class citizen* by Tom Barber](https://jaxenter.com/tom-barber-nasa-interview-apache-oodt-127821.html)
- [RADiX Powered By OODT](https://cwiki.apache.org/confluence/display/OODT/RADiX+Powered+By+OODT)
- [*A Look into the Apache OODT Ecosystem* by Chris Mattmann](https://www.slideshare.net/chrismattmann/a-look-into-the-apache-oodt-ecosystem)

## Build from scratch

OODT is primarily written in Java, with some components available in Python.
It requires Java 8 and uses the Maven 3 <http://maven.apache.org/> build
system.  To build the Java components of OODT, use the following command in
this directory:

    mvn clean install

For the Python components, see the **agility** subdirectory.

## RADiX Powered By OODT

OODT isn’t an out of the box solution, but we do try to make it as easy as possible. For that, OODT provides the RADIX build system which will compile a fully operational OODT platform ready for development and deployment. Building the RADIX distribution is as simple as running the following commands:

```bash
export JAVA_HOME=/usr/lib/jvm… (adjust for your own JAVA_HOME)
curl -s ""https://git-wip-us.apache.org/repos/asf?p=oodt.git;a=blob_plain;f=mvn/archetypes/radix/src/main/resources/bin/radix;hb=HEAD"" | bash
 
mv oodt oodt-src; cd oodt-src; mvn install
mkdir ../oodt; tar -xvf distribution/target/oodt-distribution-0.1-bin.tar.gz -C ../oodt
cd ../oodt; ./bin/oodt start
./resmgr/bin/batch_stub 2001
```
Navigate to http://localhost:8080/opsui, you should see the default OPSUI system which provides system oversight and interrogation

***

# Contributing

To contribute a patch, follow these instructions.

1. File JIRA issue for your fix at https://issues.apache.org/jira/browse/OODT.
you will get issue id OODT-xxx where xxx is the issue ID.

2. [Fork the repo](http://help.github.com/fork-a-repo) on which you're working, clone your forked repo to your local computer, and set up the upstream remote:
    ```
    git clone https://github.com/<YourGitHubUserName>/oodt.git
    git remote add upstream https://github.com/apache/oodt.git
    ```
3. Go into oodt directory
    ```
    cd oodt
    ```
4. Checkout out a new local branch based on your master and update it to the latest.The convention is to name the branch after the current JIRA issue, e.g. OODT-xxx where xxx is the issue ID.
    ```
    git checkout -b OODT-xxx
    ```
5. Do the changes to the relavant files and keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.

6. Add relevant files to the staging  area.
    ```
    git add <files>
    ```
7. For every commit please write a short (max 72 characters) summary of the change. Use markdown syntax for simple styling. Please include any JIRA issue numbers in your summary.
    ```
    git commit -m “[OODT-xxx] Put change summary here ”
    ```
    **NEVER leave the commit message blank!** Provide a detailed, clear, and complete description of your commit!

8. Before submitting a pull request, update your branch to the latest code.
    ```
    git checkout master
    git pull --rebase upstream master
    git checkout OODT-xxx
    git rebase -i master
    ```
9. Push the code to your forked repository
    ```
    git push origin OODT-xxx
    ```
10. In order to make a pull request,
  * Navigate to the OODT repository you just pushed to (e.g. https://github.com/your-user-name/oodt)
  * Click ""Pull Request"".
  * Write your branch name in the branch field (this is filled with ""master"" by default)
  * Click ""Update Commit Range"".
  * Ensure the changesets you introduced are included in the ""Commits"" tab.
  * Ensure that the ""Files Changed"" incorporate all of your changes.
  * Fill in some details about your potential patch including a meaningful title.
  * Click ""Send pull request"".
## Issue Tracker

If you encounter errors in OODT or want to suggest an improvement or a new
feature, please visit the OODT issue tracker 
https://issues.apache.org/jira/browse/OODT.  There you can also find the
latest information on known issues and recent bug fixes and enhancements.

## Documentation

You can find an enormous amount of useful documentation/resources related to OODT in 
[**OODT Confluence Wiki**](https://cwiki.apache.org/confluence/display/OODT/Home).

You can build a local copy of the OODT documentation including JavaDocs using
the following Maven 2 command in the OODT source directory:

    mvn site

You can then open the OODT Documentation in a web browser:

    ./target/site/index.html

Note: all OODT source files are encoded with UTF-8.  You must set your
MAVEN_OPTS environment variable to include ""-Dfile.encoding=UTF-8"" in order to
properly generate the web site and other artifacts from source.

Note: generating the documentation requires enormous amounts of memory.  More
than likely you'll need to add to the MAVEN_OPTS environment variable in order
to set the Java heap maximum size with ""-Xmx512m"" or larger before attempting
to run ""mvn site"".

***

# License (see also LICENSE.txt)

Collective work: Copyright 2010-2012 The Apache Software Foundation.

Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the ""License""); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Apache OODT includes a number of subcomponents with separate copyright
notices and license terms. Your use of these subcomponents is subject to
the terms and conditions of the licenses listed in the LICENSE.txt file.

***

# Export control

This distribution includes cryptographic software.  The country in which you
currently reside may have restrictions on the import, possession, use, and/or
re-export to another country, of encryption software.  BEFORE using any
encryption software, please check your country's laws, regulations and
policies concerning the import, possession, or use, and re-export of
encryption software, to see if this is permitted.  See
<http://www.wassenaar.org/> for more information.

The U.S.  Government Department of Commerce, Bureau of Industry and Security
(BIS), has classified this software as Export Commodity Control Number (ECCN)
5D002.C.1, which includes information security software using or performing
cryptographic functions with asymmetric algorithms.  The form and manner of
this Apache Software Foundation distribution makes it eligible for export
under the License Exception ENC Technology Software Unrestricted (TSU)
exception (see the BIS Export Administration Regulations, Section 740.13) for
both object code and source code.

The following provides more details on the included cryptographic software:

    Apache OODT uses Apache Tika which uses the Bouncy Castle generic
    encryption libraries for extracting text content and metadata from
    encrypted PDF files.  See http://www.bouncycastle.org/ for more details on
    Bouncy Castle.

***

# Mailing Lists

Discussion about OODT takes place on the following mailing lists:

    dev@oodt.apache.org    - About using OODT and developing OODT

Notification on all code changes are sent to the following mailing list:

    commits@oodt.apache.org

The mailing lists are open to anyone and publicly archived.

You can subscribe the mailing lists by sending a message to
<LIST>-subscribe@oodt.apache.org (for example
dev-subscribe@oodt...).  To unsubscribe, send a message to
<LIST>-unsubscribe@oodt.apache.org.  For more instructions, send a
message to <LIST>-help@oodt.apache.org.
",2023-07-07 18:43:38+00:00
apacheoozie,oozie,apache/oozie,Mirror of Apache Oozie,,False,680,2023-06-27 07:45:40+00:00,2011-09-14 07:00:10+00:00,476,78,17,0,,,Apache License 2.0,2406,release-5.2.1,51,2021-02-09 08:29:14+00:00,2023-06-28 08:16:51+00:00,2023-04-20 06:22:42+00:00,"Apache Oozie
=============

What is Oozie
--------------

Oozie is an extensible, scalable and reliable system to define, manage, schedule, and execute complex Hadoop workloads via web services. More specifically, this includes:

  * XML-based declarative framework to specify a job or a complex workflow of dependent jobs.
  * Support different types of job such as Hadoop Map-Reduce, Pipe, Streaming, Pig, Hive and custom java applications.
  * Workflow scheduling based on frequency and/or data availability.
  * Monitoring capability, automatic retry and failure handing of jobs.
  * Extensible and pluggable architecture to allow arbitrary grid programming paradigms.
  * Authentication, authorization, and capacity-aware load throttling to allow multi-tenant software as a service.

Oozie Overview
----------

Oozie is a server based Workflow Engine specialized in running workflow jobs with actions that run Hadoop Map/Reduce and Pig jobs.

Oozie is a Java Web-Application that runs in a Java servlet-container.

For the purposes of Oozie, a workflow is a collection of actions (i.e. Hadoop Map/Reduce jobs, Pig jobs) arranged in a control dependency DAG (Directed Acyclic Graph). ""control dependency"" from one action to another means that the second action can't run until the first action has completed.

Oozie workflows definitions are written in hPDL (a XML Process Definition Language similar to JBOSS JBPM jPDL).

Oozie workflow actions start jobs in remote systems (i.e. Hadoop, Pig). Upon action completion, the remote systems callback Oozie to notify the action completion, at this point Oozie proceeds to the next action in the workflow.

Oozie workflows contain control flow nodes and action nodes.

Control flow nodes define the beginning and the end of a workflow ( start , end and fail nodes) and provide a mechanism to control the workflow execution path ( decision , fork and join nodes).

Action nodes are the mechanism by which a workflow triggers the execution of a computation/processing task. Oozie provides support for different types of actions: Hadoop map-reduce, Hadoop file system, Pig, SSH, HTTP, eMail and Oozie sub-workflow. Oozie can be extended to support additional type of actions.

Oozie workflows can be parameterized (using variables like ${inputDir} within the workflow definition). When submitting a workflow job values for the parameters must be provided. If properly parameterized (i.e. using different output directories) several identical workflow jobs can concurrently.

Documentations :
-----------------
Oozie web service is bundle with the built-in details documentation.

More information could be found at:
http://oozie.apache.org/

Oozie Quick Start:
http://oozie.apache.org/docs/5.0.0/DG_QuickStart.html


Supported Hadoop Versions:
----------------------------

This version of Oozie was primarily tested against Hadoop 2.4.x and 2.6.x.



If you have any questions/issues, please send an email to:

user@oozie.apache.org

Subscribe using the link:

http://oozie.apache.org/mail-lists.html
",2023-07-07 18:43:42+00:00
openalea,openalea,openalea/openalea,"Official repository for OpenAlea and OpenAleaLab. Other repos in the openalea organization contain things like the website, documentation, projects, etc.",http://openalea.github.io,False,43,2023-04-26 02:50:11+00:00,2012-03-30 09:52:33+00:00,18,14,16,1,v2.0.0,2016-10-13 16:01:51+00:00,,5471,v2.0.0,2,2016-10-13 16:01:51+00:00,2023-04-26 02:50:11+00:00,2018-02-15 21:00:52+00:00,"[![Build Status](https://travis-ci.org/openalea/openalea.svg?branch=master)](https://travis-ci.org/openalea/openalea) [![Build status](https://ci.appveyor.com/api/projects/status/303pabu8pa694bwd/branch/master?svg=true)](https://ci.appveyor.com/project/fredboudon/openalea/branch/master)


# OpenAlea Lab

## Documentation

Official documentation is available at [openalea.github.io](http://openalea.github.io)

## Install
To install OpenAlea use conda

If you want to retrieve the code from git, use git clone.
Then use this command that will retrieve all the submodules::
    
    git submodule update --init --recursive

## Contribute

If you want to contribute to code, please have a look to [github workflow](http://virtualplants.github.io/contribute/devel/git-workflow.html)

",2023-07-07 18:43:46+00:00
openbytes,OpenBytes,Project-OpenBytes/OpenBytes,This repo contains documentation related to the operation of the OpenBytes project.,,False,9,2023-05-08 08:01:42+00:00,2021-10-18 10:07:37+00:00,1,0,4,0,,,Creative Commons Attribution 4.0 International,21,,0,,2023-05-08 08:01:42+00:00,2021-10-29 08:00:22+00:00,"
![](https://us-tutu.s3.us-west-1.amazonaws.com/company-website/OpenBytes/Mobile/logo.png)

# OpenBytes

We are a non-profit open data project hosted under the Linux Foundation with neutral governance, aiming to bring transformational changes to AI by making open datasets more available and accessible.

## Discussion

You can connect with the community in a variety of ways...


- [OpenBytes Slack](https://openbytes.slack.com/join/shared_invite/zt-xmlybz36-kc7SnQnmrIyz1m79oOu6pg#/shared-invite/email)

## Contributing
Anyone can contribute to the OpenBytes project - learn more at [CONTRIBUTING.md](CONTRIBUTING.md)

## Governance
OpenBytes is a project hosted by the [LF AI and Data Foundation](https://lfaidata.foundation). This project has established it's own processes for managing day-to-day processes in the project, and the techincal charter is located in at [GOVERNANCE.md](GOVERNANCE.md).

## Reporting Issues
To report a problem, you can open an [issue](https://github.com/Project-OpenBytes/OpenBytes/issues) in repository against a specific workflow. If the issue is senstive in nature or a security related issue, please do not report in the issue tracker but instead email contact@openbytes.io.
",2023-07-07 18:43:50+00:00
opendataology,OpenDataology,OpenDataology/OpenDataology,Best practices for AI dataset metadata and license compliance,,False,26,2023-05-08 08:00:16+00:00,2022-05-01 06:52:35+00:00,11,2,4,1,,,Other,105,0.1,1,2022-05-21 07:40:16+00:00,2023-05-08 08:00:15+00:00,2023-03-27 02:05:18+00:00,"# OpenDataology

----

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6032/badge)](https://bestpractices.coreinfrastructure.org/projects/6032) <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />

## Overview

----

OpenDataology is a project for AI model trainning with trusted dataset compliance. Our project enables users of publicly available datasets and users who curate a dataset from multiple data sources (particularly for use as a part of machine learning models) to identify the potential license compliance risks. Our project is primarily comprised of three key components. 

* A dataset license compliance analysis workflow that ascertains the final allowed rights and the required obligations associated with using a publicly available dataset or a dataset that is curated from multiple data sources for any purpose. Please refer to the paper [Can I use this publicly available dataset to build commercial AI software?-A Case Study on Publicly Available Image Datasets](Paper_license_compliance.pdf) for more details.
* A growing database and a [web portal](https://github.com/OpenDataology/portal-frontend) that documents the final rights and obligations (after the license compliance analysis is conducted) associated with the datasets and the data sources analyzed in our project. The database also documents the metadata collected and used to conduct the compliance workflow.
* An online [license generation toolkit](https://github.com/OpenDataology/license-generator) that creators of datasets to generate custom licenses depending on the exact rights and obligations that they want to allow (instead of having to rely on existing available and limited dataset specific licenses)

OpenDatalogy's recommendations cannot be constituted as legal advice.

## Getting Involved

----

* Website: www.opendatalogy.com
* Chat: [OpenDataology Slack](https://join.slack.com/t/dataset-license/shared_invite/zt-1823jgzvb-3ExLy22G4fKSaTYdXb9fYQ)
* Source: https://github.com/OpenDataology

## Contributing

----

We love contributions in various forms. To contribute to OpenDataology please see [CONTRIBUTING.md](CONTRIBUTING.md)

## Governance

----

OpenDataology is a project hosted by the [LF AI and Data Foundation](https://lfaidata.foundation). The project governance details can be found at [GOVERNANCE.md](GOVERNANCE.md).

## Reporting a Problem

----

To report a problem, you can open an issue in the repository against a specific workflow. If the issue is sensitive in nature or a security related issue, please do not report it in the issue tracker but instead email main@opendataology.com.

## Meeting Schedule and Minutes

----

[Here](https://github.com/OpenDataology/community) are the records of the meeting notes and progress.

## License

----

OpenDataology is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

Copyright (c) 2022 The OpenDataology Authors

All rights reserved.",2023-07-07 18:43:54+00:00
openge,openge,adaptivegenome/openge,An accelerated framework for manipulating and interpreting high-throughput sequencing data,,False,26,2021-03-04 19:25:29+00:00,2012-04-11 17:39:57+00:00,7,4,1,0,,,Other,514,v0.2,2,2012-08-30 15:49:37+00:00,,2013-07-09 17:51:34+00:00,"## Open Genomics Engine README

OpenGE is a unified and optimized analysis platform that is open source, accelerated for parallel computing, and easily extendible. The most recent major release is 0.4 (January 2013) and there are no planned future releases at this time.

### Getting Started

Bug fixes, documentation, and code contributions are welcome. Please feel free to email the author, or contact us via the github page:

<https://github.com/adaptivegenome/openge> (project source)

<https://github.com/adaptivegenome/openge/issues> (issues, bugs, etc.)

To get started with the codebase, check out the developer guide inside the openge/docs/ directory.

### License
OpenGE is licensed under the Virginia Tech Non-Commercial Purpose License, which can be found in the LICENSE.txt

Copyright 2013 Virginia Bioinformatics Institute.
",2023-07-07 18:43:59+00:00
openlineage,OpenLineage,OpenLineage/OpenLineage,An Open Standard for lineage metadata collection,http://openlineage.io,False,1316,2023-07-07 14:39:48+00:00,2020-10-24 21:45:05+00:00,179,38,56,47,0.29.2,2023-06-30 16:41:05+00:00,Apache License 2.0,1422,0.29.2,69,2023-06-30 16:41:05+00:00,2023-07-07 15:11:16+00:00,2023-07-07 10:43:26+00:00,"<div align=""center"">
  <img src=""./doc/openlineage-logo.png"" width=""375px"" />
  <a href=""https://lfaidata.foundation/projects"">
      <img src=""./doc/lfaidata-project-badge-sandbox-black.png"" width=""115px"" />
  </a>
</div>

## Badges

[![CircleCI](https://circleci.com/gh/OpenLineage/OpenLineage/tree/main.svg?style=shield)](https://circleci.com/gh/OpenLineage/OpenLineage/tree/main)
[![status](https://img.shields.io/badge/status-active-brightgreen.svg)](#status)
[![Slack](https://img.shields.io/badge/slack-chat-blue.svg)](http://bit.ly/OpenLineageSlack)
[![license](https://img.shields.io/badge/license-Apache_2.0-blue.svg)](https://github.com/OpenLineage/OpenLineage/blob/main/LICENSE)
[![maven](https://img.shields.io/maven-central/v/io.openlineage/openlineage-java.svg)](https://search.maven.org/search?q=g:io.openlineage)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4888/badge)](https://bestpractices.coreinfrastructure.org/projects/4888)

## Overview
OpenLineage is an Open standard for metadata and lineage collection designed to instrument jobs as they are running.
It defines a generic model of run, job, and dataset entities identified using consistent naming strategies.
The core lineage model is extensible by defining specific facets to enrich those entities.

## Status

OpenLineage is an [LF AI & Data Foundation](https://lfaidata.foundation/projects/openlineage) incubation project under active development, and we'd love your help!

## Problem

### Before

- Duplication of effort: each project has to instrument all jobs
- Integrations are external and can break with new versions

![Before OpenLineage](doc/before-ol.svg)

### With OpenLineage

- Effort of integration is shared
- Integration can be pushed in each project: no need to play catch up

![With OpenLineage](doc/with-ol.svg)

## Scope
OpenLineage defines the metadata for running jobs and the corresponding events.
A configurable backend allows the user to choose what protocol to send the events to.
 ![Scope](doc/scope.svg)

## Core model

 ![Model](doc/datamodel.svg)

 A facet is an atomic piece of metadata attached to one of the core entities.
 See the spec for more details.

## Spec
The [specification](spec/OpenLineage.md) is defined using OpenAPI and allows extension through custom facets.

## Integrations

The OpenLineage repository contains integrations with several systems.

- [Apache Spark](https://github.com/OpenLineage/OpenLineage/tree/main/integration/spark)
- [Apache Airflow](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow)
- [Dagster](https://github.com/OpenLineage/OpenLineage/tree/main/integration/dagster)
- [dbt](https://github.com/OpenLineage/OpenLineage/tree/main/integration/dbt)
- [Flink](https://github.com/OpenLineage/OpenLineage/tree/main/integration/flink)

## Related projects
- [Marquez](https://marquezproject.ai/): Marquez is an [LF AI & DATA](https://lfaidata.foundation/) project to collect, aggregate, and visualize a data ecosystem's metadata. It is the reference implementation of the OpenLineage API.
  - [OpenLineage collection implementation](https://github.com/MarquezProject/marquez/blob/main/api/src/main/java/marquez/api/OpenLineageResource.java)
- [Egeria](https://egeria.odpi.org/): Egeria offers open metadata and governance for enterprises - automatically capturing, managing and exchanging metadata between tools and platforms, no matter the vendor.

## Community
- Website: [openlineage.io](http://openlineage.io)
- Slack: [OpenLineage.slack.com](http://bit.ly/OpenLineageSlack)
  - Not a member? Join [here](https://bit.ly/lineageslack).
- Twitter: [@OpenLineage](https://twitter.com/OpenLineage)
- Mailing list: [openlineage-tsc](https://lists.lfaidata.foundation/g/openlineage-tsc)
- Wiki: [OpenLineage+Home](https://wiki.lfaidata.foundation/display/OpenLineage/OpenLineage+Home)
- LinkedIn: [13927795](https://www.linkedin.com/groups/13927795/)
- YouTube: [channel](https://www.youtube.com/channel/UCRMLy4AaSw_ka-gNV9nl7VQ)
- Mastodon: [@openlineage@fostodon.org](openlineage@fosstodon.org)

## Talks
- [Data+AI Summit June 2023. Cross-Platform Data Lineage with OpenLineage](https://www.databricks.com/dataaisummit/session/cross-platform-data-lineage-openlineage/)
- [Berlin Buzzwords June 2023. Column-Level Lineage is Coming to the Rescue](https://youtu.be/xFVSZCCbZlY)
- [Berlin Buzzwords June 2022. Cross-Platform Data Lineage with OpenLineage](https://www.youtube.com/watch?v=pLBVGIPuwEo)
- [Berlin Buzzwords June 2021. Observability for Data Pipelines with OpenLineage](https://2021.berlinbuzzwords.de/member/julien-le-dem)
- [Data Driven NYC February 2021. Data Observability and Pipelines: OpenLineage and Marquez](https://mattturck.com/datakin/)
- [Big Data Technology Warsaw Summit February 2021. Data lineage and Observability with Marquez and OpenLineage](https://bigdatatechwarsaw.eu/edition-2021/)
- [Metadata Day 2020. OpenLineage Lightning Talk](https://www.youtube.com/watch?v=anlV5Er_BpM)
- [Open Core Summit 2020. Observability for Data Pipelines: OpenLineage Project Launch](https://www.coss.community/coss/ocs-2020-breakout-julien-le-dem-3eh4)

## Contributing

See [CONTRIBUTING.md](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md) for more details about how to contribute.

## Report a Vulnerability

If you discover a vulnerability in the project, please [open an issue](https://github.com/OpenLineage/OpenLineage/issues/new/choose) and attach the ""security"" label.

----
SPDX-License-Identifier: Apache-2.0\
Copyright 2018-2023 contributors to the OpenLineage project",2023-07-07 18:44:02+00:00
openmole,openmole,openmole/openmole,Workflow engine for exploration of simulation models using high throughput computing,https://openmole.org/,False,135,2023-07-05 11:20:02+00:00,2014-10-08 09:37:00+00:00,36,18,26,4,v6.2,2017-01-24 22:45:04+00:00,GNU Affero General Public License v3.0,13872,v15.4,95,2023-06-15 11:43:26+00:00,2023-07-06 21:15:20+00:00,2023-07-06 21:14:56+00:00," [![build status](https://gitlab.openmole.org/openmole/openmole/badges/13-dev/pipeline.svg)](https://gitlab.openmole.org/openmole/openmole/-/commits/13-dev)

[OpenMOLE](http://openmole.org) (Open MOdeL Experiment) has been developed since 2008 as a free and open-source platform. It offers tools to run, explore, diagnose and optimize your numerical model, taking advantage of distributed computing environments. With OpenMOLE you can explore your already developed model, in any language (Java, Binary exe, NetLogo, R, SciLab, Python, C++...). 

* The __stable version__ is available on [openmole.org](http://openmole.org).
* A __fresh build__ of the developement version is available on [next.openmole.org](http://next.openmole.org).

OpenMOLE is distributed under the [AGPLv3](http://www.gnu.org/licenses/agpl.html) free software license.

## OpenMOLE by example ##

Before you use OpenMOLE, you need:
  * a program you want to study
  * to be able to run this program using a command line
  * to be able to set some inputs of the program
  * to be able to get some outputs variable or some output files out of this program

Then use OpenMOLE:
  * embed the executable of your program in OpenMOLE using (*5 minutes*)
  * use one of the distributed exploration algorithms provided by OpenMOLE (*5 minutes*)
  * launch the exploration indeferently on your laptop (*10 seconds*)
  * or on a distributed execution environment with thousands of machines (*1 minute*).

To summarize, you can **model exploration processes at scale** reusing **legacy code** and advanced numeric methods in approximately **10 minutes**.

## Try it! ##

To checkout OpenMOLE you can play with to the [demo site](http://demo.openmole.org) (this site is wiped out every few hours). You should click on the little cart and try out some of the market place examples.

## OpenMOLE Features: ##

  - **Expressive syntax** – A Domain Specific Language to describe your exploration processes,
  - **Transparent distributed computing** – Zero-deployment (no installation step) approach to distribute the workload transparently on your multi-core machines, desktop-grids, clusters, grids, ...
  - **Works with your programs** – Embed user’s executables (**Java, Binary exe, NetLogo, R, Scilab, Python, C++, ...**),
  - **Scalable** – Handles millions of tasks and TB of data,
  - **Advanced methods** – Advanced numerical experiments (design of experiments, optimization, calibration, sensitivity analysis, ...).

## OpenMOLE Avanced Features: ##

  - **Workflow plateform** – Design scientific workflows that may use legacy code,
  - **Distributed genetic algorithms** - Distribute the computation of your fitness functions,
  - **Distributed computing** - A high level aproach to distributed computing.

",2023-07-07 18:44:06+00:00
ophidia,ophidia-analytics-framework,OphidiaBigData/ophidia-analytics-framework,Core modules and operators of the Ophidia framework,,False,5,2022-07-01 15:29:12+00:00,2016-02-01 11:43:39+00:00,4,5,4,20,v1.7.3,2023-04-12 10:56:20+00:00,GNU General Public License v3.0,1163,v1.7.3,20,2023-04-12 10:56:20+00:00,2023-07-07 12:17:05+00:00,2023-04-12 10:56:20+00:00,"# Ophidia Analytics Framework

### Description

The Ophidia Analytics Framework is an efficient big data analytics framework. 
It includes parallel operators for data analysis and mining (subsetting, reduction, metadata processing, etc.) that can run over a cluster.
The framework is fully integrated with Ophidia Server: it receives commands from the server and sends back notifications so that workflows can be executed efficiently.

### Requirements

In order to compile and run the Ophidia Analytics Framework, make sure you have the following packages (all available through CentOS official repositories and the epel repository) properly installed:

1. mpich, mpich-devel and mpich-autoload
2. jansson and jansson-devel
3. libxml2 and libxml2-devel
4. libssh2 and libssh2-devel
5. openssl and openssl-devel
6. mysql-community-server
7. nectdf and netcdf-devel
8. libcurl and libcurl-devel
9. globus-common-devel (only for GSI support)
10. globus-gsi-credential-devel (only for GSI support)
11. globus-gsi-proxy-core-devel (only for GSI support)
12. globus-gssapi-gsi-devel (only for GSI support)
13. voms-devel (only for GSI support)
14. cfitsio-devel (only for FITS support)

**Note**:
This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit.

### How to Install

If you are building from git, you also need automake, autoconf, libtool, libtool-ltdl and libtool-ltdl-devel packages. To prepare the code for building run:

```
$ ./bootstrap 
```

The source code has been packaged with GNU Autotools, so to install simply type:

```
$ ./configure --prefix=prefix
$ make
$ make install
```

Type:

```
$ ./configure --help
```

to see all available options.

If you want to use the program system-wide, remember to add its installation directory to your PATH.

Further information can be found at [http://ophidia.cmcc.it/documentation/admin/](http://ophidia.cmcc.it/documentation/admin/) and [http://ophidia.cmcc.it/documentation/users/operators/](http://ophidia.cmcc.it/documentation/users/operators/).
",2023-07-07 18:44:11+00:00
orange,orange3,biolab/orange3,🍊 :bar_chart: :bulb: Orange: Interactive data analysis,https://orangedatamining.com,False,4169,2023-07-07 18:30:03+00:00,2013-02-22 12:52:56+00:00,934,179,97,38,3.35.0,2023-05-05 11:44:07+00:00,Other,14880,checkout,70,2018-04-17 18:30:19+00:00,2023-07-07 18:30:03+00:00,2023-06-30 08:35:46+00:00,"<p align=""center"">
    <a href=""https://orange.biolab.si/download"">
    <img src=""https://raw.githubusercontent.com/irgolic/orange3/README-shields/distribute/orange-title.png"" alt=""Orange Data Mining"" height=""200"">
    </a>
</p>
<p align=""center"">
    <a href=""https://orange.biolab.si/download"" alt=""Latest release"">
        <img src=""https://img.shields.io/github/v/release/biolab/orange3?label=download"" />
    </a>
    <a href=""https://orange3.readthedocs.io/en/latest/?badge=latest"" alt=""Documentation"">
        <img src=""https://readthedocs.org/projects/orange3/badge/?version=latest"">
    </a>
    <a href=""https://discord.gg/FWrfeXV"" alt=""Discord"">
        <img src=""https://img.shields.io/discord/633376992607076354?logo=discord&color=7389D8&logoColor=white&label=Discord"">                                                                                                                                                                                                                                                  </a>
</p>

# Orange Data Mining
[Orange] is a data mining and visualization toolbox for novice and expert alike. To explore data with Orange, one requires __no programming or in-depth mathematical knowledge__. We believe that workflow-based data science tools democratize data science by hiding complex underlying mechanics and exposing intuitive concepts. Anyone who owns data, or is motivated to peek into data, should have the means to do so.

<p align=""center"">
    <a href=""https://orange.biolab.si/download"">
    <img src=""https://raw.githubusercontent.com/irgolic/orange3/README-shields/distribute/orange-example-tall.png"" alt=""Example Workflow"">
    </a>
</p>

[Orange]: https://orange.biolab.si/


## Installing

### Easy installation

For easy installation, [Download](https://orange.biolab.si/download) the latest released Orange version from our website. To install an add-on, head to `Options -> Add-ons...` in the menu bar.

### Installing with Conda

First, install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) for your OS. 

Then, create a new conda environment, and install orange3:

```Shell
# Add conda-forge to your channels for access to the latest release
conda config --add channels conda-forge

# Perhaps enforce strict conda-forge priority
conda config --set channel_priority strict

# Create and activate an environment for Orange
conda create python=3 --yes --name orange3
conda activate orange3

# Install Orange
conda install orange3
```

For installation of an add-on, use:
```Shell
conda install orange3-<addon name>
```
[See specific add-on repositories for details.](https://github.com/biolab/)


### Installing with pip

We recommend using our [standalone installer](https://orange.biolab.si/download) or conda, but Orange is also installable with pip. You will need a C/C++ compiler (on Windows we suggest using Microsoft Visual Studio Build Tools).
Orange needs PyQt to run. Install either:
- PyQt5 and PyQtWebEngine: `pip install -r requirements-pyqt.txt` 
- PyQt6 and PyQt6-WebEngine: `pip install PyQt6 PyQt6-WebEngine`

### Installing with winget (Windows only)

To install Orange with [winget](https://docs.microsoft.com/en-us/windows/package-manager/winget/), run:

```Shell
winget install --id  UniversityofLjubljana.Orange 
```

## Running

Ensure you've activated the correct virtual environment. If following the above conda instructions:

```Shell
conda activate orange3
``` 

Run `orange-canvas` or `python3 -m Orange.canvas`. Add `--help` for a list of program options.

Starting up for the first time may take a while.


## Developing

[![GitHub Actions](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fbiolab%2Forange3%2Fbadge&label=build)](https://actions-badge.atrox.dev/biolab/orange3/goto) [![codecov](https://img.shields.io/codecov/c/github/biolab/orange3)](https://codecov.io/gh/biolab/orange3) [![Contributor count](https://img.shields.io/github/contributors-anon/biolab/orange3)](https://github.com/biolab/orange3/graphs/contributors) [![Latest GitHub commit](https://img.shields.io/github/last-commit/biolab/orange3)](https://github.com/biolab/orange3/commits/master)

Want to write a widget? [Use the Orange3 example add-on template.](https://github.com/biolab/orange3-example-addon)

Want to get involved? Join us on [Discord](https://discord.gg/FWrfeXV), introduce yourself in #general! 

Take a look at our [contributing guide](https://github.com/irgolic/orange3/blob/README-shields/CONTRIBUTING.md) and [style guidelines](https://github.com/biolab/orange-widget-base/wiki/Widget-UI).

Check out our widget development [docs](https://orange-widget-base.readthedocs.io/en/latest/?badge=latest) for a comprehensive guide on writing Orange widgets.

### The Orange ecosystem

The development of core Orange is primarily split into three repositories:

[biolab/orange-canvas-core](https://www.github.com/biolab/orange-canvas-core) implements the canvas,  
[biolab/orange-widget-base](https://www.github.com/biolab/orange-widget-base) is a handy widget GUI library,  
[biolab/orange3](https://www.github.com/biolab/orange3) brings it all together and implements the base data mining toolbox.	

Additionally, add-ons implement additional widgets for more specific use cases. [Anyone can write an add-on.](https://github.com/biolab/orange3-example-addon) Some of our first-party add-ons:

- [biolab/orange3-text](https://www.github.com/biolab/orange3-text)
- [biolab/orange3-bioinformatics](https://www.github.com/biolab/orange3-bioinformatics)
- [biolab/orange3-timeseries](https://www.github.com/biolab/orange3-timeseries)    
- [biolab/orange3-single-cell](https://www.github.com/biolab/orange3-single-cell)    
- [biolab/orange3-imageanalytics](https://www.github.com/biolab/orange3-imageanalytics)    
- [biolab/orange3-educational](https://www.github.com/biolab/orange3-educational)    
- [biolab/orange3-geo](https://www.github.com/biolab/orange3-geo)    
- [biolab/orange3-associate](https://www.github.com/biolab/orange3-associate)    
- [biolab/orange3-network](https://www.github.com/biolab/orange3-network)
- [biolab/orange3-explain](https://www.github.com/biolab/orange3-explain)

### Setting up for core Orange development

First, fork the repository by pressing the fork button in the top-right corner of this page.

Set your GitHub username,

```Shell
export MY_GITHUB_USERNAME=replaceme
```

create a conda environment, clone your fork, and install it:

```Shell
conda create python=3 --yes --name orange3
conda activate orange3

git clone ssh://git@github.com/$MY_GITHUB_USERNAME/orange3

# Install PyQT and PyQtWebEngine. You can also use PyQt6
pip install -r requirements-pyqt.txt
pip install -e orange3
```

Now you're ready to work with git. See GitHub's guides on [pull requests](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/proposing-changes-to-your-work-with-pull-requests), [forks](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/working-with-forks) if you're unfamiliar. If you're having trouble, get in touch on [Discord](https://discord.gg/FWrfeXV).

#### Running

Run Orange with `python -m Orange.canvas` (after activating the conda environment).

`python -m Orange.canvas -l 2 --no-splash --no-welcome` will skip the splash screen and welcome window, and output more debug info. Use `-l 4` for more.

Add `--clear-widget-settings` to clear the widget settings before start.

To explore the dark side of the Orange, try `--style=fusion:breeze-dark`

Argument `--help` lists all available options.

To run tests, use `unittest Orange.tests Orange.widgets.tests`


### Setting up for development of all components

Should you wish to contribute Orange's base components (the widget base and the canvas), you must also clone these two repositories from Github instead of installing them as dependencies of Orange3.

First, fork all the repositories to which you want to contribute. 

Set your GitHub username,

```Shell
export MY_GITHUB_USERNAME=replaceme
```

create a conda environment, clone your forks, and install them:

```Shell
conda create python=3 --yes --name orange3
conda activate orange3

# Install PyQT and PyQtWebEngine. You can also use PyQt6
pip install -r requirements-pyqt.txt

git clone ssh://git@github.com/$MY_GITHUB_USERNAME/orange-widget-base
pip install -e orange-widget-base

git clone ssh://git@github.com/$MY_GITHUB_USERNAME/orange-canvas-core
pip install -e orange-canvas-core

git clone ssh://git@github.com/$MY_GITHUB_USERNAME/orange3
pip install -e orange3

# Repeat for any add-on repositories
```

It's crucial to install `orange-base-widget` and `orange-canvas-core` before `orange3` to ensure that `orange3` will use your local versions.
",2023-07-07 18:44:15+00:00
orchest,orchest,orchest/orchest,"Build data pipelines, the easy way 🛠️",https://orchest.readthedocs.io/en/stable/,False,3896,2023-07-07 06:55:03+00:00,2020-05-21 17:33:05+00:00,237,41,28,201,v2023.04.2,2023-04-03 13:28:42+00:00,Apache License 2.0,9688,v2023.04.2,204,2023-04-03 13:28:42+00:00,2023-07-07 06:55:03+00:00,2023-06-06 09:48:26+00:00,"<p align=""center"">
<a href=""https://orchest.io"">
  <img src=""docs/source/img/logo.png"" width=""350px"" />
</a>
</p>

<p align=""center"">
<a href=https://orchest.readthedocs.io/en/stable><img src=""https://readthedocs.org/projects/orchest/badge/?version=stable&style=flat""></a>
</p>

<p align=center><i>Notice: we’re no longer actively developing Orchest. We could not find a way to make building a workflow orchestrator commercially viable. Check out Apache Airflow for a robust workflow solution.</i></p>


## Build data pipelines, the easy way 🙌

No frameworks. No YAML. Just write your data processing code directly in **Python**, **R** or
**Julia**.

<p align=""center"">
  <img width=""100%"" src=""https://user-images.githubusercontent.com/1309307/191785568-ce4857c3-e71f-4b71-84ce-dfa5d65a98f9.gif"">
</p>

<p align=""center"">
  <i>💡 Watch the <a target=""_blank"" href=""https://vimeo.com/764866337"">full narrated video</a> to learn more about building data pipelines in Orchest.</i>
 </p>

> **Note**: Orchest is in **beta**.

## Features

- **Visually construct pipelines** through our user-friendly UI
- **Code in Notebooks** and scripts
  ([quickstart](https://docs.orchest.io/en/stable/getting_started/quickstart.html))
- Run any subset of a pipelines directly or periodically
  ([jobs](https://docs.orchest.io/en/stable/fundamentals/jobs.html))
- Easily define your dependencies to run on **any machine**
  ([environments](https://docs.orchest.io/en/stable/fundamentals/environments.html))
- Spin up services whose lifetime spans across the entire pipeline run
  ([services](https://docs.orchest.io/en/stable/fundamentals/services.html))
- Version your projects using git
  ([projects](https://docs.orchest.io/en/stable/fundamentals/projects.html))

**When to use Orchest?** Read it in the
[docs](https://docs.orchest.io/en/stable/getting_started/when_to_use_orchest.html).

👉 Get started with our
[quickstart](https://docs.orchest.io/en/stable/getting_started/quickstart.html) tutorial or have a look at our [video tutorials](https://www.orchest.io/video-tutorials) explaining some of Orchest's core concepts.

## Roadmap

Missing a feature? Have a look at [our public roadmap](https://github.com/orgs/orchest/projects/1)
to see what the team is working on in the short and medium term.
Still missing it? Please [let us know by opening an issue](https://github.com/orchest/orchest/issues/new/choose)!

## Examples

Get started with an example project:

- [Train and compare 3 regression models](https://github.com/orchest/quickstart)
- [Connecting to an external database using SQLAlchemy](https://github.com/astrojuanlu/orchest-sqlalchemy)
- [Run dbt in Orchest for a dbt + Python transform pipeline](https://github.com/ricklamers/orchest-dbt)
- [Use PySpark in Orchest](https://github.com/ricklamers/orchest-hello-spark)

👉 Check out the full list of [example projects](https://github.com/orchest/orchest-examples).

[![Open in Orchest](https://github.com/orchest/orchest-examples/raw/main/imgs/open_in_orchest_large.svg)](https://cloud.orchest.io/)

## Installation

Want to skip [the installation](https://docs.orchest.io/en/stable/getting_started/installation.html)
and jump right in? Then try out our managed service: [Orchest Cloud](https://cloud.orchest.io/signup).

## Slack Community

Join our Slack to chat about Orchest, ask questions, and share tips.

[![Join us on Slack](https://img.shields.io/badge/%20-Join%20us%20on%20Slack-blue?style=for-the-badge&logo=slack&labelColor=5c5c5c)](https://join.slack.com/t/orchest/shared_invite/zt-g6wooj3r-6XI8TCWJrXvUnXKdIKU_8w)

## License

The software in this repository is licensed as follows:

- All content residing under the ` orchest-sdk/` and `orchest-cli/` directories of this repository
  are licensed under the `Apache-2.0` license as defined in `orchest-sdk/LICENSE` and
  `orchest-cli/LICENSE` respectively.
- Content outside of the above mentioned directories is available under the `AGPL-3.0` license.

## Contributing

Contributions are more than welcome! Please see our [contributor
guides](https://docs.orchest.io/en/stable/development/contributing.html) for more details.

Alternatively, you can submit your pipeline to the curated list of [Orchest
examples](https://github.com/orchest/orchest-examples) that are automatically loaded in every
Orchest deployment! 🔥

## Contributors

<!-- To get src for img: https://api.github.com/users/username -->

<a href=""https://github.com/ricklamers""><img src=""https://avatars2.githubusercontent.com/u/1309307?v=4"" title=""ricklamers"" width=""50"" height=""50""></a>
<a href=""https://github.com/yannickperrenet""><img src=""https://avatars0.githubusercontent.com/u/26223174?v=4"" title=""yannickperrenet"" width=""50"" height=""50""></a>
<a href=""https://github.com/fruttasecca""><img src=""https://avatars3.githubusercontent.com/u/19429509?v=4"" title=""fruttasecca"" width=""50"" height=""50""></a>
<a href=""https://github.com/samkovaly""><img src=""https://avatars2.githubusercontent.com/u/32314099?v=4"" title=""samkovaly"" width=""50"" height=""50""></a>
<a href=""https://github.com/VivanVatsa""><img src=""https://avatars0.githubusercontent.com/u/56357691?v=4"" title=""VivanVatsa"" width=""50"" height=""50""></a>
<a href=""https://github.com/obulat""><img src=""https://avatars1.githubusercontent.com/u/15233243?v=4"" title=""obulat"" width=""50"" height=""50""></a>
<a href=""https://github.com/howie6879""><img src=""https://avatars.githubusercontent.com/u/17047388?v=4"" title=""howie6879"" width=""50"" height=""50""></a>
<a href=""https://github.com/FanaHOVA""><img src=""https://avatars.githubusercontent.com/u/6490430?v=4"" title=""FanaHOVA"" width=""50"" height=""50""></a>
<a href=""https://github.com/mitchglass97""><img src=""https://avatars.githubusercontent.com/u/52224377?v=4"" title=""mitchglass97"" width=""50"" height=""50""></a>
<a href=""https://github.com/joe-bell""><img src=""https://avatars.githubusercontent.com/u/7349341?v=4"" title=""joe-bell"" width=""50"" height=""50""></a>
<a href=""https://github.com/cceyda""><img src=""https://avatars.githubusercontent.com/u/15624271?v=4"" title=""cceyda"" width=""50"" height=""50""></a>
<a href=""https://github.com/MWeltevrede""><img src=""https://avatars.githubusercontent.com/u/31962715?v=4"" title=""MWeltevrede"" width=""50"" height=""50""></a>
<a href=""https://github.com/kingabzpro""><img src=""https://avatars.githubusercontent.com/u/36753484?v=4"" title=""Abid"" width=""50"" height=""50""></a>
<a href=""https://github.com/iannbing""><img src=""https://avatars.githubusercontent.com/u/627607?v=4"" title=""iannbing"" width=""50"" height=""50""></a>
<a href=""https://github.com/andtheWings""><img src=""https://avatars.githubusercontent.com/u/5892089?v=4"" title=""andtheWings"" width=""50"" height=""50""></a>
<a href=""https://github.com/jacobodeharo""><img src=""https://avatars.githubusercontent.com/jacobodeharo?v=4"" title=""jacobodeharo"" width=""50"" height=""50""></a>
<a href=""https://github.com/nhaghighat""><img src=""https://avatars.githubusercontent.com/u/3792293?v=4"" title=""nhaghighat"" width=""50"" height=""50""></a>
<a href=""https://github.com/porcupineyhairs""><img src=""https://avatars.githubusercontent.com/u/61983466?v=4"" title=""porcupineyhairs"" width=""50"" height=""50""></a>
<a href=""https://github.com/ncspost""><img src=""https://avatars.githubusercontent.com/ncspost?v=4"" title=""ncspost"" width=""50"" height=""50""></a>
<a href=""https://github.com/cavriends""><img src=""https://avatars.githubusercontent.com/u/4497501?v=4"" title=""cavriends"" width=""50"" height=""50""></a>
<a href=""https://github.com/astrojuanlu""><img src=""https://avatars.githubusercontent.com/u/316517?v=4"" title=""astrojuanlu"" width=""50"" height=""50""></a>
<a href=""https://github.com/mausworks""><img src=""https://avatars.githubusercontent.com/u/8259221?v=4"" title=""mausworks"" width=""50"" height=""50""></a>
<a href=""https://github.com/jerdna-regeiz""><img src=""https://avatars.githubusercontent.com/u/7195718?v=4"" title=""jerdna-regeiz"" width=""50"" height=""50""></a>
<a href=""https://github.com/sbarrios93""><img src=""https://avatars.githubusercontent.com/u/19554889?v=4"" title=""sbarrios93"" width=""50"" height=""50""></a>
<a href=""https://github.com/cacrespo""><img src=""https://avatars.githubusercontent.com/u/10950697?v=4"" title=""cacrespo"" width=""50"" height=""50""></a>
",2023-07-07 18:44:19+00:00
overseer,overseer,framed-data/overseer,Overseer is a library for building and running data pipelines in Clojure.,,False,86,2023-05-20 21:09:05+00:00,2015-01-20 18:13:02+00:00,10,13,4,0,,,Eclipse Public License 1.0,239,,0,,2023-05-20 21:09:04+00:00,2019-02-20 20:19:24+00:00,"# Overseer

Overseer is a library for building and running data pipelines in Clojure.
It allows for defining workflows as a graph (DAG) of dependent tasks, and handles
scheduling, execution, and failure handling among a pool of workers.

## Installation

Overseer is available on Clojars, and can be included in your leiningen `project.clj` by adding the following to `:dependencies`:

[![Clojars Project](http://clojars.org/io.framed/overseer/latest-version.svg)](http://clojars.org/io.framed/overseer)

![Build Status](https://circleci.com/gh/framed-data/overseer.svg?style=shield&circle-token=caf3faafe0f68217721b26e571a84bc1088b5801)

## Usage
If you're looking to get up and running in just a few minutes, check out the [quickstart](https://github.com/framed-data/overseer/wiki/Quickstart).
The [User Guide](https://www.gitbook.com/book/framed/overseer/) contains comprehensive information on installing, configuring, and using Overseer. Finally, you can find nitty-gritty details in the [API docs](https://framed-data.github.io/overseer).

## License
Eclipse Public License v1.0 (see [LICENSE](https://github.com/framed-data/overseer/blob/master/LICENSE))
",2023-07-07 18:44:23+00:00
pachyderm,pachyderm,pachyderm/pachyderm,Data-Centric Pipelines and Data Versioning,https://www.pachyderm.com/,False,5941,2023-07-07 03:39:32+00:00,2014-09-04 07:50:02+00:00,558,162,157,450,v2.6.5,2023-06-30 17:44:47+00:00,Apache License 2.0,21851,vv1.7.7,1240,2018-10-09 00:23:04+00:00,2023-07-07 18:38:34+00:00,2023-07-07 16:54:28+00:00,"<p align=""center"">
	<img src='./Pachyderm_Icon-01.svg' height='225' title='Pachyderm'>
</p>

[![GitHub release](https://img.shields.io/github/release/pachyderm/pachyderm.svg?style=flat-square)](https://github.com/pachyderm/pachyderm/releases)
[![GitHub license](https://img.shields.io/badge/license-Pachyderm-blue)](https://github.com/pachyderm/pachyderm/blob/master/LICENSE)
[![GoDoc](https://godoc.org/github.com/pachyderm/pachyderm?status.svg)](https://pkg.go.dev/github.com/pachyderm/pachyderm/v2/src/client)
[![Go Report Card](https://goreportcard.com/badge/github.com/pachyderm/pachyderm)](https://goreportcard.com/report/github.com/pachyderm/pachyderm)
[![Slack Status](https://badge.slack.pachyderm.io/badge.svg)](https://slack.pachyderm.io)
[![CLA assistant](https://cla-assistant.io/readme/badge/pachyderm/pachyderm)](https://cla-assistant.io/pachyderm/pachyderm)

# Pachyderm – Automate data transformations with data versioning and lineage


Pachyderm is cost-effective at scale, enabling data engineering teams to automate complex pipelines with sophisticated data transformations across any type of data. Our unique approach provides parallelized processing of multi-stage, language-agnostic pipelines with data versioning and data lineage tracking. Pachyderm delivers the ultimate CI/CD engine for data. 

## Features

- Data-driven pipelines automatically trigger based on detecting data changes.
- Immutable data lineage with data versioning of any data type. 
- Autoscaling and parallel processing built on Kubernetes for resource orchestration.
- Uses standard object stores for data storage with automatic deduplication.  
- Runs across all major cloud providers and on-premises installations.


## Getting Started
To start deploying your end-to-end version-controlled data pipelines, run Pachyderm [locally](https://docs.pachyderm.com/latest/getting-started/local-installation/) or you can also [deploy on AWS/GCE/Azure](https://docs.pachyderm.com/latest/deploy-manage/deploy/amazon_web_services/) in about 5 minutes. 

You can also refer to our complete [documentation](https://docs.pachyderm.com) to see tutorials, check out example projects, and learn about advanced features of Pachyderm.

If you'd like to see some examples and learn about core use cases for Pachyderm:
- [Examples](https://github.com/pachyderm/examples)
- [Use Cases](https://www.pachyderm.com/use-cases/)
- [Case Studies](https://www.pachyderm.com/case-studies/)

## Documentation

[Official Documentation](https://docs.pachyderm.com/)

## Community
Keep up to date and get Pachyderm support via:
- [![Twitter](https://img.shields.io/twitter/follow/pachyderminc?style=social)](https://twitter.com/pachyderminc) Follow us on Twitter.
- [![Slack Status](https://badge.slack.pachyderm.io/badge.svg)](https://slack.pachyderm.io) Join our community [Slack Channel](https://slack.pachyderm.io) to get help from the Pachyderm team and other users.

## Contributing
To get started, sign the [Contributor License Agreement](https://cla-assistant.io/pachyderm/pachyderm).

You should also check out our [contributing guide](https://docs.pachyderm.com/latest/contributing/setup/).

Send us PRs, we would love to see what you do! You can also check our GH issues for things labeled ""help-wanted"" as a good place to start. We're sometimes bad about keeping that label up-to-date, so if you don't see any, just let us know.

## Join Us

WE'RE HIRING! Love Docker, Go and distributed systems? Learn more about [our open positions](https://boards.greenhouse.io/pachyderm)

## Usage Metrics

Pachyderm automatically reports anonymized usage metrics. These metrics help us
understand how people are using Pachyderm and make it better.  They can be
disabled by setting the env variable `METRICS` to `false` in the pachd
container.

## License Information
Pachyderm has moved some components of Pachyderm Platform to a [source-available limited license](LICENSE). 

We remain committed to the culture of open source, developing our product transparently and collaboratively with our community, and giving our community and customers source code access and the ability to study and change the software to suit their needs.

Under the Pachyderm Community License, you can access the source code and modify or redistribute it; there is only one thing you cannot do, and that is use it to make a competing offering. 

Check out our [License FAQ Page](https://www.pachyderm.com/community-license-faq/) for more information.
",2023-07-07 18:44:27+00:00
pandaworkflowmanagementsystem,pandawms,PanDAWMS/pandawms,PanDA workload management system,,False,4,2021-12-22 06:48:39+00:00,2014-02-04 20:12:56+00:00,1,25,1,0,,,GNU General Public License v3.0,5,,0,,,2014-02-04 22:56:01+00:00,"PanDA WMS
=========

This is the open source repository for the PanDA Workload Management System.

The PanDA (Production and Distributed Analysis) system was originally developed for the ATLAS experiment at CERN's Large Hadron Collider (LHC) by teams at Brookhaven National Laboratory and the University of Texas at Arlington. 

While PanDA continues to be developed for ATLAS, where it is used for the massively scaled distributed data-intensive production and analysis processing of the experiment at over 100 sites globally (150k concurrent jobs around the clock, a million jobs a day, analyzing a data set currently 150 petabytes in size), it is also being generalized, extended and packaged for use by other scientific communities through the BigPanDA project supported by the US Department of Energy.

For more information see:

PanDA twiki at CERN: https://twiki.cern.ch/twiki/bin/view/PanDA/PanDA

PanDA WMS project site: http://pandawms.org

ATLAS experiment: http://atlas.ch/
",2023-07-07 18:44:32+00:00
papy,papy,mcieslik-mctp/papy,The papy package provides an implementation of the flow-based programming paradigm in Python,http://mcieslik-mctp.github.io/papy/,False,28,2023-05-03 12:34:10+00:00,2013-12-05 16:11:33+00:00,7,4,2,0,,,MIT License,20,,0,,2023-05-03 12:34:10+00:00,2014-11-11 03:09:02+00:00,"**PaPy** - Parallel Pipelines in Python
#######################################

The ``papy`` package provides an implementation of the flow-based programming 
paradigm in Python that enables the construction and deployment of distributed
workflows.

The ``NuMap`` package is a parallel (thread- or process-based, local or 
remote), buffered, multi-task, ``itertools.imap`` or 
``multiprocessing.Pool.imap`` function replacment. Like ``imap`` it 
evaluates a function on elements of a sequence or iterable, and it does so 
lazily. Laziness can be adjusted via  the ""stride"" and ""buffer"" arguments. 
Unlike ``imap``, ``NuMap`` supports  **multiple pairs** of function and 
iterable **tasks**. The **tasks** are **not** queued rather they are 
**interwoven** and share a pool or **worker** ""processes"" or ""threads"" and 
a memory ""buffer"".

Documentation can be found `here <http://mcieslik-mctp.github.io/papy/>`_

The package is tested on Python 2.7.6
",2023-07-07 18:44:35+00:00
parsl,parsl,Parsl/parsl,Parsl - a Python parallel scripting library,http://parsl-project.org,False,387,2023-06-27 11:17:42+00:00,2016-10-20 16:27:51+00:00,120,29,76,0,,,Apache License 2.0,4252,benc-wip,62,2018-12-07 13:01:56+00:00,2023-07-07 18:15:41+00:00,2023-07-07 08:52:06+00:00,"Parsl - Parallel Scripting Library
==================================
|licence| |build-status| |docs| |NSF-1550588| |NSF-1550476| |NSF-1550562| |NSF-1550528|

Parsl extends parallelism in Python beyond a single computer.

You can use Parsl
`just like Python's parallel executors <https://parsl.readthedocs.io/en/stable/userguide/workflow.html#parallel-workflows-with-loops>`_
but across *multiple cores and nodes*.
However, the real power of Parsl is in expressing multi-step workflows of functions.
Parsl lets you chain functions together and will launch each function as inputs and computing resources are available.

.. code-block:: python

    import parsl
    from parsl import python_app

    # Start Parsl on a single computer
    parsl.load()

    # Make functions parallel by decorating them
    @python_app
    def f(x):
        return x + 1

    @python_app
    def g(x):
        return x * 2

    # These functions now return Futures, and can be chained
    future = f(1)
    assert future.result() == 2

    future = g(f(1))
    assert future.result() == 4


Start with the `configuration quickstart <https://parsl.readthedocs.io/en/stable/quickstart.html#getting-started>`_ to learn how to tell Parsl how to use your computing resource,
then explore the `parallel computing patterns <https://parsl.readthedocs.io/en/stable/userguide/workflow.html>`_ to determine how to use parallelism best in your application.

.. |licence| image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg
   :target: https://github.com/Parsl/parsl/blob/master/LICENSE
   :alt: Apache Licence V2.0
.. |build-status| image:: https://github.com/Parsl/parsl/actions/workflows/ci.yaml/badge.svg
   :target: https://github.com/Parsl/parsl/actions/workflows/ci.yaml
   :alt: Build status
.. |docs| image:: https://readthedocs.org/projects/parsl/badge/?version=stable
   :target: http://parsl.readthedocs.io/en/stable/?badge=stable
   :alt: Documentation Status
.. |NSF-1550588| image:: https://img.shields.io/badge/NSF-1550588-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=1550588
   :alt: NSF award info
.. |NSF-1550476| image:: https://img.shields.io/badge/NSF-1550476-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=1550476
   :alt: NSF award info
.. |NSF-1550562| image:: https://img.shields.io/badge/NSF-1550562-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=1550562
   :alt: NSF award info
.. |NSF-1550528| image:: https://img.shields.io/badge/NSF-1550528-blue.svg
   :target: https://nsf.gov/awardsearch/showAward?AWD_ID=1550528
   :alt: NSF award info
   
Quickstart
==========

Install Parsl using pip::

    $ pip3 install parsl

To run the Parsl tutorial notebooks you will need to install Jupyter::

    $ pip3 install jupyter

Detailed information about setting up Jupyter with Python is available `here <https://jupyter.readthedocs.io/en/latest/install.html>`_

Note: Parsl uses an opt-in model to collect anonymous usage statistics for reporting and improvement purposes. To understand what stats are collected and enable collection please refer to the `usage tracking guide <http://parsl.readthedocs.io/en/stable/userguide/usage_tracking.html>`__

Documentation
=============

The complete parsl documentation is hosted `here <http://parsl.readthedocs.io/en/stable/>`_.

The Parsl tutorial is hosted on live Jupyter notebooks `here <https://mybinder.org/v2/gh/Parsl/parsl-tutorial/master>`_


For Developers
--------------

1. Download Parsl::

    $ git clone https://github.com/Parsl/parsl


2. Build and Test::

    $ make   # show all available makefile targets
    $ make virtualenv # create a virtual environment
    $ source .venv/bin/activate # activate the virtual environment
    $ make deps # install python dependencies from test-requirements.txt
    $ make test # make (all) tests. Run ""make config_local_test"" for a faster, smaller test set.
    $ make clean # remove virtualenv and all test and build artifacts

3. Install::

    $ cd parsl
    $ python3 setup.py install

4. Use Parsl!

Requirements
============

Parsl is supported in Python 3.8+. Requirements can be found `here <requirements.txt>`_. Requirements for running tests can be found `here <test-requirements.txt>`_.

Code of Conduct
===============

Parsl seeks to foster an open and welcoming environment - Please see the `Parsl Code of Conduct <https://github.com/Parsl/parsl/blob/master/CoC.md>`_ for more details.

Contributing
============

We welcome contributions from the community. Please see our `contributing guide <https://github.com/Parsl/parsl/blob/master/CONTRIBUTING.rst>`_.
",2023-07-07 18:44:38+00:00
pegasus,pegasus,pegasus-isi/pegasus,"Pegasus Workflow Management System - Automate, recover, and debug scientific computations.",https://pegasus.isi.edu/,False,157,2023-06-28 06:27:12+00:00,2013-02-04 21:18:31+00:00,68,26,24,0,,,Apache License 2.0,15346,5.0.6,68,2023-06-30 20:06:04+00:00,2023-07-03 15:52:08+00:00,2023-07-03 15:51:55+00:00,"<p align=""center"">
      <img src=""doc/sphinx/images/pegasusfront-black-reduced.png"" width=""200"" alt=""Pegasus WMS"" />
</p>

Pegasus Workflow Management System
----------------------------------
<p align=""left"">
    <img src=""https://img.shields.io/github/license/pegasus-isi/pegasus?color=blue&label=Licence""/>
    <img src=""https://img.shields.io/github/v/tag/pegasus-isi/pegasus?label=Latest""/>
    <img src=""https://img.shields.io/pypi/dm/pegasus-wms?color=green&label=PyPI%20Downloads""/>
    <img src=""https://img.shields.io/github/contributors-anon/pegasus-isi/pegasus?color=green&label=Contributors""/>
</p>

Pegasus WMS is a configurable system for mapping and executing scientific
workflows over a wide range of computational infrastructures including laptops,
campus clusters, supercomputers, grids, and commercial and academic clouds.
Pegasus has been used to run workflows with up to 1 million tasks that process
tens of terabytes of data at a time.

Pegasus WMS bridges the scientific domain and the execution environment by
automatically mapping high-level workflow descriptions onto distributed
resources. It automatically locates the necessary input data and computational
resources required by a workflow, and plans out all of the required data
transfer and job submission operations required to execute the workflow.
Pegasus enables scientists to construct workflows in abstract terms without
worrying about the details of the underlying execution environment or the
particulars of the low-level specifications required by the middleware (Condor,
Globus, Amazon EC2, etc.). In the process, Pegasus can plan and optimize the
workflow to enable efficient, high-performance execution of large
workflows on complex, distributed infrastructures.

Pegasus has a number of features that contribute to its usability and
effectiveness:

* Portability / Reuse – User created workflows can easily be run in different
environments without alteration. Pegasus currently runs workflows on top of
Condor pools, Grid infrastructures such as Open Science Grid and XSEDE,
Amazon EC2, Google Cloud, and HPC clusters. The same workflow can run on a
single system or across a heterogeneous set of resources.
* Performance – The Pegasus mapper can reorder, group, and prioritize tasks in
order to increase overall workflow performance.
* Scalability – Pegasus can easily scale both the size of the workflow, and
the resources that the workflow is distributed over. Pegasus runs workflows
ranging from just a few computational tasks up to 1 million. The number of
resources involved in executing a workflow can scale as needed without any
impediments to performance.
* Provenance – By default, all jobs in Pegasus are launched using the
Kickstart wrapper that captures runtime provenance of the job and helps in
debugging. Provenance data is collected in a database, and the data can be
queried with tools such as pegasus-statistics, pegasus-plots, or directly
using SQL.
* Data Management – Pegasus handles replica selection, data transfers and
output registration in data catalogs. These tasks are added to a workflow as
auxilliary jobs by the Pegasus planner.
* Reliability – Jobs and data transfers are automatically retried in case of
failures. Debugging tools such as pegasus-analyzer help the user to debug the
workflow in case of non-recoverable failures.
* Error Recovery – When errors occur, Pegasus tries to recover when possible
by retrying tasks, by retrying the entire workflow, by providing workflow-level
checkpointing, by re-mapping portions of the workflow, by trying alternative
data sources for staging data, and, when all else fails, by providing a rescue
workflow containing a description of only the work that remains to be done.
It cleans up storage as the workflow is executed so that data-intensive
workflows have enough space to execute on storage-constrained resources.
Pegasus keeps track of what has been done (provenance) including the locations
of data used and produced, and which software was used with which parameters.


Getting Started
---------------

You can find more information about Pegasus on the [Pegasus Website](http://pegasus.isi.edu).

Pegasus has an extensive [User Guide](http://pegasus.isi.edu/documentation/)
that documents how to create, plan, and monitor workflows.

We recommend you start by completing the Pegasus Tutorial from [Chapter 3 of the
Pegasus User Guide](https://pegasus.isi.edu/documentation/user-guide/tutorial.html).

The easiest way to install Pegasus is to use one of the binary packages
available on the [Pegasus downloads page](http://pegasus.isi.edu/downloads).
Consult [Chapter 2 of the Pegasus User Guide](https://pegasus.isi.edu/documentation/user-guide/installation.html)
for more information about installing Pegasus from binary packages.

There is documentation on the Pegasus website for the Python, Java and R
[Abstract Workflow Generator APIs](https://pegasus.isi.edu/documentation/reference-guide/api-reference.html).
We strongly recommend using the Python API which is feature complete, and also
allows you to invoke all the pegasus command line tools.

You can use *pegasus-init* command line tool to run several examples
on your local machine. Consult [Chapter 4 of the Pegasus
User Guide](https://pegasus.isi.edu/documentation/user-guide/example-workflows.html)
for more information.

There are also examples of how to [Configure Pegasus for Different Execution
Environments](https://pegasus.isi.edu/documentation/user-guide/execution-environments.html)
in the Pegasus User Guide.

If you need help using Pegasus, please contact us. See the [contact page]
(http://pegasus.isi.edu/contact) on the Pegasus website for more information.


Building from Source
--------------------

Pegasus can be compiled on any recent Linux or Mac OS X system.

### Source Dependencies

In order to build Pegasus from source, make sure you have the following installed:

* Git
* Java 8 or higher
* Python 3.5 or higher
* R
* Ant
* gcc
* g++
* make
* tox 3.14.5 or higher
* mysql (optional, required to access MySQL databases)
* postgresql (optional, required to access PostgreSQL databases)
* Python pyyaml
* Python GitPython

Other packages may be required to run unit tests, and build MPI tools.

### Compiling

Ant is used to compile Pegasus.

To get a list of build targets run:

    $ ant -p

The targets that begin with ""dist"" are what you want to use.

To build a basic binary tarball (excluding documentation), run:

    $ ant dist

To build the release tarball (including documentation), run:

    $ ant dist-release

The resulting packages will be created in the `dist` subdirectory.
",2023-07-07 18:44:42+00:00
pentahokettle,pentaho-kettle,pentaho/pentaho-kettle,Pentaho Data Integration ( ETL ) a.k.a Kettle,,False,6729,2023-07-07 13:07:49+00:00,2013-10-08 18:27:11+00:00,3289,466,189,1,5.2.0.2-C-185-R,2015-01-05 19:15:15+00:00,Apache License 2.0,23180,pre-merge-future-develop,1337,2015-06-16 17:44:17+00:00,2023-07-07 13:07:49+00:00,2023-06-29 13:57:03+00:00,"# Pentaho Data Integration # 

Pentaho Data Integration ( ETL ) a.k.a Kettle

### Project Structure

* **assemblies:** 
Project distribution archive is produced under this module
* **core:** 
Core implementation
* **dbdialog:** 
Database dialog
* **ui:** 
User interface
* **engine:** 
PDI engine
* **engine-ext:** 
PDI engine extensions
* **[plugins:](plugins/README.md)** 
PDI core plugins
* **integration:** 
Integration tests

How to build
--------------

Pentaho Data Integration uses the Maven framework. 


#### Pre-requisites for building the project:
* Maven, version 3+
* Java JDK 11
* This [settings.xml](https://raw.githubusercontent.com/pentaho/maven-parent-poms/master/maven-support-files/settings.xml) in your <user-home>/.m2 directory

#### Building it

This is a Maven project, and to build it use the following command:

```
$ mvn clean install
```
Optionally you can specify -Drelease to trigger obfuscation and/or uglification (as needed)

Optionally you can specify -Dmaven.test.skip=true to skip the tests (even though
you shouldn't as you know)

The build result will be a Pentaho package located in ```target```.

#### Packaging / Distributing it

Packages can be built by using the following command:
```
$ mvn clean package
```

The packaged results will be in the `target/` sub-folders of `assemblies/*`.

For example, a distribution of the Desktop Client (CE) can then be found in: `assemblies/client/target/pdi-ce-*-SNAPSHOT.zip`.

#### Running the tests

__Unit tests__

This will run all unit tests in the project (and sub-modules). To run integration tests as well, see Integration Tests below.

```
$ mvn test
```

If you want to remote debug a single Java unit test (default port is 5005):

```
$ cd core
$ mvn test -Dtest=<<YourTest>> -Dmaven.surefire.debug
```

__Integration tests__

In addition to the unit tests, there are integration tests that test cross-module operation. This will run the integration tests.

```
$ mvn verify -DrunITs
```

To run a single integration test:

```
$ mvn verify -DrunITs -Dit.test=<<YourIT>>
```

To run a single integration test in debug mode (for remote debugging in an IDE) on the default port of 5005:

```
$ mvn verify -DrunITs -Dit.test=<<YourIT>> -Dmaven.failsafe.debug
```

To skip test

```
$ mvn clean install -DskipTests
```

To get log as text file

```
$ mvn clean install test >log.txt
```


__IntelliJ__

* Don't use IntelliJ's built-in maven. Make it use the same one you use from the commandline.
  * Project Preferences -> Build, Execution, Deployment -> Build Tools -> Maven ==> Maven home directory


### Contributing

1. Submit a pull request, referencing the relevant [Jira case](https://jira.pentaho.com/secure/Dashboard.jspa)
2. Attach a Git patch file to the relevant [Jira case](https://jira.pentaho.com/secure/Dashboard.jspa)

Use of the Pentaho checkstyle format (via `mvn checkstyle:check` and reviewing the report) and developing working 
Unit Tests helps to ensure that pull requests for bugs and improvements are processed quickly.

When writing unit tests, you have at your disposal a couple of ClassRules that can be used to maintain a healthy
test environment. Use [RestorePDIEnvironment](core/src/test/java/org/pentaho/di/junit/rules/RestorePDIEnvironment.java)
and [RestorePDIEngineEnvironment](engine/src/test/java/org/pentaho/di/junit/rules/RestorePDIEngineEnvironment.java)
for core and engine tests respectively.

pex.:
```java
public class MyTest {
  @ClassRule public static RestorePDIEnvironment env = new RestorePDIEnvironment();
  #setUp()...
  @Test public void testSomething() { 
    assertTrue( myMethod() ); 
  }
}
```  

### Asking for help
Please go to https://community.hitachivantara.com/community/products-and-solutions/pentaho/ to ask questions and get help.
",2023-07-07 18:44:47+00:00
phpflo,phpflo,phpflo/phpflo,Flow-based programming for PHP,,False,214,2023-06-14 22:11:10+00:00,2011-08-17 11:08:01+00:00,21,25,9,7,v2.1.1,2017-03-31 18:28:09+00:00,MIT License,211,v2.1.1,9,2017-03-31 18:28:09+00:00,2023-06-14 22:11:10+00:00,2017-05-06 07:02:31+00:00,"PhpFlo: Flow-based programming for PHP
==============================================

[![Build Status](https://secure.travis-ci.org/phpflo/phpflo.png)](http://travis-ci.org/phpflo/phpflo)
[![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/phpflo/phpflo/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/phpflo/phpflo/?branch=master)
[![Code Coverage](https://scrutinizer-ci.com/g/phpflo/phpflo/badges/coverage.png?b=master)](https://scrutinizer-ci.com/g/phpflo/phpflo/?branch=master)
[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org)

PhpFlo is a simple [flow-based programming](http://en.wikipedia.org/wiki/Flow-based_programming) implementation for PHP. It is a PHP port of [NoFlo](https://noflojs.org), a similar tool for Node.js. From WikiPedia:

> In computer science, flow-based programming (FBP) is a programming paradigm that defines applications as networks of ""black box"" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.

Developers used to the [Unix philosophy](http://en.wikipedia.org/wiki/Unix_philosophy) should be immediately familiar with FBP:

> This is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.

It also fits well in Alan Kay's [original idea of object-oriented programming](http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en):

> I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning -- it took a while to see how to do messaging in a programming language efficiently enough to be useful).

The system has been heavily inspired by [J. Paul Morrison's](http://www.jpaulmorrison.com/) book [Flow-Based Programming](http://www.jpaulmorrison.com/fbp/#More).

PhpFlo is still quite experimental, but may be useful for implementing flow control in PHP applications.

## Installing

PhpFlo can be installed from [Packagist.org](http://packagist.org/view/PhpFlo/PhpFlo) with the [composer](https://github.com/composer/composer) package manager. Just ensure your `composer.json` has the following:

```sh
php composer.phar require phpflo/phpflo
```

This gives you phpflo, [common](https://github.com/phpflo/phpflo-common), [fbp](https://github.com/phpflo/phpflo-fbp) and [flowtrace](https://github.com/phpflo/phpflo-flowtrace) packages, provided by a [monorepository](https://developer.atlassian.com/blog/2015/10/monorepos-in-git/). This helps us to manage the code in a easier way.
Every package is also split as a separate repository on github and is installable by itself via packagist.

## Autoloading

To use PhpFlo, you need a [PHP Standards Group -compatible autoloader](http://groups.google.com/group/php-standards/web/psr-0-final-proposal). You can use the Composer-supplied autoloader:

```php
<?php

require 'vendor/autoload.php';
```

## Examples

You can find examples on how to use phpflo in the [phpflo-component](https://github.com/phpflo/phpflo-component) package.

## Terminology

* Component: individual, pluggable and reusable piece of software. In this case a PHP class implementing `PhpFlo\Common\ComponentInterface`
* Graph: the control logic of a FBP application, can be either in programmatical or file format
* Inport: inbound port of a component
* Network: collection of processes connected by sockets. A running version of a graph
* Outport: outbound port of a component
* Process: an instance of a component that is running as part of a graph

## Components

A component is the main ingredient of flow-based programming. Component is a PHP class providing a set of input and output port handlers. These ports are used for connecting components to each other.

PhpFlo processes (the boxes of a flow graph) are instances of a component, with the graph controlling connections between ports of components.

### Structure of a component

Functionality a component provides:

* List of inports (named inbound ports)
* List of outports (named outbound ports)
* Handler for component initialization that accepts configuration
* Handler for connections for each inport

A minimal component would look like the following:

```php
<?php

use PhpFlo\Core\ComponentTrait;
use PhpFlo\Common\ComponentInterface;

class Forwarder implements ComponentInterface
{
    use ComponentTrait;
    protected $description = ""This component receives data on a single input port and sends the same data out to the output port"";

    public function __construct()
    {
        // Register ports
        $this->inPorts()->add('in', ['datatype' => 'all']);
        $this->outPorts()->add('out', ['datatype' => 'all']);

        // Forward data when we receive it
        $this->inPorts()->in->on('data', array($this, 'forward'));

        // Disconnect output port when input port disconnects
        $this->inPorts()->in->on('disconnect', array($this, 'disconnect'));
    }

    public function forward($data)
    {
        $this->outPorts()->out->send($data);
    }

    public function disconnect()
    {
        $this->outPorts->out->disconnect();
    }
}
```

Alternatively you can use ```PhpFlo\Core\Component``` via direct inheritance, which internally uses the trait.
This example component register two ports: _in_ and _out_. When it receives data in the _in_ port, it opens the _out_ port and sends the same data there. When the _in_ connection closes, it will also close the _out_ connection. So basically this component would be a simple repeater.
You can find more examples of components in the [phpflo-compoent](https://github.com/phpflo/phpflo-component) package.
Please mind that there's an mandatory second parameter for the ""add"" command. This array receives the port's meta information and has following defaults:
 
``` php
    $defaultAttributes = [
        'datatype' => 'all',
        'required' => false,
        'cached' => false,
        'addressable' => false,
    ];
```
This is but a subset of the available attributes, a noflo port can take.

* _datatype_ defines the ""to be expected"" datatype of the dataflow. Currently _all_ datatypes from noflo are implemented
* _required_ not implemented yet
* _cached_ not implemented yet
* _addressable_ decides if a port needs to be either an instance of Port (false) or ArrayPort (true)

Defining the datatype is mandatory, since there is a port matching check during graph building, according to this matrix:

| out\in   | all      | bang     | string   | bool     | number   | int      | object   | array    | date     | function |
| -------- |:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:| --------:|
| all      |    x     |    x     |          |          |          |          |          |          |          |          |
| bang     |    x     |    x     |          |          |          |          |          |          |          |          |
| string   |    x     |    x     |    x     |          |          |          |          |          |          |          |
| bool     |    x     |    x     |          |    x     |          |          |          |          |          |          |
| number   |    x     |    x     |          |          |    x     |          |          |          |          |          |
| int      |    x     |    x     |          |          |    x     |    x     |          |          |          |          |
| object   |    x     |    x     |          |          |          |          |    x     |          |          |          |
| array    |    x     |    x     |          |          |          |          |          |    x     |          |          |
| date     |    x     |    x     |          |          |          |          |          |          |    x     |          |
| function |    x     |    x     |          |          |          |          |          |          |          |    x     |

These types are only implicitly checked. There is no data validation during runtime!

### Some words on component design

Components should aim to be reusable, to do one thing and do it well. This is why often it is a good idea to split functionality traditionally done in one function to multiple components. For example, counting lines in a text file could happen in the following way:

* Filename is sent to a _Read File_ component
* _Read File_ reads it and sends the contents onwards to _Split String_ component
* _Split String_ splits the contents by newlines, and sends each line separately to a _Count_ component
* _Count_ counts the number of packets it received, and sends the total to a _Output_ component
* _Output_ displays the number

This way the whole logic of the application is in the graph, in how the components are wired together. And each of the components is easily reusable for other purposes.

If a component requires configuration, the good approach is to set sensible defaults in the component, and to allow them to be overridden via an input port. This method of configuration allows the settings to be kept in the graph itself, or for example to be read from a file or database, depending on the needs of the application.

The components should not depend on a particular global state, either, but instead attempt to keep the input and output ports their sole interface to the external world. There may be some exceptions, like a component that listens for HTTP requests or Redis pub-sub messages, but even in these cases the server, or subscription should be set up by the component itself.

### Ports and events

Being a flow-based programming environment, the main action in PhpFlo happens through ports and their connections. There are five events that can be associated with ports:

* _Attach_: there is a connection to the port
* _Connect_: the port has started sending or receiving a data transmission
* _Data_: an individual data packet in a transmission. There might be multiple depending on how a component operates
* _Disconnect_: end of data transmission
* _Detach_: A connection to the port has been removed

It depends on the nature of the component how these events may be handled. Most typical components do operations on a whole transmission, meaning that they should wait for the _disconnect_ event on inports before they act, but some components can also act on single _data_ packets coming in.

When a port has no connections, meaning that it was initialized without a connection, or a _detach_ event has happened, it should do no operations regarding that port.

## Graph file format

In addition to using PhpFlo in _embedded mode_ where you create the FBP graph programmatically (see [example](https://github.com/phpflo/phpflo/blob/master/examples/linecount/count.php)), you can also initialize and run graphs defined using a FBP file.
This format gives you the advantage of much less definition work, compared to the deprecated (but still valid) JSON files.

If you have older JSON definitions, you can still use them or convert then to FBP, using the dumper wrapped by the graph or directly from definition:
```php
$builder = new \PhpFlo\Core\Builder\ComponentFactory();
$network = new PhpFlo\Core\Network($builder);
$network->boot(__DIR__.'/count.json', $builder);
file_put_contents('./count.fbp', $network->getGraph()->toFbp());
```

The PhpFlo FBP files declare the processes used in the FBP graph, and the connections between them. The file format is shared between PhpFlo and NoFlo, and looks like the following:

```
ReadFile(ReadFile) out -> in SplitbyLines(SplitStr)
ReadFile(ReadFile) error -> in Display(Output)
SplitbyLines(SplitStr) out -> in CountLines(Counter)
CountLines(Counter) count -> in Display(Output)
```
Other supported formats are YAML and JSON, but keep in mind that the FBP domain specific language (DSL) should be the way to go if you want to use something like the noflo ui.

JSON example:
```json
{
    ""properties"": {
        ""name"": ""Count lines in a file""
    },
    ""processes"": {
        ""ReadFile"": {
            ""component"": ""ReadFile""
        },
        ""SplitbyLines"": {
            ""component"": ""SplitStr""
        },
        ""CountLines"": {
            ""component"": ""Counter""
        },
        ""Display"": {
            ""component"": ""Output""
        }
    },
    ""connections"": [
        {
            ""src"": {
                ""process"": ""ReadFile"",
                ""port"": ""out""
            },
            ""tgt"": {
                ""process"": ""SplitbyLines"",
                ""port"": ""in""
            }
        },
        {
            ""src"": {
                ""process"": ""ReadFile"",
                ""port"": ""error""
            },
            ""tgt"": {
                ""process"": ""Display"",
                ""port"": ""in""
            }
        },
        {
            ""src"": {
                ""process"": ""SplitbyLines"",
                ""port"": ""out""
            },
            ""tgt"": {
                ""process"": ""CountLines"",
                ""port"": ""in""
            }
        },
        {
            ""src"": {
                ""process"": ""CountLines"",
                ""port"": ""count""
            },
            ""tgt"": {
                ""process"": ""Display"",
                ""port"": ""in""
            }
        }
    ]
}
```

YAML example:
```yaml
properties:
    name: 'Count lines in a file'
initializers: {  }
processes:
    ReadFile:
        component: ReadFile
        metadata: { label: ReadFile }
    SplitbyLines:
        component: SplitStr
        metadata: { label: SplitStr }
    Display:
        component: Output
        metadata: { label: Output }
    CountLines:
        component: Counter
        metadata: { label: Counter }
connections:
    -
        src: { process: ReadFile, port: OUT }
        tgt: { process: SplitbyLines, port: IN }
    -
        src: { process: ReadFile, port: ERROR }
        tgt: { process: Display, port: IN }
    -
        src: { process: SplitbyLines, port: OUT }
        tgt: { process: CountLines, port: IN }
    -
        src: { process: CountLines, port: COUNT }
        tgt: { process: Display, port: IN }
```

To run a graph file, load it via the PhpFlow API:

```php
<?php

$builder = new PhpFlo\Core\Builder\ComponentFactory();

// create network
$network = new PhpFlo\Core\Network($builder);
$network
    ->boot(__DIR__.'/count.fbp')
    ->run($fileName, ""ReadFile"", ""source"")
    ->shutdown();
```

Note that after this the graph is _live_, meaning that you can add and remove nodes and connections, or send new _initial data_ to it. See [example](https://github.com/phpflo/phpflo/blob/master/examples/linecount/countFromJson.php).

Since the network now also features the ```HookableInterface```, you can easily add callbacks on events for e.g. debugging purposes:

```php
<?php
$builder = new PhpFlo\Core\Builder\ComponentFactory();

// create network
$network = new PhpFlo\Core\Network($builder);
$network
    ->hook(
        'data',
        'trace',
        function ($data, $socket) {
            echo $socket->getId() . print_r($data, true) . ""\n"";
        }
    )
    ->boot(__DIR__.'/count.fbp')
    ->run($fileName, ""ReadFile"", ""source"")
    ->shutdown();
```

As you can see, there's a lot of potential in the callbacks, since they can also use object references to store and/or manipulate data, but natively receive socket and data from the supported events.
This feature is also used in [phpflo/flowtrace](https://github.com/phpflo/flowtrace) library, which decorates the ```Network``` class and adds PSR-3 compatible logging.

## Testing

To be able to test your components, a trait is provided in [phpflo-common](https://github.com/phpflo/phpflo-core) which is automatically included as a dependency for phpflo. 
```PhpFlo\Test\ComponentTestTrait``` and ```PhpFlo\Test\Stub\Trait``` contain the necessary tools to make testing easier.

```php
<?php
namespace Tests\PhpFlo\Component;

use PhpFlo\Common\Test\TestUtilityTrait;

class CounterTest extends \PHPUnit_Framework_TestCase
{
    use TestUtilityTrait;

    public function testBehavior()
    {
        $counter = new Counter();
        $this->connectPorts($counter);

        $this->assertTrue($counter->inPorts()->has('in'));
        $this->assertTrue($counter->outPorts()->has('count'));

        $counter->appendCount(1);
        $counter->appendCount(""2"");
        $counter->appendCount(null);

        $counter->sendCount();

        $countData = $this->getOutPortData('count');
        $this->assertEquals(3, $countData[0]);
    }
}

```

Within this code example you can see that the outPorts are available via ```getOutportData('alias'')``` method. This will always return an array of data sent to that specific port, because you can iteratively call ports within a component.
On codelevel this is nothing more than callbacks with an internal storage of your data, so you can test a component and its interaction in isolation.
To be able to use the component in testing you will first need to ```connectPorts($component)``` or separately ```connectInPorts($component); connectOutPorts($component)```, so phpflo won't throw any ""port not connected"" errors at you.

## Development

PhpFlo development happens on GitHub. Just fork the [main repository](https://github.com/phpflo/phpflo), make modifications and send a pull request.

To run the unit tests you need PHPUnit. Run the tests with in development:

```sh
$ bin/phpunit
```

### Some ideas

* Use [phpDaemon](http://daemon.io/) to make the network run asynchronously, Node.js -like
",2023-07-07 18:44:51+00:00
piflow,piflow,cas-bigdatalab/piflow,πflow is a big data flow engine with spark support,,False,490,2023-06-29 14:49:51+00:00,2018-05-03 01:47:46+00:00,158,43,22,12,v1.5,2023-05-05 07:23:07+00:00,Apache License 2.0,1013,v1.5,12,2023-05-05 07:23:07+00:00,2023-06-29 14:49:52+00:00,2023-06-28 06:38:25+00:00,"![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-logo3.png)  
[![GitHub releases](https://img.shields.io/github/release/cas-bigdatalab/piflow.svg)](https://github.com/cas-bigdatalab/piflow/releases)
[![GitHub stars](https://img.shields.io/github/stars/cas-bigdatalab/piflow.svg)](https://github.com/cas-bigdatalab/piflow/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/cas-bigdatalab/piflow.svg)](https://github.com/cas-bigdatalab/piflow/network)
[![GitHub downloads](https://img.shields.io/github/downloads/cas-bigdatalab/piflow/total.svg)](https://github.com/cas-bigdatalab/piflow/releases)
[![GitHub issues](https://img.shields.io/github/issues/cas-bigdatalab/piflow.svg)](https://github.com/cas-bigdatalab/piflow/issues)  
 


πFlow is an easy to use, powerful big data pipeline system.

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Requirements](#requirements)
- [Getting Started](#getting-started)
- [PiFlow Docker](#docker-started)
- [Use Interface](#use-interface)
- [Principled Stand](https://github.com/cas-bigdatalab/piflow/blob/master/Governance/%E5%8E%9F%E5%88%99.md)
- [Contact Us](#contact-us)

## Features

- Easy to use
  - provide a WYSIWYG web interface to configure data flow
  - monitor data flow status
  - check the logs of data flow
  - provide checkpoints
- Strong scalability:
  - Support customized development of data processing components
- Superior performance
  - based on distributed computing engine Spark 
- Powerful
  - 100+ data processing components available
  - include spark、mllib、hadoop、hive、hbase、solr、redis、memcache、elasticSearch、jdbc、mongodb、http、ftp、xml、csv、json，etc.

## Architecture
![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/architecture.png) 
## Requirements
* JDK 1.8 
* Scala-2.11.8
* Apache Maven 3.1.0 or newer  
* Spark-2.1.0、 Spark-2.2.0、 Spark-2.3.0
* Hadoop-2.6.0
* Apache Livy-0.7.1

## Getting Started

### To Build:  
- `install external package`
          
          mvn install:install-file -Dfile=/../piflow/piflow-bundle/lib/spark-xml_2.11-0.4.2.jar -DgroupId=com.databricks -DartifactId=spark-xml_2.11 -Dversion=0.4.2 -Dpackaging=jar
          mvn install:install-file -Dfile=/../piflow/piflow-bundle/lib/java_memcached-release_2.6.6.jar -DgroupId=com.memcached -DartifactId=java_memcached-release -Dversion=2.6.6 -Dpackaging=jar
          mvn install:install-file -Dfile=/../piflow/piflow-bundle/lib/ojdbc6-11.2.0.3.jar -DgroupId=oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar
          mvn install:install-file -Dfile=/../piflow/piflow-bundle/lib/edtftpj.jar -DgroupId=ftpClient -DartifactId=edtftp -Dversion=1.0.0 -Dpackaging=jar
          

- `mvn clean package -Dmaven.test.skip=true`

          [INFO] Replacing original artifact with shaded artifact.
          [INFO] Reactor Summary:
          [INFO]
          [INFO] piflow-project ..................................... SUCCESS [  4.369 s]
          [INFO] piflow-core ........................................ SUCCESS [01:23 min]
          [INFO] piflow-configure ................................... SUCCESS [ 12.418 s]
          [INFO] piflow-bundle ...................................... SUCCESS [02:15 min]
          [INFO] piflow-server ...................................... SUCCESS [02:05 min]
          [INFO] ------------------------------------------------------------------------
          [INFO] BUILD SUCCESS
          [INFO] ------------------------------------------------------------------------
          [INFO] Total time: 06:01 min
          [INFO] Finished at: 2020-05-21T15:22:58+08:00
          [INFO] Final Memory: 118M/691M
          [INFO] ------------------------------------------------------------------------

### Run πFlow Server：

- `run piflow server on Intellij`:   
  - download piflow: git clone https://github.com/cas-bigdatalab/piflow.git
  - import piflow into Intellij
  - edit config.properties file
  - build piflow to generate piflow jar:   
    - Edit Configurations --> Add New Configuration --> Maven  
    - Name: package
    - Command line: clean package -Dmaven.test.skip=true -X  
    - run 'package' (piflow jar file will be built in ../piflow/piflow-server/target/piflow-server-0.9.jar)  
    
  - run HttpService:   
    - Edit Configurations --> Add New Configuration --> Application  
    - Name: HttpService
    - Main class : cn.piflow.api.Main  
    - Environment Variable: SPARK_HOME=/opt/spark-2.2.0-bin-hadoop2.6(change the path to your spark home)  
    - run 'HttpService'
  
  - test HttpService:   
    - run /../piflow/piflow-server/src/main/scala/cn/piflow/api/HTTPClientStartMockDataFlow.scala
    - change the piflow server ip and port to your configure
  
  
- `run piflow server by release version`:

  - download piflow.tar.gz:   
    https://github.com/cas-bigdatalab/piflow/releases/download/v1.2/piflow-server-v1.5.tar.gz  
    
  - unzip piflow.tar.gz:  
    tar -zxvf piflow.tar.gz
    
  - edit config.properties  
  
  - run start.sh、stop.sh、 restart.sh、 status.sh  
  
  - test piflow server
    - set PIFLOW_HOME  
      - vim /etc/profile  
        export PIFLOW_HOME=/yourPiflowPath/bin  
      	 export PATH=$PATH:$PIFLOW_HOME/bin  
        
      - command   
        piflow flow start example/mockDataFlow.json  
        piflow flow stop appID  
        piflow flow info appID  
        piflow flow log appID  
      
        piflow flowGroup start example/mockDataGroup.json  
        piflow flowGroup stop groupId  
        piflow flowGroup info groupId  
        
- `how to configure config.properties`
     
      #spark and yarn config
      spark.master=yarn
      spark.deploy.mode=cluster
      
      #hdfs default file system
      fs.defaultFS=hdfs://10.0.86.191:9000
      
      #yarn resourcemanager.hostname
      yarn.resourcemanager.hostname=10.0.86.191
      
      #if you want to use hive, set hive metastore uris
      #hive.metastore.uris=thrift://10.0.88.71:9083
      
      #show data in log, set 0 if you do not want to show data in logs
      data.show=10
      
      #server port
      server.port=8002
      
      #h2db port
      h2.port=50002
      
      #If you want to upload python stop,please set hdfs configs
      #example hdfs.cluster=hostname:hostIP
      #hdfs.cluster=master:127.0.0.1
      #hdfs.web.url=master:50070


  
### Run πFlow Web：
  - Visit address, download the corresponding *.tar.gz file, and modify the corresponding configuration file（`The version must be consistent with piflow-server`） 
    - https://github.com/cas-bigdatalab/piflow-web/releases/tag/v1.5 
  - If you want to upload python stops, please modify docker.service
  ```
    vim /usr/lib/systemd/system/docker.service
    ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock
    systemctl daemon-reload
    systemctl restart docker
  ```
  
### Restful API：

- flow json
  <details>
    <summary>flow example</summary>
    <pre>
      <code>
        {
  ""flow"": {
    ""name"": ""MockData"",
    ""executorMemory"": ""1g"",
    ""executorNumber"": ""1"",
    ""uuid"": ""8a80d63f720cdd2301723b7461d92600"",
    ""paths"": [
      {
        ""inport"": """",
        ""from"": ""MockData"",
        ""to"": ""ShowData"",
        ""outport"": """"
      }
    ],
    ""executorCores"": ""1"",
    ""driverMemory"": ""1g"",
    ""stops"": [
      {
        ""name"": ""MockData"",
        ""bundle"": ""cn.piflow.bundle.common.MockData"",
        ""uuid"": ""8a80d63f720cdd2301723b7461d92604"",
        ""properties"": {
          ""schema"": ""title:String, author:String, age:Int"",
          ""count"": ""10""
        },
        ""customizedProperties"": {

        }
      },
      {
        ""name"": ""ShowData"",
        ""bundle"": ""cn.piflow.bundle.external.ShowData"",
        ""uuid"": ""8a80d63f720cdd2301723b7461d92602"",
        ""properties"": {
          ""showNumber"": ""5""
        },
        ""customizedProperties"": {

        }
      }
    ]
  }
}</code>
    </pre>
  </details>
- CURL POST：
  - curl -0 -X POST http://10.0.86.191:8002/flow/start -H ""Content-type: application/json"" -d 'this is your flow json'
  
- Command line： 
  - set PIFLOW_HOME  
    vim /etc/profile  
  	export PIFLOW_HOME=/yourPiflowPath/piflow-bin  
    export PATH=$PATH:$PIFLOW_HOME/bin  

  - command example  
    piflow flow start yourFlow.json  
    piflow flow stop appID  
    piflow flow info appID  
    piflow flow log appID  

    piflow flowGroup start yourFlowGroup.json  
    piflow flowGroup stop groupId  
    piflow flowGroup info groupId  
    
## docker-started  
  - pull piflow images  
    docker pull registry.cn-hangzhou.aliyuncs.com/cnic_piflow/piflow:v1.5    
    
  - show docker images  
    docker images
    
  - run a container with  piflow imageID ， all services run automatically. Please Set HOST_IP and some docker configs.  
    docker run -h master -itd --env HOST_IP=\*.\*.\*.\* --name piflow-v1.5 -p 6001:6001 -v /usr/bin/docker:/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock --add-host docker.host:\*.\*.\*.\* [imageID]
    
  - please visit ""HOST_IP:6001"", it may take a while  
  
  - if somethings goes wrong,  all the application are in /opt  folder  
  
## use-interface
- `Login`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-login.png)
  
- `Dashboard`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/dashboard.png)
  
- `Flow list`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-flowlist.png)
  
- `Create flow`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-createflow.png)
  
- `Configure flow`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-flowconfig.png)
  
- `Load flow`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-loadflow.png)
  
- `Monitor flow`:  

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-monitor.png)

- `Flow logs`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-log.png)
  
- `Group list`:  

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-group-list.png)

- `Configure group`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-configure-group.png)

- `Monitor group`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-monitor-group.png)

- `Process List`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-processlist.png)
  
- `Template List`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-templatelist.png)

- `DataSource List`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-datasourcelist.png)
  
- `Schedule List`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-schedulelist.png)
  
- `StopHub List`:

  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/piflow-stophublist.png)
  
## Contact Us
- Name:吴老师  
- Mobile Phone：18910263390  
- WeChat：18910263390  
- Email: wzs@cnic.cn  
- QQ Group：1003489545  
  ![](https://github.com/cas-bigdatalab/piflow/blob/master/doc/PiFlowUserGroup_QQ.jpeg)  



 
",2023-07-07 18:44:55+00:00
pinball,pinball,pinterest/pinball,Pinball is a scalable workflow manager,,True,1048,2023-06-23 14:26:22+00:00,2015-03-04 03:13:18+00:00,142,53,20,0,,,Apache License 2.0,151,,0,,2023-05-06 23:15:24+00:00,2019-12-10 19:01:57+00:00,".. attention::
    This project is no longer actively maintained by Pinterest.

============
Pinball
============

Pinball is a scalable workflow manager.

.. contents::
    :local:
    :depth: 1
    :backlinks: none

Design Principles
----------------
* **Simple**: based on easy to grasp abstractions
* **Extensible**: component-based approach
* **Transparent**: state stored in a readable format
* **Reliable**: stateless computing components
* **Scalable**: scales horizontally
* **Admin-friendly**: can be upgraded without aborting workflows
* **Feature-rich**: auto-retries, per-job-emails, runtime alternations, priorities, overrun policies, etc.


Installation
----------------------
If you haven't already installed *libmysqlclient-dev*, *graphviz*. Please install them, e.g., ::

   $ sudo apt-get install libmysqlclient-dev
   $ sudo apt-get install graphviz

If you want to install *Pinball* through pypi package, please do ::

  $ sudo pip install pinball

Pinball uses mysql as persistent storage. Please also make sure mysql is available, and properly configured.


Quick Start
----------------------

Start Pinball
~~~~~~~~~~~~~
Once Pinball is installed either through pypi package installation or source code clone, we are ready to run it. There are four important components in Pinball.

* **Master**: A frontend to a persistent state repository with an interface supporting atomic job token updates. To start master, ::

  $ python -m pinball.run_pinball -c path/to/pinball/yaml/configuration/file -m master

* **UI**: A service reading directly from the storage layer used by the Master. To start UI, ::

  $ python -m pinball.run_pinball -c path/to/pinball/yaml/configuration/file -m ui

* **Scheduler**: Scheduler is responsible for running workflows on a schedule. To start scheduler, ::

  $ python -m pinball.run_pinball -c path/to/pinball/yaml/configuration/file -m scheduler

* **Worker**: A client of the Master. To start worker, ::

  $ python -m pinball.run_pinball -c path/to/pinball/yaml/configuration/file -m workers


Configure Pinball
~~~~~~~~~~~~~~~~~
In order to start Pinball, user needs to provide a pinball configuration file. A sample pinball configuraiton can be retrived at here_.

.. _here: https://github.com/pinterest/pinball/blob/master/pinball/config/default.yaml

There are a few parameters to configure. For example:

* MySQL db configuration ::

   databases:
        default:
            ENGINE:       django.db.backends.mysql
            NAME:         pinball
            USER:         flipper
            PASSWORD:     flipper123
            HOST:         127.0.0.1
            PORT:         ""3306""
        pinball.persistence:
            ENGINE:       django.db.backends.mysql
            NAME:         pinball
            USER:         flipper
            PASSWORD:     flipper123
            HOST:         127.0.0.1
            PORT:         ""3306""

* Pinball UI configuration ::

   ui_host:                  pinball
   ui_port:                  8080

.. _example: https://github.com/pinterest/pinball/blob/master/pinball_ext/examples/workflows.py
.. _parser: https://github.com/pinterest/pinball/blob/master/pinball_ext/workflow/parser.py
* Application Configuration ::

    parser:                    pinball_ext.workflows.parser.PyWorkflowParser

  *parser* tells Pinball how to interpret your defined workflow and jobs. The above configuration links to a Python parser_ provided by Pinball.
  You can also provide your own parser to intepretate your own definition of workflow and jobs. Please check the tutorial for details. ::

    parser_params:
      workflows_config:       pinball_ext.examples.workflows.WORKFLOWS
      job_repo_dir:           ""~""
      job_import_dirs_config: pinball_ext.examples.jobs.JOB_IMPORT_DIRS

  *parser_params* will be taken by *parser*. Name of the variable that stores workflows config is *workflows_config*;
  root dir of the repo that stores all user defined jobs is stored at *job_repo_dir*; *job_import_dirs_config* keeps list of
  dirs where job class should be imported from.


* Email configuration ::

    default_email:              your@email.com

  *default_email* configures default sender of email service of Pinball.


Use Pinball
~~~~~~~~~~~
After starting Pinball with the proper configuration, user can access Pinball at *pinball:8080*.
You may find there is no workflow or jobs listed in Pinball UI when you first start Pinball. To import your workflow into Pinball,
do the following command. ::

    python -m pinball.tools.workflow_util -c path/to/pinball/yaml/configuration/file -f reschedule

After this, you should be able to see your workflows in Pinball UI. They will be scheduled and run according to the specified schedules.

.. image:: instance_view.png
   :alt: Workflow Instance View

Detailed Design
-------------
Design details are available in `Pinball Architecture Overview <https://github.com/pinterest/pinball/blob/master/ARCHITECTURE.rst>`_

User Guide
-----------------
Detail user guide is available in `Pinball User Guide <https://github.com/pinterest/pinball/blob/master/USER_GUIDE.rst>`_

Admin Guide
------------------
Administrator guide is available in `Pinball Administrator Guide <https://github.com/pinterest/pinball/blob/master/ADMIN_GUIDE.rst>`_

License
-------
Pinball is distributed under `Apache License, Version 2.0 <http://www.apache.org/licenses/LICENSE-2.0.html>`_.

Maintainers
----------
* `Pawel Garbacki <https://github.com/pgarbacki>`_
* `Mao Ye <https://github.com/MaoYe>`_
* `Changshu Liu <https://github.com/csliu>`_

Contributing
-----------
* `Contributors <https://github.com/pinterest/pinball/blob/master/AUTHORS.rst>`_
* `How to contribute <https://github.com/pinterest/pinball/blob/master/CONTRIBUTING.rst>`_


Help
-----
If you have any questions or comments, you can reach us at `pinball-users@googlegroups.com <https://groups.google.com/forum/#!forum/pinball-users>`_.

",2023-07-07 18:45:00+00:00
pipelinedb,pipelinedb,pipelinedb/pipelinedb,High-performance time-series aggregation for PostgreSQL,https://www.pipelinedb.com,False,2562,2023-07-04 08:42:11+00:00,2013-11-26 00:11:48+00:00,238,107,12,49,1.0.0-13,2019-02-19 19:59:27+00:00,Apache License 2.0,1023,1.0.0,57,2018-10-24 17:13:20+00:00,2023-06-28 16:50:06+00:00,2019-05-01 19:32:42+00:00,"PipelineDB [has joined Confluent](https://www.confluent.io/blog/pipelinedb-team-joins-confluent), read the blog post [here](https://www.pipelinedb.com/blog/pipelinedb-is-joining-confluent).

PipelineDB will not have new releases beyond `1.0.0`, although critical bugs will still be fixed.

# PipelineDB

[![Gitter chat](https://img.shields.io/badge/gitter-join%20chat-brightgreen.svg?style=flat-square)](https://gitter.im/pipelinedb/pipelinedb)
[![Twitter](https://img.shields.io/badge/twitter-@pipelinedb-55acee.svg?style=flat-square)](https://twitter.com/pipelinedb)

## Overview

PipelineDB is a PostgreSQL extension for high-performance time-series aggregation, designed to power realtime reporting and analytics applications.

PipelineDB allows you to define [continuous SQL queries](http://docs.pipelinedb.com/continuous-views.html) that perpetually aggregate time-series data and store **only the aggregate output** in regular, queryable tables. You can think of this concept as extremely high-throughput, incrementally updated materialized views that never need to be manually refreshed.

Raw time-series data is never written to disk, making PipelineDB extremely efficient for aggregation workloads.

Continuous queries produce their own [output streams](http://docs.pipelinedb.com/streams.html#output-streams), and thus can be [chained together](http://docs.pipelinedb.com/continuous-transforms.html) into arbitrary networks of continuous SQL.

## PostgreSQL compatibility

PipelineDB runs on 64-bit architectures and currently supports the following PostgreSQL versions:

* **PostgreSQL 10**: 10.1, 10.2, 10.3, 10.4, 10.5
* **PostgreSQL 11**: 11.0

## Getting started

If you just want to start using PipelineDB right away, head over to the [installation docs](http://docs.pipelinedb.com/installation.html) to get going.

If you'd like to build PipelineDB from source, keep reading!

## Building from source

Since PipelineDB is a PostgreSQL extension, you'll need to have the [PostgreSQL development packages](https://www.postgresql.org/download/) installed to build PipelineDB.

Next you'll have to install [ZeroMQ](http://zeromq.org/) which PipelineDB uses for inter-process communication. [Here's](https://gist.github.com/derekjn/14f95b7ceb8029cd95f5488fb04c500a) a gist with instructions to build and install ZeroMQ from source.
You'll also need to install some Python dependencies if you'd like to run PipelineDB's Python test suite:

```
pip install -r src/test/py/requirements.txt
```

#### Build PipelineDB:

Once PostgreSQL is installed, you can build PipelineDB against it:

```
make USE_PGXS=1
make install
```

#### Test PipelineDB *(optional)*
Run the following command:

```
make test
```

#### Bootstrap the PipelineDB environment
Create PipelineDB's physical data directories, configuration files, etc:

```
make bootstrap
```

**`make bootstrap` only needs to be run the first time you install PipelineDB**. The resources that `make bootstrap` creates may continue to be used as you change and rebuild PipeineDB.


#### Run PipelineDB
Run all of the daemons necessary for PipelineDB to operate:

```
make run
```

Enter `Ctrl+C` to shut down PipelineDB.

`make run` uses the binaries in the PipelineDB source root compiled by `make`, so you don't need to `make install` before running `make run` after code changes--only `make` needs to be run.

The basic development flow is:

```
make
make run
^C

# Make some code changes...
make
make run
```

#### Send PipelineDB some data

Now let's generate some test data and stream it into a simple continuous view. First, create the stream and the continuous view that reads from it:

    $ psql
    =# CREATE FOREIGN TABLE test_stream (key integer, value integer) SERVER pipelinedb;
    CREATE FOREIGN TABLE
    =# CREATE VIEW test_view WITH (action=materialize) AS SELECT key, COUNT(*) FROM test_stream GROUP BY key;
    CREATE VIEW

Events can be emitted to PipelineDB streams using regular SQL `INSERTS`. Any `INSERT` target that isn't a table is considered a stream by PipelineDB, meaning streams don't need to have a schema created in advance. Let's emit a single event into the `test_stream` stream since our continuous view is reading from it:

    $ psql
    =# INSERT INTO test_stream (key, value) VALUES (0, 42);
    INSERT 0 1

The 1 in the `INSERT 0 1` response means that 1 event was emitted into a stream that is actually being read by a continuous query. Now let's insert some random data:

    =# INSERT INTO test_stream (key, value) SELECT random() * 10, random() * 10 FROM generate_series(1, 100000);
    INSERT 0 100000

Query the continuous view to verify that the continuous view was properly updated. Were there actually 100,001 events counted?

    $ psql -c ""SELECT sum(count) FROM test_view""
      sum
    -------
    100001
    (1 row)

What were the 10 most common randomly generated keys?

    $ psql -c ""SELECT * FROM test_view ORDER BY count DESC limit 10""
	key  | count 
	-----+-------
	 2   | 10124
	 8   | 10100
	 1   | 10042
	 7   |  9996
	 4   |  9991
	 5   |  9977
	 3   |  9963
	 6   |  9927
	 9   |  9915
	10   |  4997
	 0   |  4969

	(11 rows)
",2023-07-07 18:45:03+00:00
pipelinedog,pipelinedog-web,zhouanbo/pipelinedog-web,A web interface of pipelinedog,,False,0,2016-08-05 15:16:19+00:00,2016-08-03 19:42:24+00:00,0,3,1,0,,,GNU General Public License v3.0,103,,0,,,2017-10-30 16:47:16+00:00,"<img src=""http://pipeline.dog/icon.png"" alt=""pipelinedog logo"" height=""128"" >

# PipelineDog Docs

## Overview

This is the documentation of PipelineDog, a tool that helps you better construct and maintain your scientific pipelines.

## Links

- To get an idea on the essential building blocks of PipelineDog steps, take a look at the [LEASH Expression Specification](https://github.com/ysunlab/PipelineDog/blob/master/pipelineDog.LEASHexpression.md).

- The [Step Specification](https://github.com/ysunlab/PipelineDog/blob/master/web.pipelineDog.StepFormatDefinition.md) has information of how to structure a PipelineDog step.

- To combine steps into a pipeline, refer to the [Project Specification](https://github.com/ysunlab/PipelineDog/blob/master/web.pipelineDog.ProjectFormatDefinition.md).

- Try it out following the [Start From Scratch](https://github.com/ysunlab/PipelineDog/blob/master/startFromScratch.md) guide, a step-by-step tutorial with screenshots for each step.

- Tips and common pitfalls are pointed out at [YAML Tips](https://github.com/ysunlab/PipelineDog/blob/master/pipelineDog.YAMLtips.md).

Have fun pipelining!
",2023-07-07 18:45:07+00:00
pipengine,bioruby-pipengine,fstrozzi/bioruby-pipengine,An ultra light YAML-based pipeline execution engine,,False,19,2020-05-29 13:55:36+00:00,2012-10-15 12:13:56+00:00,3,5,3,4,v0.9.3,2016-05-09 16:01:30+00:00,MIT License,280,v0.9.7,13,2017-08-28 08:02:22+00:00,,2017-08-30 07:28:10+00:00,"PipEngine
=========

A simple launcher for complex biological pipelines.

PipEngine will generate runnable shell scripts, already configured for the PBS/Torque job scheduler, for each sample in the pipeline. It allows to run a complete pipeline or just a single step depending on the needs.

PipEngine is best suited for NGS pipelines, but it can be used for any kind of pipeline that can be runned on a job scheduling system and which is ""sample"" centric, i.e. you have from one side a list of samples with their corresponding raw data, and from the other side a pipeline that you would like to apply to them.

PipEngine was developed to combine the typical flexibility and portability of shell scripts, with the concept of pipeline templates that can be easily applied on different input data to reproduce scientific results. The overall improvement over Makefiles or customised ad-hoc shell scripts is better readability of the pipelines using the YAML format, especially for people with no coding experience, the automated scripts generation which allows adding extra functionalities like error controls and logging directly into script jobs, and an enforced separation between the description of input data and the pipeline template, which improves clarity and reusability of analysis protocols.


Installation
============

If you already have Ruby, just install PipEngine using RubyGems:

```shell
gem install bio-pipengine
```

If you don't have Ruby installed we reccomend you use the Anaconda Package Manager.

Download the installer from [here](http://conda.pydata.org/miniconda.html) and once installed you can simply type:

```shell
conda install -c bioconda ruby
```

and then install PipEngine using RubyGems:

```shell
gem install bio-pipengine
```

Pipengine has been tested and should work with Ruby >= 2.1.2

:: Topics ::
============

[Usage](https://github.com/bioinformatics-ptp/bioruby-pipengine#-usage-)

[The Pipeline YAML](https://github.com/bioinformatics-ptp/bioruby-pipengine#-the-pipeline-yaml-)

[The Samples YAML](https://github.com/bioinformatics-ptp/bioruby-pipengine#-the-samples-yaml-)

[Input and output conventions](https://github.com/bioinformatics-ptp/bioruby-pipengine#-input-and-output-conventions-)

[Sample groups and complex steps](https://github.com/bioinformatics-ptp/bioruby-pipengine#-sample-groups-and-complex-steps-)

[What happens at run-time](https://github.com/bioinformatics-ptp/bioruby-pipengine#-what-happens-at-run-time-)

[Examples](https://github.com/bioinformatics-ptp/bioruby-pipengine#-examples-)

[PBS Options](https://github.com/bioinformatics-ptp/bioruby-pipengine#-pbs-options-)

[Extending and contributing](https://github.com/bioinformatics-ptp/bioruby-pipengine#-extending-and-contributing-)

:: Usage ::
===========


```shell
> pipengine -h
List of available commands:
	run		Submit pipelines to the job scheduler
```

Command line for RUN mode
-------------------------

**Command line**
```shell
> pipengine run -p pipeline.yml -f samples.yml -s mapping --tmp /tmp
```

**Parameters**
```shell
  -p, --pipeline=<s>            YAML file with pipeline and sample details (default: pipeline.yml)
  -f, --samples-file=<s>        YAML file with samples name and directory paths (default: samples.yml)
  -l, --samples=<s+>            List of sample names to run the pipeline
  -s, --steps=<s+>              List of steps to be executed
  -d, --dry                     Dry run. Just create the job script without submitting it to the batch system
  -t, --tmp=<s>                 Temporary output folder
  -c, --create-samples=<s+>     Create samples.yml file from a Sample directory (only for CASAVA projects)
  -m, --multi=<s+>              List of samples to be processed by a given step (the order matters)
  -g, --group=<s>               Specify the group of samples to run the pipeline steps on (do not specify --multi)
  -a, --allgroups               Apply the step(s) to all the groups defined into the samples file
  -n, --name=<s>                Analysis name
  -o, --output-dir=<s>          Output directory (override standard output directory names)
  -b, --pbs-opts=<s+>           PBS options
  -q, --pbs-queue=<s>           PBS queue
  -i, --inspect-pipeline=<s>    Show steps
  --log=<s>                     Log script activities, by default stdin. Options are fluentd (default: stdin)
  -e, --log-adapter=<s>         (stdin|syslog|fluentd) In case of fluentd use http://destination.hostname:port/yourtag
  --tag=<s+>                    Overwrite tags present in samples.yml and pipeline.yml files (e.g. tag1=value1 tag2=value2)
  -h, --help                    Show this message
```

PipEngine accepts two input files:
* A YAML file describing the pipeline steps
* A YAML file describing samples names, samples location and other samples-specific information


:: The Pipeline YAML ::
=======================

The basic structure of a pipeline YAML is divided into three parts: 1) pipeline name, 2) resources, 3) steps.

An example YAML file is like the following:

```yaml

pipeline: resequencing

resources:
  fastqc: /software/FastQC/fastqc
  bwa: /software/bwa
  gatk: /software/gatk-lite/GenomeAnalysisTk.jar
  samtools: /software/samtools
  samsort: /software/picard-tools-1.77/SortSam.jar
  mark_dup: /software/picard-tools-1.77/MarkDuplicates.jar
  bam: /software/bam
  pigz: /software/pigz

steps:
  mapping:
    desc: Run BWA on each sample to perform alignment
    run:
     - ls <sample_path>/*_R1_*.gz | xargs zcat | <pigz> -p 10 >> R1.fastq.gz
     - ls <sample_path>/*_R2_*.gz | xargs zcat | <pigz> -p 10 >> R2.fastq.gz
     - <bwa> sampe -P <index> <(<bwa> aln -t 4 -q 20 <index> R1.fastq.gz) <(<bwa> aln -t 4 -q 20 <index> R2.fastq.gz) R1.fastq.gz R2.fastq.gz | <samtools> view -Su - | java -Xmx4g -jar /storage/software/picard-tools-1.77/AddOrReplaceReadGroups.jar I=/dev/stdin O=<sample>.sorted.bam SO=coordinate LB=<pipeline> PL=illumina PU=PU SM=<sample> TMP_DIR=/data/tmp CREATE_INDEX=true MAX_RECORDS_IN_RAM=1000000
     - rm -f R1.fastq.gz R2.fastq.gz
    cpu: 11

  mark_dup:
    run: java -Xmx4g -jar <mark_dup> VERBOSITY=INFO MAX_RECORDS_IN_RAM=500000 VALIDATION_STRINGENCY=SILENT INPUT=<mapping/sample>.sorted.bam OUTPUT=<sample>.md.sort.bam METRICS_FILE=<sample>.metrics REMOVE_DUPLICATES=false

  realign_target:
    run: java -Xmx4g -jar <gatk> -T RealignerTargetCreator -I <mark_dup/sample>.md.sort.bam -nt 8 -R <genome> -o <sample>.indels.intervals
    cpu: 8

  realign:
    run: java -Xmx4g -jar <gatk> -T IndelRealigner -LOD 0.4 -model USE_READS --disable_bam_indexing --target_intervals <realign_target/sample>.indels.intervals -R <genome> -I <mark_dup/sample>.md.sort.bam -o <sample>.realigned.bam

  fixtags:
    run: <samtools> calmd -r -E -u <realign/sample>.realigned.bam <genome> | <bam> squeeze --in -.ubam --out <sample>.final.bam --rmTags 'XM:i;XG:i;XO:i' --keepDups

  bam_index:
    run: <samtools> index <fixtags/sample>.final.bam

  clean:
    run: ls | grep -v final | xargs rm -fr

```

Resources definition
--------------------

PipEngine is entirely based on the placeholder and substitution logic. For example in the Pipeline YAML, each tool is declared under the resources and at run time PipEngine will search for the corresponding placeholder in the command lines.

So, for instance, if I have declared a software **bwa** under resources, PipEngine will search for a ```<bwa>``` placeholder in all the command lines and will substitute it with the software complete path declared in resources.

This makes command lines definition shorter and easier to read and avoid problems when moving from one software version to another (i.e. you just need to change the bwa definition once, and not 10 times in 5 different command lines)

The same thing happens for samples names, input and output directories and intermediate output files. This allows to create true pipelines templates that can be reused and applied to different samples sets.

Step definition
---------------

A step must be defined using standard keys:

* the first key must be the step name
* under the step name, a **run** key must be defined to hold the actual command line that will be executed
* a **cpu** key must be defined if the command line uses more than 1 CPU at runtime
* a **multi** key must be defined if the command line takes as input more than one sample (more details later)
* a **desc** key has been added to insert a short description that will be displayed using the **-i** option of PipEngine
* a **nodes** and **mem** keys can be used to specify the resources needed for this job

A note on the **run** key. If a single step need more than a command line to execute the required actions, these multiple command lines must be defined as an array in YAML (see the mapping step in the above example).


:: The Samples YAML ::
=====================

The samples YAML is much simpler then the pipeline YAML:

```yaml
resources:
  index: /storage/genomes/bwa_index/genome
  genome: /storage/genomes/genome.fa
  output: /storage/results

samples:
  sampleA: /ngs_reads/sampleA
  sampleB: /ngs_reads/sampleB
  sampleC: /ngs_reads/sampleC
  sampleD: /ngs_reads/sampleD
```

In this YAML there is again a **resources** key, but this time the tags defined here are dependent on the samples described in the YAML.

For instance, if I am working with human RNA-seq samples, these data must be aligned on the human genome, so it makes sense that the **genome** tag must be defined here and not in the pipeline YAML, which must be as much generic as possible.

Generally, the tags defined under the samples **resources** are dependent on the pipeline and analysis one wants to run. So if using BWA to perform reads alignemnt, an **index** tag must be defined here to set the BWA index prefix and it will be substituted in the pipelines command lines every time an ```<index>``` placeholder will be found in the pipeline YAML.

Sample groups
-------------

If you want to organize your samples by groups, it is possible to do it directly in the samples.yml file:


```yaml
resources:
  index: /storage/genomes/bwa_index/genome
  genome: /storage/genomes/genome.fa
  output: /storage/results

samples:
  Group1:
    sampleA: /ngs_reads/sampleA
    sampleB: /ngs_reads/sampleB
  Group2:
    sampleC: /ngs_reads/sampleC
    sampleD: /ngs_reads/sampleD
```

Then, by using the **-g** option of PipEngine, it is possible to run steps and pipelines directly on groups of samples.


How to create the Samples file
------------------------------

PipEngine is created to work primarly for NGS pipelines and with Illumina data in mind. So, the easiest thing to do if you have your samples already organized into a typical Illumina folder is to run:

```shell
> pipengine run -c /path/to/illumina/data
```

This will generate a samples.yml file with all the sample names and path derived from the run folder. The ""resources"" part is left blank for you to fill.

As a plus, if you have your samples scattered thoughout many different run folders, you can specify all the paths that you want to PipEngine and it will combine all the paths in the same samples file. So if you have your samples spread across let's say 3 runs, you can call PipEngine in this way:

```shell
> pipengine run -c /path/to/illumina/run1 /path/to/illumina/run2 /path/to/illumina/run3
```

If a sample is repeated in more than one run, all the paths will be combined in the samples.yml and PipEngine will take care of handling the multiple paths correctly.



:: Input and output conventions ::
==================================

The inputs in the steps defined in the pipeline YAML are expressed by the ```<sample>``` placeholder that will be substituted with a sample name and the ```<sample_path>```, which will be changed with the location where initial data (i.e. raw sequencing reads) are stored for that particular sample. Both this information are provided in the sample YAML file.

The ```<output>``` placeholder is a generic one to define the root location for the pipeline outputs. This parameter is also defined in the samples YAML. By default, PipEngine will write jobs scripts and will save stdout and stderr files from PBS in this folder. 

By convention, each sample output is saved under a folder with the sample name and each step is saved in a sub-folder with the step name.

That is, given a generic /storage/pipeline_results ```<output>``` folder, the outputs of the **mapping** step will be organized in this way:

```shell
/storage/pipeline_results/SampleA/mapping/SampleA.bam
                         /SampleB/mapping/SampleB.bam
                         /SampleC/mapping/SampleC.bam
                         /SampleD/mapping/SampleD.bam
```

This simple convention keeps things clean and organized. The output file name can be decided during the pipeline creation, but it's a good habit to name it using the sample name.

When new steps of the same pipeline are run, output folders are updated accordingly. So for example if after the **mapping** step a **mark_dup** step is run, the output folder will look like this:

```shell
/storage/pipeline_results/SampleA/mapping
                         /SampleA/mark_dup

/storage/pipeline_results/SampleB/mapping
                         /SampleB/mark_dup
                  .....
```

In case you are working with group of samples, specified by the **-g** option, the output folder will be changed to reflect the samples grouping. So for example if a **mapping** step is called on the **Group1** group of samples, all the outputs will be saved under the ```<output>/Group1``` folder and results of mapping for SampleA, will be found under ```<output>/Group1/SampleA/mapping``` .


How steps are connected together
--------------------------------

One step is connected to another by simply requiring that its input is the output of another preceding step. This is just achived by a combination of ```<output>``` and ```<sample>``` placeholders in the pipeline command line definitions.

For instance, if I have a resequencing pipeline that will first run BWA to map the reads and then a mark duplicate step, the mark_dup step will be dependent from the BWA output.

```yaml
  mapping:
    run:
     - ls <sample_path>/*_R1_*.gz | xargs zcat | <pigz> -p 10 >> R1.fastq.gz
     - ls <sample_path>/*_R2_*.gz | xargs zcat | <pigz> -p 10 >> R2.fastq.gz
     - <bwa> sampe -P <index> <(<bwa> aln -t 4 -q 20 <index> R1.fastq.gz) <(<bwa> aln -t 4 -q 20 <index> R2.fastq.gz) R1.fastq.gz R2.fastq.gz > <sample>.sorted.bam
     - rm -f R1.fastq.gz R2.fastq.gz
    cpu: 11

  mark_dup:
    run: java -Xmx4g -jar <mark_dup> INPUT=<mapping/sample>.sorted.bam OUTPUT=<sample>.md.sort.bam
```

So in the **mark_dup** step the input placeholder (defined under the **run** key in the pipeline YAML) will be written as:

```
<mapping/sample>.sorted.bam
```

If the ```<output>``` tag is defined for instance as ""/storage/results"", this will be translated at run-time into:

```
/storage/results/SampleA/mapping/SampleA.sorted.bam
```

for SampleA outputs. Basically the ```<mapping/sample>``` placeholder is a shortcut for ```<output>/<sample>/{step name, mapping in this case}/<sample>```

Following the same idea, using a ```<mapping/>``` placeholder (note the / at the end) will be translated into ```<output>/<sample>/{step name, mapping in this case}/``` , to address the scenario when a user wants to point to the previous step output directory, but without having the ```<sample>``` appended to the end of the path.

More complex dependences can be defined by combinations of ```<output>``` and ```<sample>``` placeholders, or using the ```<step/>``` and ```<step/sample>``` placeholders, without having to worry about the actual sample name and the complete input and output paths.

Jobs dependencies
-------------------------
Steps can also be defined with dependencies so the user can just call the final step and all the upper chain is called automatically. To achieve this task Pipengine requires that the user defines a
```
 pre:
```
tag in the step definition:

```
  root_step:
    desc: root step to test dependencies
    run:
     - echo ""root""

  child_step:
    desc: child step to test dependencies
    pre: root_step
    run:
      - echo ""I am the child""
```


:: Multi-Samples and complex steps ::
=====================================

The pipeline steps can be defined to run on a single sample or to take as input more than one sample data, depending on the command line used.

A typical example is running a differential expression step for example with CuffDiff. This requires to take all the output generated from the previous Cufflinks step (i.e. the gtf files) and process them to generate a unique transcripts reference (CuffCompare) and then perform the differential expression across the samples using the BAM files generated by, let's say, TopHat in a **mapping** step.

This is an extract of the step definition in the pipeline YAML to describe these two steps:

```yaml
  diffexp:
    multi:
      - <output>/<sample>/cufflinks/transcripts.gtf
      - <mapping/sample>_tophat/accepted_hits.bam
    run:
      - echo '<multi1>' | sed -e 's/,/ /g' | xargs ls >> gtf_list.txt
      - <cuffcompare> -s <genome> -r <gtf> -i gtf_list.txt
      - <cuffdiff> -p 12 -N -u -b <genome> ./*combined.gtf <multi2>
    cpu: 12
```

In this case we need to combine the outputs of all the samples from the cufflinks step and pass that information to cuffcompare and combine the outputs of the mapping steps and pass them to the cuffdiff command line.

This is achived in two ways. First, the step definition must include a **multi** key, that simply defines what, for each sample, will be substituted where the ```<multi>``` placeholder is found.

In the example above, the step includes two command lines, one for cuffcompare and the other for cuffdiff. Cuffcompare requires the transcripts.gtf for each sample, while Cuffdiff requires the BAM file for each sample, plus the output of Cuffcompare.

So the two command lines need two different kind of files as input from the same set of samples, therefore two **multi** keywords are defined as well as two placeholders ```<multi1>``` and ```<multi2>```

Once the step has been defined in the pipeline YAML, PipEngine must be invoked using the **-m** parameter, to specify the samples that should be grouped together by this step:

```shell
pipengine run -p pipeline.yml -m SampleA,SampleB SampleC,SampleB
```

Note that the use of commas is not casual, since the **-m** parameter specifies not only which samples should be used for this step, but also how they should be organized on the corresponding command line. The **-m** parameter takes the sample names and underneath it will combine the sample name with the 'multi' keywords and then it will substitute back the command line by keeping the samples in the same order as provided with the **-m**.

The above command line will be translated, for the **cuffdiff** command line in the following:

```shell
/software/cuffdiff -p 12 -N -u -b /storage/genome.fa combined.gtf /storage/results/SampleA/cufflinks/transcripts.gtf,/storage/results/SampleB/cufflinks/transcripts.gtf /storage/results/SampleC/cufflinks/transcripts.gtf /storage/results/SampleD/cufflinks/transcripts.gtf
```

and this will correspond to the way CuffDiff wants biological replicates for each condition to be described on the command line.

**Note**

Multi-samples step management is complex and it's a task that can't be easily generalized since every software has it's own way to require and organize the inputs on the command line. This approach it's probably not the most elegant solution but works quite well, even if there are some drawbacks. For instance, as stated above, the samples groups is processed and passed to command lines as it is taken from the **-m** parameter.

So for Cuffdiff, the presence of commas is critical to divide biological replicates from different conditions, but for Cuffcompare the commas are not needed and will raise an error on the command line. That's the reason of the:

```shell
echo '<multi1>' | sed -e 's/,/ /g' | xargs ls >> gtf_list.txt
```

This line generates the input file for Cuffcompare with the list of the transcripts.gtf files for each sample, generated using the 'multi' definition in the pipeline YAML and the line passed through the **-m** parameter, but getting rid of the commas that separate sample names. It's a workaround and it's not a super clean solution, but PipEngine wants to be a general tool not binded to specific corner cases and it always lets the user define it's own custom command lines to manage particular steps, as in this case.

Composable & Modular steps definition
------------------------------------

Since now steps are defined inside a single YAML file. This approach is usefult to have a stable and reproducible analysis pipeline. But what if, multiple users whant to collaborate on the same pipeline improving it and, most importantly, re-using the same steps in different analyses ? What happend is a proliferation of highly similar pipelines that are very complicate to compare and to maintain over time.
In this scenario, the very first thing that a developer imagine is the ability to include external files, unfortunately YAML does not implement this feature. A possible workaround, remember that we are in the Ruby land, is to embed some Ruby code into the YAML file and include external steps.

Creating a file `mapping.yml` that describe the mapping step with BWA

```
mapping:
  cpu: 8
  desc: Run BWA MEM and generates a sorted BAM file
  run:
   - <bwa> mem -t <cpu> -R '@RG\tID:<flowcell>\tLB:<sample>\tPL:ILLUMINA\tPU:<flowcell>\tCN:PTP\tSM:<sample>' <index> <trim/sample>.trim.fastq | <samtools> view -bS - > <sample>.bam
   - <samtools> sort -@ <cpu> <sample>.bam <sample>.sort
   - rm -f <sample>.bam
```

is then possible to include the `mapping.yml` file inside your pipeline with a snipped of Ruby code `<%= include :name_of_the_step, ""file_step.yml"" %>
Right now is very important that you place the tag at the very first start of the line ( no spaces at the beginning of the line)

```
steps:
<%= include :mapping, ""./mapping.yml"" %>

  index:
    desc: Make BAM index
    run: <samtools> index <mapping/sample>.sort.bam
````

are later run pipengine as usual.
TODO: Dump the whole pipeline file for reproducibility purposes.


:: What happens at run-time ::
==============================

When invoking PipEngine, the tool will look for the pipeline YAML specified and for the sample YAML file. It will load the list of samples (names and paths of input data) and for each sample it will load the information of the step specified in the command line ( **-s** parameter ).

PipEngine will then combine the data from the two YAML, generating the specific command lines of the selected steps and substituing all the placeholders to generate the final command lines.

A shell script will be finally generated, for each sample, that will contain all the instructions to run a specific step of the pipeline plus the meta-data for the PBS scheduler. The shell scripts are written inside the directory specified on the ```output:``` key in the ```samples.yml``` file, the directory is created if it does not exist.

If not invoked with the **-d** option (dry-run) PipEngine will directly submit the jobs to the PBS scheduler using the ""qsub"" command.

Dry Run
-------

The **-d** parameter lets you create the runnable shell scripts without submitting them to PBS. Use it often to check that the pipeline that will be executed is correct and it is doing what you thought. The runnable scripts are saved by default in the ```<output>``` directory.

Use it also to learn how the placeholders works, especially the dependency placeholders (e.g. ```<mapping/sample>```) and to cross-check that all the placeholders in the pipeline command lines were substituted correctly before submitting the jobs.

Temporary output folder
-------------------

By using the '--tmp' option, PipEngine will generate a job script (for each sample) that will save all the output files or folders for a particular step in a directory (e.g. /tmp) that is different from the one provided with the ```<output>```.

By default PipEngine will generate output folders directly under the location defined by the ```<ouput>``` tag in the Sample YAML. The --tmp solution instead can be useful when we don't want to save directly to the final location (e.g maybe a slower network storage) or we don't want to keep all the intermediate files but just the final ones.

With this option enabled, PipEngine will also generate instructions in the job script to copy, at the end of the job, all the outputs from the temporary directory to the final output folder (i.e. ```<output>```) and then to remove the temporary copy.

When '--tmp' is used, a UUID is generated for each job and prepended to the job name and to the temporary output folder, to avoid possible name collisions and data overwrite if more jobs with the same name (e.g. mapping) are running and writing to the same temporary location.

One job with multiple steps
---------------------------

It is of course possible to aggregate multiple steps of a pipeline and run them in one single job. For instance let's say I want to run in the same job the steps mapping, mark_dup and realign_target (see pipeline YAML example above).

From the command line it's just:

```shell
pipengine run -p pipeline.yml -s mapping mark_dup realign_target
```

A single job script, for each sample, will be generated with all the instructions for these steps. If more than one step declares a **cpu** key, the highest cpu value will be assigned for the whole job.

Each step will save outputs into a separated folder, under the ```<output>```, exactly if they were run separately. This way, if the job fails for some reason, it will be possible to check which steps were already completed and restart from there.

When multiple steps are run in the same job, by default PipEngine will generate the job name as the concatenation of all the steps names. Since this could be a problem when a lot of steps are run together in the same job, a '--name' parameter it's available to rename the job in a more convenient way.

:: Examples ::
==============

All these files can be found into the test/examples directory of the repository.

Example 1: One step and multiple command lines
----------------------------------------------

This is an example on how to prepare the inputs for BWA and run it along with Samtools:

**pipeline.yml**
```yaml
pipeline: resequencing

resources:
  bwa: /software/bwa
  samtools: /software/samtools
  pigz: /software/pigz

steps:
  mapping:
    run:
     - ls <sample_path>/*_R1_*.gz | xargs zcat | <pigz> -p 10 >> R1.fastq.gz
     - ls <sample_path>/*_R2_*.gz | xargs zcat | <pigz> -p 10 >> R2.fastq.gz
     - <bwa> sampe -P <index> <(<bwa> aln -t 4 -q 20 <index> R1.fastq.gz) <(<bwa> aln -t 4 -q 20 <index> R2.fastq.gz) R1.fastq.gz R2.fastq.gz | <samtools> view -Sb - > <sample>.bam
     - rm -f R1.fastq.gz R2.fastq.gz
    cpu: 12
```

**samples.yml**
```yaml
resources:
  index: /storage/genomes/bwa_index/genome
  genome: /storage/genomes/genome.fa
  output: ./working

samples:
  sampleA: /ngs_reads/sampleA
  sampleB: /ngs_reads/sampleB
  sampleC: /ngs_reads/sampleC
  sampleD: /ngs_reads/sampleD
```

Running PipEngine with the following command line:

```
pipengine run -p pipeline.yml -f samples.yml -s mapping -d
```

will generate a runnable shell script for each sample (available in the ./working directory):

```shell
#!/usr/bin/env bash
#PBS -N 2c57c1a853-sampleA-mapping
#PBS -d ./working
#PBS -l nodes=1:ppn=12
if [ ! -f ./working/sampleA/mapping/checkpoint ]
then
echo ""mapping 2c57c1a853-sampleA-mapping start `whoami` `hostname` `pwd` `date`.""

mkdir -p ./working/sampleA/mapping
cd ./working/sampleA/mapping
ls /ngs_reads/sampleA/*_R1_*.gz | xargs zcat | /software/pigz -p 10 >> R1.fastq.gz || { echo ""mapping 2c57c1a853-sampleA-mapping FAILED 0 `whoami` `hostname` `pwd` `date`.""; exit 1; }
ls /ngs_reads/sampleA/*_R2_*.gz | xargs zcat | /software/pigz -p 10 >> R2.fastq.gz || { echo ""mapping 2c57c1a853-sampleA-mapping FAILED 1 `whoami` `hostname` `pwd` `date`.""; exit 1; }
/software/bwa sampe -P /storage/genomes/bwa_index/genome <(/software/bwa aln -t 4 -q 20 /storage/genomes/bwa_index/genome R1.fastq.gz) <(/software/bwa aln -t 4 -q 20 /storage/genomes/bwa_index/genome R2.fastq.gz) R1.fastq.gz R2.fastq.gz | /software/samtools view -Sb - > sampleA.bam || { echo ""mapping 2c57c1a853-sampleA-mapping FAILED 2 `whoami` `hostname` `pwd` `date`.""; exit 1; }
rm -f R1.fastq.gz R2.fastq.gz || { echo ""mapping 2c57c1a853-sampleA-mapping FAILED 3 `whoami` `hostname` `pwd` `date`.""; exit 1; }
echo ""mapping 2c57c1a853-sampleA-mapping finished `whoami` `hostname` `pwd` `date`.""
touch ./working/sampleA/mapping/checkpoint
else
echo ""mapping 2c57c1a853-sampleA-mapping already executed, skipping this step `whoami` `hostname` `pwd` `date`.""
fi
```
As you can see the command line described in the pipeline YAML are translated into normal Unix command lines, therefore every solution that works on a standard Unix shell (pipes, bash substitutions) is perfectly acceptable. Pipengine addes extra lines in the script for steps checkpoint controls to avoid re-running already executed steps, and error controls with logging.

In this case also, the **run** key defines three different command lines, that are described using YAML array (a line prepended with a -). This command lines are all part of the same step, since the first two are required to prepare the input for the third command line (BWA), using standard bash commands.

As a rule of thumb you should put more command line into an array under the same step if these are all logically correlated and required to manipulate intermidiate files. Otherwise if command lines executes conceptually different actions they should go into different steps.

Example 2: Multiple steps in one job
------------------------------------

Now I want to execute more steps in a single job for each sample. The pipeline YAML is defined in this way:

```yaml

pipeline: resequencing

resources:
  bwa: /software/bwa
  samtools: /software/samtools
  mark_dup: /software/picard-tools-1.77/MarkDuplicates.jar
  gatk: /software/GenomeAnalysisTK/GenomeAnalysisTK.jar

steps:
  mapping:
    run:
     - ls <sample_path>/*_R1_*.gz | xargs zcat | pigz -p 10 >> R1.fastq.gz
     - ls <sample_path>/*_R2_*.gz | xargs zcat | pigz -p 10 >> R2.fastq.gz
     - <bwa> sampe -P <index> <(<bwa> aln -t 4 -q 20 <index> R1.fastq.gz) <(<bwa> aln -t 4 -q 20 <index> R2.fastq.gz) R1.fastq.gz R2.fastq.gz | <samtools> view -Su - | java -Xmx4g -jar /storage/software/picard-tools-1.77/AddOrReplaceReadGroups.jar I=/dev/stdin O=<sample>.sorted.bam SO=coordinate LB=<sample> PL=illumina PU=PU SM=<sample> TMP_DIR=/data/tmp CREATE_INDEX=true MAX_RECORDS_IN_RAM=1000000
     - rm -f R1.fastq.gz R2.fastq.gz
    cpu: 12

  mark_dup:
    pre: mapping
    run: java -Xmx4g -jar <mark_dup> VERBOSITY=INFO MAX_RECORDS_IN_RAM=500000 VALIDATION_STRINGENCY=SILENT INPUT=<mapping/sample>.sorted.bam OUTPUT=<sample>.md.sort.bam METRICS_FILE=<sample>.metrics REMOVE_DUPLICATES=false

  realign_target:
    pre: mark_dup
    run: java -Xmx4g -jar <gatk> -T RealignerTargetCreator -I <mark_dup/sample>.md.sort.bam -nt 8 -R <genome> -o <sample>.indels.intervals
    cpu: 8
```

The sample YAML file is the same as the example above. Now to execute together the 3 steps defined in the pipeline, PipEngine can be invoked with this command line:

```
pipengine run -p pipeline_multi.yml  -f samples.yml -s realign_target -d
```
Since dependencies have been defined for the steps using the ```pre``` key, it is sufficient to invoke Pipengine with the last step and the other two are automatically included in the script. Messages will be prompted in this case since Pipengine just warns that the directories for certain steps, that are needed for other steps in the pipeline, are not yet available (and thus the corresponding steps will be executed to generate the necessary data). The command line will generate the following shell script (one for each sample, available in the ./working directory):

```shell
#!/usr/bin/env bash
#PBS -N 6f3c911c49-sampleC-realign_target
#PBS -d ./working
#PBS -l nodes=1:ppn=12
if [ ! -f ./working/sampleC/mapping/checkpoint ]
then
echo ""mapping 6f3c911c49-sampleC-realign_target start `whoami` `hostname` `pwd` `date`.""

mkdir -p ./working/sampleC/mapping
cd ./working/sampleC/mapping
ls /ngs_reads/sampleC/*_R1_*.gz | xargs zcat | pigz -p 10 >> R1.fastq.gz || { echo ""mapping 6f3c911c49-sampleC-realign_target FAILED 0 `whoami` `hostname` `pwd` `date`.""; exit 1; }
ls /ngs_reads/sampleC/*_R2_*.gz | xargs zcat | pigz -p 10 >> R2.fastq.gz || { echo ""mapping 6f3c911c49-sampleC-realign_target FAILED 1 `whoami` `hostname` `pwd` `date`.""; exit 1; }
/software/bwa sampe -P /storage/genomes/bwa_index/genome <(/software/bwa aln -t 4 -q 20 /storage/genomes/bwa_index/genome R1.fastq.gz) <(/software/bwa aln -t 4 -q 20 /storage/genomes/bwa_index/genome R2.fastq.gz) R1.fastq.gz R2.fastq.gz | /software/samtools view -Su - | java -Xmx4g -jar /storage/software/picard-tools-1.77/AddOrReplaceReadGroups.jar I=/dev/stdin O=sampleC.sorted.bam SO=coordinate LB=sampleC PL=illumina PU=PU SM=sampleC TMP_DIR=/data/tmp CREATE_INDEX=true MAX_RECORDS_IN_RAM=1000000 || { echo ""mapping 6f3c911c49-sampleC-realign_target FAILED 2 `whoami` `hostname` `pwd` `date`.""; exit 1; }
rm -f R1.fastq.gz R2.fastq.gz || { echo ""mapping 6f3c911c49-sampleC-realign_target FAILED 3 `whoami` `hostname` `pwd` `date`.""; exit 1; }
echo ""mapping 6f3c911c49-sampleC-realign_target finished `whoami` `hostname` `pwd` `date`.""
touch ./working/sampleC/mapping/checkpoint
else
echo ""mapping 6f3c911c49-sampleC-realign_target already executed, skipping this step `whoami` `hostname` `pwd` `date`.""
fi
if [ ! -f ./working/sampleC/mark_dup/checkpoint ]
then
echo ""mark_dup 6f3c911c49-sampleC-realign_target start `whoami` `hostname` `pwd` `date`.""

mkdir -p ./working/sampleC/mark_dup
cd ./working/sampleC/mark_dup
java -Xmx4g -jar /software/picard-tools-1.77/MarkDuplicates.jar VERBOSITY=INFO MAX_RECORDS_IN_RAM=500000 VALIDATION_STRINGENCY=SILENT INPUT=./working/sampleC/mapping/sampleC.sorted.bam OUTPUT=sampleC.md.sort.bam METRICS_FILE=sampleC.metrics REMOVE_DUPLICATES=false || { echo ""mark_dup 6f3c911c49-sampleC-realign_target FAILED `whoami` `hostname` `pwd` `date`.""; exit 1; }
echo ""mark_dup 6f3c911c49-sampleC-realign_target finished `whoami` `hostname` `pwd` `date`.""
touch ./working/sampleC/mark_dup/checkpoint
else
echo ""mark_dup 6f3c911c49-sampleC-realign_target already executed, skipping this step `whoami` `hostname` `pwd` `date`.""
fi
if [ ! -f ./working/sampleC/realign_target/checkpoint ]
then
echo ""realign_target 6f3c911c49-sampleC-realign_target start `whoami` `hostname` `pwd` `date`.""

mkdir -p ./working/sampleC/realign_target
cd ./working/sampleC/realign_target
java -Xmx4g -jar /software/GenomeAnalysisTK/GenomeAnalysisTK.jar -T RealignerTargetCreator -I ./working/sampleC/mark_dup/sampleC.md.sort.bam -nt 8 -R /storage/genomes/genome.fa -o sampleC.indels.intervals || { echo ""realign_target 6f3c911c49-sampleC-realign_target FAILED `whoami` `hostname` `pwd` `date`.""; exit 1; }
echo ""realign_target 6f3c911c49-sampleC-realign_target finished `whoami` `hostname` `pwd` `date`.""
touch ./working/sampleC/realign_target/checkpoint
else
echo ""realign_target 6f3c911c49-sampleC-realign_target already executed, skipping this step `whoami` `hostname` `pwd` `date`.""
fi
```


Logging
---------------------------

It is always usefult to log activities and collect the output from your software. Pipengine can log to:

* stdin, just print on the terminal
* syslog send the log to the system log using logger
* fluentd send the log to a collector/centralized logging system (http://fluentd.org)


:: PBS Options ::
=================

If there is the need to pass to PipEngine specific PBS options, the ```--pbs-opts``` parameter can be used.

This parameter accepts a list of options and each one will be added to the PBS header in the shell script, along with the ```-l``` PBS parameter.

So for example, the following options passed to ```--pbs-opts```:

```shell
--pbs-opts nodes=2:ppn=8 host=node5
```

will become, in the shell script:

```shell
#PBS -l nodes=2:ppn=8
#PBS -l host=node5
```

Note also that from version 0.5.2, it is possible to specify common PBS options like ""nodes"" and ""mem"" (for memory) directly within a step defition in the Pipeline yaml, exactly as it's done with the ""cpu"" parameter. So in a step it is possible to write:

```yaml
  realign_target:
    run: java -Xmx4g -jar <gatk> -T RealignerTargetCreator -I <mark_dup/sample>.md.sort.bam -nt 8 -R <genome> -o <sample>.indels.intervals
    cpu: 8
    nodes: 2
    mem: 8G
```

to have PipEngine translate this into:

```shell
#PBS -l nodes=2:ppn=8,mem=8G
```

within the job script.

If a specific queue needs to be selected for sending the jobs to PBS, the ```--pbs-queue``` (short version **-q**) parameter can be used. This will pass to the ```qsub``` command the ```-q <queue name>``` taken from the command line.

:: Extending and contributing ::
================================

Pipengine code is organized around main methods allowing for YAML parsing and command line arguments substitutions that are available in lib/bio/pipengine.rb. Specific logic for jobs, pipeline steps and samples is described in dedicated classes called Bio::Pipengine::Job, Bio::Pipengine::Step and Bio::Pipengine::Sample.

For instance, in case the support for different schedulers needs to be introduced, it will be sufficient to modify or extend the Job.to_script method, which is the one defining scheduler-specific options in the runnable bash script.

Copyright
=========

&copy;2017 Francesco Strozzi, Raoul Jean Pierre Bonnal 
",2023-07-07 18:45:11+00:00
piper,piper,NationalGenomicsInfrastructure/piper,A genomics pipeline build on top of the GATK Queue framework,,False,9,2019-12-19 01:05:49+00:00,2014-06-04 13:33:33+00:00,9,13,7,7,1.5.1,2017-03-07 14:34:51+00:00,,879,v1.5.0,11,2016-09-06 11:47:05+00:00,,2017-03-07 14:34:51+00:00,"<p align=""center"">
  <a href=""https://github.com/NationalGenomicsInfrastructure/piper"">
    <img width=""512"" height=""206"" src=""artwork/logo.png""/>
  </a>
</p>
-----------------------------

[![Build Status](https://travis-ci.org/johandahlberg/piper.png?branch=master)](https://travis-ci.org/johandahlberg/piper)

A pipeline project started at the [SNP&SEQ Technology platform](http://www.molmed.medsci.uu.se/SNP+SEQ+Technology+Platform/) built on top of [GATK Queue](http://www.broadinstitute.org/gatk/guide/topic?name=intro#intro1306). Since then Piper has been adopted by the Swedish [National Genomics Infrastructure (NGI)](http://www.scilifelab.se/platforms/ngi/) for use on all human whole genome samples sequenced at NGI.

Piper builds on the concept of standardized workflows for different next-generation sequencing applications. At the moment Piper supports the following workflows:

* WholeGenome: For human whole genome sequencing data. This goes through alignment, alignment quality control, data processing, variant calling, and variant filtration according to the [best practice recommended by the Broad Institute](http://www.broadinstitute.org/gatk/guide/topic?name=best-practices), using primarily the GATK.
* Exome: TruSeq and SureSelect human exome sequencing: These use basically the same pipeline as the whole genome pipeline, but with the modifications suggested in the [best practice document](http://www.broadinstitute.org/gatk/guide/topic?name=best-practices) for exome studies.
* Haloplex: Haloplex targeted sequencing analysis. Including alignment, data processing, and variant calling.
* RNACounts: Produces [FPKMs](http://cufflinks.cbcb.umd.edu/faq.html#fpkm) for transcripts of an existing reference annotation using Tophat for mapping and Cufflinks to produce the FPKMs.

All supported workflows are available in the `workflows` directory in the project root.

Prerequisites and installation
==============================

Piper has been tested on the Java(TM) SE Runtime Environment (build 1.7.0_25) on the [UPPMAX](http://www.uppmax.uu.se) cluster Milou. It might run in other environments, but this is untested. Besides the JVM, Piper depends on [Maven (version 3+)](http://maven.apache.org/) for building (the GATK), [SBT](http://www.scala-sbt.org/) and [git](http://git-scm.com/) to checkout the source. To install piper, make sure that these programs are in you path and then clone this repository and run the setup script:

    git clone https://github.com/NationalGenomicsInfrastructure/piper.git
    cd piper
    sbt universal:packageBin
    # This will create two zip files.
    # Unzip these where you wish to install piper
    ./piper/target/universal/piper-v[version].zip
    ./setup-file-creator/target/universal/setupfilecreator-v[version].zip
    # Then add the workflows and piper bin folders to your PATH, e.g.
    export PATH=$PATH:/path/to/piper/bin:/path/to/piper/workflows
    # You also need to add the setupcreator to the path
    export PATH=$PATH:path/to/setupfilecreator/bin
    
As Piper acts as a wrapper for several standard bioinformatics programs, it requires that these are installed. At this point it requires that the following programs are installed (depending somewhat on the application):

* [bwa](http://bio-bwa.sourceforge.net/)
* [samtools](http://samtools.sourceforge.net/)
* [tophat](http://tophat.cbcb.umd.edu/)
* [cutadapt](https://code.google.com/p/cutadapt/)
* [cufflinks](http://cufflinks.cbcb.umd.edu/)
* [qualimap](http://qualimap.bioinfo.cipf.es/)

The paths for these programs are setup in the `uppmax_global_config.xml` file. If you are running on UPPMAX these should already be pointing to the correct locations. If not, you need to change them there.

Resource files
==============

For the standard application of alignment, data processing, and variant calling in human data, Piper relies on data available in the GATK bundle from the Broad Institute. This is available for download from their [website](http://gatkforums.broadinstitute.org/discussion/1213/what-s-in-the-resource-bundle-and-how-can-i-get-it). If you are working on UPPMAX these resources are available at `/pica/data/uppnex/reference/biodata/GATK/ftp.broadinstitute.org/bundle/2.8/`, however you might want to create your own directory for these in which you soft link the files, as you will be required to create, for example, bwa indexes.

The path to the GATK bundle needs to be setup in the `globalConfig.sh`.

Running the pipeline
====================

There are a number of workflows currently supported by Piper (See below). For most users these should just be called accoring to the run examples below. If you want to make customizations, open the appropriate workflow bash script and make the changes there. If you need to know what parameters are available, run the script with `--help`, e.g:

    piper -S <path to Script> --help

Setup for run
-------------

All workflows start with an xml file, for example: `pipelineSetup.xml`. This contains information about the raw data (run folders) that you want to run in the project. This is created using the `setupFileCreator` command.

** Directory structures and report files**
Piper depends on one of two specifications for folder structure to be parse metadata about the samples. The first structure looks like this (and is the one used by projects at the SNP&SEQ technology platform in Uppsala):

    Top level
    |---Runfolder1
        |---report.xml/report.tsv
        |---Sample_1
            |--- 1_<index>_<lane>_<read1>_xxx.fastq.gz
            |--- 1_<index>_<lane>_<read2>_xxx.fastq.gz (optional)
        |---Sample_2
            |--- 2_<index>_<lane>_<read1>_xxx.fastq.gz
            |--- 2_<index>_<lane>_<read2>_xxx.fastq.gz (optional)
    |---Runfolder1
        |---report.xml/report.tsv
        |---Sample_1
            |--- 1_<index>_<lane>_<read1>_xxx.fastq.gz
            |--- 1_<index>_<lane>_<read2>_xxx.fastq.gz (optional)
        |---Sample_2
            |--- 2_<index>_<lane>_<read1>_xxx.fastq.gz
            |--- 2_<index>_<lane>_<read2>_xxx.fastq.gz (optional)

As evident from this, the structure of each runfolder needs to have a file named either `report.xml` or `report.tsv`. In the `report.xml` file case, this is a file that is generated by the software [Sisyphus](https://github.com/Molmed/sisyphus), which is used by the SNP&SEQ Technology Platform to process data. If your data is not delivered from the SNP&SEQ Technology Platform you are probably better off using the simple `report.tsv` format. This is a tab-separated file with the following format:

        #SampleName     Lane    ReadLibrary     FlowcellId
        MyFirstSample   1       FirstLib        9767892AVF
        MyFirstSample   2       SecondLib       9767892AVF
        MySecondSample  1       SomeOtherLib    9767892AVF


The other allowed format is the Illumina Genome Network (IGN) file structure as it has been defined by the NGI.

    Project top level
    |---Sample
        |---Library
            |---Runfolder
               |--- <project>_<sample_name>_<index>_<lane>_xxx.fastq.gz

**Running non-interactively**
Run `setupFileCreator` without any arguments. This will show you the list of parameters that you need to set. This will produce a setup file on the following format:

```
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?>
<project xmlns=""setup.xml.molmed"">
    <metadata>
        <name>NA-001</name>
        <sequenceingcenter>NGI</sequenceingcenter>
        <platform>Illumina</platform>
        <uppmaxprojectid>a2009002</uppmaxprojectid>
        <uppmaxqos></uppmaxqos>
        <reference>/home/MOLMED/johda411/workspace/piper/src/test/resources/testdata/exampleFASTA.fasta</reference>
    </metadata>
    <inputs>
        <sample>
            <samplename>F15</samplename>
            <library>
                <libraryname>SX396_MA140710.1</libraryname>
                <platformunit>
                    <unitinfo>000000000-AA3LB.1</unitinfo>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/140812_M00485_0148_000000000-AA3LB/Projects/MD-0274/140812_M00485_0148_000000000-AA3LB/Sample_F15/F15_CCGAAGTA_L001_R1_001.fastq.gz</path>
                    </fastqfile>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/140812_M00485_0148_000000000-AA3LB/Projects/MD-0274/140812_M00485_0148_000000000-AA3LB/Sample_F15/F15_CCGAAGTA_L001_R2_001.fastq.gz</path>
                    </fastqfile>
                </platformunit>
            </library>
        </sample>
        <sample>
            <samplename>E14</samplename>
            <library>
                <libraryname>SX396_MA140710.1</libraryname>
                <platformunit>
                    <unitinfo>000000000-AA3LB.1</unitinfo>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/140812_M00485_0148_000000000-AA3LB/Projects/MD-0274/140812_M00485_0148_000000000-AA3LB/Sample_E14/E14_AGTCACTA_L001_R2_001.fastq.gz</path>
                    </fastqfile>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/140812_M00485_0148_000000000-AA3LB/Projects/MD-0274/140812_M00485_0148_000000000-AA3LB/Sample_E14/E14_AGTCACTA_L001_R1_001.fastq.gz</path>
                    </fastqfile>
                </platformunit>
            </library>
        </sample>
        <sample>
            <samplename>P1171_104</samplename>
            <library>
                <libraryname>A</libraryname>
                <platformunit>
                    <unitinfo>AC41A2ANXX.2</unitinfo>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/src/test/resources/testdata/Sthlm2UUTests/sthlm_runfolder_root/P1171_104/A/140702_AC41A2ANXX/P1171_104_ATTCAGAA-GGCTCTGA_L002_R1_001.fastq.gz</path>
                    </fastqfile>
                </platformunit>
                <platformunit>
                    <unitinfo>AC41A2ANXX.1</unitinfo>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/src/test/resources/testdata/Sthlm2UUTests/sthlm_runfolder_root/P1171_104/A/140702_AC41A2ANXX/P1171_104_ATTCAGAA-GGCTCTGA_L001_R1_001.fastq.gz</path>
                    </fastqfile>
                    <fastqfile>
                        <path>/home/MOLMED/johda411/workspace/piper/src/test/resources/testdata/Sthlm2UUTests/sthlm_runfolder_root/P1171_104/A/140702_AC41A2ANXX/P1171_104_ATTCAGAA-GGCTCTGA_L001_R2_001.fastq.gz</path>
                    </fastqfile>
                </platformunit>
            </library>
        </sample>
    </inputs>
</project>
```

Running
-------

Pick the workflow (they are located in the Piper directory under `workflows`) that you want to run, e.g. haloplex. Then initiate it (by simply running it, or if you do not have access to a node where you can continually run a JVM by using `sbatch` to send it to a node) accoding to the examples below.

Note that all workflows are by default setup to be run with the human_g1k_v37.fasta reference and associated annotations. This means that if you need some other reference, you will have to set it up manually by configuring the workflow script (and depending somewhat on the use case, make changes to the qscripts themself). 

It's also worth mentioning that all the scripts have a optional `alignments_only` flag which can be set if you are only interested in running the aligments. This is useful what you are working on a project where data is added over time, but you want to create the aligments as data comes in, and then join across the sequencing runs and continue with the downstream analysis.

Unless the `run` flag is added to the workflow commandline the pipeline will only dry run (i.e. it will not actually run any jobs), this is useful to make sure that all commandline have been setup up properly. Once you are happy with the setup add `run` to the commandline to run the pipeline.


**Haloplex**

    Haloplex.sh --xml_input <setup.xml> --intervals <regions file> --amplicons <amplicon file> [--alignments_only] [--run]

The files associated with the Haloplex design can be downloaded from Agilent's homepage. Please note that the design files will be converted to interval files to work with Picard. In this process the names in the files are converted to work with the ""b37"" genome reference, rather than ""hg19"", which is the reference used by agilent. This means that if you want to use ""hg19"" then you must specify the `--do_not_convert` flag in the qscript.

**RNACounts**

    RNACounts.sh --xml_input <setup.xml> --library_type <fr-secondstrand/fr-firststrand/fr-unstranded> [--alignments_only] [--run]

Library types depends on the protcol used. For example, for ScriptSeq libraries (EpiCentre) the library type should be set to `fr-secondstrand`.

**Exome**
    
    Exome.sh --xml_input <setup.xml> <--sureselect> || <--truseq> [--alignments_only] [--run]

Pick one of either `--sureselect` or `--truseq` to set which exome intervals should be used. If you wish to use another interval file - open up the workflow file and set the `INTERVALS` to the path of your interval file.

**WholeGenome**

    WholeGenome.sh --xml_input <setup.xml> [--alignments_only] [--run]


Monitoring progress
-------------------

To follow the progress of the run look in the `pipeline_output/logs` folder. There you will find the logs for the different scripts. By searching the log file for ""Run"", you can see how many jobs are currently running, how many have finished, and how many have failed. A recommendation is to use e.g. `less -S` to view the file with unwrapped lines, as it is quite difficult to read otherwise.


Development
===========

The heavy lifting in Piper is primarily done in Scala, with Bash glueing together the different scripts to into workflows. Some additional Java and the occasional Perl component is used, but the main body of the code is written in Scala.

Coding
------

For an introduction to Queue, on which Piper is built, see: http://gatkforums.broadinstitute.org/discussion/1306/overview-of-queue

To work on the Piper project I recommend using the [Scala IDE](http://scala-ide.org/). To start developing follow the installation procedure outlined above. When you have finised the installation you can set the project up for your IDE by running:

    sbt eclipse

This will create the necessary project file for you to be able to import the project into the Scala IDE and start developing.

Although the Scala IDE will compile the code as you type, you will probably also want to get the hang of a few basic SBT commands (which you can either run from the interactive sbt console which you start by typing `sbt` in the project root folder, or by typing `sbt <command>` to run it straight from the CLI):

    compile

Will compile your project.

    clean

If something looks strange it is probably a good idea to run this. It deletes all of your class files so that you can be sure you have a totally clean build.

    test

Run the tests (for more on testing, see the testing chapter) - note that by default this only dry runs the qscript integration tests, which basically makes sure that they compile, but giving you no guarantees for runtime functionality.

### Making Piper generate graph files
Queue includes functionallity to generate dot files to visualize the jobs graph. This is highly useful when debugging new qscripts as it lets you see how the jobs connect to one another. So, if you have made a mistake in the chaining of the dependencies it is easy to spot. "".dot"" files can be opened with e.g. [xdot](https://github.com/jrfonseca/xdot.py).


### Using the XML binding compiler (xjc):
To generate the xml read classes I use xjc, which uses an xml schema in xsd format to generate a number of java classes, which can then be used to interact with the setup and report xml files. These classes are used by the SetupXMLReader and the SetupFileCreator. An example of how to generate the classes is seen below:

	 xjc -d piper/src/main/java/ piper/src/main/resources/xml_schemas/PipelineSetupSchema.xsd

Testing
-------

### Running pipeline tests
Running the tests is done by `sbt test`. However, there are some things which need to be noted. As the pipeline tests take a long time and have dependencies on outside programs (such as bwa for alignment, etc.) these can only be run on machines that have all the required programs installed and that have all the correct resources. This means that by default the tests are setup to just compile the qscripts, but not run them. If you want to run the qscripts you need to go into `src/test/resources/testng.xml` and set the value of the runpipeline parameter to 'true'.

### Writing pipeline tests
Pipeline tests are setup to run a certain QScript and check the md5sums of the outputs. If md5sums do not match, it will show you what the differences between the files are so that you can decide if the changes to the output are reasonable considering the changes to the code you have made. At the moment, pipeline tests are just setup to run (with all the necessary resources, etc) on my workstation. In furture versions I hope to be able to make this more portable.

### Continuous integration using Travis:
Piper uses [Travis](https://travis-ci.org/) for continious integration. For instruction on how to set this up with a github repository see: http://about.travis-ci.org/docs/user/getting-started/

Troubleshooting
===============

Old projects
------------
In projects where data was generated before the spring of 2013, the report.xml files do not fulfill the current specification. To fix this you need to find the following row in the `report.xml`:
    
    <SequencingReport>

and substitute it for:

    <SequencingReport  xmlns=""illuminareport.xml.molmed"">

Licence
=======

The MIT License (MIT)

Copyright (c) 2013  The SNP&SEQ Technology Platform, Uppsala

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
",2023-07-07 18:45:15+00:00
plumbing,plumbing,plumatic/plumbing,Prismatic's Clojure(Script) utility belt,,False,1471,2023-06-21 13:07:15+00:00,2013-01-25 00:39:58+00:00,113,91,25,0,,,,377,v0.1.0,22,2013-05-01 18:00:38+00:00,2023-06-21 13:07:16+00:00,2023-02-01 03:46:03+00:00,"# Plumbing and Graph: the Clojure utility belt

<img src=""https://raw.github.com/wiki/plumatic/plumbing/images/prismatic-swiss-army-knife.png"" alt=""prismatic/plumbing logo"" title=""prismatic/plumbing logo"" align=""right"" width=""250"" />

This first release includes our '[Graph](http://plumatic.github.io/prismatics-graph-at-strange-loop)' library, our `plumbing.core` library of very commonly used functions (the only namespace we `:use` across our codebase), and a few other supporting namespaces.

*New in 0.3.0: support for ClojureScript*

*New in 0.2.0: support for schema.core/defn-style schemas on fnks and Graphs.  See `(doc fnk)` for details.*

Leiningen dependency (Clojars): 

[![Clojars Project](http://clojars.org/prismatic/plumbing/latest-version.svg)](http://clojars.org/prismatic/plumbing) 

[Latest API docs](http://plumatic.github.io/plumbing).

**This is an alpha release.  We  are using it internally in production, but the API and organizational structure are subject to change.  Comments and suggestions are much appreciated.**

Check back often, because we'll keep adding more useful namespaces and functions as we work through cleaning up and open-sourcing our stack of Clojure libraries.

## Graph: the Functional Swiss-Army Knife

Graph is a simple and *declarative* way to specify a structured computation, which is easy to analyze, change, compose, and monitor. Here's a simple example of an ordinary function definition, and its Graph equivalent:

```clojure
(require '[plumbing.core :refer (sum)])
(defn stats
  ""Take a map {:xs xs} and return a map of simple statistics on xs""
  [{:keys [xs] :as m}]
  (assert (contains? m :xs))
  (let [n  (count xs)
        m  (/ (sum identity xs) n)
        m2 (/ (sum #(* % %) xs) n)
        v  (- m2 (* m m))]
    {:n n   ; count
     :m m   ; mean
     :m2 m2 ; mean-square
     :v v   ; variance
     }))

(require '[plumbing.core :refer (fnk sum)])
(def stats-graph
  ""A graph specifying the same computation as 'stats'""
  {:n  (fnk [xs]   (count xs))
   :m  (fnk [xs n] (/ (sum identity xs) n))
   :m2 (fnk [xs n] (/ (sum #(* % %) xs) n))
   :v  (fnk [m m2] (- m2 (* m m)))})
```

A Graph is just a map from keywords to keyword functions ([learn more](#fnk)).  In this case, `stats-graph` represents the steps in taking a sequence of numbers (`xs`) and producing univariate statistics on those numbers (i.e., the mean `m` and the variance `v`).  The names of arguments to each `fnk` can refer to other steps that must happen before the step executes. For instance, in the above, to execute `:v`, you must first execute the `:m` and `:m2` steps (mean and mean-square respectively).

We can ""compile"" this Graph to produce a single function (equivalent to `stats`), which also checks that the map represents a valid Graph:

```clojure
(require '[plumbing.graph :as graph] '[schema.core :as s])
(def stats-eager (graph/compile stats-graph))

(= {:n 4
    :m 3
    :m2 (/ 25 2)
    :v (/ 7 2)}
   (into {} (stats-eager {:xs [1 2 3 6]})))

;; Missing :xs key exception
(thrown? Throwable (stats-eager {:ys [1 2 3]}))
```

Moreover, as of the 0.1.0 release, `stats-eager` is *fast* -- only about 30% slower than the hand-coded `stats` if `xs` has a single element, and within 5% of `stats` if `xs` has ten elements.

Unlike the opaque `stats` fn, however, we can modify and extend `stats-graph` using ordinary operations on maps:

```clojure
(def extended-stats
  (graph/compile
    (assoc stats-graph
      :sd (fnk [^double v] (Math/sqrt v)))))

(= {:n 4
    :m 3
    :m2 (/ 25 2)
    :v (/ 7 2)
    :sd (Math/sqrt 3.5)}
   (into {} (extended-stats {:xs [1 2 3 6]})))
```

A Graph encodes the structure of a computation, but not how it happens, allowing for many execution strategies. For example, we can compile a Graph lazily so that step values are computed as needed.  Or, we can parallel-compile the Graph so that independent step functions are run in separate threads:

```clojure
(def lazy-stats (graph/lazy-compile stats-graph))

(def output (lazy-stats {:xs [1 2 3 6]}))
;; Nothing has actually been computed yet
(= (/ 25 2) (:m2 output))
;; Now :n and :m2 have been computed, but :v and :m are still behind a delay


(def par-stats (graph/par-compile stats-graph))

(def output (par-stats {:xs [1 2 3 6]}))
;; Nodes are being computed in futures, with :m and :m2 going in parallel after :n
(= (/ 7 2) (:v output))
```

We can also ask a Graph for information about its inputs and outputs (automatically computed from its definition):

```clojure
(require '[plumbing.fnk.pfnk :as pfnk])

;; stats-graph takes a map with one required key, :xs
(= {:xs s/Any}
   (pfnk/input-schema stats-graph))

;; stats-graph outputs a map with four keys, :n, :m, :m2, and :v
(= {:n s/Any :m s/Any :m2 s/Any :v s/Any}
   (pfnk/output-schema stats-graph))
```

If schemas are provided on the inputs and outputs of the node functions, these propagate through into the Graph schema as expected.

We can also have higher-order functions on Graphs to wrap the behavior on each step. For instance, we can automatically profile each sub-function in 'stats' to see how long it takes to execute:

```clojure
(def profiled-stats (graph/compile (graph/profiled ::profile-data stats-graph)))

;;; times in milliseconds for each step:
(= {:n 1.001, :m 0.728, :m2 0.996, :v 0.069}
   @(::profile-data (profiled-stats {:xs (range 10000)})))
```

… and so on.  For more examples and details about Graph, check out the [graph examples test](https://github.com/plumatic/plumbing/blob/master/test/plumbing/graph_examples_test.cljc).

<a name=""fnk""/>

## Bring on (de)fnk

Many of the functions we write (in Graph and elsewhere) take a single (nested) map argument with keyword keys and have expectations about which keys must be present and which are optional. We developed a new style of binding ([read more here](https://github.com/plumatic/plumbing/tree/master/src/plumbing/fnk)) to make this a lot easier and to check that input data has the right 'shape'. We call these 'keyword functions' (defined by `defnk`) and here's what one looks like:

```clojure
(use 'plumbing.core)
(defnk simple-fnk [a b c]
  (+ a b c))

(= 6 (simple-fnk {:a 1 :b 2 :c 3}))
;; Below throws: Key :c not found in (:a :b)
(thrown? Throwable (simple-fnk {:a 1 :b 2}))
```

You can declare a key as optional and provide a default:
```clojure
(defnk simple-opt-fnk [a b {c 1}]
  (+ a b c))

(= 4 (simple-opt-fnk {:a 1 :b 2}))
```

You can do nested map bindings:
```clojure
(defnk simple-nested-fnk [a [:b b1] c]
  (+ a b1 c))

(= 6 (simple-nested-fnk {:a 1 :b {:b1 2} :c 3}))
;; Below throws: Expected a map at key-path [:b], got type class java.lang.Long
(thrown? Throwable (simple-nested-fnk {:a 1 :b 1 :c 3}))
```

Of course, you can bind multiple variables from an inner map and do multiple levels of nesting:
```clojure
(defnk simple-nested-fnk2 [a [:b b1 [:c {d 3}]]]
  (+ a b1 d))

(= 4 (simple-nested-fnk2 {:a 1 :b {:b1 2 :c {:d 1}}}))
(= 5 (simple-nested-fnk2 {:a 1 :b {:b1 1 :c {}}}))
```

You can also use this binding style in a `let` statement using `letk`
or within an anonymous function by using `fnk`.


## More good stuff

There are a bunch of functions in `plumbing.core` that we can't live without. Here are a few of our favorites.

When we build maps, we often use `for-map`, which works like `for` but for maps:

```clojure
(use 'plumbing.core)
(= (for-map [i (range 3)
             j (range 3)
	         :let [s (+ i j)]
			 :when (< s 3)]
	  [i j]
	  s)
   {[0 0] 0, [0 1] 1, [0 2] 2, [1 0] 1, [1 1] 2, [2 0] 2})
```

`safe-get` is like `get` but throws when the key doesn't exist:

```clojure
;; IllegalArgumentException Key :c not found in {:a 1, :b 2}
(thrown? Exception (safe-get {:a 1 :b 2} :c))
```

Another frequently used map function is `map-vals`:

```clojure
;; return k -> (f v) for [k, v] in map
(= (map-vals inc {:a 0 :b 0})
   {:a 1 :b 1})
```

Ever wanted to conditionally do steps in a `->>` or `->`? Now you can with our
'penguin' operators. Here's a few examples:

```clojure
(use 'plumbing.core)
(= (let [add-b? false]
     (-> {:a 1}
         (merge {:c 2})
         (?> add-b? (assoc :b 2))))
   {:a 1 :c 2})

(= (let [inc-all? true]
     (->> (range 10)
          (filter even?)
          (?>> inc-all? (map inc))))
	[1 3 5 7 9])
```

Check out [`plumbing.core`](https://github.com/plumatic/plumbing/blob/master/src/plumbing/core.cljc) for many other useful functions.

## ClojureScript

As of 0.3.0, plumbing is available in ClojureScript! The vast majority of the
library supports ClojureScript, with the only exceptions that are JVM-specific
optimizations.

Here's an example usage of `for-map`:

```clojure
(ns plumbing.readme
  (:require [plumbing.core :refer-macros [for-map]]))

(defn js-obj->map
  ""Recursively converts a JavaScript object into a map with keyword keys""
  [obj]
  (for-map [k (js-keys obj)
            :let [v (aget obj k)]]
    (keyword k) (if (object? v) (js-obj->map v) v)))

(is (= {:a 1 :b {:x ""x"" :y ""y""}}
       (js-obj->map
        (js-obj ""a"" 1
                ""b"" (js-obj ""x"" ""x""
                            ""y"" ""y"")))))

;; Note: this is a contrived example; you would normally use `cljs.core/clj->js`
```

## Community

Plumbing now has a [mailing list](https://groups.google.com/forum/#!forum/prismatic-plumbing).  Please feel free to join and ask questions or discuss how you're using Plumbing and Graph.

## Supported Clojure versions

Plumbing is currently supported on Clojure 1.8 or later, and the latest ClojureScript version.

## License

Distributed under the Eclipse Public License, the same as Clojure.
",2023-07-07 18:45:18+00:00
polyaxon,polyaxon,polyaxon/polyaxon,MLOps Tools For Managing & Orchestrating The Machine Learning LifeCycle,https://polyaxon.com,False,3349,2023-07-07 10:08:40+00:00,2016-12-26 12:48:47+00:00,321,79,90,0,,,Apache License 2.0,10150,v2.0.0-rc26,175,2023-07-03 18:46:54+00:00,2023-07-06 02:26:56+00:00,2023-07-03 18:46:54+00:00,"[![License: Apache 2](https://img.shields.io/badge/License-apache2-blue.svg?style=flat&longCache=true)](LICENSE)
[![Polyaxon API](https://img.shields.io/docker/pulls/polyaxon/polyaxon-api)](https://hub.docker.com/r/polyaxon/polyaxon-api)
[![Slack](https://img.shields.io/badge/Slack-1.5k%20members-blue.svg?style=flat&logo=slack&longCache=true)](https://polyaxon.com/slack/)

[![Docs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat&longCache=true)](https://polyaxon.com/docs/)
[![Release](https://img.shields.io/badge/release-v1.20.0-brightgreen.svg?longCache=true)](https://polyaxon.com/docs/releases/1-20/)
[![GitHub](https://img.shields.io/badge/issue_tracker-github-blue?style=flat&logo=github&longCache=true)](https://github.com/polyaxon/polyaxon/issues)
[![GitHub](https://img.shields.io/badge/roadmap-github-blue?style=flat&logo=github&longCache=true)](https://github.com/orgs/polyaxon/projects/5)

[![CLI](https://github.com/polyaxon/polyaxon/actions/workflows/cli.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/cli.yml)
[![Haupt](https://github.com/polyaxon/polyaxon/actions/workflows/haupt.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/haupt.yml)
[![Hypertune](https://github.com/polyaxon/polyaxon/actions/workflows/hypertune.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/hypertune.yml)
[![Traceml](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/90c05b6b112548c1a88b950beceacb69)](https://www.codacy.com/app/polyaxon/polyaxon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=polyaxon/polyaxon&amp;utm_campaign=Badge_Grade)


<a href=""https://polyaxon.com""><img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/logo/vector/primary-white-default-monochrome.svg"" width=""125"" height=""125"" align=""right"" /></a>

# Reproduce, Automate, Scale your data science


Welcome to Polyaxon, a platform for building, training, and monitoring large scale deep learning applications.
We are making a system to solve reproducibility, automation, and scalability for machine learning applications.

Polyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, and it supports all the major deep learning frameworks such as Tensorflow, MXNet, Caffe, Torch, etc.

Polyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. And it turns GPU servers into shared, self-service resources for your team or organization.

<br>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/demo.gif"" alt=""demo"" width=""80%"">
</p>
<br>

# Install

#### TL;DR;

* Install CLI

    ```bash
    # Install Polyaxon CLI
    $ pip install -U polyaxon
    ```

 * Create a deployment

    ```bash
    # Create a namespace
    $ kubectl create namespace polyaxon

    # Add Polyaxon charts repo
    $ helm repo add polyaxon https://charts.polyaxon.com

    # Deploy Polyaxon
    $ polyaxon admin deploy -f config.yaml

    # Access API
    $ polyaxon port-forward
    ```

Please check [polyaxon installation guide](https://polyaxon.com/docs/setup/)

# Quick start

#### TL;DR;

 * Start a project

    ```bash
    # Create a project
    $ polyaxon project create --name=quick-start --description='Polyaxon quick start.'
    ```

 * Train and track logs & resources

    ```bash
    # Upload code and start experiments
    $ polyaxon run -f experiment.yaml -u -l
    ```

 * Dashboard

    ```bash
    # Start Polyaxon dashboard
    $ polyaxon dashboard

    Dashboard page will now open in your browser. Continue? [Y/n]: y
    ```

<br>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/compare.png"" alt=""compare"" width=""400"">
  <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/dashboards.png"" alt=""dashboards"" width=""400"">
</p>
<br>

 * Notebook
    ```bash
    # Start Jupyter notebook for your project
    $ polyaxon run --hub notebook
    ```

<br>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/notebook.png"" alt=""compare"" width=""400"">
</p>
<br>

 * Tensorboard
    ```bash
    # Start TensorBoard for a run's output
    $ polyaxon run --hub tensorboard -P uuid=UUID
    ```

<br>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/tensorboard.png"" alt=""tensorboard"" width=""400"">
</p>
<br>

Please check our [quick start guide](https://polyaxon.com/docs/intro/quick-start/) to start training your first experiment.

# Distributed job

Polyaxon supports and simplifies distributed jobs.
Depending on the framework you are using, you need to deploy the corresponding operator, adapt your code to enable the distributed training,
and update your polyaxonfile.

Here are some examples of using distributed training:

 * [Distributed Tensorflow](https://polyaxon.com/docs/experimentation/distributed/tf-jobs/)
 * [Distributed Pytorch](https://polyaxon.com/docs/experimentation/distributed/pytorch-jobs/)
 * [Distributed MPI](https://polyaxon.com/docs/experimentation/distributed/mpi-jobs/)
 * [Horovod](https://polyaxon.com/integrations/horovod/)
 * [Spark](https://polyaxon.com/docs/experimentation/distributed/spark-jobs/)
 * [Dask](https://polyaxon.com/docs/experimentation/distributed/dask-jobs/)

# Hyperparameters tuning

Polyaxon has a concept for suggesting hyperparameters and managing their results very similar to Google Vizier called experiment groups.
An experiment group in Polyaxon defines a search algorithm, a search space, and a model to train.

 * [Grid search](https://polyaxon.com/docs/automation/optimization-engine/grid-search/)
 * [Random search](https://polyaxon.com/docs/automation/optimization-engine/random-search/)
 * [Hyperband](https://polyaxon.com/docs/automation/optimization-engine/hyperband/)
 * [Bayesian Optimization](https://polyaxon.com/docs/automation/optimization-engine/bayesian-optimization/)
 * [Hyperopt](https://polyaxon.com/docs/automation/optimization-engine/hyperopt/)
 * [Custom Iterative Optimization](https://polyaxon.com/docs/automation/optimization-engine/iterative/)

# Parallel executions

You can run your processing or model training jobs in parallel, Polyaxon provides a [mapping](https://polyaxon.com/docs/automation/mapping/) abstraction to manage concurrent jobs.

# DAGs and workflows

[Polyaxon DAGs](https://polyaxon.com/docs/automation/flow-engine/) is a tool that provides container-native engine for running machine learning pipelines.
A DAG manages multiple operations with dependencies. Each operation is defined by a component runtime.
This means that operations in a DAG can be jobs, services, distributed jobs, parallel executions, or nested DAGs.


# Architecture

![Polyaxon architecture](artifacts/polyaxon_architecture.png)

# Documentation

Check out our [documentation](https://polyaxon.com/docs/) to learn more about Polyaxon.

# Dashboard

Polyaxon comes with a dashboard that shows the projects and experiments created by you and your team members.

To start the dashboard, just run the following command in your terminal

```bash
$ polyaxon dashboard -y
```

# Project status

Polyaxon is stable and it's running in production mode at many startups and Fortune 500 companies.

# Contributions

Please follow the contribution guide line: *[Contribute to Polyaxon](CONTRIBUTING.md)*.


# Research

If you use Polyaxon in your academic research, we would be grateful if you could cite it.

Feel free to [contact us](mailto:contact@polyaxon.com), we would love to learn about your project and see how we can support your custom need.
",2023-07-07 18:45:23+00:00
popper,popper,getpopper/popper,Container-native task automation engine.,https://getpopper.io,False,289,2023-06-17 18:44:33+00:00,2015-10-02 20:58:39+00:00,60,14,40,20,v2020.09.1,2020-09-08 01:31:01+00:00,MIT License,674,v2020.09.1,23,2020-09-08 01:29:21+00:00,2023-06-17 18:44:33+00:00,2021-09-29 17:10:02+00:00,"# <img src=""https://raw.githubusercontent.com/getpopper/website/bcba4c8/assets/images/popper_logo_just_jug.png"" width=""64"" valign=""middle"" alt=""Popper""/> Popper

[![Downloads](https://pepy.tech/badge/popper)](https://pepy.tech/project/popper)
[![Build Status](https://travis-ci.org/getpopper/popper.svg?branch=master)](https://travis-ci.org/getpopper/popper)
[![codecov](https://codecov.io/gh/getpopper/popper/branch/master/graph/badge.svg)](https://codecov.io/gh/getpopper/popper)
[![Join the chat at https://gitter.im/systemslab/popper](https://badges.gitter.im/systemslab/popper.svg)](https://gitter.im/falsifiable-us/popper?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![slack](https://img.shields.io/badge/chat-on_slack-C03C20.svg?logo=slack)](https://join.slack.com/t/getpopper/shared_invite/zt-dtn0se2s-c50myMHNpeoikQXDeNbPew)
[![CROSS](https://img.shields.io/badge/supported%20by-CROSS-green)](https://cross.ucsc.edu)

Popper is a tool for defining and executing container-native testing 
workflows in Docker. With Popper, you define a workflow in a YAML 
file, and then execute it with a single command. A workflow file looks 
like this:

```yaml
steps:
- id: dev-init
  uses: docker://rikorose/gcc-cmake:gcc-9
  runs: [sh, -uexc]
  args:
  - |
    rm -rf build/
    cmake -DCMAKE_BUILD_TYPE=Release -S . -B build

- id: build
  uses: docker://rikorose/gcc-cmake:gcc-9
  runs: [cmake, --build, build/, --parallel, '4']

- id: test
  uses: docker://rikorose/gcc-cmake:gcc-9
  dir: /workspace/build/
  runs: [ctest]
```

Assuming the above is stored in a `ci.yml` file in the root of your 
project folder, this entire workflow gets executed by running:

```bash
popper run -f ci.yml
```

Running a single step:

```bash
popper run -f ci.yml build
```

Starting a shell inside the `build` step container:

```bash
popper run -f ci.yml build
```

Running on another engine (Podman):

```bash
popper run -f ci.yml -e podman build
```

See the [`examples/`](./examples) folder for examples for tests for 
other languages, as well as other types of tests (integration, 
regresssion, etc.).

## Installation

To install or upgrade Popper, run the following in your terminal:

```bash
curl -sSfL https://raw.githubusercontent.com/getpopper/popper/master/install.sh | sh
```

[Docker][docker] is required to run Popper and the installer will 
abort if the `docker` command cannot be invoked from your shell. For 
other installation options, including installing for use with the 
other supported engines (Singularity and Podman), or for setting up a 
developing environment for Popper, [read the complete installation 
instructions][installation].

Once installed, you can get an overview and list of available 
commands:

```bash
popper help
```

Read the [Quickstart Guide][getting_started] to learn the basics of 
how to use Popper. Or browse the [Official documentation][docs].

## Features

  * **Lightweight workflow and task automation syntax.** Defining a list of 
    steps is as simple as writing file in a [lightweight YAML syntax][cnwf] and 
    invoking `popper run` (see demo above). If you're familiar with 
    [Docker Compose][compose], you can think of Popper as Compose but 
    for end-to-end tasks instead of services.

  * **An abstraction over container runtimes**. In addition to Docker, 
    Popper can seamlessly execute workflows in other runtimes by 
    interacting with distinct container engines. Popper currently 
    supports [Docker][docker], [Singularity][sylabs] and 
    [Podman][podman].

  * **An abstraction over CI services**. Define a pipeline once and 
    then instruct Popper to generate configuration files for distinct 
    CI services, allowing users to run the exact same workflows they 
    run locally on Travis, Jenkins, Gitlab, Circle and others. See the 
    [`examples/`](./examples/) folder for examples on how to automate 
    CI tasks for multiple projects (Go, C++, Node, etc.).

  * **An abstraction over resource managers**. Popper can also execute 
    workflows on a variety of resource managers and schedulers such as 
    Kubernetes and SLURM, without requiring any modifications to a 
    workflow YAML file. We currently support SLURM and are working on 
    adding support for Kubernetes.

  * **Aid in workflow development**. Aid in the implementation and 
    [debugging][pp-sh] of workflows, and provide with an extensive 
    list of [example 
    workflows](https://github.com/getpopper/popper-examples) that can 
    serve as a starting point.

## What Problem Does Popper Solve?

Popper is a container-native workflow execution and task automation 
engine. In practice, when we work following the 
[container-native](docs/sections/concepts.md) paradigm, we end up 
interactively executing multiple Docker commands (`docker pull`, 
`docker build`, `docker run`, etc.) so that we can build containers, 
compile code, test applications, deploy software, among others. 
Keeping track of which `docker` commands we have executed, in which 
order, and which flags were passed to each, can quickly become 
unmanageable, difficult to document (think of outdated README 
instructions) and error prone.

On top of this, having the same workflow work in other environments 
(CI, K8S, etc.) is time-consuming and defeats the purpose of using 
containers (portability). The goal of Popper is to bring order to this 
chaotic scenario by providing a framework for clearly and explicitly 
defining container-native tasks. You can think of Popper as tool for 
wrapping all these manual tasks in a lightweight, machine-readable, 
self-documented format (YAML).

While this sounds simple at first, it has significant implications: 
results in time-savings, improves communication and in general unifies 
development, testing and deployment workflows. As a developer or user 
of ""Popperized"" container-native projects, you only need to learn one 
tool, and leave the execution details to Popper, whether is to build 
and tests applications locally, on a remote CI server or a Kubernetes 
cluster.

## Contributing

Anyone is welcome to contribute to Popper! To get started, take a look 
at our [contributing guidelines](CONTRIBUTING.md), then dive in with 
our [list of good first issues][gfi].

## Participation Guidelines

Popper adheres to the code of conduct [posted in this 
repository](CODE_OF_CONDUCT.md). By participating or contributing to 
Popper, you're expected to uphold this code. If you encounter unacceptable 
behavior, please immediately [email us](mailto:ivotron@ucsc.edu).

## How to Cite Popper

> Ivo Jimenez, Michael Sevilla, Noah Watkins, Carlos Maltzahn, Jay 
> Lofstead, Kathryn Mohror, Andrea Arpaci-Dusseau and Remzi 
> Arpaci-Dusseau. _The Popper Convention: Making Reproducible Systems 
> Evaluation Practical_. In 2017 IEEE International Parallel and 
> Distributed Processing Symposium Workshops (IPDPSW), 1561–70, 2017. 
> (https://doi.org/10.1109/IPDPSW.2017.157)

PDF for a pre-print version [available here](https://raw.githubusercontent.com/systemslab/popper-paper/master/paper/paper.pdf). 
For BibTeX, [click here](https://raw.githubusercontent.com/systemslab/popper-paper/master/popper.bib).

[gfi]: https://github.com/getpopper/popper/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22good+first+issue%22+is%3Aopen
[docker]: https://docs.docker.com/get-docker/
[getting_started]: https://popper.readthedocs.io/en/latest/sections/getting_started.html
[docs]: https://popper.readthedocs.io/en/latest/
[sylabs]: https://sylabs.io/
[compose]: https://docs.docker.com/compose/
[podman]: https://podman.io
[pp-sh]: docs/sections/cli_features.md#executing-a-step-interactively
[installation]: docs/installation.md
[cnwf]: ./docs/sections/cn_workflows.md#syntax
",2023-07-07 18:45:27+00:00
porcupine,Porcupine,TimVanMourik/Porcupine,Visual programming for neuroimaging analysis,https://timvanmourik.github.io/Porcupine,False,61,2023-05-04 07:38:11+00:00,2015-05-02 21:20:20+00:00,16,14,5,6,1.4.0,2018-01-12 15:55:50+00:00,GNU General Public License v3.0,453,v1.1.0,6,2017-06-19 08:30:04+00:00,2023-05-04 07:38:11+00:00,2019-12-06 20:39:25+00:00,"# PORcupine Creates Ur PipelINE

[![Build status](https://ci.appveyor.com/api/projects/status/2451r3j95fngau7r?svg=true)](https://ci.appveyor.com/project/lukassnoek/porcupine)
[![Build Status](https://travis-ci.org/TimVanMourik/Porcupine.svg?branch=master)](https://travis-ci.org/TimVanMourik/Porcupine)
[![DOI](https://zenodo.org/badge/34963696.svg)](https://zenodo.org/badge/latestdoi/34963696)


**This app has been superseded by a web version and is thus no longer under active development**

Porcupine has moved to the clouds! As a web application, it will be a **G**raphical **I**nterface for **R**eproducible **A**nalysis o**F** work**F**low **E**xperiments, GiraffeTools. Read the introductory post on Medium [here](https://medium.com/@TimVanMourik/giraffetools-a-generic-online-workflow-editor-8e48e288a6c7) or check out the website: [https://giraffe.tools](https://giraffe.tools).


“PORcupine Creates Ur PipelINE, the worst recursive acronym with bad capitalisation and annoying use of slang.”
Please find all documentation, examples, and much more on https://timvanmourik.github.io/Porcupine

## Installation
Installer files for all platforms can be found in the releases https://github.com/TimVanMourik/Porcupine/releases

## Compilation
After pulling this repository, it should readily compile with Qt Creator, provided that your system is up-to-date and you're using the latest version of Qt (currently: 5.8.0) and Qt Creator (currently: 4.2.1). It's been tested on Mac, Windows and Linux and basically, pressing the big green button is the only thing you need to make it compile and run. If you experience any issues, please contact us or leave an 'issue' in this repository

## Modules
Porcupine features interfaces to
* My own fMRI analysis toolbox, including laminar fMRI https://github.com/TimVanMourik/OpenFmriAnalysis
* NiPype https://github.com/nipy/nipype

",2023-07-07 18:45:31+00:00
predictionio,predictionio,apache/predictionio,"PredictionIO, a machine learning server for developers and ML engineers.",https://predictionio.apache.org,True,12555,2023-07-02 04:48:42+00:00,2013-01-25 19:42:32+00:00,2016,759,132,1,v0.9.6,2016-04-11 19:42:44+00:00,Apache License 2.0,4495,v0.14.0,43,2019-03-11 05:01:49+00:00,2023-07-02 04:48:42+00:00,2019-12-12 16:15:01+00:00,"<!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the ""License""); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# [Apache PredictionIO](http://predictionio.apache.org)

[![Build
Status](https://api.travis-ci.org/apache/predictionio.svg?branch=develop)](https://travis-ci.org/apache/predictionio)

Apache PredictionIO is an open source machine learning framework
for developers, data scientists, and end users. It supports event collection,
deployment of algorithms, evaluation, querying predictive results via REST APIs.
It is based on scalable open source services like Hadoop, HBase (and other DBs),
Elasticsearch, Spark and implements what is called a Lambda Architecture.

To get started, check out http://predictionio.apache.org!


## Table of contents
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Bugs and Feature Requests](#bugs-and-feature-requests)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Community](#community)


## Installation

A few installation options available.

*   [Installing Apache PredictionIO from
    Binary/Source](http://predictionio.apache.org/install/install-sourcecode/)
*   [Installing Apache PredictionIO with
    Docker](http://predictionio.apache.org/install/install-docker/)

## Quick Start

*   [Recommendation Engine Template Quick
    Start](http://predictionio.apache.org/templates/recommendation/quickstart/)
    Guide
*   [Similiar Product Engine Template Quick
    Start](http://predictionio.apache.org/templates/similarproduct/quickstart/)
    Guide
*   [Classification Engine Template Quick
    Start](http://predictionio.apache.org/templates/classification/quickstart/)
    Guide


## Bugs and Feature Requests

Use [Apache JIRA](https://issues.apache.org/jira/browse/PIO) to report bugs or request new features.

## Documentation

Documentation, included in this repo in the `docs/manual` directory, is built
with [Middleman](http://middlemanapp.com/) and publicly hosted at
[predictionio.apache.org](http://predictionio.apache.org/).

Interested in helping with our documentation? Read [Contributing
Documentation](http://predictionio.apache.org/community/contribute-documentation/).


## Community

Keep track of development and community news.

*   Subscribe to the user mailing list <mailto:user-subscribe@predictionio.apache.org>
    and the dev mailing list <mailto:dev-subscribe@predictionio.apache.org>
*   Follow [@predictionio](https://twitter.com/predictionio) on Twitter.


## Contributing

Read the [Contribute Code](http://predictionio.apache.org/community/contribute-code/) page.

You can also list your projects on the [Community Project
page](http://predictionio.apache.org//community/projects/).


## License

Apache PredictionIO is under [Apache 2
license](http://www.apache.org/licenses/LICENSE-2.0.html).
",2023-07-07 18:45:35+00:00
prefect,prefect,PrefectHQ/prefect,"Prefect is a workflow orchestration tool empowering developers to build, observe, and react to data pipelines",https://prefect.io,False,12278,2023-07-07 18:20:08+00:00,2018-06-29 21:59:26+00:00,1274,155,170,195,2.10.19,2023-07-06 18:10:20+00:00,Apache License 2.0,14791,2.10.19,197,2023-07-06 18:10:20+00:00,2023-07-07 18:42:08+00:00,2023-07-07 18:40:16+00:00,"<p align=""center""><img src=""https://images.ctfassets.net/gm98wzqotmnx/6rIpC9ZCAewsRGLwOw5BRe/bb17e1ef62f60d1ec32c1ae69487704c/prefect-2-logo-dark.png"" width=1000></p>

<p align=""center"">
    <a href=""https://pypi.python.org/pypi/prefect/"" alt=""PyPI version"">
        <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/prefect?color=0052FF&labelColor=090422""></a>
    <a href=""https://github.com/prefecthq/prefect/"" alt=""Stars"">
        <img src=""https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://pepy.tech/badge/prefect/"" alt=""Downloads"">
        <img src=""https://img.shields.io/pypi/dm/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://github.com/prefecthq/prefect/pulse"" alt=""Activity"">
        <img src=""https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <br>
    <a href=""https://prefect-community.slack.com"" alt=""Slack"">
        <img src=""https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&labelColor=090422&logo=slack"" /></a>
    <a href=""https://discourse.prefect.io/"" alt=""Discourse"">
        <img src=""https://img.shields.io/badge/discourse-browse_forum-red.svg?color=0052FF&labelColor=090422&logo=discourse"" /></a>
    <a href=""https://www.youtube.com/c/PrefectIO/"" alt=""YouTube"">
        <img src=""https://img.shields.io/badge/youtube-watch_videos-red.svg?color=0052FF&labelColor=090422&logo=youtube"" /></a>
</p>

# Prefect

Prefect is an orchestrator for data-intensive workflows. It's the simplest way to transform any Python function into a unit of work that can be observed and orchestrated. With Prefect, you can build resilient, dynamic workflows that react to the world around them and recover from unexpected changes. With just a few decorators, Prefect supercharges your code with features like automatic retries, distributed execution, scheduling, caching, and much more. Every activity is tracked and can be monitored with the Prefect server or Prefect Cloud dashboard.

```python
from prefect import flow, task
from typing import List
import httpx


@task(retries=3)
def get_stars(repo: str):
    url = f""https://api.github.com/repos/{repo}""
    count = httpx.get(url).json()[""stargazers_count""]
    print(f""{repo} has {count} stars!"")


@flow(name=""GitHub Stars"")
def github_stars(repos: List[str]):
    for repo in repos:
        get_stars(repo)


# run the flow!
github_stars([""PrefectHQ/Prefect""])
```

After running some flows, fire up the Prefect UI to see what happened:

```bash
prefect server start
```

![](/docs/img/ui/flow-run-page.png)

From here, you can continue to use Prefect interactively or [deploy your flows](https://docs.prefect.io/concepts/deployments) to remote environments, running on a scheduled or event-driven basis.

## Getting Started

Prefect requires Python 3.8 or later. To [install Prefect](https://docs.prefect.io/getting-started/installation/), run the following command in a shell or terminal session:

```bash
pip install prefect
```

Start by then exploring the [core concepts of Prefect workflows](https://docs.prefect.io/concepts/), then follow one of our [friendly tutorials](https://docs.prefect.io/tutorials/first-steps) to learn by example.

## Join the community

Prefect is made possible by the fastest growing community of thousands of friendly data engineers. Join us in building a new kind of workflow system. The [Prefect Slack community](https://prefect.io/slack) is a fantastic place to learn more about Prefect, ask questions, or get help with workflow design. The [Prefect Discourse](https://discourse.prefect.io/) is a community-driven knowledge base to find answers to your Prefect-related questions. All community forums, including code contributions, issue discussions, and slack messages are subject to our [Code of Conduct](https://discourse.prefect.io/faq).

## Contribute

See our [documentation on contributing to Prefect](https://docs.prefect.io/contributing/overview/).

Thanks for being part of the mission to build a new kind of workflow system and, of course, **happy engineering!**
",2023-07-07 18:45:40+00:00
presto,presto,prestodb/presto,The official home of the Presto distributed SQL query engine for big data,http://prestodb.github.io,False,14858,2023-07-07 13:56:57+00:00,2012-08-09 01:03:37+00:00,5054,875,331,0,,,Apache License 2.0,21327,list,350,2017-03-27 23:39:50+00:00,2023-07-07 18:42:19+00:00,2023-07-07 17:48:40+00:00,"# Presto

Presto is a distributed SQL query engine for big data.

See the [User Manual](https://prestodb.github.io/docs/current/) for deployment instructions and end user documentation.

## Requirements

* Mac OS X or Linux
* Java 8 Update 151 or higher (8u151+), 64-bit. Both Oracle JDK and OpenJDK are supported.
* Maven 3.3.9+ (for building)
* Python 2.4+ (for running with the launcher script)

## Building Presto

Presto is a standard Maven project. Simply run the following command from the project root directory:

    ./mvnw clean install

On the first build, Maven will download all the dependencies from the internet and cache them in the local repository (`~/.m2/repository`), which can take a considerable amount of time. Subsequent builds will be faster.

Presto has a comprehensive set of unit tests that can take several minutes to run. You can disable the tests when building:

    ./mvnw clean install -DskipTests


## Presto native and Velox

[Presto native](https://github.com/prestodb/presto/tree/master/presto-native-execution) is a C++ rewrite of Presto worker. [Presto native](https://github.com/prestodb/presto/tree/master/presto-native-execution) uses [Velox](https://github.com/facebookincubator/velox) as its primary engine to run presto workloads.

[Velox](https://github.com/facebookincubator/velox) is a C++ database library which provides reusable, extensible, and high-performance data processing components.

Check out [building instructions](https://github.com/prestodb/presto/tree/master/presto-native-execution#building) to get started. 


## Running Presto in your IDE

### Overview

After building Presto for the first time, you can load the project into your IDE and run the server. We recommend using [IntelliJ IDEA](http://www.jetbrains.com/idea/). Because Presto is a standard Maven project, you can import it into your IDE using the root `pom.xml` file. In IntelliJ, choose Open Project from the Quick Start box or choose Open from the File menu and select the root `pom.xml` file.

After opening the project in IntelliJ, double check that the Java SDK is properly configured for the project:

* Open the File menu and select Project Structure
* In the SDKs section, ensure that a 1.8 JDK is selected (create one if none exist)
* In the Project section, ensure the Project language level is set to 8.0 as Presto makes use of several Java 8 language features

Presto comes with sample configuration that should work out-of-the-box for development. Use the following options to create a run configuration:

* Main Class: `com.facebook.presto.server.PrestoServer`
* VM Options: `-ea -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -Xmx2G -Dconfig=etc/config.properties -Dlog.levels-file=etc/log.properties`
* Working directory: `$MODULE_WORKING_DIR$` or `$MODULE_DIR$`(Depends your version of IntelliJ)
* Use classpath of module: `presto-main`

The working directory should be the `presto-main` subdirectory. In IntelliJ, using `$MODULE_DIR$` accomplishes this automatically.

Additionally, the Hive plugin must be configured with location of your Hive metastore Thrift service. Add the following to the list of VM options, replacing `localhost:9083` with the correct host and port (or use the below value if you do not have a Hive metastore):

    -Dhive.metastore.uri=thrift://localhost:9083

### Using SOCKS for Hive or HDFS

If your Hive metastore or HDFS cluster is not directly accessible to your local machine, you can use SSH port forwarding to access it. Setup a dynamic SOCKS proxy with SSH listening on local port 1080:

    ssh -v -N -D 1080 server

Then add the following to the list of VM options:

    -Dhive.metastore.thrift.client.socks-proxy=localhost:1080

### Running the CLI

Start the CLI to connect to the server and run SQL queries:

    presto-cli/target/presto-cli-*-executable.jar

Run a query to see the nodes in the cluster:

    SELECT * FROM system.runtime.nodes;

In the sample configuration, the Hive connector is mounted in the `hive` catalog, so you can run the following queries to show the tables in the Hive database `default`:

    SHOW TABLES FROM hive.default;

## Development

See [Contributions](CONTRIBUTING.md) for guidelines around making new contributions and reviewing them.

## Building the Documentation

To learn how to build the docs, see the [docs README](presto-docs/README.md).

## Building the Web UI

The Presto Web UI is composed of several React components and is written in JSX and ES6. This source code is compiled and packaged into browser-compatible JavaScript, which is then checked in to the Presto source code (in the `dist` folder). You must have [Node.js](https://nodejs.org/en/download/) and [Yarn](https://yarnpkg.com/en/) installed to execute these commands. To update this folder after making changes, simply run:

    yarn --cwd presto-main/src/main/resources/webapp/src install

If no JavaScript dependencies have changed (i.e., no changes to `package.json`), it is faster to run:

    yarn --cwd presto-main/src/main/resources/webapp/src run package

To simplify iteration, you can also run in `watch` mode, which automatically re-compiles when changes to source files are detected:

    yarn --cwd presto-main/src/main/resources/webapp/src run watch

To iterate quickly, simply re-build the project in IntelliJ after packaging is complete. Project resources will be hot-reloaded and changes are reflected on browser refresh.

## Release Notes

When authoring a pull request, the PR description should include its relevant release notes.
Follow [Release Notes Guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) when authoring release notes. 
",2023-07-07 18:45:44+00:00
produce,produce,texttheater/produce,Replacement for Make geared towards processing data rather than compiling code,,False,17,2022-03-03 12:07:52+00:00,2013-06-26 19:52:16+00:00,4,4,2,14,v0.7.0,2022-09-02 09:08:56+00:00,MIT License,442,v0.7.0,15,2022-09-02 09:08:56+00:00,,2022-09-03 03:55:21+00:00,"![Produce logo](https://raw.githubusercontent.com/texttheater/produce/master/img/logo/Produce_Logo_300.png)
==============================================

Produce is an incremental build system for the command line, like Make or redo,
but different: it is scriptable in Python and it supports multiple variable
parts in file names. This makes it ideal for doing things beyond compiling
code, like setting up replicable scientific experiments.

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents** 

- [Requirements](#requirements)
- [Installing Produce](#installing-produce)
- [Usage](#usage)
- [Motivation](#motivation)
- [Build automation: basic requirements](#build-automation-basic-requirements)
- [Make syntax vs. Produce syntax and a tour of the basic features](#make-syntax-vs-produce-syntax-and-a-tour-of-the-basic-features)
  - [Rules, expansions, escaping and comments](#rules-expansions-escaping-and-comments)
  - [Named and unnamed dependencies](#named-and-unnamed-dependencies)
  - [Multiple wildcards, regular expressions and matching conditions](#multiple-wildcards-regular-expressions-and-matching-conditions)
  - [Special targets vs. special attributes](#special-targets-vs-special-attributes)
  - [Python expressions and global variables](#python-expressions-and-global-variables)
- [Running Produce](#running-produce)
  - [Status and debugging messages](#status-and-debugging-messages)
  - [Error handling and aborting](#error-handling-and-aborting)
  - [How targets are matched against rules](#how-targets-are-matched-against-rules)
- [Advanced usage](#advanced-usage)
  - [Whitespace and indentation in values](#whitespace-and-indentation-in-values)
  - [The prelude](#the-prelude)
  - [`shell`: choosing the recipe interpreter](#shell-choosing-the-recipe-interpreter)
  - [Running jobs in parallel](#running-jobs-in-parallel)
  - [Dependency files](#dependency-files)
  - [Rules with multiple outputs](#rules-with-multiple-outputs)
    - [“Sideways” dependencies](#sideways-dependencies)
  - [Producing the outputs for all inputs](#producing-the-outputs-for-all-inputs)
- [All special attributes at a glance](#all-special-attributes-at-a-glance)
  - [In rules](#in-rules)
  - [In the global section](#in-the-global-section)
- [Getting in touch](#getting-in-touch)
- [Acknowledgments](#acknowledgments)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

Requirements
------------

* A Unix-like operating system such as Linux or Mac OS X. Windows Subsystem for
  Linux may also work.
* Python 3.4 or higher
* Git (for downloading Produce)

Installing Produce
------------------

Install the latest release using pip:

    pip3 install produce

Or get the development version by running the following command in a convenient
location:

    git clone https://github.com/texttheater/produce

This will create a directory called `produce`. To update to the latest version
of Produce later, you can just go into that directory and run:

    git pull

The `produce` directory contains an executable Python script also called
`produce`. This is all you need to run Produce. Just make sure it is in your
`PATH`, e.g. by copying it to `/usr/local/bin` or by linking to it from your
`$HOME/bin` directory.

Usage
-----

When invoked, Produce will first look for a file called `produce.ini` in the
current working directory. Its format is documented in this document. If you
want a quick start, have a look at
[an example project](https://github.com/texttheater/produce/tree/master/doc/samples/tokenization).

You may also have a look at the
[PyGrunn 2014 slides](https://texttheater.github.io/produce-pygrunn2014)
for a quick introduction.

Motivation
----------

Produce is a build automation tool. Build automation is useful whenever you
have one or several input files from which one or several output files are
generated automatically – possibly in multiple steps, so that you have
intermediate files.

The classic case for this is compiling C programs, where a simple project might
look like this:

![example dependency chart for compiling a C program](img/compiling.png)

But build automation is also useful in other areas, such as science. For
example, in the [Groningen Meaning Bank](http://gmb.let.rug.nl/) project, a
Natural Language Processing pipeline is combined with corrections from human
experts to build a collection of texts with linguistic annotations in a
bootstraping fashion.

In the following simplified setup, processing starts with a text file
(`en.txt`) which is first part-of-speech-tagged (`en.pos`), then analyzed
syntactically (`en.syn`) by a parser and finally analyzed semantically
(`en.sem`). Each step is first carried out automatically by an NLP tool
(`*.auto`) but then corrections by human annotators (`*.corr`) are applied
to build the main version of the file which then serves as input to further
processing. Every time a new human correction is added, parts of the
pipeline must be re-run:

![example dependency chart for running an NLP pipeline](img/pipeline.png)

Or take running machine learning experiments: we have a collection of labeled
data, split into a training portion and testing portions. We have various
feature sets and want to know which one produces the best model. So we train a
separate model based on each feature set and on the training data, and generate
corresponding labeled outputs and evaluation reports based on the development
test data:

![example dependency chart for running machine learning experiments](img/ml.png)

A [number](http://kbroman.github.io/minimal_make/)
[of](http://bost.ocks.org/mike/make/) [articles](http://zmjones.com/make/)
point out that build automation is an invaluable help in setting up experiments
in a self-documenting manner, so that they can still be understood, replicated
and modified months or years later, by you, your colleagues or other
researchers. Many people use Make for this purpose, and so did I, for a while.
I specifically liked:

* *The declarative notation.* Every step of the workflow is expressed as a
  _rule_, listing the _target_, its direct dependencies and the command to run
  (the _recipe_). Together with a good file naming scheme, this almost
  eliminates the need for documentation.
* *The Unix philosophy.* Make is, at its core, a thin wrapper around shell
  scripts. For orchestrating the steps, you use Make, and for executing them,
  you use the full power of shell scripts. Each tool does one thing, and does
  it well. This reliance on shell scripts is something that sets Make apart
  from specialized build tools such as Ant or A-A-P.
* *The wide availability.* Make is installed by default on almost every Unix
  system, making it ideal for disseminating and exchanging code because the
  Makefile format is widely known and can be run everywhere.

So, if Make has so many advantages, why yet another build automation tool?
There are two reasons:

* *Make’s syntax.* Although the basic syntax is extremely simple, as soon as
  you want to go a _little bit_ beyond what it offers and use more advanced
  features, things get quite arcane very quickly.
* *Wildcards are quite limited.* If you want to match on the name of a specific
  target to generate its dependencies dynamically, you can only use one
  wildcard. If your names are a bit more complex than that, you have to resort
  to black magic like Make’s built-in string manipulation functions that don’t
  compare favorably to languages like Python or even Perl, or rely on external
  tools. In either case, your Makefiles become extremely hard to read, bugs
  slip in easily and the simplicity afforded by the declarative paradigm is
  largely lost.

Produce is thus designed as a tool that copies Make’s virtues and improves a
great deal on its deficiencies by using a still simple, but much more powerful
syntax for mapping targets to dependencies. Only the core functionality of Make
is mimicked – advanced functions of Make such as built-in rules specific to
compiling C programs are not covered. Produce is general-purpose.

Produce is written in Python 3 and scriptable in Python 3. Whenever I write
Python below, I mean Python 3.

Build automation: basic requirements
------------------------------------

Let’s review the basic functionality we expect of a build automation tool:

* Allows you to run multiple steps of a workflow with a single command, in the
  right order.
* Notices when inputs have changed and runs exactly those steps again that are
  needed to bring the outputs up to speed, no more or less.

In addition, some build automation tools satisfy the following requirement
(Produce currently doesn’t):

* Intermediate files can be deleted without affecting up-to-dateness – if the
  outputs are newer than the inputs, the workflow will not be re-run.

Make syntax vs. Produce syntax and a tour of the basic features
---------------------------------------------------------------

When you run the `produce` command (usually followed by the targets you want
built), Produce will look for a file in the current directory, called
`produce.ini` by default. This is the “Producefile”. Let’s introduce
Producefile syntax by comparing it to Makefile syntax.

### Rules, expansions, escaping and comments

Here is a Makefile for a tiny C project:

    # Compile
    %.o : %.c
    	cc -c $<
    
    # Link
    % : %.o
    	cc -o $@ $<

And here is the corresponding `produce.ini`:

    # Compile
    [%{name}.o]
    dep.c = %{name}.c
    recipe = cc -c %{c}
    
    # Link
    [%{name}]
    dep.o = %{name}.o
    recipe = cc -o %{target} %{o}

Easy enough, right? Produce syntax is a dialect of the widely known INI syntax,
consisting of sections with headings in square brackets, followed by
attribute-value pairs separated by `=`. In Produce’s case, sections represent
_rules_, the section headings are _target patterns_ matching _targets_ to
build, and the attribute-value pairs specify the target’s direct dependencies
and the recipe to run it.

Dependencies are typically listed each as one attribute of the form `dep.name`
where `name` stands for a name you give to the dependency – e.g., its file
type. This way, you can refer to it in the recipe using an _expansion_.

Expansions have the form `%{...}`. In the target pattern, they are used as
wildcards. When the rule is invoked on a specific target, they match any string
and assign it to the variable name specified between the curly braces. In
attribute values, they are used like variables, expanding to the value
associated with the variable name. Besides target matching, values can also be
assigned to variable names by attribute-value pairs, as with e.g.
`dep.c = %{name}.c`. Here, `c` is the variable name; the `dep.` prefix just
tells Produce that this particular value is also a dependency.

If you need a literal percent sign in some attribute value, you need to escape
it as `%%`.

The `target` variable is automatically available when the rule is invoked,
containing the target matched by the target pattern.

Lines starting with `#` are for comments and ignored.

So far, so good – a readable syntax, I hope, but a bit more verbose than that
of Makefiles. What does this added verbosity buy us? We will see in the next
subsections.

### Named and unnamed dependencies

To see why naming dependencies is a good idea, consider the following Makefile
rule:

    out/%.pos : out/%.pos.auto out/%.pos.corr
    	./src/scripts/apply_corrections $< \
            --corrections out/$*.pos.corr > $@

This could be from the Natural Language Processing project we saw as the second
example above: the rule is for making the final `pos` file from the
automatically generated `pos.auto` file and the `pos.corr` file with manual
corrections, thus it has two direct dependencies, specified on the first line.
The recipe refers to the first dependency using the shorthand `$<`, but there
is no such shorthand for other dependencies. So we have to type out the second
dependency again in the recipe, taking care to replace the wildcard `%` with
the magic variable `$*`. This is ugly because it violates the golden principle
“Don’t repeat yourself!” If we write something twice in a Makefile, not only is
it more work to type, but also if we want to change it later, we have to change
it in two places, and there’s a good chance we’ll forget that.

Produce’s named dependencies avoid this problem: once specified, you can refer
to every dependency using its name. Here is the Produce rule corresponding to
the above Makefile rule:

    [out/%{name}.pos]
    dep.auto = %{name}.pos.auto
    dep.corr = %{name}.pos.corr
    recipe = ./src/scripts/apply_corrections %{auto} %{corr} > %{target}

Note that you don’t _have_ to name dependencies. Sometimes you don’t need to
refer back to them. Here is an example rule that compiles a LaTeX document:

    [%{name}.pdf]
    deps = %{name}.tex bibliography.bib
    recipe =
    	pdflatex %{name}
    	bibtex %{name}
    	pdflatex %{name}
    	pdflatex %{name}

The TeX tools are smart enough to fill in the file name extension if we just
give them the basename that we got by matching the target. In such cases, it
can be more convenient not to name the dependencies and list them all on one
line. This is what the `deps` attribute is for. It is parsed using Python’s
[`shlex.split`](https://docs.python.org/3/library/shlex.html?highlight=shlex#shlex.split)
function – consult the Python documentation for escaping rules and such. You
can also mix `dep.*` attributes and `deps` in one rule.

Note that, as in many INI dialects, attribute values (here: the recipe) can
span multiple lines as long as each line after the first is indented. See
[Whitespace and indentation in values](#whitespace-and-indentation-in-values)
below for details.

Note also that dependency lists can also be generated dynamically – see the
section on [dependency files](#dependency-files) below.

### Multiple wildcards, regular expressions and matching conditions

The ability to use more than one wildcard in target patterns is Produce’s
killer feature because not many other build automations tools offer it.
The only one I know of so far is [plmake](https://github.com/cmungall/plmake).
Rake and others do offer full regular expressions which are strictly more
powerful but not as easy to read. Don’t worry, Produce supports them too and
more, we will come to that. But first consider the following Produce rule,
which might stem from the third example project we saw in the introduction,
the machine learning one:

    [out/%{corpus}.%{portion}.%{fset}.labeled]
    dep.model = out/%{corpus}.train.%{fset}.model
    dep.input = out/%{corpus}.%{portion}.feat
    recipe = wapiti label -m %{model} %{input} > %{target}

Labeled output files here follow a certain naming convention: four parts,
separated by periods. The first one specifies the data collection (e.g. a
linguistic corpus), the second one the portion of the data that is
automatically labeled in this step (either the development portion or the test
portion), the third one specifies the feature set used and the fourth one is
the extension `labeled`. For each of the three first parts, we use a wildcard
to match it. We can then freely use these three wildcards to specify the
dependencies: the model we use for labelling depends on the corpus and on the
feature set but not on the portion to label: the portion used for training the
model is always the training portion. The input to labelling is a file
containing the data portion to label, together with the extracted features. We
assume that this file always contains all features we can extract even if we’re
not going to use them in a particular model, so this dependency does not depend
on the feature set.

A Makefile rule to achieve something similar would look something like this:

    .SECONDEXPANSION:
    out/%.labeled : out/$$(subst test,train,$$(subst dev,train,$$*)).model \
                    out/$$(basename $$*).feat
            wapiti label -m $< out/$(basename $*).feat > $@

If you are like me, this is orders of magnitude less readable than the Produce
version. Getting a Makefile rule like this to function properly will certainly
make you feel smart, but hopefully also feel miserable about the brain cycles
wasted getting your head around the bizarre syntax, the double dollars and the
second expansion.

A wildcard will match _anything_. If you need more control about which targets
are matched, you can use a
[Python regular expression](https://docs.python.org/3/library/re.html?highlight=re#module-re)
between slashes as the target pattern. For example, if we want to make sure
that our rule only matches targets where the second part of the filename is
either `dev` or `test`, we could do it like this:

    [/out/(?P<corpus>.*)\.(?P<portion>dev|test)\.(?P<fset>.*)\.labeled/]
    dep.model = out/%{corpus}.train.%{fset}.model
    dep.input = out/%{corpus}.%{portion}.feat
    recipe = wapiti label -m %{model} %{input} > %{target}

The regular expression in this rule’s header is almost precisely what the above
header with three wildcards is translated to by Produce internally, with the
difference that the subexpression matching the second part is now `dev|test`
rather than `.*`. We are using a little-known feature of regular expressions
here, namely the `(?P<...>)` syntax that allows us to assign names to
subexpressions by which you can refer to the matched part later.

Note the slashes at the beginning and end are just a signal to Produce to
interpret what is in-between as a regular expressions. You do not have to
escape slashes within your regular expression.

While regular expressions are powerful, they make your Producefile less
readable. A better way to write the above rule is by sticking to ordinary
wildcards and using a separate _matching condition_ to check for `dev|test`:

    [out/%{corpus}.%{portion}.%{fset}.labeled]
    cond = %{portion in ('dev', 'test')}
    dep.model = out/%{corpus}.train.%{fset}.model
    dep.input = out/%{corpus}.%{portion}.feat
    recipe = wapiti label -m %{model} %{input} > %{target}

A matching condition is specified as the `cond` attribute. We can use any
Python expression. It is evaluated only if the target pattern matches the
requested target. If it evaluates to a “truthy” value, the rule matches and
the recipe is executed. If it evaluates to a “falsy” value, the rule does
not match, and Produce moves on, trying to match the next rule in the
Producefile.

Note that the Python expression is given as an expansion. At this point we
should explain a few fine points:

1. Whenever we used expansions so far, the variable names inside were actually
   Python expressions, albeit of a simple kind: single variable names. But as
   we see now, we can use arbitrary Python expressions. Expansions used as
   wildcards in the target pattern are an exception, of course: they can only
   consist of a single variable name.
2. The variables we use in rules are actually Python variables.
3. Attribute values are always strings, so if a Python expression is used to
   generate (part of) an attribute value, not the value of the expression
   itself is used but whatever its `__str__` method returns. Thus, in the
   above rule, the value of the `cond` variable is not `True` or `False`, but
   `'True'` or `'False'`. In order to interpret the value as a Boolean, Produce
   calls
   [ast.literal\_eval](https://docs.python.org/3/library/ast.html?highlight=literal_eval#ast.literal_eval)
   on the string. So if the string contains anything other than a literal
   Python expression, this is an error.

As an exception to what we said about `__str__`, if an expansion evaluates to
something that is not a string but has an `__iter__` method, it will be treated
as a sequence and rendered as a white-space separated list, the elements
properly shell-quoted and escaped. Note also that parentheses are automatically
added around an expansion so it is very convenient to use generator expressions
for expansions. All of this is illustrated in the following rule:

    [Whole.txt]
    deps = %{'Part {}.txt'.format(i) for i in range(4)}
    recipe = cat %{deps} > %{target}

### Special targets vs. special attributes

Besides not naming all dependencies, there is another reason why Make’s syntax
is too simple for its own good. When some rule needs to have a special
property, Make usually requires a “special target” that syntactically looks
like a target but is actually a declaration and has no obvious visual
connection to the rule(s) it applies to. We have already seen an example of the
dreaded `.SECONDEXPANSION`. Another common special target is `.PHONY`, marking
targets that are just jobs to be run, without producing an output file. For
example:

    .PHONY: clean
    clean:
    	rm *.o temp

It would be easier and more logical if the “phoniness” was declared as part of
the rule rather than some external declaration. This is was Produce does. The
Produce equivalent of declaring targets phony is to set the `type` attribute of
their rule to `task` (the default is `file`). With this the rule above is
written as follows:

    [vacuum]
    type = task
    recipe = rm *.o temp

Note that since it is ungrammatical to “produce a clean”, I invented a naming
convention according to which the task that cleans up your project directory is
called `vacuum` because it produces a vacuum. It’s silly, I know.

For other special attributes besides `task`, see [All special attributes at a
glance](#all-special-attributes-at-a-glance) below.

### Python expressions and global variables

As we have already seen, Produce’s expansions can contain arbitrary Python
expressions. This is not only useful for specifying Boolean matching
conditions, but also for string manipulation, in particular for playing with
dependencies. This is a pain in Make, because Make implements its own string
manipulation language which from today’s perspective (since we have Python)
not only reinvents the wheel, but reinvents it poorly, with a rather dangerous
syntax. Consider the following (contrived) example from the GNU Make manual
where you have a list of dependencies in a global variable and filter them to
retain only those ending in `.c` or `.s`:

    sources := foo.c bar.c baz.s ugh.h
    foo: $(sources)
    	cc $(filter %.c %.s,$(sources)) -o foo

With Produce, we can just hand the string manipulation to Python, a language
we already know and (hopefully) like:

    []
    sources = foo.c bar.c baz.s ugh.h

    [foo]
    deps = %{sources}
    recipe = cc %{f for f in sources.split() \
    		if f.endswith('.c') or f.endswith('.s')}

This example also introduces the _global section_, a section headed by `[]`,
thus named with the empty string. The attributes here define global variables
accessible from all rules. The global section may only appear once and only at
the beginning of a Producefile.

Running Produce
---------------

Produce is invoked from the command line by the command `produce`, usually
followed by the target(s) to produce. These can be omitted if the Producefile
specifies one or more default targets. By default, Produce will look for
`produce.ini` in the current working directory and complain if it does not
exist.

A number of options can be used to control Produce’s behavior, as listed in its
help message:

usage: produce [-h] [-B | -b] [-d] [-f FILE] [-j JOBS] [-n] [-u PATTERN]
               [target ...]

positional arguments:
  target                The target(s) to produce - if omitted, default target
                        from Producefile is used

options:
  -h, --help            show this help message and exit
  -B, --always-build    Unconditionally build all specified targets and their
                        dependencies
  -b, --always-build-specified
                        Unconditionally build all specified targets, but treat
                        their dependencies normally (only build if out of
                        date)
  -d, --debug           Print debugging information. Give this option multiple
                        times for more information.
  -f FILE, --file FILE  Use FILE as a Producefile
  -j JOBS, --jobs JOBS  Specifies the number of jobs (recipes) to run
                        simultaneously
  -n, --dry-run         Print status messages, but do not run recipes
  -u PATTERN, --pretend-up-to-date PATTERN
                        Do not rebuild targets matching PATTERN or their
                        dependencies (unless the latter are also depended on
                        by other targets) even if out of date, but make sure
                        that future invocations of Produce will still treat
                        them as out of date by increasing the modification
                        times of their changed dependencies as necessary.
                        PATTERN can be a Produce pattern or a regular
                        expression enclosed in forward slashes, as in rules.

### Status and debugging messages

When it starts (re)building a target, Produce will tell you so with a status
message in green where the target is indented according to how deep in the
dependency graph it is. On successful completion of a target, a similar message
with `complete` is printed. If an error occurs while a target is being built,
Produce instead prints an `incomplete` message in red. The latter indicates
controlled shutdown: the recipe has been killed and incomplete outputs have
been renamed (see below). If you see a `(re)building` message but no
`(in)complete` message for some target, something went really wrong – this
should never happen. In that case, better check for yourself if any incomplete
outputs are still hanging around.

Giving the `-d`/`--debug` option one, two or three times will cause Produce to
additionally flood your terminal with a few, some more or lots of messages that
may be helpful for debugging.

### Error handling and aborting

When a recipe fails, i.e. its interpreter returns an exit status other than 0,
the corresponding target file (if any) may already have been created or
touched, potentially leading the next invocation of Produce to believe that it
is up to date, even though it probably doesn’t have the correct contents. Such
inconsistencies can lead to users tearing their hair out. In order to avoid
this, Produce will, when a recipe fails, make sure that the target file does
not stay there. It could just delete it, but that might be unwise because the
user might want to inspect the output file of the erroneous recipe for
debugging. So, Produce renames the target file by appending a `~` to the
filename (a common naming convention for short-lived “backups”).

If multiple recipes are running in parallel and one fails, Produce will kill
all of them, do the renaming and abort immediately.

The same is true if Produce receives an interrupt signal. So you can safely
abort a production process in your terminal by pressing `Ctrl+C`.

### How targets are matched against rules

When producing a target, either because asked to by the user or because the
target is required by another one, Produce will always work through the
Producefile from top to bottom and use the first rule that matches the target.
A rule matches a target if both the target pattern matches and the matching
condition (if any) subsequently evaluates to true.

Note that unlike most INI dialects, Produce allows for multiple sections with
the same heading. It makes sense to have the same target pattern multiple times
when there are matching conditions to make subdistinctions.

If no rule matches a target, Produce aborts with an error message.

Advanced usage
--------------

### Whitespace and indentation in values

An attribute value can span multiple lines as long as each line after the first
is indented with some whitespace. The recommended indentation is either one tab
or four spaces. If you make use of this, it is recommended to leave the first
line (after the attribute name and the `=`) blank so all lines of the value are
consistently aligned.

The _second_ line of a value (i.e. the first indented one) determines the kind
and amount of whitespace expected to start each subsequent line. This
whitespace will _not_ be part of the attribute value. _Additional_ whitespace
after the initial amount is, however, preserved. This is important e.g. for
Python code and the reason why Produce is no longer using Python’s
`configparser` module.

All whitespace at the very beginning and at the very end of an attribute value
will be stripped away.

For example, in the following rule, the recipe spans two lines:

    [paper.pdf]
    dep.tex = paper.tex
    dep.bib = paper.bib
    recipe =
        pdflatex paper
        pdflatex paper

### The prelude

If you use Python expressions in your recipes, you will often need to import
Python modules or define functions to use in these expressions. You can do this
by putting the imports, function definitions and other Python code into the
special `prelude` attribute in the [global
section](#python-expressions-and-global-variables). For example, put this at
the beginning of your Producefile to import the `errno`, `glob` and `os`
modules and define a helper function for creating directories.

    []
    prelude =
        import errno
        import glob
        import os

        def makedirs(path):
            try:
                os.makedirs(path)
            except OSError, error:
                if error.errno != errno.EEXIST:
                    raise error

### `shell`: choosing the recipe interpreter

By default, recipes are (after doing expansions) handed to the `bash` command
for execution. If you would rather write your recipe in `zsh`, `perl`, `python`
or any other language, that’s no problem. Just specify the interpreter in the
`shell` attribute of the rule.

### Running jobs in parallel

Use the `-j JOBS` command line option to specify the number of jobs Produce
runs in parallel. By default, Produce reserves one job slot for each recipe.
For recipes that run multiple parallel jobs themselves, it is recommended to
specify the number of jobs via the `jobs` attribute. Produce will then reserve
that many job slots for this recipe (but no more than `JOBS`).

Here is an example where the target `b` is created by a recipe that runs in
parallel:

    [a]
    deps = b c d
    recipe = touch %{target}

    [b]
    dep.input = input.txt
    dep.my_script = ./my_script.sh
    jobs = 8
    recipe = parallel --gnu -n %{jobs} -k %{my_script} %{input} > %{target}

    [c]
    dep.my_script = ./my_script.sh
    recipe = %{my_script} c > %{target}

    [d]
    dep.my_script = ./my_script.sh
    recipe = %{my_script} d > %{target}

Running `produce -j 8 a` will run up to 8 jobs in parallel. In this example,
the recipes for `c` and `d` may run in parallel. The recipe for `b` will not
run in parallel with any other recipe because it uses all 8 job slots.

### Dependency files

Sometimes the question which other files a file depends on is more complex and
may change frequently over the lifetime of a project, e.g. in the cases of
source files that import other header files, modules etc. In such cases, it
would be nice to have the dependencies automatically listed by a script.
Produce supports this via the `depfile` attribute in rules: here, you can
specify the name of a _dependency file_, a text file that contains
dependencies, one per line. Produce will read them and add them to the list of
dependencies for the matched target. Also, Produce will try to produce the
dependency file (i.e. make it up to date) _prior_ to reading it. So you can
write another rule that tells Produce how to generate each dependency file, and
the rest is automatic.

For example, the following rule might be used to generate a dependency file
listing the source file and header files required for compiling a C object.
This example uses `.d` as the extension for dependency files. It runs `cc -MM`
to use the C compiler’s dependency discovery feature and then some shell magic
to convert the output from a Makefile rule into a simple dependency list:

    [%{name}.d]
    dep.c = %{name}.c
    recipe =
        cc -MM -I. %{name} | sed -e 's/.*: //' | sed -e 's/^ *//' | \
        perl -pe 's/ (\\\n)?/\n/g' > %{target}

The following rule could then be used to create the actual object file. The
`depfile` attribute makes sure that whenever an included header file changes,
the object file will be rebuilt:

    [%{name}.o]
    dep.src = %{name}.c
    depfile = %{name}.d
    recipe =
        cc -c -o %{target} %{src}

Note that the `.c` file will end up in the dependency list twice, once from
`dep.src` and once from the dependency file. This does not matter, Produce is
smart enough not to do the same thing twice.

Warning: dependency files are made up to date even in dry-run mode!

### Rules with multiple outputs

Sometimes you have a command that creates multiple files at once because their
creation is inherently linked to the same process – it wouldn’t make sense to
try and create them in neatly separated steps. Splitting a file up into
multiple chunks is such a case:

    split -n 4 data.txt

This command creates four files called `xaa`, `xab`, `xac` and `xad`. It gets
complicated when these output files individually are dependencies of further
targets, as in this example:

    [split_and_zip]
    type = task
    deps = xaa.zip xab.zip xac.zip xad.zip

    [%{name}.zip]
    dep.file = %{name}
    recipe = zip %{target} %{file}

    [%{chunk}]
    dep.txt = data.txt
    recipe = split -n 4 %{txt}

If we run the task `split_and_zip`, it will try to create its (indirect)
dependencies `xaa`, `xab`, `xac` and `xad` independently of each other. Each
time, the last rule will match, and each time, the exact same recipe will be
executed. This is unncecessary work, one time would be sufficient because it
creates all four files in each case. Worse, if we run Produce in parallel,
multiple instances of the recipe may run in parallel and corrupt the data.

The solution is to explicitly declare which files a rule produces, other than
the target. The `outputs` attribute serves this purpose. With it, the last rule
is rewritten as follows:

    [%{chunk}]
    outputs = xaa xab xac xad
    dep.txt = data.txt
    recipe = split -n 4 %{txt}

Additionally, it is good style to add a matching condition to prevent that the
rule accidentally matches something that is not its output:

    [%{chunk}]
    outputs = xaa xab xac xad
    cond = %{target in outputs.split()}
    dep.txt = data.txt
    recipe = split -n 4 %{txt}

Instead of a single `outputs` attribute, separate attributes with the `out.`
prefix can be used, and both styles can also be mixed, similar to
`dep.`/`deps`. Here is an example of a rule using the `out.` style to declare
that while producing a `.pdf` file it will also produce an `.aux` file:

    [%{name}.pdf]
    dep.tex = %{name}.tex
    out.aux = %{name}.aux
    recipe =
        pdflatex %{tex}

#### “Sideways” dependencies

Suppose there is a target A that has some additional output file B. What if a
target C wants to declare a dependency on B? For this to work, there must be a
rule matching B. B, of course, is produced when A is produced. So, effectively,
in order to produce B, A must be produced. We can express this as a dependency:
B depends on A. You can write a rule that will tell Produce to produce A when B
is requested:

    [B]
    dep.a = A

(TODO: What if A is up to date but B does not exist?)

Such a rule only serves to “guide” Produce from B to A. It cannot contain its
own recipe. This would not make the sense as it is the rule for A that creates
B. If you included a recipe, Produce would complain about a cyclic dependency.

Here is a more concrete example: the rule for `paper.pdf` produces an
additional output `paper.aux`. Another rule, for `paper.info`, depends on
`paper.aux`. In order for Produce to be able to satisfy this dependency,
`paper.aux` is declared as depending on `paper.pdf`.

    [paper.info]
    dep.aux = paper.aux
    recipe = cat %{aux} | ./my_tool > %{target}

    [paper.aux]
    dep.pdf = paper.pdf

    [paper.pdf]
    dep.tex = paper.tex
    outputs = paper.aux
    recipe =
        pdflatex paper

There is one final problem here: after running the recipe for `paper.pdf`, the
modification time of `paper.pdf` may well be greater than that of `paper.aux`.
Since we declared `paper.aux` dependent on `paper.pdf`, this means that
`paper.aux` appears as out of date to Produce even though we just produced it.
A simple and effective way to prevent this is to include `touch %{outputs}` as 
the last line of any rule with multiple outputs. The last rule above thus
becomes:

    [paper.pdf]
    dep.tex = paper.tex
    outputs = paper.aux
    recipe =
        pdflatex paper
        touch %{outputs}

### Producing the outputs for all inputs

Suppose you have a number of input files (say `inputs/input001.txt` to
`inputs/input100.txt`). Each input can be processed to yield an output file
(say `models/model001` to `models/model100`) – for example, by the following
rule:

    [models/model%{num}]
    dep.input = inputs/input%{num}.txt
    dep.train = bin/train
    recipe = ./%{train} %{input} %{target}

Now you would like to automatically produce the model for every input that is
there. You can do this by writing a _task_, i.e., a rule for a target that is
not a file but is just invoked. The task for the example might look like this:

    [all_models]
    type = task
    deps = %{'models/{}'.format(i.replace('input', 'model').replace('.txt, \
             '') for i in os.listdir('inputs')}

This task does not need a recipe because all it does is pull in all the models
through its dependencies. The dependencies are specified through an arbitrary
Python expression, in this case it looks at the inputs directory and returns
the names of the models corresponding to each input. It uses the `os` module,
which needs to be imported. So let’s add a global section with a prelude to do
this. The whole Producefile then looks like this:

    []
    prelude =
        import os

    [models/model%{num}]
    dep.input = inputs/input%{num}.txt
    dep.train = bin/train
    recipe = ./%{train} %{input} %{target}

    [all_models]
    type = task
    deps = %{'models/{}'.format(i.replace('input', 'model').replace('.txt, \
             '') for i in os.listdir('inputs')}

And to produce all models, all you need to do is tell Produce to produce the
`all_models` task:

    $ produce all_models

## All special attributes at a glance

For your reference, here are all the rule attributes that currently have a
special meaning to Produce:

### In rules

<dl>
    <dt><code>target</code></dt>
    <dd>When a rule matches a target, this variable is always set to that
    target, mainly so you can refer to it in the recipe. It is illegal to set
    the <code>target</code> attribute yourself. Also see
    <a href=""#rules-expansions-escaping-and-comments"">Rules, expansions, escaping and comments</a>.</dd>
    <dt><code>cond</code></dt>
    <dd>Allows to specify a _matching condition_ in addition to the target
    pattern. Typically it is given as a single expansion with a boolean Python
    expression. It is expanded immediately after a target matches the rule. The
    resulting string must be a Python literal. If “truthy”, the rule matches
    and its expansion/execution continues. If “falsy”, the rule does not match
    the target and Produce proceeds with the next rule, trying to match the
    target. Also see <a href=""#multiple-wildcards-regular-expressions-and-matching-conditions"">Multiple wildcards, regular expressions and matching conditions</a>.</dd>
    <dt><code>dep.*</code></dt>
    <dd>The asterisk stands for a name chosen by you, which is the actual name
    of the variable the attribute value will be assigned to. The <code>dep.</code> prefix,
    not part of the variable name, tells Produce that this is a dependency,
    i.e. that the target given by the value must be made up to date before the
    recipe of this rule can be run. Also see
    <a href=""#named-and-unnamed-dependencies"">Named an unnamed depenencies</a>.</dd>
    <dt><code>deps</code></dt>
    <dd>Like <code>dep.*</code>, but allows for specifying multiple unnamed dependencies
    in one attribute value. The format is roughly a space-separated list. For
    details, see
    <a href=""https://docs.python.org/3/library/shlex.html?highlight=shlex#shlex.split""><code>shlex.split</code></a>.
    Also see <a href=""#named-and-unnamed-dependencies"">Named an unnamed depenencies</a>.</dd>
    <dt><code>depfile</code></dt>
    <dd>Another way to specify (additional) dependencies: the name of a file
    from which dependencies are read, one per line. Additionally, Produce will
    try to make that file up to date prior to reading it. Also see
    <a href=""#dependency-files"">Dependency files</a>.</dd>
    <dt><code>type</code></dt>
    <dd>Is either <code>file</code> (default) or <code>task</code>. If <code>file</code>, the target is supposed
    to be a file that the recipe creates/updates if it runs successfully. If
    <code>task</code>, the target is an arbitrary name given to some task that the recipe
    executes. Crucially, task-type targets are always assumed to be out of
    date, regardless of the possible existence and age of a file with the same
    name. Also see
    <a href=""#special-targets-vs-special-attributes"">Special targets vs. special attributes</a></dd>
    <dt><code>recipe</code></dt>
    <dd>The command(s) to run to build the target, typically a single shell
    command or a short shell script. Unlike Make, each line is not run in
    isolation, but the whole script is passed to the interpreter as a whole,
    after doing expansions. This way, you can e.g. define a shell variable
    on one line and use it on the next. Also see
    <a href=""#rules-expansions-escaping-and-comments"">Rules, expansions, escaping and comments</a>.</dd>
    <dt><code>shell</code></dt>
    <dd>See <a href=""#shell-choosing-the-recipe-interpreter""><code>shell</code>: choosing the recipe interpreter</a></dd>
    <dt><code>out.*</code></dt>
    <dd>See <a href=""#rules-with-multiple-outputs"">Rules with multiple outputs</a></dd>
    <dt><code>outputs</code></dt>
    <dd>See <a href=""#rules-with-multiple-outputs"">Rules with multiple outputs</a></dd>
    <dt><code>jobs</code></dt>
    <dd>See <a href=""#running-jobs-in-parallel"">Running jobs in parallel</a></dd>
</dl>

### In the global section

<dl>
    <dt><code>default</code></dt>
    <dd>A list
    (parsed by <a href=""https://docs.python.org/3/library/shlex.html?highlight=shlex#shlex.split""><code>shlex.split</code></a>)
    of default targets that are produced if the user does not specify any
    targets when calling Produce.</dd>
    <dt><code>prelude</code></dt>
    <dd>See <a href=""#the-prelude"">The prelude</a></dd>
</dl>

Getting in touch
----------------

Produce is being developed by Kilian Evang <%{firstname}@%{lastname}.name>.
I would love to hear from you if you find it useful, if you have questions, bug
reports or feature requests.

Acknowledgments
---------------

The Produce logo was designed by [Valerio Basile](https://valeriobasile.github.io).
",2023-07-07 18:45:48+00:00
pwrake,Workflows,misshie/Workflows,Pwrake for bioinfomatics workflows using GATK and Dindel ,,False,12,2020-01-03 02:55:44+00:00,2011-03-28 07:30:33+00:00,2,2,1,0,,,Other,21,,0,,,2012-12-10 06:49:32+00:00,"= Rakefiles for GATK and Dindel workflows

Author:: Hiroyuki Mishima (missy at be.to / hmishima at nagasaki-u.ac.jp)
Copyright:: Hiroyuki Mishima, 2010-2011
License:: the MIT license. See the LICENSE file.

= Workflows
== The Dindel workflow. 
see Dindel's web page http://www.sanger.ac.uk/resources/software/dindel/ .

== The GATK 'better' workflow
A rakefile for the Genome Analysis Toolkit (GATK) workflow. 

see GATK web page
http://www.broadinstitute.org/gsa/wiki/index.php/The_Genome_Analysis_Toolkit .

This workflow describes ""Better - sample-level realignment with known indels
and recalibration"".
See http://www.broadinstitute.org/gsa/wiki/index.php/Best_Practice_Variant_Detection_with_the_GATK_v2#Better:_sample-level_realignment_with_known_indels_and_recalibration .

== Combined Rakefile for GATK and Dindel workflows
A combined rakefile calling both GATK and Dindel workflows workflows. 

== The GATK 'better' workflow July 2011
The newest UnifiedGenotyper implements the Dindel algorithm and can reports both SNVs and indels. This workflows is based on the newest recommended workflow on the GATK web page. Now Rakefile.invoke is devided into Rakefile.invoke.Gatk and Rakefile.invoke.Picard. Each file containes Gatk or Picard specific methods. This made shareing Rakefile.invoke easier between different workflows.

= How to run
* Rake dry-run: rake -n
* Rake run: rake
* Pwrake dry-run: pwrake NODEFILE=nodefile -n
* Pwrake run: pwrake NODEFILE=nodefile
* details of Pwrake: see https://github.com/masa16/Pwrake/ and http://bioruby.open-bio.org/wiki/Workflows

= Package contents
Workflow directories consist of the folloing files:
== Rakefile
Main rakefile. In the start, target files of each workflow step are defined in constants. These constants are used in definition of the :default task. This makes finding overview of the workflow easy and setting break points in workflow execution. Each workflow steps can be defined using the ""rule"" method if the dependency of the step is defined by naming rules such as file extensions (suffix). The ""file"" method also can be used. This methods defines dependency using fixed filenames instead of rules. You can flexibly define dependencies using regular ruby syntax such as Enumerable#each. 
== Rakefile.invoke
Refered from Rakefile. Command-line options of tools to be invoked are described in this file. Each invoke method should receive a Task object (sometimes ""t"" is used for a parameter name). An optional Hash object can be used if the method needs extra information.
== Rakefile.helper
Defining helper methods simplifying Rakefile descriptions. These methods are defined in the top level.
=== suffix(objfile, dependency)
""objfile"" is an array of String objects (filenames). ""dependency"" is a Hash but expected to have only one key. To replace the "".bam"" file extension (or suffix) to "".dedup.bam"", ""dependency"" should be {"".bam"" => "".dedup.bam""}. Note that you do not have to use ""\."" to indicate a dot. 
=== suffix_proc(dependency)
The ""rule"" method of Rake requires an Array of Proc objects (object of code block or procedure) to define dependent files. This method returnes a Proc object to replace suffix. ""dependency"" is as same as that in the suffix method. 

== nodefile
A file with contains a line ""localhost 16"" to allow to run maximum 16 processes simultaneously in the localhost. Multiple lines are allowed in this file.

= Procedure to describe new workflows
As a summary of the agile workflow development, the general procedure for describing new workflows in Pwrake is given below.

== (1) Workflow definition phase.
Describe file dependencies in Rakefile.
 task ""output.dat"" => ""input.dat"" do |t|
   RakefileInvoke::generate_terget t
 end

== (2) Parameter adjustment phase
Define the RakefileInvoke::generate_terget method in Rake.invoke.
 module RakefileInvoke
   def generate_target(t)
     sh ""command-line #{t.prerequisite} > #{t.name}""
   end
 end

== (3) Iteration of phases.
Parameter adjustments require modifications to Rakefile.invoke only. Similarly, changes in file dependencies require modification to Rakefile only.

== Tips
* In Rakefile and Rakefile.invoke, all the fixed values in the command-line should be given using constants instead of hard coding.
* In Rakefile, the rule method is useful if the order of tasks can be defined by the file naming rule such as file name extensions.
* For syntax check, the -n option of the Pwrake/Rake command for dry-run is useful.
* To check the correctness of the generated command-line, the command-line can be shown by replacing the sh method by the puts method in Rakefile.invoke.
* Redirecting the standard output and the standard error to files is a good practice for trouble shooting.

= Copyright and license
copyright (c) Hiroyuki Mishima, 2010-2011. See the LICENSE file.
",2023-07-07 18:45:52+00:00
pydra,pydra,nipype/pydra,Pydra Dataflow Engine,https://nipype.github.io/pydra/,False,100,2023-06-12 13:59:46+00:00,2018-10-08 18:26:17+00:00,49,13,23,34,0.22,2023-02-27 03:41:55+00:00,Other,1890,0.22,35,2023-02-27 03:11:51+00:00,2023-07-01 23:52:24+00:00,2023-05-03 20:43:02+00:00,"|GHAction| |CircleCI| |codecov|

|Pydralogo|

.. |Pydralogo| image:: https://raw.githubusercontent.com/nipype/pydra/master/docs/logo/pydra_logo.jpg
   :width: 200px
   :alt: pydra logo

.. |GHAction| image:: https://github.com/nipype/pydra/workflows/Pydra/badge.svg
   :alt: GitHub Actions CI
   :target: https://github.com/nipype/Pydra/actions

.. |CircleCI| image:: https://circleci.com/gh/nipype/pydra.svg?style=svg
   :alt: CircleCI

.. |codecov| image:: https://codecov.io/gh/nipype/pydra/branch/master/graph/badge.svg
   :alt: codecov

======================
Pydra: Dataflow Engine
======================

A simple dataflow engine with scalable semantics.

Pydra is a rewrite of the Nipype engine with mapping and joining as
first-class operations. It forms the core of the Nipype 2.0 ecosystem.

The goal of pydra is to provide a lightweight Python dataflow engine for DAG
construction, manipulation, and distributed execution.

Feature list:
=============
1. Python 3.7+ using type annotation and `attrs <https://www.attrs.org/en/stable/>`_
2. Composable dataflows with simple node semantics. A dataflow can be a node of another dataflow.
3. `splitter` and `combiner` provides many ways of compressing complex loop semantics
4. Cached execution with support for a global cache across dataflows and users
5. Distributed execution, presently via ConcurrentFutures, SLURM, and Dask (this is an experimental implementation with limited testing)

`API Documentation <https://nipype.github.io/pydra/>`_

Learn more about Pydra
======================

* `SciPy 2020 Proceedings <http://conference.scipy.org/proceedings/scipy2020/pydra.html>`_
* `PyCon 2020 Poster <https://docs.google.com/presentation/d/10tS2I34rS0G9qz6v29qVd77OUydjP_FdBklrgAGmYSw/edit?usp=sharing>`_
* `Explore Pydra interactively <https://github.com/nipype/pydra-tutorial>`_ (the tutorial can be also run using Binder service)

|Binder|

.. |Binder| image:: https://mybinder.org/badge_logo.svg
   :alt: Binder


Please note that mybinder times out after an hour.

Installation
============

::

    pip install pydra


Note that installation fails with older versions of pip on Windows. Upgrade pip before installing:

::

   pip install –upgrade pip
   pip install pydra


Developer installation
======================

Pydra requires Python 3.7+. To install in developer mode:

::

    git clone git@github.com:nipype/pydra.git
    cd pydra
    pip install -e "".[dev]""


In order to run pydra's test locally:

::

    pytest -vs pydra


If you want to test execution with Dask:

::

    git clone git@github.com:nipype/pydra.git
    cd pydra
    pip install -e "".[dask]""



It is also useful to install pre-commit:

::

    pip install pre-commit
    pre-commit
",2023-07-07 18:45:55+00:00
pyflow,pyflow,Illumina/pyflow,A lightweight parallel task engine,http://Illumina.github.io/pyflow/,False,134,2023-07-04 14:07:04+00:00,2013-05-08 00:58:25+00:00,42,31,3,19,v1.1.20,2018-03-02 17:24:08+00:00,,120,v1.1.20,23,2018-03-02 17:17:44+00:00,2023-07-04 14:05:40+00:00,2020-07-01 21:52:27+00:00,"pyFlow - a lightweight parallel task engine
===========================================

[![Build Status][tcistatus]][tcihome]
[![Build status][acistatus]][acihome]


pyFlow is a tool to manage tasks in the context of a task dependency
graph. It has some similarities to make. pyFlow is not a program – it
is a python module, and workflows are defined using pyFlow by writing
regular python code with the pyFlow API

For more information, please see the [pyFlow website][site].

[site]:http://illumina.github.io/pyflow/

[tcistatus]:https://travis-ci.org/Illumina/pyflow.svg?branch=master
[tcihome]:https://travis-ci.org/Illumina/pyflow

[acistatus]:https://ci.appveyor.com/api/projects/status/fkovw5ife59ae48t/branch/master?svg=true
[acihome]:https://ci.appveyor.com/project/ctsa/pyflow/branch/master


License
-------

pyFlow source code is provided under the [BSD 2-Clause License](pyflow/COPYRIGHT.txt).


Releases
--------

Recent release tarballs can be found on the github release list here:

https://github.com/Illumina/pyflow/releases

To create a release tarball corresponding to any other version, run:

    git clone git://github.com/Illumina/pyflow.git pyflow
    cd pyflow
    git checkout ${VERSION}
    ./scratch/make_release_tarball.bash
    # tarball is ""./pyflow-${VERSION}.tar.gz""

Note this README is at the root of the pyflow development repository
and is not part of the python source release.


Contents
--------

For the development repository (this directory), the sub-directories are:

pyflow/

Contains all pyflow code intended for distribution, plus demo code and
documentation.

scratch/

This directory contains support scripts for tests/cleanup/release
tarballing.. etc.

",2023-07-07 18:46:00+00:00
pyhive,PyHive,dropbox/PyHive,Python interface to Hive and Presto. 🐝,,False,1614,2023-06-27 06:51:15+00:00,2014-02-01 09:05:07+00:00,546,64,40,17,v0.6.3,2020-08-05 17:46:25+00:00,Other,185,v0.6.3,19,2020-08-05 17:46:25+00:00,2023-07-07 17:22:21+00:00,2023-06-20 11:07:18+00:00,"================================
Project is currently unsupported
================================




.. image:: https://travis-ci.org/dropbox/PyHive.svg?branch=master
    :target: https://travis-ci.org/dropbox/PyHive
.. image:: https://img.shields.io/codecov/c/github/dropbox/PyHive.svg

======
PyHive
======

PyHive is a collection of Python `DB-API <http://www.python.org/dev/peps/pep-0249/>`_ and
`SQLAlchemy <http://www.sqlalchemy.org/>`_ interfaces for `Presto <http://prestodb.io/>`_ and
`Hive <http://hive.apache.org/>`_.

Usage
=====

DB-API
------
.. code-block:: python

    from pyhive import presto  # or import hive or import trino
    cursor = presto.connect('localhost').cursor()
    cursor.execute('SELECT * FROM my_awesome_data LIMIT 10')
    print cursor.fetchone()
    print cursor.fetchall()

DB-API (asynchronous)
---------------------
.. code-block:: python

    from pyhive import hive
    from TCLIService.ttypes import TOperationState
    cursor = hive.connect('localhost').cursor()
    cursor.execute('SELECT * FROM my_awesome_data LIMIT 10', async=True)

    status = cursor.poll().operationState
    while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):
        logs = cursor.fetch_logs()
        for message in logs:
            print message

        # If needed, an asynchronous query can be cancelled at any time with:
        # cursor.cancel()

        status = cursor.poll().operationState

    print cursor.fetchall()

In Python 3.7 `async` became a keyword; you can use `async_` instead:

.. code-block:: python

    cursor.execute('SELECT * FROM my_awesome_data LIMIT 10', async_=True)


SQLAlchemy
----------
First install this package to register it with SQLAlchemy (see ``setup.py``).

.. code-block:: python

    from sqlalchemy import *
    from sqlalchemy.engine import create_engine
    from sqlalchemy.schema import *
    # Presto
    engine = create_engine('presto://localhost:8080/hive/default')
    # Trino
    engine = create_engine('trino://localhost:8080/hive/default')
    # Hive
    engine = create_engine('hive://localhost:10000/default')
    logs = Table('my_awesome_data', MetaData(bind=engine), autoload=True)
    print select([func.count('*')], from_obj=logs).scalar()

    # Hive + HTTPS + LDAP or basic Auth
    engine = create_engine('hive+https://username:password@localhost:10000/')
    logs = Table('my_awesome_data', MetaData(bind=engine), autoload=True)
    print select([func.count('*')], from_obj=logs).scalar()

Note: query generation functionality is not exhaustive or fully tested, but there should be no
problem with raw SQL.

Passing session configuration
-----------------------------

.. code-block:: python

    # DB-API
    hive.connect('localhost', configuration={'hive.exec.reducers.max': '123'})
    presto.connect('localhost', session_props={'query_max_run_time': '1234m'})
    trino.connect('localhost',  session_props={'query_max_run_time': '1234m'})
    # SQLAlchemy
    create_engine(
        'presto://user@host:443/hive',
        connect_args={'protocol': 'https',
                      'session_props': {'query_max_run_time': '1234m'}}
    )
    create_engine(
        'trino://user@host:443/hive',
        connect_args={'protocol': 'https',
                      'session_props': {'query_max_run_time': '1234m'}}
    )
    create_engine(
        'hive://user@host:10000/database',
        connect_args={'configuration': {'hive.exec.reducers.max': '123'}},
    )
    # SQLAlchemy with LDAP
    create_engine(
        'hive://user:password@host:10000/database',
        connect_args={'auth': 'LDAP'},
    )

Requirements
============

Install using

- ``pip install 'pyhive[hive]'`` for the Hive interface and
- ``pip install 'pyhive[presto]'`` for the Presto interface.
- ``pip install 'pyhive[trino]'`` for the Trino interface

PyHive works with

- Python 2.7 / Python 3
- For Presto: Presto install
- For Trino: Trino install
- For Hive: `HiveServer2 <https://cwiki.apache.org/confluence/display/Hive/Setting+up+HiveServer2>`_ daemon

Changelog
=========
See https://github.com/dropbox/PyHive/releases.

Contributing
============
- Please fill out the Dropbox Contributor License Agreement at https://opensource.dropbox.com/cla/ and note this in your pull request.
- Changes must come with tests, with the exception of trivial things like fixing comments. See .travis.yml for the test environment setup.
- Notes on project scope:

  - This project is intended to be a minimal Hive/Presto client that does that one thing and nothing else.
    Features that can be implemented on top of PyHive, such integration with your favorite data analysis library, are likely out of scope.
  - We prefer having a small number of generic features over a large number of specialized, inflexible features.
    For example, the Presto code takes an arbitrary ``requests_session`` argument for customizing HTTP calls, as opposed to having a separate parameter/branch for each ``requests`` option.

Testing
=======
.. image:: https://travis-ci.org/dropbox/PyHive.svg
    :target: https://travis-ci.org/dropbox/PyHive
.. image:: http://codecov.io/github/dropbox/PyHive/coverage.svg?branch=master
    :target: http://codecov.io/github/dropbox/PyHive?branch=master

Run the following in an environment with Hive/Presto::

    ./scripts/make_test_tables.sh
    virtualenv --no-site-packages env
    source env/bin/activate
    pip install -e .
    pip install -r dev_requirements.txt
    py.test

WARNING: This drops/creates tables named ``one_row``, ``one_row_complex``, and ``many_rows``, plus a
database called ``pyhive_test_database``.

Updating TCLIService
====================

The TCLIService module is autogenerated using a ``TCLIService.thrift`` file. To update it, the
``generate.py`` file can be used: ``python generate.py <TCLIServiceURL>``. When left blank, the
version for Hive 2.3 will be downloaded.
",2023-07-07 18:46:05+00:00
pyinvoke,invoke,pyinvoke/invoke,Pythonic task management & command execution.,http://pyinvoke.org,False,4004,2023-07-06 19:52:34+00:00,2012-02-29 23:59:23+00:00,350,92,50,0,,,"BSD 2-Clause ""Simplified"" License",3694,just-rebased-414,65,2018-08-06 19:45:59+00:00,2023-07-06 19:52:35+00:00,2023-06-14 23:05:36+00:00,"|version| |python| |license| |ci| |coverage|

.. |version| image:: https://img.shields.io/pypi/v/invoke
    :target: https://pypi.org/project/invoke/
    :alt: PyPI - Package Version
.. |python| image:: https://img.shields.io/pypi/pyversions/invoke
    :target: https://pypi.org/project/invoke/
    :alt: PyPI - Python Version
.. |license| image:: https://img.shields.io/pypi/l/invoke
    :target: https://github.com/pyinvoke/invoke/blob/main/LICENSE
    :alt: PyPI - License
.. |ci| image:: https://img.shields.io/circleci/build/github/pyinvoke/invoke/main
    :target: https://app.circleci.com/pipelines/github/pyinvoke/invoke
    :alt: CircleCI
.. |coverage| image:: https://img.shields.io/codecov/c/gh/pyinvoke/invoke
    :target: https://app.codecov.io/gh/pyinvoke/invoke
    :alt: Codecov

Welcome to Invoke!
==================

Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented
subprocesses and organizing executable Python code into CLI-invokable tasks. It
draws inspiration from various sources (``make``/``rake``, Fabric 1.x, etc) to
arrive at a powerful & clean feature set.

To find out what's new in this version of Invoke, please see `the changelog
<https://pyinvoke.org/changelog.html#{}>`_.

The project maintainer keeps a `roadmap
<https://bitprophet.org/projects#roadmap>`_ on his website.
",2023-07-07 18:46:09+00:00
pyiron,pyiron,pyiron/pyiron,pyiron - an integrated development environment (IDE) for computational materials science.,http://pyiron.org,False,291,2023-07-04 15:20:06+00:00,2018-03-22 13:14:40+00:00,41,14,32,11,pyiron-0.4.7,2022-10-17 15:05:10+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",7389,pyiron-0.5.0,57,2023-06-08 03:35:55+00:00,2023-07-05 16:24:39+00:00,2023-07-05 16:24:34+00:00,"pyiron
======

.. image:: https://coveralls.io/repos/github/pyiron/pyiron/badge.svg?branch=main
    :target: https://coveralls.io/github/pyiron/pyiron?branch=main
    :alt: Coverage Status

.. image:: https://api.codacy.com/project/badge/Grade/c513254f10004df5a1f5c76425c6584b
    :target: https://app.codacy.com/app/pyiron-runner/pyiron?utm_source=github.com&utm_medium=referral&utm_content=pyiron/pyiron&utm_campaign=Badge_Grade_Settings
    :alt: Codacy Badge

.. image:: https://anaconda.org/conda-forge/pyiron/badges/latest_release_date.svg
    :target: https://anaconda.org/conda-forge/pyiron/
    :alt: Release_Date

.. image:: https://github.com/pyiron/pyiron/workflows/Python%20package/badge.svg
    :target: https://github.com/pyiron//pyiron/actions
    :alt: Build Status

.. image:: https://anaconda.org/conda-forge/pyiron/badges/downloads.svg
    :target: https://anaconda.org/conda-forge/pyiron/
    :alt: Downloads

.. image:: https://readthedocs.org/projects/pyiron/badge/?version=latest
    :target: https://pyiron.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status


.. image:: docs/_static/screenshot.png
    :align: center
    :alt: Screenshot of pyiron running inside jupyterlab.


pyiron - an integrated development environment (IDE) for computational materials science. It combines several tools in a common platform:

 - Atomic structure objects – compatible to the `Atomic Simulation Environment (ASE) <https://wiki.fysik.dtu.dk/ase/>`_.
 - Atomistic simulation codes – like `LAMMPS <http://lammps.sandia.gov>`_ and `VASP <https://www.vasp.at>`_.
 - Feedback Loops – to construct dynamic simulation life cycles.
 - Hierarchical data management – interfacing with storage resources like SQL and `HDF5 <https://support.hdfgroup.org/HDF5/>`_.
 - Integrated visualization – based on `NGLview <https://github.com/arose/nglview>`_.
 - Interactive simulation protocols - based on `Jupyter notebooks <http://jupyter.org>`_.
 - Object oriented job management – for scaling complex simulation protocols from single jobs to high-throughput simulations.

pyiron (called pyron) is developed in the `Computational Materials Design department <https://www.mpie.de/CM>`_ of `Joerg Neugebauer <https://www.mpie.de/person/43010/2763386>`_ at the `Max Planck Institut für Eisenforschung (Max Planck Institute for iron research) <https://www.mpie.de/2281/en>`_. While its original focus was to provide a framework to develop and run complex simulation protocols as needed for ab initio thermodynamics it quickly evolved into a versatile tool to manage a wide variety of simulation tasks. In 2016 the `Interdisciplinary Centre for Advanced Materials Simulation (ICAMS) <http://www.icams.de>`_ joined the development of the framework with a specific focus on high throughput applications. In 2018 pyiron was released as open-source project.
See the `Documentation <http://pyiron.org>`_ page for more details.

.. note::
   **pyiron**: This is the documentation page for the pyiron meta package, that combines the other packages in a common
   interface.  The API documentation for `pyiron_base <https://pyiron_base.readthedocs.io/en/latest/>`_ and
   `pyiron_atomistics <https://pyiron_atomistics.readthedocs.io/en/latest/>`_ are available as separate pages.


Installation
------------
You can test pyiron on `Mybinder.org (beta) <https://mybinder.org/v2/gh/pyiron/pyiron/main?urlpath=lab>`_.
For a local installation we recommend to install pyiron inside an `anaconda <https://www.anaconda.com>`_  environment::

    conda install -c conda-forge pyiron


After the installation of pyiron you need to configure pyiron. The default configuration can be generated automatically. Start a new Python session and import pyiron::

   > import pyiron
   > pyiron.install()
   >>> It appears that pyiron is not yet configured, do you want to create a default start configuration (recommended: yes). [yes/no]:
   > yes
   > exit()


See the `Documentation-Installation <https://pyiron.readthedocs.io/en/latest/source/installation.html>`_ page for more details.


Example
-------
After the successful configuration you can start your first pyiron calculation. Navigate to the the projects directory and start a jupyter notebook or jupyter lab session correspondingly::

    cd ~/pyiron/projects
    jupyter notebook

Open a new jupyter notebook and inside the notebook you can now validate your pyiron calculation by creating a test project, setting up an initial structure of bcc Fe and visualize it using NGLview::

    from pyiron import Project
    pr = Project('test')
    structure = pr.create_structure('Fe', 'bcc', 2.78)
    structure.plot3d()

Finally a first lammps calculation can be executed by::

    job = pr.create_job(job_type=pr.job_type.Lammps, job_name='lammpstestjob')
    job.structure = structure
    job.potential = job.list_potentials()[0]
    job.run()


Getting started:
----------------
Test pyiron with mybinder:

.. image:: https://mybinder.org/badge_logo.svg
     :target: https://mybinder.org/v2/gh/pyiron/pyiron/main
     :alt: mybinder


License and Acknowledgments
---------------------------
``pyiron`` is licensed under the BSD license.

If you use pyiron in your scientific work, `please consider citing <http://www.sciencedirect.com/science/article/pii/S0927025618304786>`_ ::

  @article{pyiron-paper,
    title = {pyiron: An integrated development environment for computational materials science},
    journal = {Computational Materials Science},
    volume = {163},
    pages = {24 - 36},
    year = {2019},
    issn = {0927-0256},
    doi = {https://doi.org/10.1016/j.commatsci.2018.07.043},
    url = {http://www.sciencedirect.com/science/article/pii/S0927025618304786},
    author = {Jan Janssen and Sudarsan Surendralal and Yury Lysogorskiy and Mira Todorova and Tilmann Hickel and Ralf Drautz and Jörg Neugebauer},
    keywords = {Modelling workflow, Integrated development environment, Complex simulation protocols},
  }
",2023-07-07 18:46:13+00:00
pypeflow,pypeFLOW,PacificBiosciences/pypeFLOW,a simple lightweight workflow engine for data analysis scripting,,True,10,2023-01-28 02:20:35+00:00,2014-02-03 15:33:01+00:00,23,8,8,8,,,BSD 3-Clause Clear License,506,v0.1.0,9,2012-08-01 21:12:24+00:00,2023-04-14 02:40:57+00:00,2019-05-09 08:48:29+00:00,"What is pypeFLOW
================

pypeFLOW is light weight and reusable make / flow data process
library written in Python.

Most of bioinformatics analysis or general data analysis
includes various steps combining data files, transforming
files between different formats and calculating statistics
with a variety of tools. Ian Holmes has a great summary and
opinions about bioinformatics workflow at
http://biowiki.org/BioinformaticsWorkflows.  It is
interesting that such analysis workflow is really similar to
constructing software without an IDE in general.  Using a
""makefile"" file for managing bioinformatics analysis
workflow is actually great for generating reproducible and
reusable analysis procedure.  Combining with a proper
version control tool, one will be able to manage to work
with a divergent set of data and tools over a period of time
for a project especially when there are complicate
dependence between the data, tools and customized code
for the analysis tasks.

However, using ""make"" and ""makefile"" implies all data
analysis steps are done by some command line tools. If you
have some customized analysis tasks, you will have to write
some scripts and to make them into command line tools.  In
my personal experience, I find it is convenient to bypass
such burden and to combine those quick and simple steps in a
single scripts. The only caveat is that if an analyst does
not save the results of any intermediate steps, he or she
has to repeat the computation all over again for every steps
from the beginning. This will waste a lot of computation
cycles and personal time.  Well, the solution is simple,
just like the traditional software building process, one
have to track the dependencies and analyze them and only
reprocess those parts that are necessary to get the most
up-to-date final results.

General Design Principles
=========================

    - Explicitly modeling data and task dependency
    - Support declarative programming style within Python while
      maintaining some thing that imperative programming dose the
      best
    - Utilize RDF meta-data framework
    - Keep it simple if possible

Features
========

    - Multiple concurrent task scheduling and running
    - Support task as simple shell script (considering clustering
      job submission in mind)
    - reasonable simple interface for declarative programming

General Installation
====================

pypeFlow uses the standard python setup.py for installation::
    
    python setup.py install

Once install, a brief documentation can be generated by::

    cd doc
    make html

The generate sphinx html document can be viewed by point your web browser 
to ``_build/html/index.html`` in the ``doc`` directory.

DISCLAIMER
----------
THIS WEBSITE AND CONTENT AND ALL SITE-RELATED SERVICES, INCLUDING ANY DATA, ARE PROVIDED ""AS IS,"" WITH ALL FAULTS, WITH NO REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, SATISFACTORY QUALITY, NON-INFRINGEMENT OR FITNESS FOR A PARTICULAR PURPOSE. YOU ASSUME TOTAL RESPONSIBILITY AND RISK FOR YOUR USE OF THIS SITE, ALL SITE-RELATED SERVICES, AND ANY THIRD PARTY WEBSITES OR APPLICATIONS. NO ORAL OR WRITTEN INFORMATION OR ADVICE SHALL CREATE A WARRANTY OF ANY KIND. ANY REFERENCES TO SPECIFIC PRODUCTS OR SERVICES ON THE WEBSITES DO NOT CONSTITUTE OR IMPLY A RECOMMENDATION OR ENDORSEMENT BY PACIFIC BIOSCIENCES.
",2023-07-07 18:46:18+00:00
pypeln,pypeln,cgarciae/pypeln,Concurrent data pipelines in Python   >>>,https://cgarciae.github.io/pypeln,False,1426,2023-07-05 01:09:33+00:00,2018-09-01 13:43:31+00:00,89,41,11,14,0.4.9,2022-01-06 15:29:14+00:00,MIT License,240,0.4.9,24,2022-01-06 15:29:14+00:00,2023-07-06 15:58:01+00:00,2023-03-22 14:52:27+00:00,"# Pypeln

[![Coverage](https://img.shields.io/codecov/c/github/cgarciae/pypeln?color=%2334D058)](https://codecov.io/gh/cgarciae/pypeln)

-----------------

_Pypeln (pronounced as ""pypeline"") is a simple yet powerful Python library for creating concurrent data pipelines._

#### Main Features

* **Simple**: Pypeln was designed to solve _medium_ data tasks that require parallelism and concurrency where using frameworks like Spark or Dask feels exaggerated or unnatural.
* **Easy-to-use**: Pypeln exposes a familiar functional API compatible with regular Python code.
* **Flexible**: Pypeln enables you to build pipelines using Processes, Threads and asyncio.Tasks via the exact same API.
* **Fine-grained Control**: Pypeln allows you to have control over the memory and cpu resources used at each stage of your pipelines.

For more information take a look at the [Documentation](https://cgarciae.github.io/pypeln).

![diagram](https://github.com/cgarciae/pypeln/blob/master/docs/images/diagram.png?raw=true)

## Installation

Install Pypeln using pip:
```bash
pip install pypeln
```

## Basic Usage
With Pypeln you can easily create multi-stage data pipelines using 3 type of workers:

### Processes
You can create a pipeline based on [multiprocessing.Process](https://docs.python.org/3.4/library/multiprocessing.html#multiprocessing.Process) workers by using the `process` module:

```python
import pypeln as pl
import time
from random import random

def slow_add1(x):
    time.sleep(random()) # <= some slow computation
    return x + 1

def slow_gt3(x):
    time.sleep(random()) # <= some slow computation
    return x > 3

data = range(10) # [0, 1, 2, ..., 9] 

stage = pl.process.map(slow_add1, data, workers=3, maxsize=4)
stage = pl.process.filter(slow_gt3, stage, workers=2)

data = list(stage) # e.g. [5, 6, 9, 4, 8, 10, 7]
```
At each stage the you can specify the numbers of `workers`. The `maxsize` parameter limits the maximum amount of elements that the stage can hold simultaneously.

### Threads
You can create a pipeline based on [threading.Thread](https://docs.python.org/3/library/threading.html#threading.Thread) workers by using the `thread` module:
```python
import pypeln as pl
import time
from random import random

def slow_add1(x):
    time.sleep(random()) # <= some slow computation
    return x + 1

def slow_gt3(x):
    time.sleep(random()) # <= some slow computation
    return x > 3

data = range(10) # [0, 1, 2, ..., 9] 

stage = pl.thread.map(slow_add1, data, workers=3, maxsize=4)
stage = pl.thread.filter(slow_gt3, stage, workers=2)

data = list(stage) # e.g. [5, 6, 9, 4, 8, 10, 7]
```
Here we have the exact same situation as in the previous case except that the worker are Threads.

### Tasks
You can create a pipeline based on [asyncio.Task](https://docs.python.org/3.4/library/asyncio-task.html#asyncio.Task) workers by using the `task` module:
```python
import pypeln as pl
import asyncio
from random import random

async def slow_add1(x):
    await asyncio.sleep(random()) # <= some slow computation
    return x + 1

async def slow_gt3(x):
    await asyncio.sleep(random()) # <= some slow computation
    return x > 3

data = range(10) # [0, 1, 2, ..., 9] 

stage = pl.task.map(slow_add1, data, workers=3, maxsize=4)
stage = pl.task.filter(slow_gt3, stage, workers=2)

data = list(stage) # e.g. [5, 6, 9, 4, 8, 10, 7]
```
Conceptually similar but everything is running in a single thread and Task workers are created dynamically. If the code is running inside an async task can use `await` on the stage instead to avoid blocking:

```python
import pypeln as pl
import asyncio
from random import random

async def slow_add1(x):
    await asyncio.sleep(random()) # <= some slow computation
    return x + 1

async def slow_gt3(x):
    await asyncio.sleep(random()) # <= some slow computation
    return x > 3


def main():
    data = range(10) # [0, 1, 2, ..., 9] 

    stage = pl.task.map(slow_add1, data, workers=3, maxsize=4)
    stage = pl.task.filter(slow_gt3, stage, workers=2)

    data = await stage # e.g. [5, 6, 9, 4, 8, 10, 7]

asyncio.run(main())
```
### Sync
The `sync` module implements all operations using synchronous generators. This module is useful for debugging or when you don't need to perform heavy CPU or IO tasks but still want to retain element order information that certain functions like `pl.*.ordered` rely on.

```python
import pypeln as pl
import time
from random import random

def slow_add1(x):
    return x + 1

def slow_gt3(x):
    return x > 3

data = range(10) # [0, 1, 2, ..., 9] 

stage = pl.sync.map(slow_add1, data, workers=3, maxsize=4)
stage = pl.sync.filter(slow_gt3, stage, workers=2)

data = list(stage) # [4, 5, 6, 7, 8, 9, 10]
```
Common arguments such as `workers` and `maxsize` are accepted by this module's functions for API compatibility purposes but are ignored.

## Mixed Pipelines
You can create pipelines using different worker types such that each type is the best for its given task so you can get the maximum performance out of your code:
```python
data = get_iterable()
data = pl.task.map(f1, data, workers=100)
data = pl.thread.flat_map(f2, data, workers=10)
data = filter(f3, data)
data = pl.process.map(f4, data, workers=5, maxsize=200)
```
Notice that here we even used a regular python `filter`, since stages are iterables Pypeln integrates smoothly with any python code, just be aware of how each stage behaves.


## Pipe Operator
In the spirit of being a true pipeline library, Pypeln also lets you create your pipelines using the pipe `|` operator:

```python
data = (
    range(10)
    | pl.process.map(slow_add1, workers=3, maxsize=4)
    | pl.process.filter(slow_gt3, workers=2)
    | list
)
```

## Run Tests
A sample script is provided to run the tests in a container (either Docker or Podman is supported), to run tests:

```bash
$ bash scripts/run-tests.sh
```

This script can also receive a python version to check test against, i.e

```bash
$ bash scripts/run-tests.sh 3.7
```


## Related Stuff
* [Making an Unlimited Number of Requests with Python aiohttp + pypeln](https://medium.com/@cgarciae/making-an-infinite-number-of-requests-with-python-aiohttp-pypeln-3a552b97dc95)
* [Process Pools](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process#module-multiprocessing.pool)
* [Making 100 million requests with Python aiohttp](https://www.artificialworlds.net/blog/2017/06/12/making-100-million-requests-with-python-aiohttp/)
* [Python multiprocessing Queue memory management](https://stackoverflow.com/questions/52286527/python-multiprocessing-queue-memory-management/52286686#52286686)
* [joblib](https://joblib.readthedocs.io/en/latest/)
* [mpipe](https://vmlaker.github.io/mpipe/)

## Contributors
* [cgarciae](https://github.com/cgarciae)
* [Davidnet](https://github.com/Davidnet)

## License
MIT",2023-07-07 18:46:23+00:00
pyperator,pyperator,baffelli/pyperator,Simple python workflow engine based on asyncio and a DAG structure.,,False,53,2023-06-23 12:14:43+00:00,2017-03-17 21:47:03+00:00,7,3,0,0,,,MIT License,290,,0,,2023-06-23 12:14:43+00:00,2017-05-09 11:28:43+00:00,"[![Travis](https://travis-ci.org/baffelli/pyperator.svg?branch=master)](https://travis-ci.org/baffelli/pyperator.svg?branch=master)

# Pyperator
Pyperator is a simple python workflow library based on asyncio. Freely inspired by other flow-based programming tools such as [noflo](https://noflojs.org/)
[scipipe](https://github.com/scipipe/scipipe/) and many [others](https://github.com/pditommaso/awesome-pipeline).
A network of components communicating through named ports is build; the execution happens asynchronously by scheduling all processes and sending/receving on the input/output ports.
## Simple example


A simple example workflow summing two numbers and printing the result can be built as follows:
```python
        from pyperator.DAG import  Multigraph
        from pyperator.components import GeneratorSource, ShowInputs, BroadcastApplyFunction, ConstantSource, Filter, OneOffProcess
        from pyperator import components
        from pyperator.nodes import Component
        from pyperator.utils import InputPort, OutputPort, FilePort, Wildcards
        #Sum function
        def adder(**kwargs):
                out = sum([item for k, item in kwargs.items() if item])
                return out
        #Create two source generating data from generator compherensions
        source1 = GeneratorSource('s1',  (i for i in range(100)))
        source2 = GeneratorSource('s2',  (i for i in range(100)))
        #Add a printer to display the result
        shower = ShowInputs('printer')
        #add input port
        shower.inputs.add(InputPort('in1'))
        #Function that applies a function of all input packets and sends it to all output ports
        summer = BroadcastApplyFunction('summer', adder )
        #Add ports
        summer.inputs.add(InputPort('g1'))
        summer.inputs.add(InputPort('g2'))
        summer.outputs.add(OutputPort('sum'))
        #Initialize DAG
        graph = Multigraph()
        #Connect ports
        graph.connect(source1.outputs.OUT, summer.inputs.g1)
        graph.connect(source2.outputs.OUT, summer.inputs.g2)
        graph.connect(summer.outputs.sum, shower.inputs.in1)
        #Execute dag
        graph()
```     
## How to use Pyperator

Creating a pyperator workflow requires three steps:
1. Define/import components
2. Define input/output ports 
3. create a graph by connecting ports

### Define components
Each pyperator component should be built subclassing `pyperator.nodes.Component`, which is in turn subclassed from the `AbstractComponent`, which has `__call__` as a abstract method. Your component should implement the `__call__` 
coroutine. An example of a simple component summing the content of the packets received in ""IN1"" and ""IN2"":
```python
from pyperator import components
from pyperator.nodes import Component
from pyperator.utils import InputPort, OutputPort, FilePort, Wildcards
from pyperator.IP import InformationPacket
from pyperator.DAG import  Multigraph
import asyncio
import random

class Summer(Component):
    """"""
    This is a component that sums the value of the recieved packets
    """"""

    def __init__(self, name):
        super(Summer, self).__init__(name)
        self.inputs.add(InputPort(""IN1""))
        self.inputs.add(InputPort(""IN2""))
        self.outputs.add(OutputPort(""OUT""))

    @log_schedule
    async def __call__(self):
        while True:
                in_packets_1 = await sefl.inputs.IN1.receive()
                in_packets_2 = await sefl.inputs.IN1.receive()
                #InformationPackets have the attribute ""value"" that contains the data
                summed = InformatioPacket(in_packets_1.value() + in_packets_2.value())
                await self.outputs.OUT.send_packet(summed)

```
To receive packets from a channel named ""IN"", you should use `packet = await self.inputs.IN.receive_packet()`. The input and output PortRegister support two forms of indexing:

* Selecting ports using dict-style keys, i.e `self.inputs[""IN""]`

* Selecting them as attributes of the PortRegister, i.e `self.inputs.IN`

Likewise, sending packets is accomplished by issuing `await self.outputs.OUT.send(packet)`.

Now, we define a second components that creates some interesting packets. This component will be a source component, it will not have any input ports and will generate packtes consisting of random numbers:
```python
class RandomSource(Component):
        """"""
        This component generates random packets
        """"""
        def __init__(self, name):
                super(RandomSource, self).__init__(name)
                self.name = name
                self.outputs.add(OutputPort(""OUT""))
                
        async def __call__(self):
                while True:
                        #generate IP containing random number
                        a = InformationPacket(random.random())
                        #send to the output port
                        await self.outputs.OUT.send_packet(a)
                        await asyncio.sleep(0)


```

### Define Input/Output Ports

There are two main ways to add ports to a component:

* Adding them when defining the component, using its __init__ method:

```python
class CustomComponent(Component):
    """"""
    This is my useless component
    """"""

    def __init__(self, name):
        super(CustomComponent, self).__init__(name)
        self._gen = generator
        self.outputs.add(OutputPort(""OUT""))
        self.inputs.add(OutputPort(""IN""))
```
this is what we have done before to define the `Summer` and the `RandomSource`components.
* Adding ports to an existing compononent instance using the method `add` of the input and output PortRegister:

```python

c1 = CustomComponent(""test"")
#This component already has one input and output ports.
c1.inputs.add.InputPort(""IN1"")
c1.outputs.add.OutputPort(""OUT1"")

```
Now, c1 will have two `InputPort`s, ""IN"" and ""IN1"" and two `OutputPorts`""OUT"" and ""OUT1"".

### Create a graph

Finally, several components can be tied together using a MultiGraph. In this case, we want to compute the sum of two random packets and print the result. To print the result we import the `ShowInputs` component from `pyperator.components` that will print the packet it receives from each port.
```python

#Define a graph
g = Multigraph()

#Instantiate components

s1 = RandomSource(""s1"")
s2 = RandomSource(""s2"")
adder = Summer(""sum_them"")
printer = ShowInputs(""show_it"")
#The printer needs an input port
printer.inputs.add(InputPort(""IN""))

#Now we connect the components

s1.outputs.OUT.connect(adder.inputs.IN1)
s2.outputs.OUT.connect(adder.inputs.IN2)
adder.outputs.OUT.connect(printer.inputs.IN)

#Now we need to add them to a Graph instance to be able to run them.

g.add_node(s1)
g.add_node(s2)
g.add_node(adder)
g.add_node(printer)

#We can vrun the computation by calling the graph
g()

```

### Create a graph using magic methods
Pyperator also supports a nicer syntax to express graphs and connect ports. The same graph of the previous example can be expressed
in a more suggestive way using:

```python

#all nodes defined within this context manager will be automatically be added to the graph `g`
with Multigraph() as g:
        s1 = RandomSource(""s1"")
        s2 = RandomSource(""s2"")
        adder = Summer(""sum_them"")
        printer = ShowInputs(""show_it"")
        #The printer needs an input port
        printer.inputs.add(InputPort(""IN""))
        #Connect 
        s1.outputs.OUT >> adder.inputs.IN1
        s2.outputs.OUT >> adder.inputs.IN2
        adder.outputs.OUT >> printer.inputs.IN
        
g()
        
        
```

### 

## Advanced example
In this example we will see how pyperator supports shell commands and files, including wildcards and the generation of dynamic filenames. Many of these features are inspired by [scipipe](https://github.com/scipipe/scipipe/).
```python
        from pyperator.DAG import  Multigraph
        from pyperator.components import GeneratorSource, ShowInputs, BroadcastApplyFunction, ConstantSource, Filter, OneOffProcess
        from pyperator import components
        from pyperator.nodes import Component
        from pyperator.utils import InputPort, OutputPort, FilePort, Wildcards
        #Source
        source1 = GeneratorSource('s1', (i for i in range(5)))
        source2 = GeneratorSource('s2', (i for i in range(5)))
        toucher = components.Shell('shell', ""echo '{inputs.i.value}, {inputs.j.value}' > {outputs.f1.path}"")
        toucher.outputs.add(FilePort('f1'))
        toucher.inputs.add(InputPort('i'))
        toucher.inputs.add(InputPort('j'))
        toucher.DynamicFormatter('f1', ""{inputs.j.value}_{inputs.i.value}.txt1"")
        printer = ShowInputs('show_path')
        printer.inputs.add(FilePort('f2'))
        graph = Multigraph()
        graph.connect(source1.outputs.OUT, toucher.inputs.i)
        graph.connect(source2.outputs.OUT, toucher.inputs.j)
        graph.connect(toucher.outputs.f1, printer.inputs.f2)
        graph()
```
",2023-07-07 18:46:28+00:00
pypipegraph,pypipegraph,IMTMarburg/pypipegraph,python make on steroids,,False,0,2021-07-12 10:18:06+00:00,2018-11-15 10:56:52+00:00,2,2,2,2,v0.197,2021-07-12 10:18:00+00:00,Other,573,v0.197,6,2021-07-12 10:17:50+00:00,,2021-07-12 10:17:50+00:00,"
# pypipegraph 

| Build status: | [![Build Status](https://travis-ci.com/TyberiusPrime/pypipegraph.svg?branch=master)](https://travis-ci.com/TyberiusPrime/pypipegraph)|
|---------------|-----------------------------------------------------------------------------|
| Documentation | https://pypipegraph.readthedocs.io/en/latest/
| Code style    | ![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

## Introduction

[pypipegraph](https://github.com/IMTMarburg/pypipegraph): is an
MIT-licensed library for constructing a workflow piece by piece and
executing just the parts of it that need to be (re-)done. It supports
using multiple cores (SMP) and (eventually, alpha code right now)
machines (cluster) and is a hybrid between a dependency tracker (think
'make') and a cluster engine.

More specifically, you construct Jobs, which encapsulate output (i.e.
stuff that needs to be done), invariants (which force re-evaluation of
output jobs if they change), and stuff inbetween (e.g. load data from
disk).

From your point of view, you create a pypipegraph, you create jobs,
chain them together, then ask the pypipegraph to run. It examines all
jobs for their need to run (either because the have not been finished,
or because they have been invalidated), distributes them across multiple
python instances, and get's them executed in a sensible order.

It is robust against jobs dying for whatever reason (only the failed job
and everything 'downstream' will be affected, independend jobs will
continue running), allows you to resume at any point 'in between' jobs,
and isolates jobs against each other.

pypipegraph supports Python 3 only.

## 30 second summary

```python
    pypipegraph.new_pipeline()
    output_filenameA = 'sampleA.txt'
    def do_the_work():
        op = open(output_filename, 'wb').write(""hello world"")
    jobA = pypipegraph.FileGeneratingJob(output_filenameA, do_the_work)
    output_filenameB = 'sampleB.txt'
    def do_the_work():
         op = open(output_filenameB, 'wb').write(open(output_filenameA, 'rb').read() + "",  once again"")
    jobB = pypipegraph.FileGeneratingJob(output_filenameB, do_the_work)
    jobB.depends_on(jobA)
    pypipegraph.run()
    print('the pipegraph is done and has returned control to you.')
    print('sampleA.txt contains ""hello world""')
    print('sampleB.txt contains ""hello world, once again"")
```
",2023-07-07 18:46:31+00:00
pyppl,pipen,pwwang/pipen,pipen - A pipeline framework for python,https://pwwang.github.io/pipen/,False,90,2023-03-26 12:07:01+00:00,2017-01-27 22:39:34+00:00,14,11,5,77,0.10.6,2023-06-29 07:23:33+00:00,Apache License 2.0,1162,0.10.6,77,2023-06-29 07:23:33+00:00,2023-06-29 07:27:26+00:00,2023-06-29 07:23:33+00:00,"<div align=""center"">
    <img src=""./pipen.png"" width=""320px"">

**A pipeline framework for python**

</div>

______________________________________________________________________

[![Pypi][6]][7] [![Github][8]][9] ![Building][10] [![Docs and API][11]][1] [![Codacy][12]][13] [![Codacy coverage][14]][13] [![Deps][5]][23]

[Documentation][1] | [ChangeLog][2] | [Examples][3] | [API][4]

## Features

- Easy to use
- Nearly zero-configuration
- Nice logging
- Highly extendable

## Installation

```bash
pip install -U pipen
```

## Quickstart

`example.py`

```python
from pipen import Proc, Pipen

class P1(Proc):
    """"""Sort input file""""""
    input = ""infile""
    input_data = [""/tmp/data.txt""]
    output = ""outfile:file:intermediate.txt""
    script = ""cat {{in.infile}} | sort > {{out.outfile}}""

class P2(Proc):
    """"""Paste line number""""""
    requires = P1
    input = ""infile""
    output = ""outfile:file:result.txt""
    script = ""paste <(seq 1 3) {{in.infile}} > {{out.outfile}}""

class MyPipeline(Pipen):
    starts = P1

if __name__ == ""__main__"":
    MyPipeline().run()
```

```shell
> echo -e ""3\n2\n1"" > /tmp/data.txt
> python example.py
```

```log
06-09 23:15:29 I core                  _____________________________________   __
06-09 23:15:29 I core                  ___  __ \___  _/__  __ \__  ____/__  | / /
06-09 23:15:29 I core                  __  /_/ /__  / __  /_/ /_  __/  __   |/ /
06-09 23:15:29 I core                  _  ____/__/ /  _  ____/_  /___  _  /|  /
06-09 23:15:29 I core                  /_/     /___/  /_/     /_____/  /_/ |_/
06-09 23:15:29 I core
06-09 23:15:29 I core                              version: 0.10.2
06-09 23:15:29 I core
06-09 23:15:29 I core    ╔══════════════════════════════════════════════════════════════════╗
06-09 23:15:29 I core    ║                            MYPIPELINE                            ║
06-09 23:15:29 I core    ╚══════════════════════════════════════════════════════════════════╝
06-09 23:15:29 I core    plugins         : verbose v0.7.0
06-09 23:15:29 I core    # procs         : 2
06-09 23:15:29 I core    profile         : default
06-09 23:15:29 I core    outdir          : /home/pwwang/github/pipen/MyPipeline-output
06-09 23:15:29 I core    cache           : True
06-09 23:15:29 I core    dirsig          : 1
06-09 23:15:29 I core    error_strategy  : ignore
06-09 23:15:29 I core    forks           : 1
06-09 23:15:29 I core    lang            : bash
06-09 23:15:29 I core    loglevel        : info
06-09 23:15:29 I core    num_retries     : 3
06-09 23:15:29 I core    scheduler       : local
06-09 23:15:29 I core    submission_batch: 8
06-09 23:15:29 I core    template        : liquid
06-09 23:15:29 I core    workdir         : /home/pwwang/github/pipen/.pipen/MyPipeline
06-09 23:15:29 I core    plugin_opts     :
06-09 23:15:29 I core    template_opts   :
06-09 23:15:31 I core
06-09 23:15:31 I core    ╭─────────────────────────────── P1 ───────────────────────────────╮
06-09 23:15:31 I core    │ Sort input file                                                  │
06-09 23:15:31 I core    ╰──────────────────────────────────────────────────────────────────╯
06-09 23:15:31 I core    P1: Workdir: '/home/pwwang/github/pipen/.pipen/MyPipeline/P1'
06-09 23:15:31 I core    P1: <<< [START]
06-09 23:15:31 I core    P1: >>> ['P2']
06-09 23:15:31 I verbose P1: size: 1
06-09 23:15:31 I verbose P1: [0/0] in.infile: /tmp/data.txt
06-09 23:15:31 I verbose P1: [0/0] out.outfile:
                 /home/pwwang/github/pipen/.pipen/MyPipeline/P1/0/output/intermediate.txt
06-09 23:15:33 I verbose P1: Time elapsed: 00:00:02.018s
06-09 23:15:33 I core
06-09 23:15:33 I core    ╭═══════════════════════════════ P2 ═══════════════════════════════╮
06-09 23:15:33 I core    ║ Paste line number                                                ║
06-09 23:15:33 I core    ╰══════════════════════════════════════════════════════════════════╯
06-09 23:15:33 I core    P2: Workdir: '/home/pwwang/github/pipen/.pipen/MyPipeline/P2'
06-09 23:15:33 I core    P2: <<< ['P1']
06-09 23:15:33 I core    P2: >>> [END]
06-09 23:15:33 I verbose P2: size: 1
06-09 23:15:33 I verbose P2: [0/0] in.infile:
                 /home/pwwang/github/pipen/.pipen/MyPipeline/P1/0/output/intermediate.txt
06-09 23:15:33 I verbose P2: [0/0] out.outfile:
                 /home/pwwang/github/pipen/MyPipeline-output/P2/result.txt
06-09 23:15:35 I verbose P2: Time elapsed: 00:00:02.009s
06-09 23:15:35 I core

              MYPIPELINE: 100%|█████████████████████████████| 2/2 [00:06<00:00, 0.36 procs/s]
```

```shell
> cat ./MyPipeline-output/P2/result.txt
1       1
2       2
3       3
```

## Examples

See more examples at `examples/` and a more realcase example at:
https://github.com/pwwang/pipen-report/tree/master/example

## Plugin gallery

Plugins make `pipen` even better.

- [`pipen-verbose`][15]: Add verbosal information in logs for pipen.
- [`pipen-lock`][25]: Process lock for pipen to prevent multiple runs at the same time.
- [`pipen-report`][16]: Generate report for pipen
- [`pipen-filters`][17]: Add a set of useful filters for pipen templates.
- [`pipen-diagram`][18]: Draw pipeline diagrams for pipen
- [`pipen-annotate`][26]: Use docstring to annotate pipen processes
- [`pipen-args`][19]: Command line argument parser for pipen
- [`pipen-dry`][20]: Dry runner for pipen pipelines
- [`pipen-log2file`][28]: Save running logs to file for pipen
- [`pipen-board`][27]: Visualize configuration and running of pipen pipelines on the web
- [`pipen-runinfo`][29]: Save running information to file for pipen
- [`pipen-cli-init`][21]: A pipen CLI plugin to create a pipen project (pipeline)
- [`pipen-cli-run`][22]: A pipen cli plugin to run a process or a pipeline
- [`pipen-cli-require`][24]: A pipen cli plugin check the requirements of a pipeline


[1]: https://pwwang.github.io/pipen
[2]: https://pwwang.github.io/pipen/CHANGELOG
[3]: https://pwwang.github.io/pipen/examples
[4]: https://pwwang.github.io/pipen/api/pipen
[5]: https://img.shields.io/librariesio/release/pypi/pipen?style=flat-square
[6]: https://img.shields.io/pypi/v/pipen?style=flat-square
[7]: https://pypi.org/project/pipen/
[8]: https://img.shields.io/github/v/tag/pwwang/pipen?style=flat-square
[9]: https://github.com/pwwang/pipen
[10]: https://img.shields.io/github/actions/workflow/status/pwwang/pipen/build.yml?style=flat-square
[11]: https://img.shields.io/github/actions/workflow/status/pwwang/pipen/docs.yml?label=docs&style=flat-square
[12]: https://img.shields.io/codacy/grade/cf1c6c97e5c4480386a05b42dec10c6e?style=flat-square
[13]: https://app.codacy.com/gh/pwwang/pipen
[14]: https://img.shields.io/codacy/coverage/cf1c6c97e5c4480386a05b42dec10c6e?style=flat-square
[15]: https://github.com/pwwang/pipen-verbose
[16]: https://github.com/pwwang/pipen-report
[17]: https://github.com/pwwang/pipen-filters
[18]: https://github.com/pwwang/pipen-diagram
[19]: https://github.com/pwwang/pipen-args
[20]: https://github.com/pwwang/pipen-dry
[21]: https://github.com/pwwang/pipen-cli-init
[22]: https://github.com/pwwang/pipen-cli-run
[23]: https://libraries.io/github/pwwang/pipen#repository_dependencies
[24]: https://github.com/pwwang/pipen-cli-require
[25]: https://github.com/pwwang/pipen-lock
[26]: https://github.com/pwwang/pipen-annotate
[27]: https://github.com/pwwang/pipen-board
[28]: https://github.com/pwwang/pipen-log2file
[29]: https://github.com/pwwang/pipen-runinfo
",2023-07-07 18:46:36+00:00
pypyr,pypyr,pypyr/pypyr,"pypyr task-runner cli & api for automation pipelines. Automate anything by combining commands, different scripts in different languages & applications into one pipeline process.",https://pypyr.io,False,531,2023-06-29 23:34:27+00:00,2017-03-24 11:26:15+00:00,25,15,4,81,v5.8.0,2023-03-13 01:34:36+00:00,Apache License 2.0,708,v5.8.0,93,2023-03-13 01:34:36+00:00,2023-06-29 23:34:28+00:00,2023-03-13 01:34:36+00:00,"![pypyr task runner for automation pipelines](https://pypyr.io/images/2x1/pypyr-taskrunner-yaml-pipeline-automation-1200x600.1bd2401e4f8071d85bcb1301128e4717f0f54a278e91c9c350051191de9d22c0.png)

# pypyr automation task runner
All documentation is here: <https://pypyr.io/>

[![build status](https://github.com/pypyr/pypyr/workflows/lint-test-build/badge.svg?branch=main)](https://github.com/pypyr/pypyr/actions)
[![coverage status](https://codecov.io/gh/pypyr/pypyr/branch/main/graph/badge.svg)](https://codecov.io/gh/pypyr/pypyr)
[![pypi version](https://badge.fury.io/py/pypyr.svg)](https://pypi.python.org/pypi/pypyr/)
[![apache 2.0 license](https://img.shields.io/github/license/pypyr/pypyr)](https://opensource.org/licenses/Apache-2.0)

*pypyr*

>   pronounce how you like, but I generally say *piper* as in ""piping
    down the valleys wild""

pypyr is a free & open-source task-runner that lets you define and run
sequential steps in a pipeline.

Like a turbo-charged shell script, but less finicky. Less annoying than
a makefile.

pypyr runs pipelines defined in yaml. A pipeline is pretty much anything
you want to automate with a sequence of steps.

Automate anything by combining commands, different scripts in different
languages & applications into one pipeline process.

You can run loops, conditionally execute steps based on conditions you
specify, wait for status changes before continuing, break on failure
conditions or swallow errors. Pretty useful for orchestrating continuous
integration, continuous deployment & devops operations.

pypyr gives you simple variable substitution & configuration file
management so you can read, merge and write configuration files to and
from yaml, json or just text.

## Installation

```console
$ pip install --upgrade pypyr
```

Tested against Python \>=3.7

pypyr runs on Linux, MacOS & Windows. pypyr also runs fine on CI servers &
containers - pretty much anywhere with a Python run-time will work.

## Usage
### This is a pipeline
Example pipeline that runs a sequence of steps and takes an optional
custom cli input argument:

```yaml
# ./show-me-what-you-got.yaml
context_parser: pypyr.parser.keyvaluepairs
steps:
  - name: pypyr.steps.echo
    in:
      echoMe: o hai!
  - name: pypyr.steps.cmd
    in:
      cmd: echo any cmd you like
  - name: pypyr.steps.shell
    in:
      cmd: echo ninja shell power | grep '^ninja.*r$' 
  - name: pypyr.steps.py
    in:
      py: print('any python you like')
  - name: pypyr.steps.cmd
    while:
      max: 3
    in:
      cmd: echo gimme a {whileCounter}
  - name: pypyr.steps.cmd
    foreach: [once, twice, thrice]
    in:
      cmd: echo say {i}
  - name: pypyr.steps.default
    in:
      defaults:
        sayBye: False
  - name: pypyr.steps.echo
    run: '{sayBye}'
    in:
      echoMe: k bye!
```

### This is how you run a pipeline
This is what happens when you run this pipeline:

```console
$ pypyr show-me-what-you-got
o hai!
any cmd you like
ninja shell power
any python you like
gimme a 1
gimme a 2
gimme a 3
say once
say twice
say thrice

$ pypyr show-me-what-you-got sayBye=true  
o hai!
any cmd you like
ninja shell power
any python you like
gimme a 1
gimme a 2
gimme a 3
say once
say twice
say thrice
k bye!
```

## Help!
Don't Panic! Check the [pypyr technical docs](https://pypyr.io/docs/)
to begin. For help, community & talk, check [pypyr
twitter](https://twitter.com/pypyrpipes/), or join the chat at the 
[pypyr community discussion forum](https://github.com/pypyr/pypyr/discussions)!

## Contribute
### Developers
For information on how to help with pypyr, run tests and coverage,
please do check out the [contribution guide](CONTRIBUTING.md).

### Bugs
Well, you know. No one's perfect. Feel free to [create an
issue](https://github.com/pypyr/pypyr/issues/new).

## License
pypyr is free & open-source software distributed under the Apache 2.0 License.

Please see [LICENSE file](LICENSE) in the root of the repo..

Copyright 2017 the pypyr contributors.
",2023-07-07 18:46:41+00:00
qsubsec,qsubsec,alastair-droop/qsubsec,Simple tokenised template system for SGE ,,False,9,2020-05-13 14:03:40+00:00,2015-07-08 13:42:33+00:00,1,1,1,4,3.0v28,2019-08-30 11:56:51+00:00,GNU General Public License v3.0,84,3.0v28,4,2019-08-30 11:56:51+00:00,,2019-08-30 11:56:51+00:00,"# A Simple SGE Template Preprocessor Engine


`qsubsec` is a template language for generating script files for submission using the [SGE grid system](https://arc.liv.ac.uk/trac/SGE). By using this system, you can separate the logic of your qsub jobs from the data required for a specific run.

## Overview

The qsubsec utility processes qsubsec-formatted template files to generate code sections that can be submitted for execution (for example through the SGE scheduler). Before processing, template files are checked for token placeholders and these are filled in. The resulting Python code is then executed to generate a set of sections.

The stages in template processing are:

* The template file and token file(s) are read;
* Any token placeholders found in the template file are filled in using the tokens file(s) read;
* The resulting Python code is executed to yield a set of sections;
* Each section is output in turn

Processed sections can be output in multiple formats (currently `bash` and `qsub`), allowing the same templates to be run on multiple different systems.

For more information on the TFF token definition syntax and the qsubsec template syntax, see the documentation directory.


## Installation

Installation should be as simple as:

~~~bash
git clone https://github.com/alastair-droop/qsubsec.git
cd qsubsec
python setup.py install
~~~

You will need python3 (>=3.3) to use qsubsec.

If you do not have admin privileges, you can install this locally using `python setup.py install --user`.

After installation, you can verify that you have the correct version using `qsubsec -v`.

Although `qsubsec` can be run on most machines, the qsub executable must be available and functional for automatic qsub submission to work (using the `-s` argument with the qsub format).

## Licence

These tools are released under the [GNU General Public License version 3](http://www.gnu.org/licenses/gpl.html).

## Reference

If you use qsubsec for any published work, please reference the [original paper](https://doi.org/10.1093/bioinformatics/btv698) as:

Alastair P. Droop; ""qsubsec: a lightweight template system for defining sun grid engine workflows"", *Bioinformatics*, Volume **32**, Issue 8, 15 April 2016, Pages 1267–1268, `https://doi.org/10.1093/bioinformatics/btv698`


BibTex:

~~~latex
@article{doi:10.1093/bioinformatics/btv698,
author = {Droop, Alastair P.},
title = {qsubsec: a lightweight template system for defining sun grid engine workflows},
journal = {Bioinformatics},
volume = {32},
number = {8},
pages = {1267-1268},
year = {2016},
doi = {10.1093/bioinformatics/btv698},
URL = { + http://dx.doi.org/10.1093/bioinformatics/btv698},
eprint = {/oup/backfile/content_public/journal/bioinformatics/32/8/10.1093_bioinformatics_btv698/3/btv698.pdf}
}
~~~

",2023-07-07 18:46:45+00:00
quiltdata,quilt,quiltdata/quilt,Quilt is a data mesh for connecting people with actionable data,https://quiltdata.com,False,1263,2023-07-07 02:37:59+00:00,2017-02-10 02:46:03+00:00,91,22,33,52,3.2.1,2020-10-15 04:51:57+00:00,Apache License 2.0,4269,v3.2.0,100,2020-09-08 20:02:21+00:00,2023-07-07 02:38:01+00:00,2023-07-07 01:35:08+00:00,"<!-- markdownlint-disable -->
[![docs on_gitbook](https://img.shields.io/badge/docs-on_gitbook-blue.svg?style=flat-square)](https://docs.quiltdata.com/)
[![chat on_slack](https://img.shields.io/badge/chat-on_slack-blue.svg?style=flat-square)](https://slack.quiltdata.com/)
[![codecov](https://codecov.io/gh/quiltdata/quilt/branch/master/graph/badge.svg)](https://codecov.io/gh/quiltdata/quilt)
[![pypi](https://img.shields.io/pypi/v/quilt3.svg?style=flat-square)](https://pypi.org/project/quilt3/)

# Quilt is a self-organizing data hub

## Python Quick start, tutorials
If you have Python and an S3 bucket, you're ready to create versioned datasets with Quilt.
Visit the [Quilt docs](https://docs.quiltdata.com/installation) for installation instructions,
a quick start, and more.

## Quilt in action
* [open.quiltdata.com](https://open.quiltdata.com/) is a petabyte-scale open
data portal that runs on Quilt
* [quiltdata.com](https://quiltdata.com) includes case studies, use cases, videos,
and instructions on how to run a private Quilt instance
* [Versioning data and models for rapid experimentation in machine learning](https://medium.com/pytorch/how-to-iterate-faster-in-machine-learning-by-versioning-data-and-models-featuring-detectron2-4fd2f9338df5)
shows how to use Quilt for real world projects

## Who is Quilt for?
Quilt is for data-driven teams and offers features for coders (data scientists,
data engineers, developers) and business users alike.

## What does Quilt do?
Quilt manages data like code so that teams in machine learning, biotech,
and analytics can experiment faster, build smarter models, and recover from errors.

## How does Quilt work?
Quilt consists of a Python client, web catalog, lambda
functions&mdash;all of which are open source&mdash;plus
a suite of backend services and Docker containers
orchestrated by CloudFormation.

The backend services are available under a paid license
on [quiltdata.com](https://quiltdata.com).

## Use cases
* **Share** data at scale. Quilt wraps AWS S3 to add simple URLs, web preview for large files, and sharing via email address (no need to create an IAM role).
* **Understand** data better through inline documentation (Jupyter notebooks, markdown) and visualizations (Vega, Vega Lite)
* **Discover** related data by indexing objects in ElasticSearch
* **Model** data by providing a home for large data and models that don't fit in git, and by providing immutable versions for objects and data sets (a.k.a. ""Quilt Packages"")
* **Decide** by broadening data access within the organization and supporting the documentation of decision processes through audit-able versioning and inline documentation
",2023-07-07 18:46:49+00:00
rabix,rabix,rabix/rabix,[Historical] Reproducible Analyses for Bioinformatics,http://rabix.io,True,107,2023-01-28 18:31:11+00:00,2014-06-09 16:37:45+00:00,26,29,10,0,,,GNU Affero General Public License v3.0,812,0.6.5,10,2015-03-17 18:35:01+00:00,,2019-03-11 19:13:04+00:00,"# Rabix has grown up!

**Reproducible Analyses for Bioinformatics is now a reality!**

Please see
- [Common Workflow Language](https://www.commonwl.org/) ([github](https://github.com/common-workflow-language/common-workflow-language)) for the specfication that the language part of Rabix merged into.
- [Rabix Composer](https://github.com/rabix/composer) for a visual editor for CWL workflows
- This list of [executors/runners](https://www.commonwl.org/#Implementations) that can run workflows in CWL on anything from your laptop to your HPC to your cloud.

If you want to see the old documentation for the project please see [here](OLDREADME.md)
",2023-07-07 18:46:54+00:00
rain,rain,substantic/rain,Framework for large distributed pipelines,https://substantic.github.io/rain/docs/,False,722,2023-06-29 15:34:00+00:00,2018-02-13 17:04:46+00:00,52,44,9,10,v0.4.0,2018-08-31 22:08:45+00:00,MIT License,738,v0.4.0,11,2018-08-31 22:08:45+00:00,2023-06-29 15:34:00+00:00,2018-12-16 16:29:10+00:00,"# Rain

[![](https://img.shields.io/github/license/substantic/rain.svg)](https://github.com/substantic/rain/blob/master/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/rain-python.svg)](https://pypi.org/project/rain-python/)
[![Crates.io](https://img.shields.io/crates/v/rain_core.svg)](https://www.crates.io/crates/rain_core)
[![Build Status](https://travis-ci.org/substantic/rain.svg?branch=master)](https://travis-ci.org/substantic/rain)
[![Gitter](https://badges.gitter.im/substantic/rain.svg)](https://gitter.im/substantic/rain?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

<img align=""right"" width=""35%"" src=""docs/imgs/logo.svg?sanitize=true"">

**Rain** is an open-source distributed computational framework for processing
of large-scale task-based pipelines.

Rain aims to lower the entry barrier to the world of distributed computing. Our
intention is to provide a light yet robust distributed framework that features
an intuitive Python API, straightforward installation and deployment with
insightful monitoring on top.

> Despite that this is an early release of Rain, it is a fully functional
> project that can be used out-of-the box. Being aware that there is still
> a lot that can be improved and added, we are looking for external
> users and collaborators to help to move this work forward.
> Talk to us online at Gitter or via email and let us know what your
> projects and use-cases need, submit bugs or feature
> requests at GitHub or even contribute with pull requests.

## Features

- **Dataflow programming.** Computation in Rain is defined as a flow graph of
  tasks. Tasks may be built-in functions, Python/C++/Rust code, or an external
  applications, short and light or long-running and heavy. The system is
  designed to integrate any code into a pipeline, respecting its resource
  requirements, and to handle very large task graphs (hundreds thousands tasks).

- **Easy to use.** Rain was designed to be easy to deployed anywhere, ranging
  from a single node deployments to large-scale distributed systems and clouds
  ranging thousands of cores.

- **Rust core, Python/C++/Rust API.** Rain is written in Rust for safety and
  efficiency and has a high-level Python API to Rain core infrastructure, and
  even supports Python tasks out-of-the-box. Rain also provides libraries for
  writing own tasks in C++ and Rust.

- **Monitoring.** Rain is designed to support both online and postmortem
  monitoring.

  ![Dashboard screencast](docs/imgs/rain-dashboard.gif)

## Documentation

[Overview](http://substantic.github.io/rain/docs/overview.html) &bull; [Quickstart](http://substantic.github.io/rain/docs/quickstart.html) &bull; [User guide](http://substantic.github.io/rain/docs/user.html) &bull; [Python API](http://substantic.github.io/rain/docs/python_api.html) &bull; [Examples](http://substantic.github.io/rain/docs/examples.html)

## Quick start

- Download binary

```
$ wget https://github.com/substantic/rain/releases/download/v0.4.0/rain-v0.4.0-linux-x64.tar.xz
$ tar xvf rain-v0.4.0-linux-x64.tar.xz
```

- Install Python API

```
$ pip3 install rain-python
```

- Start server & a single local governor

```
$ ./rain-v0.4.0-linux-x64/rain start --simple
```

- Rain ""Hello world"" in Python

```python
from rain.client import Client, tasks, blob

client = Client(""localhost"", 7210)

with client.new_session() as session:
    task = tasks.Concat((blob(""Hello ""), blob(""world!"")))
    task.output.keep()
    session.submit()
    result = task.output.fetch().get_bytes()
    print(result)
```

### Installation via cargo

If you have installed Rust, you can install and start Rain as follows:

```
$ cargo install rain_server

$ pip3 install rain-python

$ rain start --simple
```

[Read the docs](http://substantic.github.io/rain/docs/examples.html) for more examples.
",2023-07-07 18:46:58+00:00
ray,ray,ray-project/ray,Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for accelerating ML workloads.,https://ray.io,False,26500,2023-07-07 18:13:38+00:00,2016-10-25 19:38:30+00:00,4572,450,419,67,ray-2.5.1,2023-06-13 04:55:33+00:00,Apache License 2.0,18109,ray-2.5.1,75,2023-06-13 04:55:33+00:00,2023-07-07 18:42:58+00:00,2023-07-07 18:24:07+00:00,".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png

.. image:: https://readthedocs.org/projects/ray/badge/?version=master
    :target: http://docs.ray.io/en/master/?badge=master

.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue
    :target: https://forms.gle/9TSdDYUgxYs8SA9e8

.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue
    :target: https://discuss.ray.io/

.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter
    :target: https://twitter.com/raydistributed

|

Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for simplifying ML compute:

.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg

..
  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit

Learn more about `Ray AIR`_ and its libraries:

- `Data`_: Scalable Datasets for ML
- `Train`_: Distributed Training
- `Tune`_: Scalable Hyperparameter Tuning
- `RLlib`_: Scalable Reinforcement Learning
- `Serve`_: Scalable and Programmable Serving

Or more about `Ray Core`_ and its key abstractions:

- `Tasks`_: Stateless functions executed in the cluster.
- `Actors`_: Stateful worker processes created in the cluster.
- `Objects`_: Immutable values accessible across the cluster.

Monitor and debug Ray applications and clusters using the `Ray dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.

Ray runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing
`ecosystem of community integrations`_.

Install Ray with: ``pip install ray``. For nightly wheels, see the
`Installation page <https://docs.ray.io/en/latest/installation.html>`__.

.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html
.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html
.. _`Workflow`: https://docs.ray.io/en/latest/workflows/concepts.html
.. _`Train`: https://docs.ray.io/en/latest/train/train.html
.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html
.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html
.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html


Why Ray?
--------

Today's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.

Ray is a unified way to scale Python and AI applications from a laptop to a cluster.

With Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.

More Information
----------------

- `Documentation`_
- `Ray Architecture whitepaper`_
- `Ray AIR Technical whitepaper`_
- `Exoshuffle: large-scale data shuffle in Ray`_
- `Ownership: a distributed futures system for fine-grained tasks`_
- `RLlib paper`_
- `Tune paper`_

*Older documents:*

- `Ray paper`_
- `Ray HotOS paper`_
- `Ray Architecture v1 whitepaper`_

.. _`Ray AIR`: https://docs.ray.io/en/latest/ray-air/getting-started.html
.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html
.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html
.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html
.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html
.. _`Documentation`: http://docs.ray.io/en/latest/index.html
.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview
.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview
.. _`Ray AIR Technical whitepaper`: https://docs.google.com/document/d/1bYL-638GN6EeJ45dPuLiPImA8msojEDDKiBx3YzB4_s/preview
.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072
.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf
.. _`Ray paper`: https://arxiv.org/abs/1712.05889
.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924
.. _`RLlib paper`: https://arxiv.org/abs/1712.09381
.. _`Tune paper`: https://arxiv.org/abs/1807.05118

Getting Involved
----------------

.. list-table::
   :widths: 25 50 25 25
   :header-rows: 1

   * - Platform
     - Purpose
     - Estimated Response Time
     - Support Level
   * - `Discourse Forum`_
     - For discussions about development and questions about usage.
     - < 1 day
     - Community
   * - `GitHub Issues`_
     - For reporting bugs and filing feature requests.
     - < 2 days
     - Ray OSS Team
   * - `Slack`_
     - For collaborating with other Ray users.
     - < 2 days
     - Community
   * - `StackOverflow`_
     - For asking questions about how to use Ray.
     - 3-5 days
     - Community
   * - `Meetup Group`_
     - For learning about Ray projects and best practices.
     - Monthly
     - Ray DevRel
   * - `Twitter`_
     - For staying up-to-date on new features.
     - Daily
     - Ray DevRel

.. _`Discourse Forum`: https://discuss.ray.io/
.. _`GitHub Issues`: https://github.com/ray-project/ray/issues
.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray
.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/
.. _`Twitter`: https://twitter.com/raydistributed
.. _`Slack`: https://forms.gle/9TSdDYUgxYs8SA9e8

",2023-07-07 18:47:02+00:00
reana,reana,reanahub/reana,REANA: Reusable research data analysis platform,https://docs.reana.io,False,112,2023-06-30 00:55:55+00:00,2017-01-16 10:00:18+00:00,43,19,17,35,0.9.1-alpha.3,2023-06-26 08:38:28+00:00,MIT License,492,v0.6.1,35,2020-06-09 12:38:16+00:00,2023-07-07 12:39:42+00:00,2023-06-29 12:51:47+00:00,".. image:: docs/logo-reana.png
   :target: http://docs.reana.io
   :align: center

===========================
 REANA - Reusable Analyses
===========================

.. image:: https://github.com/reanahub/reana/workflows/CI/badge.svg
   :target: https://github.com/reanahub/reana/actions

.. image:: https://readthedocs.org/projects/reana/badge/?version=latest
   :target: https://reana.readthedocs.io/en/latest/?badge=latest

.. image:: https://codecov.io/gh/reanahub/reana/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/reanahub/reana

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :target: https://gitter.im/reanahub/reana?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge

.. image:: https://img.shields.io/github/license/reanahub/reana.svg
   :target: https://github.com/reanahub/reana/blob/master/LICENSE

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/psf/black

About
-----

`REANA <http://www.reana.io>`_ is a reusable and reproducible research data
analysis platform. It helps researchers to structure their input data, analysis
code, containerised environments and computational workflows so that the
analysis can be instantiated and run on remote compute clouds.

REANA was born to target the use case of particle physics analyses, but is
applicable to any scientific discipline. The system paves the way towards
reusing and reinterpreting preserved data analyses even several years after the
original publication.

Features
--------

- structure research data analysis in reusable manner
- instantiate computational workflows on remote clouds
- rerun analyses with modified input data, parameters or code
- support for several compute clouds (Kubernetes/OpenStack)
- support for several workflow specifications (CWL, Serial, Yadage, Snakemake)
- support for several shared storage systems (Ceph)
- support for several container technologies (Docker)

Getting started
---------------

You can `install REANA locally <https://docs.reana.io/administration/deployment/deploying-locally/>`_, `deploy it at scale on premises
<https://docs.reana.io/administration/deployment/deploying-at-scale/>`_ (in about 10 minutes) or use https://reana.cern.ch. Once the system
is ready, you can follow the guide to run `your first example <https://docs.reana.io/getting-started/first-example/>`_.
For more in depth information visit the `official REANA documentation <https://docs.reana.io/>`_.

Community
---------

- Discuss `on Forum <https://forum.reana.io/>`_
- Chat on `Mattermost <https://mattermost.web.cern.ch/it-dep/channels/reana>`_ or `Gitter <https://gitter.im/reanahub/reana>`_
- Follow us `on Twitter <https://twitter.com/reanahub>`_

Useful links
------------

- `REANA home page <http://www.reana.io/>`_
- `REANA documentation <http://docs.reana.io/>`_
- `REANA on DockerHub <https://hub.docker.com/u/reanahub/>`_
",2023-07-07 18:47:07+00:00
redun,redun,insitro/redun,Yet another redundant workflow engine,https://insitro.github.io/redun/,False,421,2023-07-07 10:16:46+00:00,2021-11-04 00:14:09+00:00,31,12,29,0,,,Apache License 2.0,317,,0,,2023-07-07 10:16:46+00:00,2023-07-04 21:32:13+00:00,"<img src=""docs/source/_static/redun.svg"" width=""200""/>

*yet another redundant workflow engine*

**redun** aims to be a more expressive and efficient workflow framework, built on top of the popular Python programming language. It takes the somewhat contrarian view that writing dataflows directly is unnecessarily restrictive, and by doing so we lose abstractions we have come to rely on in most modern high-level languages (control flow, composability, recursion, high order functions, etc). redun's key insight is that workflows can be expressed as [lazy expressions](#whats-the-trick), which are then evaluated by a scheduler that performs automatic parallelization, caching, and data provenance logging.

redun's key features are:

- Workflows are defined by lazy expressions that when evaluated emit dynamic directed acyclic graphs (DAGs), enabling complex data flows.
- Incremental computation that is reactive to both data changes as well as code changes.
- Workflow tasks can be executed on a variety of compute backend (threads, processes, AWS batch jobs, Spark jobs, etc). 
- Data changes are detected for in-memory values as well as external data sources such as files and object stores using file hashing.
- Code changes are detected by hashing individual Python functions and comparing them against historical call graph recordings.
- Past intermediate results are cached centrally and reused across workflows.
- Past call graphs can be used as a data lineage record and can be queried for debugging and auditing.

To learn more, see our [Medium](https://insitro.medium.com/when-data-science-goes-with-the-flow-insitro-introduces-redun-8b06b707a14b) and [AWS HPC](https://aws.amazon.com/blogs/hpc/data-science-workflows-at-insitro-using-redun-on-aws-batch/) blog posts, as well as our [documentation](https://insitro.github.io/redun/design.html), [tutorial](examples/README.md), and [influences](https://insitro.github.io/redun/design.html#influences).

*About the name:* The name ""redun"" is self-deprecating (there are [A LOT](https://github.com/pditommaso/awesome-pipeline) of workflow engines), but it is also a reference to its original inspiration, the [redo](https://apenwarr.ca/log/20101214) build system.

## Install

```sh
pip install redun
```

See [developing](https://insitro.github.io/redun/developing.html) for more information on working with the code.

### Postgres backend

To use postgres as a recording backend, use

```sh
pip install redun[postgres]
```

The above assumes the following dependencies are installed:
* `pg_config` (in the `postgresql-devel` package; on ubuntu: `apt-get install libpq-dev`)
* `gcc` (on ubuntu or similar `sudo apt-get install gcc`)

### Optional Visualization

To generate graphviz images and dot files, use

```sh
pip install redun[viz]
```

The above assumes the following dependencies are installed:
* `graphviz` (on ubuntu: `apt-get install graphviz graphviz-dev`, via homebrew: `brew install graphviz`)
* `gcc` (on ubuntu or similar `sudo apt-get install gcc`)

## Use cases

redun's general approach to defining workflows makes it a good choice for implementing workflows for a wide-variety of use cases:

- [Bioinformatics](examples/06_bioinfo_batch/)
- [Cheminformatics](examples/aws_glue/rdkit_workflow.py)
- [Web or API data extraction](examples/scraping/)
- [General data science](examples/word_count/)
- [And much more](examples/)

## Small taste

Here is a quick example of using redun for a familiar workflow, compiling a C program ([full example](examples/02_compile/README.md)). In general, any kind of data processing could be done within each task (e.g. reading and writing CSVs, DataFrames, databases, APIs).

```py
# make.py

import os
from typing import Dict, List

from redun import task, File


redun_namespace = ""redun.examples.compile""


@task()
def compile(c_file: File) -> File:
    """"""
    Compile one C file into an object file.
    """"""
    os.system(f""gcc -c {c_file.path}"")
    return File(c_file.path.replace("".c"", "".o""))


@task()
def link(prog_path: str, o_files: List[File]) -> File:
    """"""
    Link several object files together into one program.
    """"""
    o_files="" "".join(o_file.path for o_file in o_files)
    os.system(f""gcc -o {prog_path} {o_files}"")
    return File(prog_path)


@task()
def make_prog(prog_path: str, c_files: List[File]) -> File:
    """"""
    Compile one program from its source C files.
    """"""
    o_files = [
        compile(c_file)
        for c_file in c_files
    ]
    prog_file = link(prog_path, o_files)
    return prog_file


# Definition of programs and their source C files.
files = {
    ""prog"": [
        File(""prog.c""),
        File(""lib.c""),
    ],
    ""prog2"": [
        File(""prog2.c""),
        File(""lib.c""),
    ],
}


@task()
def make(files : Dict[str, List[File]] = files) -> List[File]:
    """"""
    Top-level task for compiling all the programs in the project.
    """"""
    progs = [
        make_prog(prog_path, c_files)
        for prog_path, c_files in files.items()
    ]
    return progs
```

Notice, that besides the `@task` decorator, the code follows typical Python conventions and is organized like a sequential program.

We can run the workflow using the `redun run` command:

```
redun run make.py make

[redun] redun :: version 0.4.15
[redun] config dir: /Users/rasmus/projects/redun/examples/compile/.redun
[redun] Upgrading db from version -1.0 to 2.0...
[redun] Start Execution 69c40fe5-c081-4ca6-b232-e56a0a679d42:  redun run make.py make
[redun] Run    Job 72bdb973:  redun.examples.compile.make(files={'prog': [File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=a2e6cbd9)], 'prog2': [File(path=prog2.c, hash=c748e4c7), File(path=lib.c, hash=a2e6cbd9)]}) on default
[redun] Run    Job 096be12b:  redun.examples.compile.make_prog(prog_path='prog', c_files=[File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=a2e6cbd9)]) on default
[redun] Run    Job 32ed5cf8:  redun.examples.compile.make_prog(prog_path='prog2', c_files=[File(path=prog2.c, hash=c748e4c7), File(path=lib.c, hash=a2e6cbd9)]) on default
[redun] Run    Job dfdd2ee2:  redun.examples.compile.compile(c_file=File(path=prog.c, hash=dfa3aba7)) on default
[redun] Run    Job 225f924d:  redun.examples.compile.compile(c_file=File(path=lib.c, hash=a2e6cbd9)) on default
[redun] Run    Job 3f9ea7ae:  redun.examples.compile.compile(c_file=File(path=prog2.c, hash=c748e4c7)) on default
[redun] Run    Job a8b21ec0:  redun.examples.compile.link(prog_path='prog', o_files=[File(path=prog.o, hash=4934098e), File(path=lib.o, hash=7caa7f9c)]) on default
[redun] Run    Job 5707a358:  redun.examples.compile.link(prog_path='prog2', o_files=[File(path=prog2.o, hash=cd0b6b7e), File(path=lib.o, hash=7caa7f9c)]) on default
[redun]
[redun] | JOB STATUS 2021/06/18 10:34:29
[redun] | TASK                             PENDING RUNNING  FAILED  CACHED    DONE   TOTAL
[redun] |
[redun] | ALL                                    0       0       0       0       8       8
[redun] | redun.examples.compile.compile         0       0       0       0       3       3
[redun] | redun.examples.compile.link            0       0       0       0       2       2
[redun] | redun.examples.compile.make            0       0       0       0       1       1
[redun] | redun.examples.compile.make_prog       0       0       0       0       2       2
[redun]
[File(path=prog, hash=a8d14a5e), File(path=prog2, hash=04bfff2f)]
```

This should have taken three C source files (`lib.c`, `prog.c`, and `prog2.c`), compiled them to three object files (`lib.o`, `prog.o`, `prog2.o`), and then linked them into two binaries (`prog` and `prog2`). Specifically, redun automatically determined the following dataflow DAG and performed the compiling and linking steps in separate threads:

<p align=""center"">
  <img width=""400"" src=""examples/02_compile/images/compile-dag.svg"">
</p>

Using the `redun log` command, we can see the full job tree of the most recent execution (denoted `-`):

```
redun log -

Exec 69c40fe5-c081-4ca6-b232-e56a0a679d42 [ DONE ] 2021-06-18 10:34:28:  run make.py make
Duration: 0:00:01.47

Jobs: 8 (DONE: 8, CACHED: 0, FAILED: 0)
--------------------------------------------------------------------------------
Job 72bdb973 [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.make(files={'prog': [File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=a2e6cbd9)], 'prog2': [File(path=prog2.c, hash=c748e4c7), Fil
  Job 096be12b [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.make_prog('prog', [File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=a2e6cbd9)])
    Job dfdd2ee2 [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.compile(File(path=prog.c, hash=dfa3aba7))
    Job 225f924d [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.compile(File(path=lib.c, hash=a2e6cbd9))
    Job a8b21ec0 [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.link('prog', [File(path=prog.o, hash=4934098e), File(path=lib.o, hash=7caa7f9c)])
  Job 32ed5cf8 [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.make_prog('prog2', [File(path=prog2.c, hash=c748e4c7), File(path=lib.c, hash=a2e6cbd9)])
    Job 3f9ea7ae [ DONE ] 2021-06-18 10:34:28:  redun.examples.compile.compile(File(path=prog2.c, hash=c748e4c7))
    Job 5707a358 [ DONE ] 2021-06-18 10:34:29:  redun.examples.compile.link('prog2', [File(path=prog2.o, hash=cd0b6b7e), File(path=lib.o, hash=7caa7f9c)])
```

Notice, redun automatically detected that `lib.c` only needed to be compiled once and that its result can be reused (a form of [common subexpression elimination](https://en.wikipedia.org/wiki/Common_subexpression_elimination)).

Using the `--file` option, we can see all files (or URLs) that were read, `r`, or written, `w`, by the workflow:

```
redun log --file

File 2b6a7ce0 2021-06-18 11:41:42 r  lib.c
File d90885ad 2021-06-18 11:41:42 rw lib.o
File 2f43c23c 2021-06-18 11:41:42 w  prog
File dfa3aba7 2021-06-18 10:34:28 r  prog.c
File 4934098e 2021-06-18 10:34:28 rw prog.o
File b4537ad7 2021-06-18 11:41:42 w  prog2
File c748e4c7 2021-06-18 10:34:28 r  prog2.c
File cd0b6b7e 2021-06-18 10:34:28 rw prog2.o
```

We can also look at the provenance of a single file, such as the binary `prog`:

```
redun log prog

File 2f43c23c 2021-06-18 11:41:42 w  prog
Produced by Job a8b21ec0

  Job a8b21ec0-e60b-4486-bcf4-4422be265608 [ DONE ] 2021-06-18 11:41:42:  redun.examples.compile.link('prog', [File(path=prog.o, hash=4934098e), File(path=lib.o, hash=d90885ad)])
  Traceback: Exec 4a2b624d > (1 Job) > Job 2f8b4b5f make_prog > Job a8b21ec0 link
  Duration: 0:00:00.24

    CallNode 6c56c8d472dc1d07cfd2634893043130b401dc84 redun.examples.compile.link
      Args:   'prog', [File(path=prog.o, hash=4934098e), File(path=lib.o, hash=d90885ad)]
      Result: File(path=prog, hash=2f43c23c)

    Task a20ef6dc2ab4ed89869514707f94fe18c15f8f66 redun.examples.compile.link

      def link(prog_path: str, o_files: List[File]) -> File:
          """"""
          Link several object files together into one program.
          """"""
          o_files="" "".join(o_file.path for o_file in o_files)
          os.system(f""gcc -o {prog_path} {o_files}"")
          return File(prog_path)


    Upstream dataflow:

      result = File(path=prog, hash=2f43c23c)

      result <-- <6c56c8d4> link(prog_path, o_files)
        prog_path = <ee510692> 'prog'
        o_files   = <f1eaf150> [File(path=prog.o, hash=4934098e), File(path=lib.o, hash=d90885ad)]

      prog_path <-- argument of <a4ac4959> make_prog(prog_path, c_files)
                <-- origin

      o_files <-- derives from
        compile_result   = <d90885ad> File(path=lib.o, hash=d90885ad)
        compile_result_2 = <4934098e> File(path=prog.o, hash=4934098e)

      compile_result <-- <45054a8f> compile(c_file)
        c_file = <2b6a7ce0> File(path=lib.c, hash=2b6a7ce0)

      c_file <-- argument of <a4ac4959> make_prog(prog_path, c_files)
             <-- argument of <a9a6af53> make(files)
             <-- origin

      compile_result_2 <-- <8d85cebc> compile(c_file_2)
        c_file_2 = <dfa3aba7> File(path=prog.c, hash=dfa3aba7)

      c_file_2 <-- argument of <74cceb4e> make_prog(prog_path, c_files)
               <-- argument of <45400ab5> make(files)
               <-- origin
```

This output shows the original `link` task source code responsible for creating the program `prog`, as well as the full derivation, denoted ""upstream dataflow"". See the full example for a [deeper explanation](examples/02_compile#data-provenance-for-files) of this output. To understand more about the data structure that powers these kinds of queries, see [call graphs](https://insitro.github.io/redun/design.html#call-graphs).

We can change one of the input files, such as `lib.c`, and rerun the workflow. Due to redun's automatic incremental compute, only the minimal tasks are rerun:

```
redun run make.py make

[redun] redun :: version 0.4.15
[redun] config dir: /Users/rasmus/projects/redun/examples/compile/.redun
[redun] Start Execution 4a2b624d-b6c7-41cb-acca-ec440c2434db:  redun run make.py make
[redun] Run    Job 84d14769:  redun.examples.compile.make(files={'prog': [File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=2b6a7ce0)], 'prog2': [File(path=prog2.c, hash=c748e4c7), File(path=lib.c, hash=2b6a7ce0)]}) on default
[redun] Run    Job 2f8b4b5f:  redun.examples.compile.make_prog(prog_path='prog', c_files=[File(path=prog.c, hash=dfa3aba7), File(path=lib.c, hash=2b6a7ce0)]) on default
[redun] Run    Job 4ae4eaf6:  redun.examples.compile.make_prog(prog_path='prog2', c_files=[File(path=prog2.c, hash=c748e4c7), File(path=lib.c, hash=2b6a7ce0)]) on default
[redun] Cached Job 049a0006:  redun.examples.compile.compile(c_file=File(path=prog.c, hash=dfa3aba7)) (eval_hash=434cbbfe)
[redun] Run    Job 0f8df953:  redun.examples.compile.compile(c_file=File(path=lib.c, hash=2b6a7ce0)) on default
[redun] Cached Job 98d24081:  redun.examples.compile.compile(c_file=File(path=prog2.c, hash=c748e4c7)) (eval_hash=96ab0a2b)
[redun] Run    Job 8c95f048:  redun.examples.compile.link(prog_path='prog', o_files=[File(path=prog.o, hash=4934098e), File(path=lib.o, hash=d90885ad)]) on default
[redun] Run    Job 9006bd19:  redun.examples.compile.link(prog_path='prog2', o_files=[File(path=prog2.o, hash=cd0b6b7e), File(path=lib.o, hash=d90885ad)]) on default
[redun]
[redun] | JOB STATUS 2021/06/18 11:41:43
[redun] | TASK                             PENDING RUNNING  FAILED  CACHED    DONE   TOTAL
[redun] |
[redun] | ALL                                    0       0       0       2       6       8
[redun] | redun.examples.compile.compile         0       0       0       2       1       3
[redun] | redun.examples.compile.link            0       0       0       0       2       2
[redun] | redun.examples.compile.make            0       0       0       0       1       1
[redun] | redun.examples.compile.make_prog       0       0       0       0       2       2
[redun]
[File(path=prog, hash=2f43c23c), File(path=prog2, hash=b4537ad7)]
```

Notice, two of the compile jobs are cached (`prog.c` and `prog2.c`), but compiling the library `lib.c` and the downstream link steps are correctly rerun.

Check out the [examples](examples/) for more example workflows and features of redun. Also, see the [design notes](https://insitro.github.io/redun/design.html) for more information on redun's design.

## Data provenance exploration

All workflow executions are recorded into a database that can be explored using the [Console (TUI)](https://insitro.github.io/redun/design.html#call-graphs). The Console is convenient for debugging large complex workflows, as well as understanding how to reproduce and extend past work.

<a href=""docs/source/_static/console-execution.svg""><img width=""45%"" src=""docs/source/_static/console-execution.svg""> <a href=""docs/source/_static/console-job.svg""><img width=""45%"" src=""docs/source/_static/console-job.svg"">

## Mixed compute backends

In the above example, each task ran in its own thread. However, more generally each task can run in its own process, Docker container, [AWS Batch job](examples/05_aws_batch), or [Spark job](examples/aws_glue). With [minimal configuration](examples/05_aws_batch/.redun/redun.ini), users can lightly annotate where they would like each task to run. redun will automatically handle the data and code movement as well as backend scheduling:

```py
@task(executor=""process"")
def a_process_task(a):
    # This task runs in its own process.
    b = a_batch_task(a)
    c = a_spark_task(b)
    return c

@task(executor=""batch"", memory=4, vcpus=5)
def a_batch_task(a):
    # This task runs in its own AWS Batch job.
    # ...

@task(executor=""spark"")
def a_spark_task(b):
    # This task runs in its own Spark job.
    sc = get_spark_context()
    # ...
```

See the [executor documentation](https://insitro.github.io/redun/executors.html) for more.

## What's the trick?

How did redun automatically perform parallel compute, caching, and data provenance in the example above? The trick is that redun builds up an [expression graph](https://en.wikipedia.org/wiki/Abstract_semantic_graph) representing the workflow and evaluates the expressions using [graph reduction](https://en.wikipedia.org/wiki/Graph_reduction). For example, the workflow above went through the following evaluation process:

<img width=""800"" src=""examples/02_compile/images/expression-graph.svg"">

For a more in-depth walk-through, see the [scheduler tutorial](examples/03_scheduler).

## Why not another workflow engine?

redun focuses on making multi-domain scientific pipelines easy to develop and deploy. The automatic parallelism, caching, code, and data reactivity, as well as data provenance features, make it a great fit for such work. However, redun does not attempt to solve all possible workflow problems, so it's perfectly reasonable to supplement it with other tools. For example, while redun provides a very expressive way to define [task parallelism](https://en.wikipedia.org/wiki/Task_parallelism), it does not attempt to perform the kind of fine-grain [data parallelism](https://en.wikipedia.org/wiki/Data_parallelism) more commonly provided by Spark or Dask. Fortunately, redun does not perform any ""dirty tricks"" (e.g. complex static analysis or call stack manipulation), and so we have found it possible to safely combine redun with other frameworks (e.g. pyspark, pytorch, Dask, etc) to achieve the benefits of each tool.

Lastly, redun does not provide its own compute cluster but instead builds upon other systems that do, such as cloud provider services for batch Docker jobs or Spark jobs.

For more details on how redun compares to other related ideas, see the [influences](https://insitro.github.io/redun/design.html#influences) section.
",2023-07-07 18:47:12+00:00
reflow,reflow,grailbio/reflow,"A language and runtime for distributed, incremental data processing in the cloud",,False,936,2023-07-03 14:32:43+00:00,2017-10-20 03:37:20+00:00,52,48,20,28,reflow1.29.0,2023-06-20 18:09:18+00:00,Apache License 2.0,1128,reflow1.29.0,28,2023-06-20 18:09:18+00:00,2023-07-03 14:32:43+00:00,2023-06-20 18:09:18+00:00,"![Reflow](reflow.svg)

[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/grailbio/reflow?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/grailbio/reflow.svg?branch=master)](https://travis-ci.org/grailbio/reflow)

Reflow is a system for incremental data processing in the cloud.
Reflow enables scientists and engineers to compose existing tools
(packaged in Docker images) using ordinary programming constructs.
Reflow then evaluates these programs in a cloud environment,
transparently parallelizing work and memoizing results. Reflow was
created at [GRAIL](http://grail.com/) to manage our NGS (next
generation sequencing) bioinformatics workloads on
[AWS](https://aws.amazon.com), but has also been used for many other
applications, including model training and ad-hoc data analyses.

Reflow comprises:

- a functional, lazy, type-safe domain specific language for writing workflow programs;
- a runtime for evaluating Reflow programs [incrementally](https://en.wikipedia.org/wiki/Incremental_computing), coordinating cluster execution, and transparent memoization;
- a cluster scheduler to dynamically provision and tear down resources from a cloud provider (AWS currently supported).

Reflow thus allows scientists and engineers to write straightforward
programs and then have them transparently executed in a cloud
environment. Programs are automatically parallelized and distributed
across multiple machines, and redundant computations (even across
runs and users) are eliminated by its memoization cache. Reflow
evaluates its programs
[incrementally](https://en.wikipedia.org/wiki/Incremental_computing):
whenever the input data or program changes, only those outputs that
depend on the changed data or code are recomputed.

In addition to the default cluster computing mode, Reflow programs
can also be run locally, making use of the local machine's Docker
daemon (including Docker for Mac).

Reflow was designed to support sophisticated, large-scale
bioinformatics workflows, but should be widely applicable to
scientific and engineering computing workloads. It was built 
using [Go](https://golang.org).

Reflow joins a [long
list](https://github.com/pditommaso/awesome-pipeline) of systems
designed to tackle bioinformatics workloads, but differ from these in
important ways:

- it is a vertically integrated system with a minimal set of external dependencies; this allows Reflow to be ""plug-and-play"": bring your cloud credentials, and you're off to the races;
- it defines a strict data model which is used for transparent memoization and other optimizations;
- it takes workflow software seriously: the Reflow DSL provides type checking, modularity, and other constructors that are commonplace in general purpose programming languages;
- because of its high level data model and use of caching, Reflow computes [incrementally](https://en.wikipedia.org/wiki/Incremental_computing): it is always able to compute the smallest set of operations given what has been computed previously.

## Table of Contents

- [Quickstart - AWS](#quickstart---aws)
- [Simple bioinformatics workflow](#simple-bioinformatics-workflow)
- [1000align](#1000align)
- [Documentation](#documentation)
- [Developing and building Reflow](#developing-and-building-reflow)
- [Debugging Reflow runs](#debugging-reflow-runs)
- [A note on Reflow's EC2 cluster manager](#a-note-on-reflows-ec2-cluster-manager)
- [Setting up a TaskDB](#setting-up-a-taskdb)
- [Support and community](#support-and-community)


## Getting Reflow

You can get binaries (macOS/amd64, Linux/amd64) for the latest
release at the [GitHub release
page](https://github.com/grailbio/reflow/releases).

If you are developing Reflow,
or would like to build it yourself,
please follow the instructions in the section
""[Developing and building Reflow](#developing-and-building-reflow).""

## Quickstart - AWS

Reflow is distributed with an EC2 cluster manager, and a memoization
cache implementation based on S3. These must be configured before
use. Reflow maintains a configuration file in `$HOME/.reflow/config.yaml`
by default (this can be overridden with the `-config` option). Reflow's
setup commands modify this file directly. After each step, the current 
configuration can be examined by running `reflow config`.

Note Reflow must have access to AWS credentials and configuration in the
environment (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`) while
running these commands.

	% reflow setup-ec2
	% reflow config
	cluster: ec2cluster
    ec2cluster:
      securitygroup: <a newly created security group here>
      maxpendinginstances: 5
      maxhourlycostusd: 10
      disktype: gp3
      instancesizetodiskspace:
        2xlarge: 300
        3xlarge: 300
        ...
      diskslices: 0
      ami: """"
	  sshkey: []
      keyname: """"
      cloudconfig: {}
      instancetypes:
      - c3.2xlarge
      - c3.4xlarge
      ...
    reflow: reflowversion,version=<hash>
    tls: tls,file=$HOME/.reflow/reflow.pem

After running `reflow setup-ec2`, we see that Reflow created a new
security group (associated with the account's default VPC), and
configured the cluster to use some default settings. Feel free to
edit the configuration file (`$HOME/.reflow/config.yaml`) to your
taste. If you want to use spot instances, add a new key under `ec2cluster`:
`spot: true`.

Reflow only configures one security group per account: Reflow will reuse
a previously created security group if `reflow setup-ec2` is run anew.
See `reflow setup-ec2 -help` for more details.

Next, we'll set up a cache. This isn't strictly necessary, but we'll
need it in order to use many of Reflow's sophisticated caching and
incremental computation features. On AWS, Reflow implements a cache
based on S3 and DynamoDB. A new S3-based cache is provisioned by
`reflow setup-s3-repository` and `reflow setup-dynamodb-assoc`, each
of which takes one argument naming the S3 bucket and DynamoDB table
name to be used, respectively. The S3 bucket is used to store file
objects while the DynamoDB table is used to store associations
between logically named computations and their concrete output. Note
that S3 bucket names are global, so pick a name that's likely to be
unique.

	% reflow setup-s3-repository reflow-quickstart-cache
	reflow: creating s3 bucket reflow-quickstart-cache
	reflow: created s3 bucket reflow-quickstart-cache
	% reflow setup-dynamodb-assoc reflow-quickstart
	reflow: creating DynamoDB table reflow-quickstart
	reflow: created DynamoDB table reflow-quickstart
	% reflow config
	assoc: dynamodb,table=reflow-quickstart
	repository: s3,bucket=reflow-quickstart-cache

	<rest is same as before>

The setup commands created the S3 bucket and DynamoDB table as
needed, and modified the configuration accordingly.

Advanced users can also optionally [setup a taskdb](#setting-up-a-taskdb).

We're now ready to run our first ""hello world"" program!

Create a file called ""hello.rf"" with the following contents:

	val Main = exec(image := ""ubuntu"", mem := GiB) (out file) {""
		echo hello world >>{{out}}
	""}

and run it:

	% reflow run hello.rf
	reflow: run ID: 6da656d1
		ec2cluster: 0 instances:  (<=$0.0/hr), total{}, waiting{mem:1.0GiB cpu:1 disk:1.0GiB
	reflow: total n=1 time=0s
			ident      n   ncache transfer runtime(m) cpu mem(GiB) disk(GiB) tmp(GiB)
			hello.Main 1   1      0B

	a948904f

Here, Reflow started a new `t2.small` instance (Reflow matches the workload with
available instance types), ran `echo hello world` inside of an Ubuntu container,
placed the output in a file, and returned its SHA256 digest. (Reflow represents
file contents using their SHA256 digest.)

We're now ready to explore Reflow more fully.

## Simple bioinformatics workflow

Let's explore some of Reflow's features through a simple task:
aligning NGS read data from the 1000genomes project. Create
a file called ""align.rf"" with the following. The code is commented
inline for clarity.

	// In order to align raw NGS data, we first need to construct an index
	// against which to perform the alignment. We're going to be using
	// the BWA aligner, and so we'll need to retrieve a reference sequence
	// and create an index that's usable from BWA.

	// g1kv37 is a human reference FASTA sequence. (All
	// chromosomes.) Reflow has a static type system, but most type
	// annotations can be omitted: they are inferred by Reflow. In this
	// case, we're creating a file: a reference to the contents of the
	// named URL. We're retrieving data from the public 1000genomes S3
	// bucket.
	val g1kv37 = file(""s3://1000genomes/technical/reference/human_g1k_v37.fasta.gz"")
	
	// Here we create an indexed version of the g1kv37 reference. It is
	// created using the ""bwa index"" command with the raw FASTA data as
	// input. Here we encounter another way to produce data in reflow:
	// the exec. An exec runs a (Bash) script inside of a Docker image,
	// placing the output in files or directories (or both: execs can
	// return multiple values). In this case, we're returning a
	// directory since BWA stores multiple index files alongside the raw
	// reference. We also declare that the image to be used is
	// ""biocontainers/bwa"" (the BWA image maintained by the
	// biocontainers project).
	//
	// Inside of an exec template (delimited by {"" and ""}) we refer to
	// (interpolate) values in our environment by placing expressions
	// inside of the {{ and }} delimiters. In this case we're referring
	// to the file g1kv37 declared above, and our output, named out.
	//
	// Many types of expressions can be interpolated inside of an exec,
	// for example strings, integers, files, and directories. Strings
	// and integers are rendered using their normal representation,
	// files and directories are materialized to a local path before
	// starting execution. Thus, in this case, {{g1kv37}} is replaced at
	// runtime by a path on disk with a file with the contents of the
	// file g1kv37 (i.e.,
	// s3://1000genomes/technical/reference/human_g1k_v37.fasta.gz)
	val reference = exec(image := ""biocontainers/bwa:v0.7.15_cv3"", mem := 6*GiB, cpu := 1) (out dir) {""
		# Ignore failures here. The file from 1000genomes has a trailer
		# that isn't recognized by gunzip. (This is not recommended practice!)
		gunzip -c {{g1kv37}} > {{out}}/g1k_v37.fa || true
		cd {{out}}
		bwa index -a bwtsw g1k_v37.fa
	""}
	
	// Now that we have defined a reference, we can define a function to
	// align a pair of reads against the reference, producing an output
	// SAM-formatted file. Functions compute expressions over a set of
	// abstract parameters, in this case, a pair of read files. Unlike almost
	// everywhere else in Reflow, function parameters must be explicitly
	// typed.
	//
	// (Note that we're using a syntactic short-hand here: parameter lists can 
	// be abbreviated. ""r1, r2 file"" is equivalent to ""r1 file, r2 file"".)
	//
	// The implementation of align is a straightforward invocation of ""bwa mem"".
	// Note that ""r1"" and ""r2"" inside of the exec refer to the function arguments,
	// thus align can be invoked for any set of r1, r2.
	func align(r1, r2 file) = 
		exec(image := ""biocontainers/bwa:v0.7.15_cv3"", mem := 20*GiB, cpu := 16) (out file) {""
			bwa mem -M -t 16 {{reference}}/g1k_v37.fa {{r1}} {{r2}} > {{out}}
		""}

	// We're ready to test our workflow now. We pick an arbitrary read
	// pair from the 1000genomes data set, and invoke align. There are a
	// few things of note here. First is the identifier ""Main"". This
	// names the expression that's evaluated by `reflow run` -- the
	// entry point of the computation. Second, we've defined Main to be
	// a block. A block is an expression that contains one or more
	// definitions followed by an expression. The value of a block is the
	// final expression. Finally, Main contains a @requires annotation.
	// This instructs Reflow how many resources to reserve for the work
	// being done. Note that, because Reflow is able to distribute work,
	// if a single instance is too small to execute fully in parallel,
	// Reflow will provision additional compute instances to help along.
	// @requires thus denotes the smallest possible instance
	// configuration that's required for the program.
	@requires(cpu := 16, mem := 24*GiB, disk := 50*GiB)	
	val Main = {
		r1 := file(""s3://1000genomes/phase3/data/HG00103/sequence_read/SRR062640_1.filt.fastq.gz"")
		r2 := file(""s3://1000genomes/phase3/data/HG00103/sequence_read/SRR062640_2.filt.fastq.gz"")
		align(r1, r2)
	}

Now we're ready to run our module. First, let's run `reflow doc`.
This does two things. First, it typechecks the module (and any
dependent modules), and second, it prints documentation for the
public declarations in the module. Identifiers that begin with an
uppercase letter are public (and may be used from other modules);
others are not.

	% reflow doc align.rf
	Declarations
	
	val Main (out file)
	    We're ready to test our workflow now. We pick an arbitrary read pair from the
	    1000genomes data set, and invoke align. There are a few things of note here.
	    First is the identifier ""Main"". This names the expression that's evaluated by
	    `reflow run` -- the entry point of the computation. Second, we've defined Main
	    to be a block. A block is an expression that contains one or more definitions
	    followed by an expression. The value of a block is the final expression. Finally,
	    Main contains a @requires annotation. This instructs Reflow how many resources
	    to reserve for the work being done. Note that, because Reflow is able to
	    distribute work, if a single instance is too small to execute fully in parallel,
	    Reflow will provision additional compute instances to help along. @requires thus
	    denotes the smallest possible instance configuration that's required for the
	    program.

Then let's run it:

	% reflow run align.rf
	reflow: run ID: 82e63a7a
	ec2cluster: 1 instances: c5.4xlarge:1 (<=$0.7/hr), total{mem:29.8GiB cpu:16 disk:250.0GiB intel_avx512:16}, waiting{}, pending{}
	82e63a7a: elapsed: 2m30s, executing:1, completed: 3/5
	  align.reference:  exec ..101f9a082e1679c16d23787c532a0107537c9c # Ignore failures here. The f..bwa index -a bwtsw g1k_v37.fa  2m4s

Reflow launched a new instance: the previously launched instance (a
`t2.small`) was not big enough to fit the requirements of align.rf.
Note also that Reflow assigns a run name for each `reflow run`
invocation. This can be used to look up run details with the `reflow
info` command. In this case:

	% reflow info 82e63a7a
	82e63a7aee201d137f8ade3d584c234b856dc6bdeba00d5d6efc9627bd988a68 (run)
	    time:      Wed Dec 12 10:45:04 2018
	    program:   /Users/you/align.rf
	    phase:     Eval
	    alloc:     ec2-34-213-42-76.us-west-2.compute.amazonaws.com:9000/5a0adaf6c879efb1
	    resources: {mem:28.9GiB cpu:16 disk:245.1GiB intel_avx:16 intel_avx2:16 intel_avx512:16}
	    log:       /Users/you/.reflow/runs/82e63a7aee201d137f8ade3d584c234b856dc6bdeba00d5d6efc9627bd988a68.execlog

Here we see that the run is currently being performed on the alloc named
`ec2-34-213-42-76.us-west-2.compute.amazonaws.com:9000/5a0adaf6c879efb1`.
An alloc is a resource reservation on a single machine. A run can
make use of multiple allocs to distribute work across multiple
machines. The alloc is a URI, and the first component is the real 
hostname. You can ssh into the host in order to inspect what's going on.
Reflow launched the instance with your public SSH key (as long as it was
set up by `reflow setup-ec2`, and `$HOME/.ssh/id_rsa.pub` existed at that time).

	% ssh core@ec2-34-213-42-76.us-west-2.compute.amazonaws.com
	...

As the run progresses, Reflow prints execution status of each task on the
console.

	...
	align.Main.r2:    intern s3://1000genomes/phase3/data/HG00103/sequence_read/SRR062640_2.filt.fastq.gz                         23s
	align.Main.r1:    intern done 1.8GiB                                                                                          23s
	align.g1kv37:     intern done 851.0MiB                                                                                        23s
	align.reference:  exec ..101f9a082e1679c16d23787c532a0107537c9c # Ignore failures here. The f..bwa index -a bwtsw g1k_v37.fa  6s

Here, Reflow started downloading r1 and r2 in parallel with creating the reference.
Creating the reference is an expensive operation. We can examine it while it's running
with `reflow ps`:

	% reflow ps 
	3674721e align.reference 10:46AM 0:00 running 4.4GiB 1.0 6.5GiB bwa

This tells us that the only task that's currently running is bwa to compute the reference.
It's currently using 4.4GiB of memory, 1 cores, and 6.5GiB GiB of disk space. By passing the -l
option, reflow ps also prints the task's exec URI.

	% reflow ps -l
	3674721e align.reference 10:46AM 0:00 running 4.4GiB 1.0 6.5GiB bwa ec2-34-213-42-76.us-west-2.compute.amazonaws.com:9000/5a0adaf6c879efb1/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e

An exec URI is a handle to the actual task being executed. It
globally identifies all tasks, and can be examined with `reflow info`:

	% reflow info ec2-34-213-42-76.us-west-2.compute.amazonaws.com:9000/5a0adaf6c879efb1/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e
	ec2-34-213-42-76.us-west-2.compute.amazonaws.com:9000/5a0adaf6c879efb1/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e (exec)
	    state: running
	    type:  exec
	    ident: align.reference
	    image: index.docker.io/biocontainers/bwa@sha256:0529e39005e35618c4e52f8f56101f9a082e1679c16d23787c532a0107537c9c
	    cmd:   ""\n\t# Ignore failures here. The file from 1000genomes has a trailer\n\t# that isn't recognized by gunzip. (This is not recommended practice!)\n\tgunzip -c {{arg[0]}} > {{arg[1]}}/g1k_v37.fa || true\n\tcd {{arg[2]}}\n\tbwa index -a bwtsw g1k_v37.fa\n""
	      arg[0]:
	        .: sha256:8b6c538abf0dd92d3f3020f36cc1dd67ce004ffa421c2781205f1eb690bdb442 (851.0MiB)
	      arg[1]: output 0
	      arg[2]: output 0
	    top:
	         bwa index -a bwtsw g1k_v37.fa

Here, Reflow tells us that the currently running process is ""bwa
index..."", its template command, and the SHA256 digest of its inputs.
Programs often print helpful output to standard error while working;
this output can be examined with `reflow logs`:

	% reflow logs ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e
	
	gzip: /arg/0/0: decompression OK, trailing garbage ignored
	[bwa_index] Pack FASTA... 18.87 sec
	[bwa_index] Construct BWT for the packed sequence...
	[BWTIncCreate] textLength=6203609478, availableWord=448508744
	[BWTIncConstructFromPacked] 10 iterations done. 99999990 characters processed.
	[BWTIncConstructFromPacked] 20 iterations done. 199999990 characters processed.
	[BWTIncConstructFromPacked] 30 iterations done. 299999990 characters processed.
	[BWTIncConstructFromPacked] 40 iterations done. 399999990 characters processed.
	[BWTIncConstructFromPacked] 50 iterations done. 499999990 characters processed.
	[BWTIncConstructFromPacked] 60 iterations done. 599999990 characters processed.
	[BWTIncConstructFromPacked] 70 iterations done. 699999990 characters processed.
	[BWTIncConstructFromPacked] 80 iterations done. 799999990 characters processed.
	[BWTIncConstructFromPacked] 90 iterations done. 899999990 characters processed.
	[BWTIncConstructFromPacked] 100 iterations done. 999999990 characters processed.
	[BWTIncConstructFromPacked] 110 iterations done. 1099999990 characters processed.
	[BWTIncConstructFromPacked] 120 iterations done. 1199999990 characters processed.
	[BWTIncConstructFromPacked] 130 iterations done. 1299999990 characters processed.
	[BWTIncConstructFromPacked] 140 iterations done. 1399999990 characters processed.
	[BWTIncConstructFromPacked] 150 iterations done. 1499999990 characters processed.
	[BWTIncConstructFromPacked] 160 iterations done. 1599999990 characters processed.
	[BWTIncConstructFromPacked] 170 iterations done. 1699999990 characters processed.
	[BWTIncConstructFromPacked] 180 iterations done. 1799999990 characters processed.
	[BWTIncConstructFromPacked] 190 iterations done. 1899999990 characters processed.
	[BWTIncConstructFromPacked] 200 iterations done. 1999999990 characters processed.
	[BWTIncConstructFromPacked] 210 iterations done. 2099999990 characters processed.
	[BWTIncConstructFromPacked] 220 iterations done. 2199999990 characters processed.
	[BWTIncConstructFromPacked] 230 iterations done. 2299999990 characters processed.
	[BWTIncConstructFromPacked] 240 iterations done. 2399999990 characters processed.
	[BWTIncConstructFromPacked] 250 iterations done. 2499999990 characters processed.
	[BWTIncConstructFromPacked] 260 iterations done. 2599999990 characters processed.
	[BWTIncConstructFromPacked] 270 iterations done. 2699999990 characters processed.
	[BWTIncConstructFromPacked] 280 iterations done. 2799999990 characters processed.
	[BWTIncConstructFromPacked] 290 iterations done. 2899999990 characters processed.
	[BWTIncConstructFromPacked] 300 iterations done. 2999999990 characters processed.
	[BWTIncConstructFromPacked] 310 iterations done. 3099999990 characters processed.
	[BWTIncConstructFromPacked] 320 iterations done. 3199999990 characters processed.
	[BWTIncConstructFromPacked] 330 iterations done. 3299999990 characters processed.
	[BWTIncConstructFromPacked] 340 iterations done. 3399999990 characters processed.
	[BWTIncConstructFromPacked] 350 iterations done. 3499999990 characters processed.
	[BWTIncConstructFromPacked] 360 iterations done. 3599999990 characters processed.
	[BWTIncConstructFromPacked] 370 iterations done. 3699999990 characters processed.
	[BWTIncConstructFromPacked] 380 iterations done. 3799999990 characters processed.
	[BWTIncConstructFromPacked] 390 iterations done. 3899999990 characters processed.
	[BWTIncConstructFromPacked] 400 iterations done. 3999999990 characters processed.
	[BWTIncConstructFromPacked] 410 iterations done. 4099999990 characters processed.
	[BWTIncConstructFromPacked] 420 iterations done. 4199999990 characters processed.
	[BWTIncConstructFromPacked] 430 iterations done. 4299999990 characters processed.
	[BWTIncConstructFromPacked] 440 iterations done. 4399999990 characters processed.
	[BWTIncConstructFromPacked] 450 iterations done. 4499999990 characters processed.

At this point, it looks like everything is running as expected.
There's not much more to do than wait. Note that, while creating an
index takes a long time, Reflow only has to compute it once. When
it's done, Reflow memoizes the result, uploading the resulting data
directly to the configured S3 cache bucket. The next time the
reference expression is encountered, Reflow will use the previously
computed result. If the input file changes (e.g., we decide to use
another reference sequence), Reflow will recompute the index again.
The same will happen if the command (or Docker image) that's used to
compute the index changes. Reflow keeps track of all the dependencies
for a particular sub computation, and recomputes them only when
dependencies have changed. This way, we always know what is being
computed is correct (the result is the same as if we had computed the
result from scratch), but avoid paying the cost of redundant
computation.

After a little while, the reference will have finished generating,
and Reflow begins alignment. Here, Reflow reports that the reference
took 52 minutes to compute, and produced 8 GiB of output.

      align.reference:  exec done 8.0GiB                                                                                            52m37s
      align.align:      exec ..101f9a082e1679c16d23787c532a0107537c9c bwa mem -M -t 16 {{reference}..37.fa {{r1}} {{r2}} > {{out}}  4s

If we query (""info"") the reference exec again, Reflow reports precisely what
was produced:

	% reflow info ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e
	ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/3674721e2d9e80b325934b08973fb3b1d3028b2df34514c9238be466112eb86e (exec)
	    state: complete
	    type:  exec
	    ident: align.reference
	    image: index.docker.io/biocontainers/bwa@sha256:0529e39005e35618c4e52f8f56101f9a082e1679c16d23787c532a0107537c9c
	    cmd:   ""\n\t# Ignore failures here. The file from 1000genomes has a trailer\n\t# that isn't recognized by gunzip. (This is not recommended practice!)\n\tgunzip -c {{arg[0]}} > {{arg[1]}}/g1k_v37.fa || true\n\tcd {{arg[2]}}\n\tbwa index -a bwtsw g1k_v37.fa\n""
	      arg[0]:
	        .: sha256:8b6c538abf0dd92d3f3020f36cc1dd67ce004ffa421c2781205f1eb690bdb442 (851.0MiB)
	      arg[1]: output 0
	      arg[2]: output 0
	    result:
	      list[0]:
	        g1k_v37.fa:     sha256:2f9cd9e853a9284c53884e6a551b1c7284795dd053f255d630aeeb114d1fa81f (2.9GiB)
	        g1k_v37.fa.amb: sha256:dd51a07041a470925c1ebba45c2f534af91d829f104ade8fc321095f65e7e206 (6.4KiB)
	        g1k_v37.fa.ann: sha256:68928e712ef48af64c5b6a443f2d2b8517e392ae58b6a4ab7191ef7da3f7930e (6.7KiB)
	        g1k_v37.fa.bwt: sha256:2aec938930b8a2681eb0dfbe4f865360b98b2b6212c1fb9f7991bc74f72d79d8 (2.9GiB)
	        g1k_v37.fa.pac: sha256:d62039666da85d859a29ea24af55b3c8ffc61ddf02287af4d51b0647f863b94c (739.5MiB)
	        g1k_v37.fa.sa:  sha256:99eb6ff6b54fba663c25e2642bb2a6c82921c931338a7144327c1e3ee99a4447 (1.4GiB)

In this case, ""bwa index"" produced a number of auxiliary index
files. These are the contents of the ""reference"" directory.

We can again query Reflow for running execs, and examine the
alignment. We see now that the reference is passed in (argument 0),
along side the read pairs (arguments 1 and 2). 

	% reflow ps -l
	6a6c36f5 align.align 5:12PM 0:00 running 5.9GiB 12.3 0B  bwa ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/6a6c36f5da6ee387510b0b61d788d7e4c94244d61e6bc621b43f59a73443a755
	% reflow info ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/6a6c36f5da6ee387510b0b61d788d7e4c94244d61e6bc621b43f59a73443a755
	ec2-34-221-0-157.us-west-2.compute.amazonaws.com:9000/0061a20f88f57386/6a6c36f5da6ee387510b0b61d788d7e4c94244d61e6bc621b43f59a73443a755 (exec)
	    state: running
	    type:  exec
	    ident: align.align
	    image: index.docker.io/biocontainers/bwa@sha256:0529e39005e35618c4e52f8f56101f9a082e1679c16d23787c532a0107537c9c
	    cmd:   ""\n\t\tbwa mem -M -t 16 {{arg[0]}}/g1k_v37.fa {{arg[1]}} {{arg[2]}} > {{arg[3]}}\n\t""
	      arg[0]:
	        g1k_v37.fa:     sha256:2f9cd9e853a9284c53884e6a551b1c7284795dd053f255d630aeeb114d1fa81f (2.9GiB)
	        g1k_v37.fa.amb: sha256:dd51a07041a470925c1ebba45c2f534af91d829f104ade8fc321095f65e7e206 (6.4KiB)
	        g1k_v37.fa.ann: sha256:68928e712ef48af64c5b6a443f2d2b8517e392ae58b6a4ab7191ef7da3f7930e (6.7KiB)
	        g1k_v37.fa.bwt: sha256:2aec938930b8a2681eb0dfbe4f865360b98b2b6212c1fb9f7991bc74f72d79d8 (2.9GiB)
	        g1k_v37.fa.pac: sha256:d62039666da85d859a29ea24af55b3c8ffc61ddf02287af4d51b0647f863b94c (739.5MiB)
	        g1k_v37.fa.sa:  sha256:99eb6ff6b54fba663c25e2642bb2a6c82921c931338a7144327c1e3ee99a4447 (1.4GiB)
	      arg[1]:
	        .: sha256:0c1f85aa9470b24d46d9fc67ba074ca9695d53a0dee580ec8de8ed46ef347a85 (1.8GiB)
	      arg[2]:
	        .: sha256:47f5e749123d8dda92b82d5df8e32de85273989516f8e575d9838adca271f630 (1.7GiB)
	      arg[3]: output 0
	    top:
	         /bin/bash -e -l -o pipefail -c ..bwa mem -M -t 16 /arg/0/0/g1k_v37.fa /arg/1/0 /arg/2/0 > /return/0 .
	         bwa mem -M -t 16 /arg/0/0/g1k_v37.fa /arg/1/0 /arg/2/0

Note that the read pairs are files. Files in Reflow do not have
names; they are just blobs of data. When Reflow runs a process that
requires input files, those anonymous files are materialized on disk,
but the filenames are not meaningful. In this case, we can see from
the ""top"" output (these are the actual running processes, as reported
by the OS), that the r1 ended up being called ""/arg/1/0"" and r2
""/arg/2/0"". The output is a file named ""/return/0"".

Finally, alignment is complete. Aligning a single read pair took
around 19m, and produced 13.2 GiB of output. Upon completion, Reflow
prints runtime statistics and the result.

	reflow: total n=5 time=1h9m57s
	        ident           n   ncache transfer runtime(m) cpu            mem(GiB)    disk(GiB)      tmp(GiB)
	        align.align     1   0      0B       17/17/17   15.6/15.6/15.6 7.8/7.8/7.8 12.9/12.9/12.9 0.0/0.0/0.0
	        align.Main.r2   1   0      0B
	        align.Main.r1   1   0      0B
	        align.reference 1   0      0B       51/51/51   1.0/1.0/1.0    4.4/4.4/4.4 6.5/6.5/6.5    0.0/0.0/0.0
	        align.g1kv37    1   0      0B
	
	becb0485

Reflow represents file values by the SHA256 digest of the file's
content. In this case, that's not very useful: you want the file,
not its digest. Reflow provides mechanisms to export data. In this
case let's copy the resulting file to an S3 bucket.

We'll make use of the ""files"" system module to copy the aligned file
to an external S3 bucket. Modify align.rf's `Main` to the following
(but pick an S3 bucket you own), and then run it again. Commentary is
inline for clarity.

	@requires(cpu := 16, mem := 24*GiB, disk := 50*GiB)	
	val Main = {
		r1 := file(""s3://1000genomes/phase3/data/HG00103/sequence_read/SRR062640_1.filt.fastq.gz"")
		r2 := file(""s3://1000genomes/phase3/data/HG00103/sequence_read/SRR062640_2.filt.fastq.gz"")
		// Instantiate the system modules ""files"" (system modules begin
		// with $), assigning its instance to the ""files"" identifier. To
		// view the documentation for this module, run `reflow doc
		// $/files`.
		files := make(""$/files"")
		// As before.
		aligned := align(r1, r2)
		// Use the files module's Copy function to copy the aligned file to
		// the provided destination.
		files.Copy(aligned, ""s3://marius-test-bucket/aligned.sam"")
	}

And run it again:

	% reflow run align.rf
	reflow: run ID: 9f0f3596
	reflow: total n=2 time=1m9s
	        ident         n   ncache transfer runtime(m) cpu mem(GiB) disk(GiB) tmp(GiB)
	        align_2.align 1   1      0B
	        align_2.Main  1   0      13.2GiB
	
	val<.=becb0485 13.2GiB>


Here we see that Reflow did not need to recompute the aligned file;
it is instead retrieved from cache. The reference index generation is
skipped altogether.  Status lines that indicate ""xfer"" (instead of
""run"") means that Reflow is performing a cache transfer in place of
running the computation. Reflow claims to have transferred a 13.2 GiB
file to `s3://marius-test-bucket/aligned.sam`. Indeed it did:

	% aws s3 ls s3://marius-test-bucket/aligned.sam
	2018-12-13 16:29:49 14196491221 aligned.sam.

## 1000align

This code was modularized and generalized in
[1000align](https://github.com/grailbio/reflow/tree/master/doc/1000align). Here,
fastq, bam, and alignment utilities are split into their own
parameterized modules. The toplevel module, 1000align, is
instantiated from the command line. Command line invocations (`reflow
run`) can pass module parameters through flags (strings, booleans,
and integers):

	% reflow run 1000align.rf -help
	usage of 1000align.rf:
	  -out string
	    	out is the target of the output merged BAM file (required)
	  -sample string
	    	sample is the name of the 1000genomes phase 3 sample (required)

For example, to align the full sample from above, we can invoke
1000align.rf with the following arguments:

	% reflow run 1000align.rf -sample HG00103 -out s3://marius-test-bucket/HG00103.bam

In this case, if your account limits allow it, Reflow will launch
additional EC2 instances in order to further parallelize the work to
be done. (Since we're aligning multiple pairs of FASTQ files).
In this run, we can see that Reflow is aligning 5 pairs in parallel
across 2 instances (four can fit on the initial m4.16xlarge instance).

	% reflow ps -l
	e74d4311 align.align.sam 11:45AM 0:00 running 10.9GiB 31.8 6.9GiB   bwa ec2-34-210-201-193.us-west-2.compute.amazonaws.com:9000/6a7ffa00d6b0d9e1/e74d4311708f1c9c8d3894a06b59029219e8a545c69aa79c3ecfedc1eeb898f6
	59c561be align.align.sam 11:45AM 0:00 running 10.9GiB 32.7 6.4GiB   bwa ec2-34-210-201-193.us-west-2.compute.amazonaws.com:9000/6a7ffa00d6b0d9e1/59c561be5f627143108ce592d640126b88c23ba3d00974ad0a3c801a32b50fbe
	ba688daf align.align.sam 11:47AM 0:00 running 8.7GiB  22.6 2.9GiB   bwa ec2-18-236-233-4.us-west-2.compute.amazonaws.com:9000/ae348d6c8a33f1c9/ba688daf5d50db514ee67972ec5f0a684f8a76faedeb9a25ce3d412e3c94c75c
	0caece7f align.align.sam 11:47AM 0:00 running 8.7GiB  25.9 3.4GiB   bwa ec2-18-236-233-4.us-west-2.compute.amazonaws.com:9000/ae348d6c8a33f1c9/0caece7f38dc3d451d2a7411b1fcb375afa6c86a7b0b27ba7dd1f9d43d94f2f9
	0b59e00c align.align.sam 11:47AM 0:00 running 10.4GiB 22.9 926.6MiB bwa ec2-18-236-233-4.us-west-2.compute.amazonaws.com:9000/ae348d6c8a33f1c9/0b59e00c848fa91e3b0871c30da3ed7e70fbc363bdc48fb09c3dfd61684c5fd9

When it completes, an approximately 17GiB BAM file is deposited to s3:

	% aws s3 ls s3://marius-test-bucket/HG00103.bam
	2018-12-14 15:27:33 18761607096 HG00103.bam.

## A note on Reflow's EC2 cluster manager

Reflow comes with a built-in cluster manager, which is responsible
for elastically increasing or decreasing required compute resources.
The AWS EC2 cluster manager keeps track of instance type availability
and account limits, and uses these to launch the most appropriate set
of instances for a given job. When instances become idle, they will
terminate themselves if they are idle for more than 10 minutes; idle
instances are reused when possible.

The cluster manager may be configured under the ""ec2cluster"" key in 
Reflow's configuration. Its parameters are documented by
[godoc](https://godoc.org/github.com/grailbio/reflow/ec2cluster#Config).
(Formal documentation is forthcoming.)

## Setting up a TaskDB
Setting up a TaskDB is entirely optional. The TaskDB is used to store a record of reflow runs,
their sub-tasks (mainly `exec`s), the EC2 instances that were instantiated, etc.
It provides the following benefits:

* Tools such as `reflow info` will work better, especially in a multi-user environment.

  That is, if you have a single AWS account and share it with other users to run `reflow`, then
a TaskDB allows you to monitor and query info about all runs within the account (using `reflow ps`, `reflow info`, etc)
* Determine cost of a particular run (included in the output of `reflow info`)
* Determine cost of the cluster (`reflow ps -p` - see documentation using `reflow ps --help`)

The following command can be used to setup a TaskDB (refer documentation:

	% reflow setup-taskdb -help

Note that the same dynamodb table and S3 bucket which were used to setup the cache (see above),
could optionally be used here.  But note that this (TaskDB) feature comes with a cost (DynamoDB),
and by keeping them separate, the costs can be managed independently.

Example:

	% reflow setup-taskdb <table_name> <s3_bucket_name>
	reflow: attempting to create DynamoDB table ...
	reflow: created DynamoDB table ...
	reflow: waiting for table to become active; current status: CREATING
	reflow: created secondary index Date-Keepalive-index
	reflow: waiting for table to become active; current status: UPDATING
	reflow: waiting for index Date-Keepalive-index to become active; current status: CREATING
	...
	reflow: created secondary index RunID-index
	reflow: waiting for table to become active; current status: UPDATING
	reflow: waiting for index RunID-index to become active; current status: CREATING
	...


## Documentation

- [Language summary](LANGUAGE.md)
- [Go package docs](https://godoc.org/github.com/grailbio/reflow)

## Developing and building Reflow

Reflow is implemented in Go, and its packages are go-gettable. 
Reflow is also a [Go module](https://github.com/golang/go/wiki/Modules)
and uses modules to fix its dependency graph.

After checking out the repository, 
the usual `go` commands should work, e.g.:

	% go test ./...

The package `github.com/grailbio/reflow/cmd/reflow`
(or subdirectory `cmd/reflow` in the repository)
defines the main command for Reflow.
Because Reflow relies on being able to
distribute its current build,
the binary must be built using the `buildreflow` tool
instead of the ordinary Go tooling.
Command `buildreflow` acts like `go build`,
but also cross compiles the binary 
for the remote target (Linux/amd64 currently supported),
and embeds the cross-compiled binary.

	% cd $CHECKOUT/cmd/reflow
	% go install github.com/grailbio/reflow/cmd/buildreflow
	% buildreflow

## Debugging Reflow runs

The `$HOME/.reflow/runs` directory contains logs, traces and other 
information for each Reflow run. If the run you're looking for is
no longer there, the `info` and `cat` tools can be used if you have 
the run ID:

	% reflow info 2fd5a9b6
	runid    user       start   end    RunLog   EvalGraph Trace
    2fd5a9b6 username   4:41PM  4:41PM 29a4b506 90f40bfc  4ec75aac
    
    % reflow cat 29a4b506 > /tmp/29a4b506.runlog

    # fetch the evalgraph data, pass to the dot tool to generate an svg image (viewable in your browser)
    % reflow cat 90f40bfc | dot -Tsvg > /tmp/90f40bfc.svg

For more information about tracing, see: [doc/tracing.md](doc/tracing.md).

## Support and community

Please join us on on [Gitter](https://gitter.im/grailbio/reflow) or 
on the [mailing list](https://groups.google.com/forum/#!forum/reflowlets)
to discuss Reflow.


",2023-07-07 18:47:16+00:00
remake,remake,richfitz/remake,Make-like declarative workflows in R,,False,338,2023-04-04 15:11:08+00:00,2014-09-06 08:14:14+00:00,31,24,11,0,,,Other,396,,0,,,2017-08-26 13:26:32+00:00,"# remake

[![Build Status](https://travis-ci.org/richfitz/remake.svg?branch=master)](https://travis-ci.org/richfitz/remake)
[![Build status](https://ci.appveyor.com/api/projects/status/yltv2lpn046a7e93/branch/master?svg=true)](https://ci.appveyor.com/project/richfitz/remake/branch/master)
[![Coverage Status](https://coveralls.io/repos/richfitz/remake/badge.svg?branch=master)](https://coveralls.io/r/richfitz/remake?branch=master)

Make-like build management, reimagined for R.

See below for [installation instructions](#installation).

# The idea

""[make](http://en.wikipedia.org/wiki/Make_(software))"",
when it works, is wonderful.  Being able to change part of a complicated system and the re-make, updating only the parts of the system that have changed is great.  While it gets some use It's very heavily tailored towards building software though.  While make can be used to create reproducible research workflows (e.g. [here](http://www.bioinformaticszen.com/post/decomplected-workflows-makefiles/) and [here](http://kbroman.org/minimal_make/)), it is a challenge.

The idea here is to re-imagine a set of ideas from make but built for R.  Rather than having a series of calls to different instances of R (as happens if you run make on R scripts), the idea is to define pieces of a pipeline within an R session.  Rather than being language agnostic (like make must be), `remake` is unapologetically R focussed.

**Note**: This package is under heavy development (as of May 2015), so things may change under you if you start using this now.  However, the core format seems to be working on some nontrivial cases that we are using in our own work.  At the same time, if you're willing to have things change around a bit feel free to start using this and post [issues](https://github.com/richfitz/remake/issues) with problems/friction/ideas etc and the package will reflect your workflow more.

**Note**: Between versions `0.1` and `0.2.0` the database format has changed.  This will require rebuilding your project.  This corresponds to adding the dependency on [`storr`](https://github.com/richfitz/storr).  Everything else should remain unchanged though.

## What remake does

You describe the beginning, intermediate and end points of your analysis, and how they flow together.

* ""**targets**"" are the points in your analysis.  They can be either files (data files for input; tables, plots, knitr reports for output) or they can be R objects (representing processed data, results, fitted models, etc).
* ""**rules**"" are how the targets in your analysis relate together and are simply the names of R functions.
* ""**dependencies**"" are the targets that need to already be made before a particular target can run (for example, a processed data set might depend on downloading a file; a plot might depend on a processed data set).

There might be very few steps or very many, but `remake` will take care of stepping through the analysis in a correct order (there can be more than one correct order!).

## Example

Here's a very simple analysis pipeline that illustrates the basic idea:

1. Download some data from the web into a local file
2. Process that file to prepare it for analysis
3. Create a plot from that file
4. Create a knitr report that uses the same set of objects

The remakefile that describes this pipline might look like this:

```yaml
sources:
  - code.R

targets:
  all:
    depends: plot.pdf

  data.csv:
    command: download_data(target_name)

  processed:
    command: process_data(""data.csv"")

  plot.pdf:
    command: myplot(processed)
    plot: true

  report.md:
    depends: processed
    knitr: true
```

(this is a [yaml](http://yaml.org) file).  The full version of this file, with explanations, is [here](doc/remake.yml).

You still need to write functions that carry out each step; that might look something like [this](doc/code.R), but it would define the functions `download_data`, `processs_data` and `myplot`.  Remake can then be run from within R:

```r
remake::make()
# [ BUILD ] data.csv            |  download_data(""data.csv"")
# [ BUILD ] processed           |  processed <- process_data(""data.csv"")
# [ BUILD ] plot.pdf            |  myplot(processed) # ==> plot.pdf
# [       ] all
```

The ""`BUILD`"": next to each target indicates that it is being run (which may take some time for a complicated step) and after the pipe a call is printed that indicates what is happening (this is a small white lie).

Rerunning remake:

```r
remake::make()
# [    OK ] data.csv
# [    OK ] processed
# [    OK ] plot.pdf
# [       ] all
```

Everything is up to date, so remake just skips over things.

There are also special [`knitr`](http://yihui.name/knitr) targets:

```r
remake::make(""report.md"")
# [    OK ] data.csv
# [    OK ] processed
# [       ] report.Rmd
# [  KNIT ] report.md            |  knitr::knit(""report.Rmd"", ""report.md"")
```

This arranges for the target `processed`, on which this depends (see [the remakefile](doc/remake.yml)) to be passed through to `knitr`, along with all the functions defined in `code.R`, and builds the report `report.md` from the knitr source `report.Rmd` (the source is [here](doc/report.Rmd)).  Note that because `processed` was already up to date, `remake` skips rebuilding it.

`remake` can also be run from the command line (outside of R), to make it easy to include as part of a bigger pipeline, perhaps using make! (I do this in my own use of remake).

Rather than require that you buy in to some all-singing, all-dancing workflow tool, `remake` tries to be agnostic about how you work: there are no special functions within your code that you need to use.  You can also create a linear version of your analysis at any time:

```r
remake::make_script()
# source(""code.R"")
# download_data(""data.csv"")
# processed <- process_data(""data.csv"")
# pdf(""plot.pdf"")
# myplot(processed)
# dev.off()
```

## Other core features:

* `remake` determines if any dependencies have changed when need running your analysis.  So if a downloaded data file changes, everything that depends on it will be rebuilt when needed.
  - however, rather than rely on file modification times, `remake` uses a hash (like a digital fingerprint) of the file or object to determine if the contents have *really* changed.  So inconsequential changes will be ignored.
* Object targets are persistent across sessions, so manual saving of objects is not necessary.  This avoids a lot of manual caching of results that tends to litter long-running analysis code.
* `remake` also checks if the *functions* used as rules (or called from those functions) have changed and will rebuild if these have changed (for the rationale here, see [here](doc/reproducible_research.md)).
* Because `remake` keeps track of which files and objects it created, it can automatically clean up after itself.  This makes it easy to rerun the entire analysis beginning-to-end.
  - three levels of cleaning (tidy, clean and purge) are provided to categorise how much you want to keep a particular target.
* Support for easily making figures and running [`knitr`](http://yihui.name/knitr) as special targets.
* Automate installation of dependencies.
* Automate curation of `.gitignore` files to prevent accidentally committing large output to your repository.
* (Very early) support for archiving a set of analyses that other users can import.
  - This means you can share results of long-running analyses/simulations and people can easily run the last steps of your analyses.
  - Eventually this will interface with [rfigshare](https://github.com/ropensci/rfigshare) and GitHub [releases](https://github.com/blog/1547-release-your-software).

## Real-world examples

* A large analysis of [adequacy of models used in comparative phylogenetics](https://github.com/richfitz/modeladequacy/blob/master/analysis/remake.yml).
* [How much of the world is woody?](https://github.com/richfitz/wood_remake) (this is a port of the make-based [original](https://github.com/richfitz/wood))
* A large [data synthesis project](https://github.com/dfalster/baad) (unfortunately complicated by being a templated remakefile).
* The accounts for a [coffee coop](https://github.com/aammd/CoffeeCoop)

## Tutorials

Some tutorials on using remake with different datasets. 

* [Tutorial on remake from Ecostats workshop](https://github.com/nicercode/2015.12.08-EcoStats)
* [Tutorial on remake using the Gapminder data](https://github.com/ropenscilabs/remake-tutorial)
* [Tutorial on remake using cooking recipes as examples](https://github.com/krlmlr/remake-tutorial)


# Installation

Install using [devtools](https://github.com/hadley/devtools):

```r
devtools::install_github(""richfitz/remake"")
```

If you don't have devtools installed you will see an error ""there is no package called 'devtools'""; if that happens install devtools with `install.packages(""devtools"")`.

remake depends on several R packages, all of which can be installed from CRAN.  The required packages are:

* [`R6`](http://cran.r-project.org/web/packages/R6) for holding things together
* [`yaml`](http://cran.r-project.org/web/packages/yaml) for reading the configuration
* [`digest`](http://cran.r-project.org/web/packages/digest) for efficiently hashing objects
* [`crayon`](https://github.com/gaborcsardi/crayon) for coloured output on the terminal (not in Rstudio or Rgui)
* [`optparse`](http://cran.r-project.org/web/packages/optparse) for a command line version (run from outside of an R session)

```r
install.packages(c(""R6"", ""yaml"", ""digest"", ""crayon"", ""optparse""))
```

We also depend on `storr` for object storage:

```r
devtools::install_github(""richfitz/storr"")
```
",2023-07-07 18:47:20+00:00
resolwe,resolwe,genialis/resolwe,Open source dataflow engine in Django,,False,33,2023-04-03 11:29:49+00:00,2015-04-09 16:07:20+00:00,27,11,17,0,,,Apache License 2.0,1998,rm,435,2021-02-09 17:31:12+00:00,2023-07-07 15:36:15+00:00,2023-07-04 06:34:33+00:00,"=======
Resolwe
=======

|build| |coverage| |docs| |black| |pypi_version| |pypi_pyversions| |pypi_downloads|

.. |build| image:: https://github.com/genialis/resolwe/workflows/Resolwe%20CI/badge.svg?branch=master
    :target: https://github.com/genialis/resolwe/actions?query=branch%3Amaster
    :alt: Build Status

.. |coverage| image:: https://img.shields.io/codecov/c/github/genialis/resolwe/master.svg
    :target: http://codecov.io/github/genialis/resolwe?branch=master
    :alt: Coverage Status

.. |docs| image:: https://readthedocs.org/projects/resolwe/badge/?version=latest
    :target: http://resolwe.readthedocs.io/
    :alt: Documentation Status

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
    :alt: Code Style Black

.. |pypi_version| image:: https://img.shields.io/pypi/v/resolwe.svg
    :target: https://pypi.org/project/resolwe
    :alt: Version on PyPI

.. |pypi_pyversions| image:: https://img.shields.io/pypi/pyversions/resolwe.svg
    :target: https://pypi.org/project/resolwe
    :alt: Supported Python versions

.. |pypi_downloads| image:: https://pepy.tech/badge/resolwe
    :target: https://pepy.tech/project/resolwe
    :alt: Number of downloads from PyPI

Resolwe is an open source dataflow package for `Django framework`_. We envision
Resolwe to follow the `Common Workflow Language`_ specification, but the
current implementation does not yet fully support it. Resolwe offers a complete
RESTful API to connect with external resources. A collection of bioinformatics
pipelines is available in `Resolwe Bioinformatics`_.

.. _Django framework: https://www.djangoproject.com/
.. _Common Workflow Language: https://github.com/common-workflow-language/common-workflow-language
.. _Resolwe Bioinformatics: https://github.com/genialis/resolwe-bio


Docs & Help
===========

Read about architecture, getting started, how to write `processes`, RESTful API
details, and API Reference in the documentation_.

To chat with developers or ask for help, join us on Slack_.

.. _documentation: http://resolwe.readthedocs.io/
.. _Slack: http://resolwe.slack.com/


Install
=======

Prerequisites
-------------

Resolwe runs on Python_ 3.6 or later If you don't have it yet, follow `these
instructions <https://docs.python.org/3/using/index.html>`__.

It's easiest to run other required services in Docker containers If you don't
have it yet, you can follow the `official Docker tutorial`_ for Mac and for
Windows or install it as a distribution's package in most of standard Linux
distributions (Fedora, Ubuntu,...).

.. _Python: https://www.python.org/
.. _official Docker tutorial: https://docs.docker.com/get-started/

Using PyPI_
-----------

.. code::

    pip install resolwe

.. _PyPI: https://pypi.org/

Using source on GitHub_
-----------------------

.. code::

   pip install https://github.com/genialis/resolwe/archive/<git-tree-ish>.tar.gz

where ``<git-tree-ish>`` can represent any commit SHA, branch name, tag name,
etc. in `Resolwe's GitHub repository`_. For example, to install the latest
Resolwe from the ``master`` branch, use:

.. code::

   pip install https://github.com/genialis/resolwe/archive/master.tar.gz

.. _`Resolwe's GitHub repository`: https://github.com/genialis/resolwe/
.. _GitHub: `Resolwe's GitHub repository`_


Contribute
==========

We welcome new contributors. To learn more, read Contributing_ section of our
documentation.

.. _Contributing: http://resolwe.readthedocs.io/en/latest/contributing.html
",2023-07-07 18:47:23+00:00
robinhood'sfaust,faust,faust-streaming/faust,Python Stream Processing. A Faust fork,https://faust-streaming.github.io/faust/,False,1098,2023-07-06 10:51:46+00:00,2020-10-22 15:32:50+00:00,151,21,129,65,v0.10.14,2023-06-26 20:57:49+00:00,Other,4529,v0.10.14,85,2023-06-26 20:57:49+00:00,2023-07-07 13:57:05+00:00,2023-06-30 16:11:10+00:00,"![faust](https://raw.githubusercontent.com/robinhood/faust/8ee5e209322d9edf5bdb79b992ef986be2de4bb4/artwork/banner-alt1.png)

# Python Stream Processing Fork

![python versions](https://img.shields.io/pypi/pyversions/faust-streaming.svg)
![version](https://img.shields.io/pypi/v/faust-streaming)
[![codecov](https://codecov.io/gh/faust-streaming/faust/branch/master/graph/badge.svg?token=QJFBYNN0JJ)](https://codecov.io/gh/faust-streaming/faust)
[![slack](https://img.shields.io/badge/slack-Faust-brightgreen.svg?logo=slack)](https://join.slack.com/t/fauststreaming/shared_invite/zt-1q1jhq4kh-Q1t~rJgpyuMQ6N38cByE9g)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
![pre-commit](https://img.shields.io/badge/pre--commit-enabled-green)
![license](https://img.shields.io/pypi/l/faust-streaming)
![downloads](https://img.shields.io/pypi/dw/faust-streaming)

## Installation

`pip install faust-streaming`

## Documentation

- `introduction`: https://faust-streaming.github.io/faust/introduction.html
- `quickstart`: https://faust-streaming.github.io/faust/playbooks/quickstart.html
- `User Guide`: https://faust-streaming.github.io/faust/userguide/index.html

## Why the fork

We have decided to fork the original `Faust` project because there is a critical process of releasing new versions which causes uncertainty in the community. Everybody is welcome to contribute to this `fork`, and you can be added as a maintainer.

We want to:

- Ensure continues release
- Code quality
- Use of latest versions of kafka drivers (for now only [aiokafka](https://github.com/aio-libs/aiokafka))
- Support kafka transactions
- Update the documentation

and more...

## Usage

```python
# Python Streams
# Forever scalable event processing & in-memory durable K/V store;
# as a library w/ asyncio & static typing.
import faust
```

**Faust** is a stream processing library, porting the ideas from
`Kafka Streams` to Python.

It is used at `Robinhood` to build high performance distributed systems
and real-time data pipelines that process billions of events every day.

Faust provides both *stream processing* and *event processing*,
sharing similarity with tools such as `Kafka Streams`, `Apache Spark`, `Storm`, `Samza`, `Flink`,

It does not use a DSL, it's just Python!
This means you can use all your favorite Python libraries
when stream processing: NumPy, PyTorch, Pandas, NLTK, Django,
Flask, SQLAlchemy, ++

Faust requires Python 3.6 or later for the new `async/await`_ syntax,
and variable type annotations.

Here's an example processing a stream of incoming orders:

```python

app = faust.App('myapp', broker='kafka://localhost')

# Models describe how messages are serialized:
# {""account_id"": ""3fae-..."", amount"": 3}
class Order(faust.Record):
    account_id: str
    amount: int

@app.agent(value_type=Order)
async def order(orders):
    async for order in orders:
        # process infinite stream of orders.
        print(f'Order for {order.account_id}: {order.amount}')
```

The Agent decorator defines a ""stream processor"" that essentially
consumes from a Kafka topic and does something for every event it receives.

The agent is an `async def` function, so can also perform
other operations asynchronously, such as web requests.

This system can persist state, acting like a database.
Tables are named distributed key/value stores you can use
as regular Python dictionaries.

Tables are stored locally on each machine using a super fast
embedded database written in C++, called `RocksDB`.

Tables can also store aggregate counts that are optionally ""windowed""
so you can keep track
of ""number of clicks from the last day,"" or
""number of clicks in the last hour."" for example. Like `Kafka Streams`,
we support tumbling, hopping and sliding windows of time, and old windows
can be expired to stop data from filling up.

For reliability, we use a Kafka topic as ""write-ahead-log"".
Whenever a key is changed we publish to the changelog.
Standby nodes consume from this changelog to keep an exact replica
of the data and enables instant recovery should any of the nodes fail.

To the user a table is just a dictionary, but data is persisted between
restarts and replicated across nodes so on failover other nodes can take over
automatically.

You can count page views by URL:

```python
# data sent to 'clicks' topic sharded by URL key.
# e.g. key=""http://example.com"" value=""1""
click_topic = app.topic('clicks', key_type=str, value_type=int)

# default value for missing URL will be 0 with `default=int`
counts = app.Table('click_counts', default=int)

@app.agent(click_topic)
async def count_click(clicks):
    async for url, count in clicks.items():
        counts[url] += count
```

The data sent to the Kafka topic is partitioned, which means
the clicks will be sharded by URL in such a way that every count
for the same URL will be delivered to the same Faust worker instance.

Faust supports any type of stream data: bytes, Unicode and serialized
structures, but also comes with ""Models"" that use modern Python
syntax to describe how keys and values in streams are serialized:

```python
# Order is a json serialized dictionary,
# having these fields:

class Order(faust.Record):
    account_id: str
    product_id: str
    price: float
    quantity: float = 1.0

orders_topic = app.topic('orders', key_type=str, value_type=Order)

@app.agent(orders_topic)
async def process_order(orders):
    async for order in orders:
        # process each order using regular Python
        total_price = order.price * order.quantity
        await send_order_received_email(order.account_id, order)
```

Faust is statically typed, using the `mypy` type checker,
so you can take advantage of static types when writing applications.

The Faust source code is small, well organized, and serves as a good
resource for learning the implementation of `Kafka Streams`.

**Learn more about Faust in the** `introduction` **introduction page**
    to read more about Faust, system requirements, installation instructions,
    community resources, and more.

**or go directly to the** `quickstart` **tutorial**
    to see Faust in action by programming a streaming application.

**then explore the** `User Guide`
    for in-depth information organized by topic.

- `Robinhood`: http://robinhood.com
- `async/await`:https://medium.freecodecamp.org/a-guide-to-asynchronous-programming-in-python-with-asyncio-232e2afa44f6
- `Celery`: http://celeryproject.org
- `Kafka Streams`: https://kafka.apache.org/documentation/streams
- `Apache Spark`: http://spark.apache.org
- `Storm`: http://storm.apache.org
- `Samza`: http://samza.apache.org
- `Flink`: http://flink.apache.org
- `RocksDB`: http://rocksdb.org
- `Aerospike`: https://www.aerospike.com/
- `Apache Kafka`: https://kafka.apache.org

## Local development

1. Clone the project
2. Create a virtualenv: `python3.7 -m venv venv && source venv/bin/activate`
3. Install the requirements: `./scripts/install`
4. Run lint: `./scripts/lint`
5. Run tests: `./scripts/tests`

## Faust key points

### Simple

Faust is extremely easy to use. To get started using other stream processing
solutions you have complicated hello-world projects, and
infrastructure requirements.  Faust only requires Kafka,
the rest is just Python, so If you know Python you can already use Faust to do
stream processing, and it can integrate with just about anything.

Here's one of the easier applications you can make::

```python
import faust

class Greeting(faust.Record):
    from_name: str
    to_name: str

app = faust.App('hello-app', broker='kafka://localhost')
topic = app.topic('hello-topic', value_type=Greeting)

@app.agent(topic)
async def hello(greetings):
    async for greeting in greetings:
        print(f'Hello from {greeting.from_name} to {greeting.to_name}')

@app.timer(interval=1.0)
async def example_sender(app):
    await hello.send(
        value=Greeting(from_name='Faust', to_name='you'),
    )

if __name__ == '__main__':
    app.main()
```

You're probably a bit intimidated by the `async` and `await` keywords,
but you don't have to know how ``asyncio`` works to use
Faust: just mimic the examples, and you'll be fine.

The example application starts two tasks: one is processing a stream,
the other is a background thread sending events to that stream.
In a real-life application, your system will publish
events to Kafka topics that your processors can consume from,
and the background thread is only needed to feed data into our
example.

### Highly Available

Faust is highly available and can survive network problems and server
crashes.  In the case of node failure, it can automatically recover,
and tables have standby nodes that will take over.

### Distributed

Start more instances of your application as needed.

### Fast

A single-core Faust worker instance can already process tens of thousands
of events every second, and we are reasonably confident that throughput will
increase once we can support a more optimized Kafka client.

### Flexible

Faust is just Python, and a stream is an infinite asynchronous iterator.
If you know how to use Python, you already know how to use Faust,
and it works with your favorite Python libraries like Django, Flask,
SQLAlchemy, NLTK, NumPy, SciPy, TensorFlow, etc.

## Bundles

Faust also defines a group of ``setuptools`` extensions that can be used
to install Faust and the dependencies for a given feature.

You can specify these in your requirements or on the ``pip``
command-line by using brackets. Separate multiple bundles using the comma:

```sh
pip install ""faust-streaming[rocksdb]""

pip install ""faust-streaming[rocksdb,uvloop,fast,redis,aerospike]""
```

The following bundles are available:

## Faust with extras

### Stores

#### RocksDB

For using `RocksDB` for storing Faust table state. **Recommended in production.**

`pip install faust-streaming[rocksdb]` (uses RocksDB 6)

`pip install faust-streaming[rocksdict]` (uses RocksDB 8, not backwards compatible with 6)


#### Aerospike

`pip install faust-streaming[aerospike]` for using `Aerospike` for storing Faust table state. **Recommended if supported**

### Aerospike Configuration
Aerospike can be enabled as the state store by specifying
`store=""aerospike://""`

By default, all tables backed by Aerospike use `use_partitioner=True` and generate changelog topic events similar
to a state store backed by RocksDB.
The following configuration options should be passed in as keys to the options parameter in [Table](https://faust-streaming.github.io/faust/reference/faust.tables.html)
`namespace` : aerospike namespace

`ttl`: TTL for all KV's in the table

`username`: username to connect to the Aerospike cluster

`password`: password to connect to the Aerospike cluster

`hosts` : the hosts parameter as specified in the [aerospike client](https://www.aerospike.com/apidocs/python/aerospike.html)

`policies`: the different policies for read/write/scans [policies](https://www.aerospike.com/apidocs/python/aerospike.html)

`client`: a dict of `host` and `policies` defined above

### Caching

`faust-streaming[redis]` for using `Redis` as a simple caching backend (Memcached-style).

### Codecs

`faust-streaming[yaml]` for using YAML and the `PyYAML` library in streams.

### Optimization

`faust-streaming[fast]` for installing all the available C speedup extensions to Faust core.

### Sensors

`faust-streaming[datadog]` for using the `Datadog` Faust monitor.

`faust-streaming[statsd]` for using the `Statsd` Faust monitor.

`faust-streaming[prometheus]` for using the `Prometheus` Faust monitor.

### Event Loops

`faust-streaming[uvloop]` for using Faust with `uvloop`.

`faust-streaming[eventlet]` for using Faust with `eventlet`

### Debugging

`faust-streaming[debug]` for using `aiomonitor` to connect and debug a running Faust worker.

`faust-streaming[setproctitle]`when the `setproctitle` module is installed the Faust worker will use it to set a nicer process name in `ps`/`top` listings.vAlso installed with the `fast` and `debug` bundles.

## Downloading and installing from source

Download the latest version of Faust from https://pypi.org/project/faust-streaming/

You can install it by doing:

```sh
$ tar xvfz faust-streaming-0.0.0.tar.gz
$ cd faust-streaming-0.0.0
$ python setup.py build
# python setup.py install
```

The last command must be executed as a privileged user if
you are not currently using a virtualenv.

## Using the development version

### With pip

You can install the latest snapshot of Faust using the following `pip` command:

```sh
pip install https://github.com/faust-streaming/faust/zipball/master#egg=faust
```

## FAQ

### Can I use Faust with Django/Flask/etc

Yes! Use ``eventlet`` as a bridge to integrate with ``asyncio``.

### Using eventlet

This approach works with any blocking Python library that can work with `eventlet`

Using `eventlet` requires you to install the `faust-aioeventlet` module,
and you can install this as a bundle along with Faust:

```sh
pip install -U faust-streaming[eventlet]
```

Then to actually use eventlet as the event loop you have to either
use the `-L <faust --loop>` argument to the `faust` program:

```sh
faust -L eventlet -A myproj worker -l info
```

or add `import mode.loop.eventlet` at the top of your entry point script:

```python
#!/usr/bin/env python3
import mode.loop.eventlet  # noqa
```

It's very important this is at the very top of the module,
and that it executes before you import libraries.

### Can I use Faust with Tornado

Yes! Use the `tornado.platform.asyncio` [bridge](http://www.tornadoweb.org/en/stable/asyncio.html)

### Can I use Faust with Twisted

Yes! Use the `asyncio` reactor implementation: https://twistedmatrix.com/documents/current/api/twisted.internet.asyncioreactor.html

### Will you support Python 2.7 or Python 3.5

No. Faust requires Python 3.8 or later, since it heavily uses features that were
introduced in Python 3.6 (`async`, `await`, variable type annotations).

### I get a maximum number of open files exceeded error by RocksDB when running a Faust app locally. How can I fix this

You may need to increase the limit for the maximum number of open files.
On macOS and Linux you can use:

```ulimit -n max_open_files``` to increase the open files limit to max_open_files.

On docker, you can use the --ulimit flag:

```docker run --ulimit nofile=50000:100000 <image-tag>```
where 50000 is the soft limit, and 100000 is the hard limit [See the difference](https://unix.stackexchange.com/a/29579).

### What kafka versions faust supports

Faust supports kafka with version >= 0.10.

## Getting Help

### Slack

For discussions about the usage, development, and future of Faust, please join the `fauststream` Slack.

- https://fauststream.slack.com
- Sign-up: https://join.slack.com/t/fauststreaming/shared_invite/zt-1q1jhq4kh-Q1t~rJgpyuMQ6N38cByE9g

## Resources

### Bug tracker

If you have any suggestions, bug reports, or annoyances please report them
to our issue tracker at https://github.com/faust-streaming/faust/issues/

## License

This software is licensed under the `New BSD License`. See the `LICENSE` file in the top distribution directory for the full license text.

### Contributing

Development of `Faust` happens at [GitHub](https://github.com/faust-streaming/faust)

You're highly encouraged to participate in the development of `Faust`.

### Code of Conduct

Everyone interacting in the project's code bases, issue trackers, chat rooms,
and mailing lists is expected to follow the Faust Code of Conduct.

As contributors and maintainers of these projects, and in the interest of fostering
an open and welcoming community, we pledge to respect all people who contribute
through reporting issues, posting feature requests, updating documentation,
submitting pull requests or patches, and other activities.

We are committed to making participation in these projects a harassment-free
experience for everyone, regardless of level of experience, gender,
gender identity and expression, sexual orientation, disability,
personal appearance, body size, race, ethnicity, age,
religion, or nationality.

Examples of unacceptable behavior by participants include:

- The use of sexualized language or imagery
- Personal attacks
- Trolling or insulting/derogatory comments
- Public or private harassment
- Publishing other's private information, such as physical or electronic addresses, without explicit permission
- Other unethical or unprofessional conduct.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct. By adopting this Code of Conduct,
project maintainers commit themselves to fairly and consistently applying
these principles to every aspect of managing this project. Project maintainers
who do not follow or enforce the Code of Conduct may be permanently removed from
the project team.

This code of conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community.

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by opening an issue or contacting one or more of the project maintainers.
",2023-07-07 18:47:28+00:00
roddy,Roddy,TheRoddyWMS/Roddy,The Roddy workflow development and management system.,http://roddy-documentation.readthedocs.io,False,9,2023-06-06 16:00:38+00:00,2016-07-21 08:04:37+00:00,3,7,5,35,3.5.11,2023-02-06 13:33:44+00:00,MIT License,1624,3.7.3,62,2023-02-06 13:28:16+00:00,2023-06-06 16:00:38+00:00,2023-05-09 13:47:16+00:00,"[![CircleCI](https://circleci.com/gh/TheRoddyWMS/Roddy/tree/master.svg?style=svg)](https://circleci.com/gh/TheRoddyWMS/Roddy/tree/master) [![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FTheRoddyWMS%2FRoddy.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FTheRoddyWMS%2FRoddy?ref=badge_shield)

# What is Roddy? 

Roddy is a framework for development and management of workflows on a batch processing cluster. It has been developed at the German Cancer Research Center (DKFZ) in Heidelberg in the eilslabs group and is used by a number of in-house workflows such as the [PanCancer Alignment Workflow](https://github.com/DKFZ-ODCF/AlignmentAndQCWorkflows) and the [ACEseq workflow](https://github.com/eilslabs/ACEseqWorkflow). The development is now continued in the Omics IT and Data Management Core Facility (ODCF) at the DKFZ.

> <table><tr><td><a href=""https://www.denbi.de/""><img src=""docs/images/denbi.png"" alt=""de.NBI logo"" width=""300"" align=""left""></a></td><td><strong>Your opinion matters!</strong> The development of Roddy is supported by the <a href=""https://www.denbi.de/"">German Network for Bioinformatic Infrastructure (de.NBI)</a>. By completing <a href=""https://www.surveymonkey.de/r/denbi-service?sc=hd-hub&tool=roddy"">this very short survey</a> you support our efforts to improve this tool.</td></tr></table>

> No new features will be implemented for Roddy! We will continue to fix bugs occurring with currently existing workflows. On the long run, existing workflows should be migrated to other workflow management systems. 

# Documentation

You can find the documentation at [Read the Docs](http://roddy-documentation.readthedocs.io), including [detailed installation instructions](https://roddy-documentation.readthedocs.io/en/latest/installationGuide.html).

# Workflows

The following workflows have been developed at the DKFZ based on Roddy as workflow management system:

  * [Alignment and QC workflows](https://github.com/DKFZ-ODCF/AlignmentAndQCWorkflows)
  * [SNV-Calling workflow](https://github.com/DKFZ-ODCF/SNVCallingWorkflow)
  * [ACEseq workflow](https://github.com/DKFZ-ODCF/ACEseqWorkflow) for copy-number variation calling
  * [InDel-Calling workflow](https://github.com/DKFZ-ODCF/IndelCallingWorkflow) workflow
  * [Sophia workflow](https://github.com/DKFZ-ODCF/SophiaWorkflow) for structural variation calling
  * [RNA-seq workflow](https://github.com/DKFZ-ODCF/RNAseqWorkflow)
  * CNVkit for copy-number variation calling on exome data (to be published)
  * Leaf-Cutter workflow
  * [Bam-to-FASTQ](https://github.com/TheRoddyWMS/BamToFastqPlugin) plugin
  
The following plugins are available as support for the workflows:

  * [COWorkflowBasePlugin](https://github.com/DKFZ-ODCF/COWorkflowsBasePlugin) with basic control code for many of the workflows
  * [PluginBase](https://github.com/TheRoddyWMS/Roddy-Base-Plugin) just the plugin base-class from which other plugins are derived
  * [DefaultPlugin](https://github.com/TheRoddyWMS/Roddy-Default-Plugin) with the `wrapInScript.sh` that wraps all Roddy cluster jobs
",2023-07-07 18:47:33+00:00
rubra,rubra,bjpop/rubra,Infrastructure code to support DNA pipeline,,False,38,2022-03-28 08:42:43+00:00,2012-01-23 01:02:06+00:00,18,9,3,0,,,MIT License,72,,0,,,2015-05-05 05:15:10+00:00,"Rubra: a bioinformatics pipeline.
---------------------------------

https://github.com/bjpop/rubra

License:
--------

Rubra is licensed under the MIT license. See LICENSE.txt.

Description:
------------

Rubra is a pipeline system for bioinformatics workflows. It is built on top
of the Ruffus (http://www.ruffus.org.uk/) Python library, and adds support
for running pipeline stages on a distributed compute cluster.

Authors:
--------

Bernie Pope, Clare Sloggett, Gayle Philip, Matthew Wakefield

Installation:
-------------

To install, clone this repository and run `setup.py`:

    git clone https://github.com/bjpop/rubra
    cd rubra
    python setup.py install

If you are on a system where you do not have administrative privileges, we
suggest using virtualenv ( http://www.virtualenv.org/ ). On HPC systems you 
may find virtualenv is already installed.

Usage:
------

usage: rubra [-h] PIPELINE_FILE --config CONFIG_FILE
                [CONFIG_FILE ...] [--verbose {0,1,2}]
                [--style {print,run,touchfiles,flowchart}] [--force TASKNAME]
                [--end TASKNAME] [--rebuild {fromstart,fromend}]

A bioinformatics pipeline system.

optional arguments:
  -h, --help            show this help message and exit
  PIPELINE_FILE         Your Ruffus pipeline stages (a Python module)
  --config CONFIG_FILE [CONFIG_FILE ...]
                        One or more configuration files (Python modules)
  --verbose {0,1,2}     Output verbosity level: 0 = quiet; 1 = normal; 2 =
                        chatty (default is 1)
  --style {print,run,touchfiles,flowchart}
                        Pipeline behaviour: print; run; touchfiles; flowchart (default is
                        print)
  --force TASKNAME      tasks which are forced to be out of date regardless of
                        timestamps
  --end TASKNAME        end points (tasks) for the pipeline
  --rebuild {fromstart,fromend}
                        rebuild outputs by working back from end tasks or
                        forwards from start tasks (default is fromstart)

Example:
--------

Below is a little example pipeline which you can find in the Rubra source
tree. It counts the number of lines in two files (test/data1.txt and
test/data2.txt), and then sums the results together.

   rubra example_pipeline.py --config example_config.py --style run

There are 2 lines in the first file and 1 line in the second file. So the
result is 3, which is written to the output file test/total.txt.

The --pipeline argument is a Python script which contains the actual
code for each pipeline stage (using Ruffus notation). The --config
argument is a Python script which contains configuration options for the
whole pipeline, plus options for each stage (including the shell command
to run in the stage). The --style argument says what to do with the pipeline:
""run"" means ""perform the out-of-date steps in the pipeline"". The default
style is ""print"" which just displays what the pipeline would do if it were
run. You can get a diagram of the pipeline using the ""flowchart"" style. You 
can touch all files in order using the ""touchfiles"" style, which is mostly 
useful for forcing Ruffus to acknowledge that a set of steps is up to date.

Configuration:
--------------

Configuration options are written into one or more Python scripts, which
are passed to Rubra via the --config command line argument.

Some options are required, and some are, well, optional.

Options for the whole pipeline:
-------------------------------

    pipeline = {
        ""logDir"": ""log"",
        ""logFile"": ""pipeline.log"",
        ""procs"": 2,
        ""end"": [""total""],
    }


Options for each stage of the pipeline:
---------------------------------------

    stageDefaults = {
        ""distributed"": False,
        ""walltime"": ""00:10:00"",
        ""memInGB"": 1,
        ""queue"": ""batch"",
        ""modules"": [""python-gcc""]
    }

    stages = {
        ""countLines"": {
            ""command"": ""wc -l %file > %out"",
        },
        ""total"": {
            ""command"": ""./test/total.py %files > %out"",
        },
    }
",2023-07-07 18:47:39+00:00
ruigi,ruigi,kirillseva/ruigi,"Ruigi is a pipeline specialist, much like his python counterpart, Luigi.",,False,40,2022-03-29 03:23:57+00:00,2015-05-11 04:06:21+00:00,6,2,3,1,0.1.2.9000,2016-02-23 16:48:16+00:00,,69,0.1.2.9000,4,2016-02-23 16:48:16+00:00,,2019-05-26 10:04:10+00:00,"Ruigi [![Build Status](https://img.shields.io/travis/kirillseva/ruigi/master.svg)](https://travis-ci.org/kirillseva/ruigi) [![Coverage Status](https://img.shields.io/coveralls/kirillseva/ruigi.svg)](https://coveralls.io/r/kirillseva/ruigi) ![Release Tag](https://img.shields.io/github/tag/kirillseva/ruigi.svg)
===========

![Ruigi himself](http://youchew.net/wiki/images/2/2b/Ruigi.jpg)

Ruigi is a pipeline specialist, much like his python counterpart,
[Luigi](https://github.com/spotify/luigi).

How to use
----
Ruigi has two simple concepts that you need to understand in order to use it.
The first one is the concept of a **target**.

A target is an abstraction of an output of a computation that also encloses methods for reading and writing.
For example, a `.csv` file can be a valid target. [Here](https://github.com/kirillseva/ruigi/blob/master/R/csv_target.R)
is how it is defined in Ruigi. To use this target you can just define it like

```r
target <- CSVtarget$new(""~/Desktop/output.csv"")
target$exists() # [1] FALSE
target$write(iris)
target$exists() # [1] TRUE
identical(iris, target$read()) # [1] TRUE
```

The second abstraction is a **task**. A task is an abstraction of a confined computation module,
which can later become a part of a big computation pipeline.
When you define a pipeline, Ruigi will automatically determine the optimal order of execution for the tasks,
discover the dependencies, and perform checks to see if you have any cyclic dependencies.

A task is defined by its input targets, the output target, and the computation that needs to be performed on
those targets. Note that a task can have 0, 1 or many inputs, but it has to have exactly one output.
If the output target for a task exists, the computation will not be run again, saving you time.

By defining separate abstractions for computation modules (tasks), and inspectable outputs (targets),
you can have your own library of data processing steps that you can combine into pipelines for different use cases.


Example
----
```r
# Prepare expample dataset called titanic.csv
download.file(""https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/ff414a1bcfcba32481e4d4e8db578e55872a2ca1/titanic.csv"",
              destfile = ""./titanic.csv"")

# These can be logically organized into folders and then `source`'d
# prior to defining the pipeline.
reader <- ruigi_task$new(
  requires = list(CSVtarget$new(""./titanic.csv"")),
  target = Rtarget$new(""titanic_data""),
  name = ""I will read a .csv file and store it on .ruigi_env"",
  runner = function(requires, target) {
    out <- requires[[1]]$read()
    target$write(out)
  }
)

writer <- ruigi_task$new(
  requires = list(Rtarget$new(""titanic_data"")),
  target = CSVtarget$new(""./output.csv""),
  name = ""I will read a file from RAM and store it in a .csv"",
  runner = function(requires, target) {
    out <- requires[[1]]$read()
    target$write(out)
  }
)

# Dependencies will be determined and the tasks will be run.
ruigi::pipeline(list(writer, reader))

# Running task:  I will read a .csv file and store it on .ruigi_env ✓
# Running task:  I will read a file from RAM and store it in a .csv ✓

# No need to run the tasks again, the results already exist
ruigi::pipeline(list(reader, writer))
# Skipping:  I will read a .csv file and store it on .ruigi_env
# Skipping:  I will read a file from RAM and store it in a .csv
```

Installation
----
```r
if (!require(""devtools"")) { install.packages(""devtools"") }
devtools::install_github(""avantcredit/AWS.tools"")
devtools::install_github(""kirillseva/cacher"")
devtools::install_github(""robertzk/s3mpi"")
devtools::install_github(""kirillseva/ruigi"")
library(ruigi)
```

Inspiration
----
1. [Luigi](https://github.com/spotify/luigi). A very powerful and
widely used python package.
2. [Make](http://www.gnu.org/software/make/). Classic.
3. [Remake](https://github.com/richfitz/remake). A make alternative
for R. If you prefer to write R code as opposed to oneliners in
`.yml` configs you might enjoy using Ruigi!
",2023-07-07 18:47:42+00:00
sake,sake,tonyfischetti/sake,A self-documenting build automation tool,,False,262,2023-06-19 19:09:37+00:00,2013-12-11 23:27:48+00:00,8,8,3,3,sake-version-1.2,2020-02-04 16:59:38+00:00,MIT License,259,v0.9.7,10,2016-03-30 01:43:33+00:00,2023-05-29 14:34:41+00:00,2020-02-04 17:20:57+00:00,"# Sake

![sake logo](http://statethatiamin.com/media/sakelogo.png)

[![Build Status](https://travis-ci.org/tonyfischetti/sake.svg?branch=master)](https://travis-ci.org/tonyfischetti/sake)
[![License](https://img.shields.io/pypi/l/master-sake.svg)](https://pypi.python.org/pypi/master-sake/)

### What is it?
Sake is a way to easily design, share, build, and visualize workflows with
intricate interdependencies. Sake is self-documenting because the
instructions for building a project also serve as the documentation of the
project's workflow. The first time it's run, sake will build all of the
components of a project in an order that automatically satisfies all
dependencies. For all subsequent runs, sake will only rebuild the parts
of the project that depend on changed files. This cuts down on unnecessary
re-building and lets the user concentrate on their work rather than memorizing
the order in which commands have to be run.

Sake is free, open source cross-platform software under a very permissive
license (MIT Expat) and is written in Python. Sake is stable.

### 

### Examples
To see an example, check out [this project's webpage](http://tonyfischetti.github.io/sake/)

### Installation
This projects depends on
 - Python 2.7, or 3.5 and above (including 3.8!)
 - The networkx python module
 - the PyYAML python module
 - [Graphviz](http://www.graphviz.org)

Assuming you have python and easy\_install installed, just run

    [sudo] easy_install pip
    [sudo] pip install master-sake

More detailed instructions for installation, including platform specific
directions, are available at this project's [webpage.](http://tonyfischetti.github.io/sake/)

### How do I use it
Again, check out [this project's webpage](http://tonyfischetti.github.io/sake/)
for more detailed information
    

### Support or Contact
If you're having trouble using sake; have a question; or want to contribute,
please email me at tony.fischetti@gmail.com
",2023-07-07 18:47:46+00:00
apachesamza,samza,apache/samza,Mirror of Apache Samza,,False,777,2023-07-01 10:52:09+00:00,2015-03-14 07:00:05+00:00,317,61,133,0,,,Apache License 2.0,2573,release-1.8.0-rc0,62,2022-12-19 18:20:23+00:00,2023-07-06 22:10:17+00:00,2023-06-20 18:09:08+00:00,"## What is Samza?  [![Build Status](https://travis-ci.org/apache/samza.svg?branch=master)](https://travis-ci.org/apache/samza)

[Apache Samza](http://samza.apache.org/) is a distributed stream processing framework. It uses [Apache Kafka](http://kafka.apache.org) for messaging, and [Apache Hadoop YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) to provide fault tolerance, processor isolation, security, and resource management.

Samza's key features include:

* **Simple API:** Unlike most low-level messaging system APIs, Samza provides a very simple callback-based ""process message"" API comparable to MapReduce.
* **Managed state:** Samza manages snapshotting and restoration of a stream processor's state. When the processor is restarted, Samza restores its state to a consistent snapshot. Samza is built to handle large amounts of state (many gigabytes per partition).
* **Fault tolerance:** Whenever a machine in the cluster fails, Samza works with YARN to transparently migrate your tasks to another machine.
* **Durability:** Samza uses Kafka to guarantee that messages are processed in the order they were written to a partition, and that no messages are ever lost.
* **Scalability:** Samza is partitioned and distributed at every level. Kafka provides ordered, partitioned, replayable, fault-tolerant streams. YARN provides a distributed environment for Samza containers to run in.
* **Pluggable:** Though Samza works out of the box with Kafka and YARN, Samza provides a pluggable API that lets you run Samza with other messaging systems and execution environments.
* **Processor isolation:** Samza works with Apache YARN, which supports Hadoop's security model, and resource isolation through Linux CGroups.

Check out [Hello Samza](https://samza.apache.org/startup/hello-samza/latest/) to try Samza. Read the [Background](https://samza.apache.org/learn/documentation/latest/introduction/background.html) page to learn more about Samza.

### Building Samza

To build Samza from a git checkout, run:

    ./gradlew clean build

To build Samza from a source release, it is first necessary to download the gradle wrapper script above. This bootstrapping process requires Gradle to be installed on the source machine.  Gradle is available through most package managers or directly from [its website](http://www.gradle.org/).  To bootstrap the wrapper, run:

    gradle -b bootstrap.gradle

After the bootstrap script has completed, the regular gradlew instructions below are available.

### Java Version Support

This project is built with Java 8 and can run in a Java 8 runtime enviornment. Additionally, it also supports running in a Java 11 runtime environment. 
If you intend to use Samza in a Java 11 runtime environment, it means you will also need to use YARN 3.3.4+ and in which case, you should also use 
the `samza-yarn3` module (built with YARN 3.3.4) instead of the `samza-yarn` (built with YARN 2.10.1). There is also a `samza-shell-yarn3` that
depends on the `samza-yarn3` module, so use that shell module if you intend on using Yarn 3.

#### Scala and YARN

Samza builds with [Scala](http://www.scala-lang.org/) 2.11 or 2.12 and [YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) 2.10.1, by default. 
Use the -PscalaSuffix switches to change Scala versions. Samza supports building Scala with 2.11 and 2.12 and provides a YARN 2 module (`samze-yarn`) and a YARN 3 module (`samza-yarn3`).

NOTE: Some modules currently do **not** officially support Java 11 Runtime and are still using the YARN 2.10.1 dependency:
* `samza-yarn`
* `samza-shell`
* `samza-test`
* `samza-hdfs`


    ./gradlew -PscalaSuffix=2.12 clean build 

Also, you can make use of `bin/check-all.sh` in order to test multiple variants of Java JDKs, Scala, and Yarn.

### Testing Samza

To run all tests:

    ./gradlew clean test

To run a single test:

    ./gradlew clean :samza-test:test -Dtest.single=TestStatefulTask

To run key-value performance tests:

    ./gradlew samza-shell:kvPerformanceTest -PconfigPath=file://$PWD/samza-test/src/main/config/perf/kv-perf.properties

To run yarn integration tests:

    ./bin/integration-tests.sh <dir> yarn-integration-tests

To run standalone integration tests:

    ./bin/integration-tests.sh <dir> standalone-integration-tests

### Running checkstyle on the java code ###

    ./gradlew checkstyleMain checkstyleTest

### Job Management

To run a job (defined in a properties file):

    ./gradlew samza-shell:runJob -PconfigPath=/path/to/job/config.properties

To inspect a job's latest checkpoint:

    ./gradlew samza-shell:checkpointTool -PconfigPath=/path/to/job/config.properties

To modify a job's checkpoint (assumes that the job is not currently running), give it a file with the new offset for each partition, in the format `systems.<system>.streams.<topic>.partitions.<partition>=<offset>`:

    ./gradlew samza-shell:checkpointTool -PconfigPath=/path/to/job/config.properties \
        -PnewOffsets=file:///path/to/new/offsets.properties

### Developers

To get Eclipse projects, run:

    ./gradlew eclipse

For IntelliJ, run:

    ./gradlew idea

### Contribution

To start contributing on Samza please read [Rules](http://samza.apache.org/contribute/rules.html) and [Contributor Corner](https://cwiki.apache.org/confluence/display/SAMZA/Contributor%27s+Corner). Notice that **Samza git repository does not support git pull request**.

### Apache Software Foundation

Apache Samza is a top level project of the [Apache Software Foundation](http://www.apache.org/).

![Apache Software Foundation Logo](http://www.apache.org/images/feather.gif)
",2023-07-07 18:47:50+00:00
sbpipe,sbpipe,pdp10/sbpipe,Pipelines for systems modelling of biological networks,http://sbpipe.readthedocs.io,False,1,2020-04-06 00:48:49+00:00,2015-12-03 11:08:50+00:00,1,3,1,5,v4.21.0,2018-06-26 14:46:50+00:00,MIT License,1786,v4.21.0,5,2018-06-26 14:46:08+00:00,,2018-06-26 14:46:08+00:00,"SBpipe package
==============

|Build Status| |Docs Status| |MIT License| |Anaconda Cloud Version| |PyPI version| |CRAN version|

Introduction
------------

SBpipe is an open source software tool for automating repetitive tasks
in model building and simulation. Using basic YAML configuration files,
SBpipe builds a sequence of repeated model simulations or parameter
estimations, performs analyses from this generated sequence, and finally
generates a LaTeX/PDF report. The parameter estimation pipeline offers
analyses of parameter profile likelihood and parameter correlation using
samples from the computed estimates. Specific pipelines for scanning of
one or two model parameters at the same time are also provided.
Pipelines can run on multicore computers, Sun Grid Engine (SGE), or Load
Sharing Facility (LSF) clusters, speeding up the processes of model
building and simulation. If desired, pipelines can also be executed via
`Snakemake`_, a powerful workflow management system. SBpipe can run
models implemented in COPASI, Python or coded in any other programming
language using Python as a wrapper module. Future support for other
software simulators can be dynamically added without affecting the
current implementation.

To install SBpipe, see the documentation: `HTML`_ or `PDF`_.

The R functions used by SBpipe for data analysis can also be retrieved separately
from `CRAN (sbpiper)`_, `bioconda (r-sbpiper)`_, or from `GitHub (sbpiper)`_.

To download the Snakemake workflows for SBpipe, visit the GitHub repository `sbpipe_snake`_.

**Citation:** Dalle Pezze P, Le Novère N. SBpipe: a collection of
pipelines for automating repetitive simulation and analysis tasks. *BMC
Systems Biology*. 2017 Apr;11:46. `DOI:10.1186/s12918-017-0423-3`_

.. figure:: https://github.com/pdp10/sbpipe/blob/master/docs/images/sbpipe_workflow.png
   :alt: SBpipe workflow


Issues / Feature requests
-------------------------

SBpipe is a relatively young project and there is a chance that some
error occurs. Issues and feature requests can be notified using the
github issue tracking system for SBpipe at the web page:
https://github.com/pdp10/sbpipe/issues. To help us better identify and
reproduce your problem, some technical info.

.. _Snakemake: https://snakemake.readthedocs.io
.. _HTML: http://sbpipe.readthedocs.io
.. _PDF: https://media.readthedocs.org/pdf/sbpipe/latest/sbpipe.pdf
.. _`CRAN (sbpiper)`: https://cran.r-project.org/package=sbpiper
.. _`bioconda (r-sbpiper)`: https://anaconda.org/bioconda/r-sbpiper
.. _`GitHub (sbpiper)`: https://github.com/pdp10/sbpiper
.. _sbpipe_snake: https://github.com/pdp10/sbpipe_snake
.. _`DOI:10.1186/s12918-017-0423-3`: https://doi.org/10.1186/s12918-017-0423-3

.. |Build Status| image:: https://travis-ci.org/pdp10/sbpipe.svg?branch=master
   :target: https://travis-ci.org/pdp10/sbpipe
.. |Docs Status| image:: https://readthedocs.org/projects/sbpipe/badge/
   :target: http://sbpipe.readthedocs.io
.. |MIT License| image:: http://img.shields.io/badge/license-MIT-blue.svg
   :target: https://opensource.org/licenses/MIT
.. |Anaconda Cloud Version| image:: https://anaconda.org/bioconda/sbpipe/badges/version.svg
   :target: https://anaconda.org/bioconda/sbpipe
.. |PyPI version| image:: https://badge.fury.io/py/sbpipe.svg
   :target: https://badge.fury.io/py/sbpipe
.. |CRAN version| image:: https://www.r-pkg.org/badges/version/sbpiper
   :target: https://cran.r-project.org/package=sbpiper
",2023-07-07 18:47:55+00:00
sciapps,sciapps,warelab/sciapps,SciApps: a cloud-based platform for reproducible bioinformatics workflows,https://www.sciapps.org,False,2,2020-10-06 14:28:16+00:00,2015-09-16 19:30:43+00:00,1,17,3,0,,,Apache License 2.0,852,,0,,,2020-10-06 14:27:57+00:00,"# SciApps: a cloud-based platform for reproducible bioinformatics workflows
## Introduction
SciApps is a bioinformatics workflow package developed to leverage local clusters or TACC/XSEDE resources for computing and CyVerse Data Store for storage. SciApps is built on top of the Agave API that can also virtualize commercial resources, e.g., Amazon EC2/S3 for computing and storage. Both GUI and RESTful API are available for interactive or batch processing of NGS data.

## Installation of SciApps
    git clone https://github.com/warelab/sciapps.git
    cd sciapps/agavedancer
    sudo npm install -g grunt-cli
    npm install
    grunt package
    sudo /usr/sbin/apachectl graceful  

## Providing CyVerse credentials
Update defaultUser to ""XXX"" in agavedancer/environments/production.yml (or development.yml), and create the following file.

    cd sciapps/agavedancer
    touch .agave
      .agave content:
          {""username"":""XXX"",""password"":""YYY""}

## Setting up iRODS (for accessing CyVerse Data Store)
    wget ftp://ftp.renci.org/pub/irods/releases/4.1.10/centos7/irods-icommands-4.1.10-centos7-x86_64.rpm
    sudo yum install fuse fuse-libs
    sudo rpm -i irods-icommands-4.1.10-centos7-x86_64.rpm 
    cd /usr/share/httpd
    sudo touch irodsEnv
    sudo chmod 664 irodsEnv
    sudo chown apache:apache irodsEnv
      irodsEnv content:
        {
          ""irods_host"": ""data.iplantcollaborative.org"",
          ""irods_user_name"": ""XXX"",
          ""irods_port"": 1247,
          ""irods_zone_name"": ""iplant"",
          ""irods_authentication_file"": ""/usr/share/httpd/irodsA""
        }
    sudo touch irodsA
    sudo chmod 664 irodsA
    sudo chown apache:apache irodsA
    sudo -u apache /bin/bash
    export IRODS_ENVIRONMENT_FILE=/usr/share/httpd/irodsEnv
    iinit

## Integrating new Apps/Tools
Follow this instruction for developing new Agave apps. And put the app json file in the following assets folder (e.g., Bismark-0.14.4.json).

    cd agavedancer/public/assets
    touch agaveAppsList.json
    agaveAppsList.json content
       {
          ""tags"": [""Methylation""],
          ""id"": ""Bismark-0.14.4"",
          ""label"": ""Bismark"",
          ""name"": ""Bismark"",
          ""version"": ""0.14.4""
       },
       {
         ...
       }

## Configuring web server
SciApps.org can be configured with an Apache server using the following demo configuration file. The sciapps.conf file should be placed under /etc/httpd/conf.d/ (Centos 7) or /usr/local/apache2/conf/ (Centos 6). Note that SSL certificate is needed to be able to authenticate to the cloud systems.

    <VirtualHost 143.48.220.100:443>
        SSLEngine on
        SSLCertificateFile /etc/letsencrypt/live/www.sciapps.org/fullchain.pem
        SSLCertificateKeyFile /etc/letsencrypt/live/www.sciapps.org/privkey.pem
        SSLCertificateChainFile /etc/letsencrypt/live/www.sciapps.org/chain.pem
        ServerName       www.sciapps.org
        ServerAlias      sciapps.org
        DocumentRoot     /home/YOURUSERNAME/sciapps/agavedancer/public
        RewriteEngine on
        RewriteRule ^/app_id/(.*)      https://www.sciapps.org/?app_id=$1 [L]
        RewriteRule ^/page/(.*)     https://www.sciapps.org/?page_id=$1 [L]
        RewriteRule ^/data/(.*)     https://www.sciapps.org/?page_id=dataWorkflows&data_item=$1 [L]

        SetEnv DANCER_ENVIRONMENT ""production""
        <Directory ""/home/YOURUSERNAME/sciapps/agavedancer/public"">
            AllowOverride none
            Require all granted
            DirectoryIndex index.html index.php
        </Directory>
        <Location />
            SetHandler perl-script
            PerlResponseHandler Plack::Handler::Apache2
            PerlSetVar psgi_app /home/YOURUSERNAME/sciapps/agavedancer/bin/app.pl
        </Location>
    </VirtualHost>

## Citation
Wang, L., Lu, Z., Van Buren, P., & Ware, D. (2018). SciApps: a cloud-based platform for reproducible bioinformatics workflows. Bioinformatics, 34(22), 3917-3920. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6223375/)
",2023-07-07 18:47:58+00:00
sciflow,SciFlow,kaizhang/SciFlow,Scientific workflow management,https://hackage.haskell.org/package/SciFlow,False,50,2022-08-20 03:08:31+00:00,2015-04-07 18:43:31+00:00,3,9,1,0,,,,214,v0.6.0,3,2017-08-16 23:30:44+00:00,,2022-08-20 05:57:54+00:00,"Scientific workflow management system
=====================================

SciFlow is a DSL for building type-safe computational workflows.
SciFlow is implemented based on the Free Arrow and is heavily inspired by the funflow package.
The differences between SciFlow and funflow are:

1. SciFlow uses Template Haskell to ease the process of workflow specification and to allow composition and reuse of defined workflows.

2. SciFlow supports distributed computing thanks to Cloud Haskell!

![Design](design.png)

Features
--------

1. Easy to use and safe: Provide a simple and flexible way to design type safe
computational pipelines in Haskell.

2. Automatic Checkpointing: The states of intermediate steps are automatically
logged, allowing easy restart upon failures.

3. Parallelism and distributed computing support.

Examples
--------

See examples in the ""examples"" directory for more details.

Featured applications
--------------------

[Here](https://github.com/Taiji-pipeline)
are some bioinformatics pipelines built with SciFlow.
",2023-07-07 18:48:02+00:00
sciluigi,sciluigi,pharmbio/sciluigi,"A light-weight wrapper library around Spotify's Luigi workflow library to make writing scientific workflows more fluent, flexible and modular",http://dx.doi.org/10.1186/s13321-016-0179-6,False,324,2023-06-06 16:59:35+00:00,2015-05-20 16:31:45+00:00,56,20,15,10,0.10.0,2023-01-07 21:48:10+00:00,MIT License,322,v0.9.1-BETA,10,2015-09-14 17:40:40+00:00,2023-06-08 15:18:37+00:00,2023-01-07 22:35:52+00:00,"![SciLuigi Logo](http://i.imgur.com/2aMT04J.png)

[![CircleCI Build status](https://img.shields.io/circleci/build/github/samuell/sciluigi_fork)](https://app.circleci.com/pipelines/github/samuell/sciluigi_fork)

## Project updates

* ***Update Jan 7, 2023***: Version (0.10.0 and) 0.10.1 are released, and should work well at least with
  with Python 3.9 and Luigi 3.1.1. Please [report any issues](https://github.com/pharmbio/sciluigi)!
* ***A paper with the motivation and design decisions behind SciLuigi [now available](http://dx.doi.org/10.1186/s13321-016-0179-6)***
  * If you use SciLuigi in your research, please cite it like this:<br>
    Lampa S, Alvarsson J, Spjuth O. Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles. *J Cheminform*. 2016. doi:[10.1186/s13321-016-0179-6](http://dx.doi.org/10.1186/s13321-016-0179-6).
* ***A Virtual Machine with a realistic, runnable, example workflow in a Jupyter Notebook, is available [here](https://github.com/pharmbio/bioimg-sciluigi-casestudy)***
* ***Watch a 10 minute screencast going through the basics of using SciLuigi [here](https://www.youtube.com/watch?v=gkKUWskRbjw)***
* ***See a poster describing the motivations behind SciLuigi [here](http://dx.doi.org/10.13140/RG.2.1.1143.6246)***

## About SciLuigi

Scientific Luigi (SciLuigi for short) is a light-weight wrapper library around [Spotify](http://spotify.com)'s [Luigi](http://github.com/spotify/luigi)
workflow system that aims to make writing scientific workflows more fluent, flexible and
modular.

Luigi is a flexile and fun-to-use library. It has turned out though
that its default way of defining dependencies by hard coding them in each task's
requires() function is not optimal for some type of workflows common e.g. in bioinformatics where multiple inputs and outputs, complex dependencies,
and the need to quickly try different workflow connectivity in an explorative fashion is central to the way of working.

SciLuigi was designed to solve some of these problems, by providing the following
""features"" over vanilla Luigi:

- Separation of dependency definitions from the tasks themselves,
  for improved modularity and composability.
- Inputs and outputs implemented as separate fields, a.k.a.
  ""ports"", to allow specifying dependencies between specific input
  and output-targets rather than just between tasks. This is again to let such
  details of the network definition reside outside the tasks.
- The fact that inputs and outputs are object fields, also allows auto-completion
  support to ease the network connection work (Works great e.g. with [jedi-vim](https://github.com/davidhalter/jedi-vim)).
- Inputs and outputs are connected with an intuitive ""single-assignment syntax"".
- ""Good default"" high-level logging of workflow tasks and execution times.
- Produces an easy to read audit-report with high level information per task.
- Integration with some HPC workload managers.
  (So far only [SLURM](http://slurm.schedmd.com/) though).

Because of Luigi's easy-to-use API these changes have been implemented
as a very thin layer on top of luigi's own API with no changes at all to the luigi
core, which means that you can continue leveraging the work already being
put into maintaining and further developing luigi by the team at Spotify and others.

## Workflow code quick demo

***For a brief 10 minute screencast going through the basics below, see [this link](https://www.youtube.com/watch?v=gkKUWskRbjw)***

Just to give a quick feel for how a workflow definition might look like in SciLuigi, check this code example
(implementation of tasks hidden here for brevity. See Usage section further below for more details):

```python
import sciluigi as sl

class MyWorkflow(sl.WorkflowTask):
    def workflow(self):
        # Initialize tasks:
        foowrt = self.new_task('foowriter', MyFooWriter)
        foorpl = self.new_task('fooreplacer', MyFooReplacer,
            replacement='bar')

        # Here we do the *magic*: Connecting outputs to inputs:
        foorpl.in_foo = foowrt.out_foo

        # Return the last task(s) in the workflow chain.
        return foorpl
```

That's it! And again, see the ""usage"" section just below for a more detailed description of getting to this!

## Support: Getting help

Please use the [issue queue](https://github.com/pharmbio/sciluigi/issues) for any support questions, rather than mailing the author(s) directly, as the solutions can then help others who face similar issues (we are a very small team with very limited time, so this is important).

## Prerequisites

- Python 2.7 - 3.4
- Luigi 1.3.x - 2.0.1

## Install

1. Install SciLuigi, including its dependencies (luigi etc), through PyPI:

    ```bash
    pip install sciluigi
    ```

2. Now you can use the library by just importing it in your python script, like so:

    ```python
    import sciluigi
    ```

    Note that you can aliase it to a shorter name, for brevity, and to save keystrokes:

    ```python
    import sciluigi as sl
    ```

## Usage

Creating workflows in SciLuigi differs slightly from how it is done in vanilla Luigi.
Very briefly, it is done in these main steps:

1. Create a workflow tasks class
2. Create task classes
3. Add the workflow definition in the workflow class's `workflow()` method.
4. Add a run method at the end of the script
5. Run the script

### Create a Workflow task

The first thing to do when creating a workflow, is to define a workflow task.

You do this by:

1. Creating a subclass of `sciluigi.WorkflowTask`
2. Implementing the `workflow()` method.

#### Example:

```python
import sciluigi

class MyWorkflow(sciluigi.WorkflowTask):
    def workflow(self):
        pass # TODO: Implement workflow here later!
```

### Create tasks

Then, you need to define some tasks that can be done in this workflow.

This is done by:

1. Creating a subclass of `sciluigi.Task` (or `sciluigi.SlurmTask` if you want Slurm support)
2. Adding fields named `in_<yournamehere>` for each input, in the new task class
3. Define methods named `out_<yournamehere>()` for each output, that return `sciluigi.TargetInfo` objects. (sciluigi.TargetInfo is initialized with a reference to the task object itself - typically `self` - and a path name, where upstream tasks paths can be used).
4. Define luigi parameters to the task.
5. Implement the `run()` method of the task.

#### Example:

Let's define a simple task that just writes ""foo"" to a file named `foo.txt`:

```python
class MyFooWriter(sciluigi.Task):
    # We have no inputs here
    # Define outputs:
    def out_foo(self):
        return sciluigi.TargetInfo(self, 'foo.txt')
    def run(self):
        with self.out_foo().open('w') as foofile:
            foofile.write('foo\n')
```

Then, let's create a task that replaces ""foo"" with ""bar"":

```python
class MyFooReplacer(sciluigi.Task):
    replacement = sciluigi.Parameter() # Here, we take as a parameter
                                  # what to replace foo with.
    # Here we have one input, a ""foo file"":
    in_foo = None
    # ... and an output, a ""bar file"":
    def out_replaced(self):
        # As the path to the returned target(info), we
        # use the path of the foo file:
        return sciluigi.TargetInfo(self, self.in_foo().path + '.bar.txt')
    def run(self):
        with self.in_foo().open() as in_f:
            with self.out_replaced().open('w') as out_f:
                # Here we see that we use the parameter self.replacement:
                out_f.write(in_f.read().replace('foo', self.replacement))
```

The last lines, we could have instead written using the command-line `sed` utility, available in linux, by calling it on the commandline, with the built-in `ex()` method:

```python
    def run(self):
        # Here, we use the in-built self.ex() method, to execute commands:
        self.ex(""sed 's/foo/{repl}/g' {inpath} > {outpath}"".format(
            repl=self.replacement,
            inpath=self.in_foo().path,
            outpath=self.out_replaced().path))
```

### Write the workflow definition

Now, we can use these two tasks we created, to create a simple workflow, in our workflow class, that we also created above.

We do this by:

1. Instantiating the tasks, using the `self.new_task(<unique_taskname>, <task_class>, *args, **kwargs)` method, of the workflow task.
2. Connect the tasks together, by pointing the right `out_*` method to the right `in_*` field.
3. Returning the last task in the chain, from the workflow method.

#### Example:

```python
import sciluigi
class MyWorkflow(sciluigi.WorkflowTask):
    def workflow(self):
        foowriter = self.new_task('foowriter', MyFooWriter)
        fooreplacer = self.new_task('fooreplacer', MyFooReplacer,
            replacement='bar')

        # Here we do the *magic*: Connecting outputs to inputs:
        fooreplacer.in_foo = foowriter.out_foo

        # Return the last task(s) in the workflow chain.
        return fooreplacer
```

### Add a run method to the end of the script

Now, the only thing that remains, is adding a run method to the end of the script.

You can use luigi's own `luigi.run()`, or our own two methods:

1. `sciluigi.run()`
2. `sciluigi.run_local()`

The `run_local()` one, is handy if you don't want to run a central scheduler daemon, but just want to run the workflow as a script.

Both of the above take the same options as `luigi.run()`, so you can for example set the main class to use (our workflow task):

```
# End of script ....
if __name__ == '__main__':
    sciluigi.run_local(main_task_cls=MyWorkflow)
```

### Run the workflow

Now, you should be able to run the workflow as simple as:

```bash
python myworkflow.py
```

... provided of course, that the workflow is saved in a file named myworkflow.py.

### More Examples

See the [examples folder](https://github.com/samuell/sciluigi/tree/master/examples) for more detailed examples!

### More links, background info etc.

The basic idea behind SciLuigi, and a preceding solution to it, was
presented in workshop (e-Infra MPS 2015) talk:
- [Slides](http://www.slideshare.net/SamuelLampa/building-workflows-with-spotifys-luigi)
- [Video](https://www.youtube.com/watch?v=f26PqSXZdWM)

See also [this collection of links](http://bionics.it/posts/our-experiences-using-spotifys-luigi-for-bioinformatics-workflows), to more of our reported experiences
using Luigi, which lead up to the creation of SciLuigi.

Known Limitations
-----------------

- Changing the workflow scheduling based on data sent as parameters, is not
  possible.
- If you have an unknown number of outputs from a task, for which you want to
  start a full branch of the workflow, this is not possible either.

Both of the limitations are due to the fact that Luigi does scheduling and
execution separately (with the exception of Luigi's [dynamic dependencies](http://luigi.readthedocs.io/en/stable/tasks.html#dynamic-dependencies),
but they work only for upstream tasks, not downstream tasks, which we would
need).

If you run into any of these problems, you might be interested in a new workflow engine we develop to overcome these limitations:
[SciPipe](http://scipipe.org).

Changelog
---------
- 0.9.3b4
  - Support for Python 3 (Thanks to @jeffcjohnson for contributing this!).
  - Bug fixes.

Contributors
----------------
- [See here](https://github.com/pharmbio/sciluigi/graphs/contributors)

Acknowledgements
----------------
This work has been supported by:
- [Faculty grants of the dept. of Pharmaceutical Biosciences, Uppsala University](http://www.farmbio.uu.se)
- [Bioinformatics Infrastructure for Life Sciences, BILS](https://bils.se)
- [Vinnova](https://www.vinnova.se/) via the project [KoDa - Kollektivtrafikens Datalab](https://www.vinnova.se/p/koda---kollektivtrafikens-datalab/)
  as granted to [Savantic](https://savantic.eu/) and others.

Many ideas and inspiration for the API is taken from:
- [John Paul Morrison's invention and works on Flow-Based Programming](http://jpaulmorrison.com/fbp)

Publications using SciLuigi
---------------------------

Below is an incomplete list of publications using SciLuigi for computational analysis. If you are using SciLuigi in a publication, please consider adding your own here.

Schulz W, Durant T, Siddon A, Torres R. Use of application containers and workflows for genomic data analysis. J Pathol Inform. 2016;7(1):53. DOI: [10.4103/2153-3539.197197](https://dx.doi.org/10.4103%2F2153-3539.197197)

See also: [SciPipe](http://scipipe.org)
---------------------------------------

If you find yourself needing some more advanced scheduling features like dynamic scheduling, or run into performance problems with Python/Luigi/SciLuigi, you might be interested to check out a new workflow engine we develop, in the Go programming language, to cope with some of the limitations we have still faced with Python/Luigi/SciLuigi:
[SciPipe](http://scipipe.org). 

[SciPipe](http://scipipe.org) leverages some of the successful parts of Luigi's API, such as the flexible file name formatting, but replaces the Luigi scheduler with a custom, novel and very light-weight implicit dataflow scheduler written in Go. We find that it makes life much easier for complex workflow constructs as those involving cross validation, and/or nested parameter sweeps.
",2023-07-07 18:48:06+00:00
scipipe,scipipe,scipipe/scipipe,"Robust, flexible and resource-efficient pipelines using Go and the commandline",https://scipipe.org,False,1004,2023-07-07 15:00:48+00:00,2015-03-07 21:47:23+00:00,72,38,10,44,v0.12.0,2021-10-14 09:06:39+00:00,MIT License,983,v0.12.0,44,2021-10-14 09:06:39+00:00,2023-07-07 15:00:49+00:00,2023-02-23 15:15:06+00:00,"<h1 style=""margin-bottom: 0;""><img src=""docs/images/scipipe_logo_bluegrey_horiz_320px.png"" alt=""SciPipe""></h1>

<big>Robust, flexible and resource-efficient pipelines using Go and the commandline</big>

[![Build Status](https://img.shields.io/circleci/project/github/scipipe/scipipe.svg)](https://app.circleci.com/pipelines/github/scipipe)
[![Test Coverage](https://img.shields.io/codecov/c/github/scipipe/scipipe.svg)](https://codecov.io/gh/scipipe/scipipe)
[![Codebeat Grade](https://codebeat.co/badges/96e93624-2ac8-42c9-9e94-2d6e5325d8ff)](https://codebeat.co/projects/github-com-scipipe-scipipe-master)
[![Go Report Card](https://goreportcard.com/badge/github.com/scipipe/scipipe)](https://goreportcard.com/report/github.com/scipipe/scipipe)
[![GoDoc](https://godoc.org/github.com/scipipe/scipipe?status.svg)](https://godoc.org/github.com/scipipe/scipipe)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/scipipe/scipipe)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1157941.svg)](https://doi.org/10.5281/zenodo.1157941)

<strong>Project links: [Documentation & Main Website](http://scipipe.org) | [Issue Tracker](https://github.com/scipipe/scipipe/issues) | [Chat](https://gitter.im/scipipe/scipipe)</strong>

## Why SciPipe?

- **Intuitive:** SciPipe works by flowing data through a network of channels
  and processes
- **Flexible:** Wrapped command-line programs can be combined with processes in
  Go
- **Convenient:** Full control over how your files are named
- **Efficient:** Workflows are compiled to binary code that run fast
- **Parallel:** Pipeline paralellism between processes as well as task
  parallelism for multiple inputs, making efficient use of multiple CPU cores
- **Supports streaming:** Stream data between programs to avoid wasting disk space
- **Easy to debug:** Use available Go debugging tools or just `println()`
- **Portable:** Distribute workflows as Go code or as self-contained executable
  files


## Project updates

- <strong>Jan 2020: New screencast:</strong> <a href=""https://www.youtube.com/watch?v=hi0Uqwddrtg"" target=""_blank"">""Hello World"" scientific workflow in SciPipe</a>
- <strong>May 2019: The SciPipe paper published open access in GigaScience:</strong> <a href=""https://doi.org/10.1093/gigascience/giz044"" target=""_blank"">SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines</a>
- <strong>Nov 2018: Scientific study using SciPipe:</strong> <a href=""https://doi.org/10.3389/fphar.2018.01256"" target=""_blank"">Predicting off-target binding profiles with confidence using Conformal Prediction</a>
- <strong>Slides:</strong> <a href=""https://pharmb.io/blog/saml-gostockholm2018/"">Presentation on SciPipe and more at Go Stockholm Conference</a>
- <strong>Blog post:</strong> <a href=""http://bionics.it/posts/provenance-reports-in-scientific-workflows"">Provenance reports in Scientific Workflows</a> - going into details about how SciPipe is addressing provenance.
- <strong>Blog post:</strong> <a href=""http://bionics.it/posts/first-production-workflow-run-with-scipipe"">First production workflow run with SciPipe</a

## Introduction

<img src=""docs/images/fbp_factory.png"" align=""right"">

SciPipe is a library for writing [Scientific
Workflows](https://en.wikipedia.org/wiki/Scientific_workflow_system), sometimes
also called ""pipelines"", in the [Go programming language](http://golang.org).

When you need to run many commandline programs that depend on each other in
complex ways, SciPipe helps by making the process of running these programs
flexible, robust and reproducible. SciPipe also lets you restart an interrupted
run without over-writing already produced output and produces an audit report
of what was run, among many other things.

SciPipe is built on the proven principles of [Flow-Based Programming](https://en.wikipedia.org/wiki/Flow-based_programming)
(FBP) to achieve maximum flexibility, productivity and agility when designing
workflows.  Compared to plain dataflow, FBP provides the benefits that
processes are fully self-contained, so that a library of re-usable components
can be created, and plugged into new workflows ad-hoc.

Similar to other FBP systems, SciPipe workflows can be likened to a network of
assembly lines in a factory, where items (files) are flowing through a network
of conveyor belts, stopping at different independently running stations
(processes) for processing, as depicted in the picture above.

SciPipe was initially created for problems in bioinformatics and
cheminformatics, but works equally well for any problem involving pipelines of
commandline applications.

**Project status:** SciPipe pretty stable now, and only very minor API changes
might still occur. We have successfully used SciPipe in a handful of both real
and experimental projects, and it has had occasional use outside the research
group as well.

## Known limitations

- There are still a number of missing good-to-have features for workflow
  design. See the [issue tracker](https://github.com/scipipe/scipipe/issues)
  for details.
- There is not (yet) support for the [Common Workflow Language](http://common-workflow-language.github.io).

## Installing

For full installation instructions, see the [intallation page](/install).
For quick getting started steps, you can do:

1. [Download](https://golang.org/dl/) and [install](https://golang.org/doc/install) Go
2. Run the following command, to install the scipipe Go library (don't miss the
   trailing dots!), and create a Go module for your script:

```bash
go install github.com/scipipe/scipipe/...@latest
go mod init myfirstworkflow-module
```

## Hello World example

Let's look at an example workflow to get a feel for what writing workflows in
SciPipe looks like:

```go
package main

import (
    // Import SciPipe, aliased to sp
    sp ""github.com/scipipe/scipipe""
)

func main() {
    // Init workflow and max concurrent tasks
    wf := sp.NewWorkflow(""hello_world"", 4)

    // Initialize processes, and file extensions
    hello := wf.NewProc(""hello"", ""echo 'Hello ' > {o:out|.txt}"")
    world := wf.NewProc(""world"", ""echo $(cat {i:in}) World > {o:out|.txt}"")

    // Define data flow
    world.In(""in"").From(hello.Out(""out""))

    // Run workflow
    wf.Run()
}
```

To create a file with a similar simple example, you can run:

```
scipipe new hello_world.go
```

## Running the example

Let's put the code in a file named `hello_world.go` and run it.

First you need to make sure that the dependencies (SciPipe in this case) is
installed in your local Go module. This you can do with:

```bash
go mod tidy
```

Then you can go ahead and run the workflow:

```bash
$ go run hello_world.go
AUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Starting workflow (Writing log to log/scipipe-20180717-214226-hello_world.log)
AUDIT   2018/07/17 21:42:26 | hello                            | Executing: echo 'Hello ' > hello.out.txt
AUDIT   2018/07/17 21:42:26 | hello                            | Finished: echo 'Hello ' > hello.out.txt
AUDIT   2018/07/17 21:42:26 | world                            | Executing: echo $(cat ../hello.out.txt) World > hello.out.txt.world.out.txt
AUDIT   2018/07/17 21:42:26 | world                            | Finished: echo $(cat ../hello.out.txt) World > hello.out.txt.world.out.txt
AUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Finished workflow (Log written to log/scipipe-20180717-214226-hello_world.log)
```

Let's check what file SciPipe has generated:

```
$ ls -1 hello*
hello.out.txt
hello.out.txt.audit.json
hello.out.txt.world.out.txt
hello.out.txt.world.out.txt.audit.json
```

As you can see, it has created a file `hello.out.txt`, and `hello.out.world.out.txt`, and
an accompanying `.audit.json` for each of these files.

Now, let's check the output of the final resulting file:

```bash
$ cat hello.out.txt.world.out.txt
Hello World
```

Now we can rejoice that it contains the text ""Hello World"", exactly as a proper
Hello World example should :)

Now, these were a little long and cumbersome filenames, weren't they? SciPipe
gives you very good control over how to name your files, if you don't want to
rely on the automatic file naming. For example, we could set the first filename
to a static one, and then use the first name as a basis for the file name for
the second process, like so:

```go
package main

import (
    // Import the SciPipe package, aliased to 'sp'
    sp ""github.com/scipipe/scipipe""
)

func main() {
    // Init workflow with a name, and max concurrent tasks
    wf := sp.NewWorkflow(""hello_world"", 4)

    // Initialize processes and set output file paths
    hello := wf.NewProc(""hello"", ""echo 'Hello ' > {o:out}"")
    hello.SetOut(""out"", ""hello.txt"")

    world := wf.NewProc(""world"", ""echo $(cat {i:in}) World >> {o:out}"")
    world.SetOut(""out"", ""{i:in|%.txt}_world.txt"")

    // Connect network
    world.In(""in"").From(hello.Out(""out""))

    // Run workflow
    wf.Run()
}
```

In the `{i:in...` part, we are re-using the file path from the file received on
the in-port named 'in', and then running a Bash-style trim-from-end command on
it to remove the `.txt` extension.

Now, if we run this, the file names get a little cleaner:

```bash
$ ls -1 hello*
hello.txt
hello.txt.audit.json
hello_world.go
hello_world.txt
hello_world.txt.audit.json
```

## The audit logs

Finally, we could have a look at one of those audit file created:

```bash
$ cat hello_world.txt.audit.json
{
    ""ID"": ""99i5vxhtd41pmaewc8pr"",
    ""ProcessName"": ""world"",
    ""Command"": ""echo $(cat hello.txt) World \u003e\u003e hello_world.txt.tmp/hello_world.txt"",
    ""Params"": {},
    ""Tags"": {},
    ""StartTime"": ""2018-06-15T19:10:37.955602979+02:00"",
    ""FinishTime"": ""2018-06-15T19:10:37.959410102+02:00"",
    ""ExecTimeNS"": 3000000,
    ""Upstream"": {
        ""hello.txt"": {
            ""ID"": ""w4oeiii9h5j7sckq7aqq"",
            ""ProcessName"": ""hello"",
            ""Command"": ""echo 'Hello ' \u003e hello.txt.tmp/hello.txt"",
            ""Params"": {},
            ""Tags"": {},
            ""StartTime"": ""2018-06-15T19:10:37.950032676+02:00"",
            ""FinishTime"": ""2018-06-15T19:10:37.95468214+02:00"",
            ""ExecTimeNS"": 4000000,
            ""Upstream"": {}
        }
    }
```

Each such audit-file contains a hierarchic JSON-representation of the full
workflow path that was executed in order to produce this file. On the first
level is the command that directly produced the corresponding file, and then,
indexed by their filenames, under ""Upstream"", there is a similar chunk
describing how all of its input files were generated. This process will be
repeated in a recursive way for large workflows, so that, for each file
generated by the workflow, there is always a full, hierarchic, history of all
the commands run - with their associated metadata - to produce that file.

You can find many more examples in the [examples folder](https://github.com/scipipe/scipipe/tree/master/examples) in the GitHub repo.

For more information about how to write workflows using SciPipe, and much more,
see [SciPipe website (scipipe.org)](http://scipipe.org)!

## More material on SciPipe

- See [a poster on SciPipe](http://dx.doi.org/10.13140/RG.2.2.34414.61760), presented at the [e-Science Academy in Lund, on Oct 12-13 2016](essenceofescience.se/event/swedish-e-science-academy-2016-2/).
- See [slides from a recent presentation of SciPipe for use in a Bioinformatics setting](http://www.slideshare.net/SamuelLampa/scipipe-a-lightweight-workflow-library-inspired-by-flowbased-programming).
- The architecture of SciPipe is based on an [flow-based programming](https://en.wikipedia.org/wiki/Flow-based_programming) like
  pattern in pure Go presented in
  [this](http://blog.gopheracademy.com/composable-pipelines-pattern) and
  [this](https://blog.gopheracademy.com/advent-2015/composable-pipelines-improvements/)
  blog posts on Gopher Academy.

## Citing SciPipe

If you use SciPipe in academic or scholarly work, please cite the following paper as source:

Lampa S, Dahlö M, Alvarsson J, Spjuth O. SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines 
_Gigascience_. 8, 5 (2019). DOI: [10.1093/gigascience/giz044](https://dx.doi.org/10.1093/gigascience/giz044)

## Acknowledgements

- SciPipe is very heavily dependent on the proven principles form [Flow-Based
  Programming (FBP)](http://www.jpaulmorrison.com/fbp), as invented by [John Paul Morrison](http://www.jpaulmorrison.com/fbp).
  From Flow-based programming, SciPipe uses the ideas of separate network
  (workflow dependency graph) definition, named in- and out-ports,
  sub-networks/sub-workflows and bounded buffers (already available in Go's
  channels) to make writing workflows as easy as possible.
- This library is has been much influenced/inspired also by the
  [GoFlow](https://github.com/trustmaster/goflow) library by [Vladimir Sibirov](https://github.com/trustmaster/goflow).
- Thanks to [Egon Elbre](http://twitter.com/egonelbre) for helpful input on the
  design of the internals of the pipeline, and processes, which greatly
  simplified the implementation.
- This work is financed by faculty grants and other financing for the [Pharmaceutical Bioinformatics group](http://pharmb.io) of [Dept. of
  Pharmaceutical Biosciences](http://www.farmbio.uu.se) at [Uppsala University](http://www.uu.se), and by [Swedish Research Council](http://vr.se)
  through the Swedish [National Bioinformatics Infrastructure Sweden](http://nbis.se).
- Supervisor for the project is [Ola Spjuth](http://www.farmbio.uu.se/research/researchgroups/pb/olaspjuth).

## Related tools

Find below a few tools that are more or less similar to SciPipe that are worth worth checking out before
deciding on what tool fits you best (in approximate order of similarity to SciPipe):

- [NextFlow](http://nextflow.io)
- [Luigi](https://github.com/spotify/luigi)/[SciLuigi](https://github.com/samuell/sciluigi)
- [BPipe](https://code.google.com/p/bpipe/)
- [SnakeMake](https://bitbucket.org/johanneskoester/snakemake)
- [Cuneiform](https://github.com/joergen7/cuneiform)
",2023-07-07 18:48:10+00:00
scoop,scoop,soravux/scoop,SCOOP (Scalable COncurrent Operations in Python),https://github.com/soravux/scoop,False,596,2023-07-02 09:48:05+00:00,2015-03-22 18:25:14+00:00,89,37,14,0,,,GNU Lesser General Public License v3.0,1022,0.7.1,16,2014-03-17 13:43:20+00:00,2023-07-02 09:48:05+00:00,2022-08-14 23:39:50+00:00,"![SCOOP logo](http://scoop.readthedocs.org/en/latest/_images/logo.png)

SCOOP (Scalable COncurrent Operations in Python) is a distributed task
module allowing concurrent parallel programming on various environments,
from heterogeneous grids to supercomputers. Its documentation is available on http://scoop.readthedocs.org/ .

Philosophy
==========

SCOOP was designed from the following ideas:

  * The **future** is parallel;
  * Simple is beautiful;
  * Parallelism should be simpler.
    
These tenets are translated concretely in a **minimum number of functions** 
allowing **maximum parallel efficiency** while keeping at **minimum the 
inner knowledge required** to use them. It is implemented with Python 3 in mind 
while being compatible with Python 2.6+ to allow fast prototyping without sacrificing 
efficiency and speed.

Some comments we received on SCOOP:

  * ""I must say that that was by far the easiest upgrade I have probably ever done.  I still need to build and test it on the cluster, but on my development machine it took about 10 minutes to upgrade and test."" [deap mailing list](https://groups.google.com/d/msg/deap-users/chQY-2HHZWM/4qZRkQuvbbIJ EBo)

Features
========

SCOOP features and advantages over 
[futures](http://docs.python.org/dev/library/concurrent.futures.html),
[multiprocessing](http://docs.python.org/dev/library/multiprocessing.html)
and similar modules are as follows:

  * Harness the power of **multiple computers** over network;
  * Ability to spawn multiple tasks inside a task;
  * API compatible with [PEP-3148](http://www.python.org/dev/peps/pep-3148/);
  * Parallelizing serial code with only minor modifications;
  * Efficient load-balancing.

Anatomy of a SCOOPed program
----------------------------

SCOOP can handle multiple diversified multi-layered tasks. With it, you can submit your different functions and data simultaneously and effortlessly while the framework executes them locally or remotely. Contrarily to most multiprocessing frameworks, it allows to launch subtasks within tasks.

![Intro tree](http://scoop.readthedocs.org/en/latest/_images/introductory_tree.png)

Through SCOOP, you can execute simultaneously tasks that are different by 
nature, shown by the task color, or different by complexity, shown by the task radius. The module will handle the physical considerations of parallelization, such as task distribution over your resources (load balancing), communications, etc.

Applications
------------

The common applications of SCOOP consist but is not limited to:

  * Evolutionary Algorithms
  * Monte Carlo simulations
  * Data mining
  * Data processing
  * Graph traversal

Citing SCOOP
============

Authors of scientific papers including results generated using SCOOP are encouraged to cite the following paper.

{{{
@inproceedings{SCOOP_XSEDE2014,
  title={Once you SCOOP, no need to fork},
  author={Hold-Geoffroy, Yannick and Gagnon, Olivier and Parizeau, Marc},
  booktitle={Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
  pages={60},
  year={2014},
  organization={ACM}
}
}}}

Useful links
============

You can [download the latest stable version](https://pypi.python.org/pypi/scoop/), check the [project  documentation](http://scoop.readthedocs.org/), post to the [mailing list](http://groups.google.com/group/scoop-users) or [submit an issue](https://github.com/soravux/scoop/issues) if you've found one.
",2023-07-07 18:48:14+00:00
secdataview,SecDATAVIEW,shiyonglu/SecDATAVIEW,A Secure Version of DATAVIEW using SGX techniques.,,False,10,2023-03-14 06:43:32+00:00,2019-03-21 19:09:09+00:00,5,2,3,1,v2.0,2019-09-20 17:20:18+00:00,,38,v2.0,1,2019-09-20 17:20:18+00:00,,2021-07-06 16:47:12+00:00,"SecDATAVIEW: A Secure Big Data Workflow Management System for Heterogeneous Computing Environments
==================================================================================================
SecDATAVIEW is a secure big data workflow management system compatible with the heterogeneous computing environment. It leverages hardware-assisted TEEs such as Intel SGX and AMD SEV to protect the execution of workflows in the untrusted cloud. 
The SecDATAVIEW paper has appeared in proceedings of The 35th Annual Computer Security Applications Conference 
(ACSAC'19), San Juan, Puerto Rico, December 2019. You may access paper here https://dl.acm.org/doi/10.1145/3359789.3359845
The first release of SecDATAVIEW implemented the artifacts of the ACSAC'19 paper. You may download and use the first release corresponding to ACSAC'19 paper.
We have enhanced the SecDATAVIEW with additional security measurements to address the attacks that mainly fake the presence of TEE with leveraging real-time Intel-based SGX attestation and attacks that mainly happen after the workflow execution is finished (e.g., when data owner shutdown VPCs and left the cloud environments). All together is available as the current edition. A new paper regarding modifications with a set of additional experimental results is currently under review. We will provide the venue name and the link to the paper later here.   

Seqence diagram of new Workflow Code Provisioning and Communication (WCPAC) protocol
------------------------------------------------------------------------------------
SecDATAVIEW implemented WCPAC protocol for (1) provisioning and attesting secure worker nodes, (2) securely provisioning the code for the ```Task Executor``` and workflow tasks on each participating worker node, (3) establishing a secure communication channel between the master node and each worker node, and (4) establishing secure communication channels among worker nodes for secure data transfer.
The detail of the WCPAC will be available in the new paper. Also WCPAC steps are mapped at code level and may be located through comments.
![](/WCPAC/WCPAC.png)

Following Video explains WCPAC integration

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/JZJtPtqN_xU/0.jpg)](https://www.youtube.com/watch?v=JZJtPtqN_xU)

Prerequisites
-------------
SecDATAVIEW has been tested on Ubuntu 16.04 LTS for SGX worker nodes and 18.04 LTS for SEV worker node. 

Setting up the SGX worker node
------------------------------
On every SGX worker node, install Ubuntu 16.04 LTS then follow the instruction below.

1- SecDATAVIEW default SGX server's username and password is user=""ubuntu"" and password=""dataview"" respectively. Also, SecDATAVIEW tries to send the sgx-lkl disk image and scripts into the ```/home/ubuntu``` folder in the SGX servers by default. It is recommended to create the same user credential in every sgx nodes for a smooth test of provided workflows in this repository. Also, the worker's credential setting can be updated within the SecDATAVIEW at any time.  
2- Install the Intel SGX driver https://github.com/01org/linux-sgx-driver is required. 
We have tested SecDATAVIEW with driver versions 2.0.

3- Install the SGX-LKL Library OS from https://github.com/lsds/sgx-lkl build it in hardware mode and sign the enclave following its instruction; make sure you can run the sample java application provided with SGX-LKL library OS.

4- Instal OpenSSH-SERVER and make sure you can make an ssh connection to your SGX machine.
```bash
sudo apt install openssh-server
ssh <your user name>@<Your SGX server ip> 
```
5- Download the pre-created sgx-lkl disk image from the link below and save the disk image in the SecDATAVIEW Master node.
 https://www.dropbox.com/sh/hywa7du0sr70nec/AABAyHqD_4tPYXVUNdw2bQAEa?dl=0



Networking support for SGX worker node
---------------------------------------

In order for SecDATAVIEW SGX-LKL application to send and receive packets via the network, a TAP interface is needed on the host. Besides, all the network traffic towards the worker node on TCP port 8000, TCP port 7700, TCP port 56000 and UDP port 56002 should be forwarded to the TAP interface. Create and configure the TAP interface as follows:
```bash
#Lets create a TAP interface
sudo ip tuntap del dev sgxlkl_tap0 mode tap
sudo ip tuntap add dev sgxlkl_tap0 mode tap user `whoami`
sudo ip link set dev sgxlkl_tap0 up
sudo ip addr add dev sgxlkl_tap0 10.0.1.254/24
```
To forward the network packets, the OS iptables should be updated with the following commands: 
```bash
#Lets backup the OS iptables first. Keep this file to restore the iptables rules at a later time if something got wrong

sudo iptables-save > ~/iptables-bak

#Rules to update the PREROUTING table for NAT

sudo iptables -t nat -A PREROUTING ! -s 10.0.1.0/24 -p tcp -m tcp --dport 8000 -j DNAT --to-destination 10.0.1.1:8000
sudo iptables -t nat -A PREROUTING ! -s 10.0.1.0/24 -p tcp -m tcp --dport 7700 -j DNAT --to-destination 10.0.1.1:7700
# Forward tcp traffic to enclave attestation endpoint
sudo iptables -t nat -A PREROUTING ! -s 10.0.1.0/24 -p tcp -m tcp --dport 56000 -j DNAT --to-destination 10.0.1.1:56000
# Forward udp traffic to enclave Wireguard endpoint (from the SGX-LKL network setup instructions)
sudo iptables -t nat -I PREROUTING -p udp -i eth0 --dport 56002 -j DNAT --to-destination 10.0.1.1:56002

#Rule to update the POSTROUTING table that provides internet access for worker node

sudo iptables -t nat -A POSTROUTING -s 10.0.1.0/24 ! -d 10.0.1.0/24 -j MASQUERADE

#Rules to update the FORWARD table for packet forwarding shoud be available in both master and work nodes

sudo iptables -I FORWARD -m state -d 10.0.0.0/8 --state NEW,RELATED,ESTABLISHED -j ACCEPT
sudo iptables -I FORWARD -m state -s 10.0.0.0/8 --state NEW,RELATED,ESTABLISHED -j ACCEPT

#You must enable the packet forwarding in the system module unless packet routing does not work.
sudo sysctl -w net.ipv4.ip_forward=1
```
Also, you can make an executable bash script for the network setup with above commands (copy and paste all the commands in your script) and execute it in your SGX worker. 

Setting up the SEV worker node
------------------------------
SEV worker node requires an AMD server that supports the SEV feature. SEV feature must be enabled in the BIOS of the server, and instructions provided below should be followed.

1- Install and prepare the SEV HOST for the UBUNTU 18 OS by following the https://github.com/AMDESE/AMDSEV

2- On the host machine, install OpenSSH-SERVER, then set the 'root' password. SecDATAVIEW uses 'root' account to remotely launch the SEV VMs in the AMD server.
```bash
sudo apt get install openssh-server
sudo su
passwd ""set the password for 'root'""
```
3-give the 'root' account the SSH access by following the instruction below

https://linuxconfig.org/allow-ssh-root-login-on-ubuntu-18-04-bionic-beaver-linux 

4- make sure you can get ssh access to the AMD server with 'root' account.
```bash
ssh root@""Your AMD server IP"" 
```
5- Download the pre-created SEV disk image <sev-image.qcow2> from the link below and save the disk image in your SecDATAVIEW Master node.

 https://www.dropbox.com/sh/hywa7du0sr70nec/AABAyHqD_4tPYXVUNdw2bQAEa?dl=0


Networking support for SEV worker node
--------------------------------------

In order SecDATAVIEW SEV VM to send and receive packets via the network, a TAP interface is needed on the host AMD server for every single SEV VM. The TAP interface should be bridged to the ethernet interface with internet access so that every SEV VM receives its IP addresses from the LAN DHCP server. Create and configure each TAP interface as follows:
```bash
sudo  brctl addbr br0
#replace ""enp98s0"" with your ethernet interface name
sudo ip addr flush dev enp98s0
sudo brctl addif br0 enp98s0
sudo ip tuntap add dev tap0 mode tap user `whoami`
sudo brctl addif br0 tap0
sudo ifconfig enp98s0 up
sudo ifconfig tap0 hw ether cb:9a:78:56:34:12 up
sudo ifconfig br0 hw ether 12:34:56:78:9a:bc up
sudo  dhclient -v br0
```
Also, you can make an executable bash script for the network setup with above commands (copy and paste all the commands in your script) and execute it in your AMD server. 

Follow the article below for more information regarding network configuration in KVM and QEMU that is used by SEV VM  
https://gist.github.com/extremecoders-re/e8fd8a67a515fee0c873dcafc81d811c

Setting up SecDATAVIEW master node
-----------------------------------

We have tested SecDATAVIEW master on Ubuntu 16.04 LTS
Please follow the instruction below to setup the master node.

1- Install OpenJDK 1.8
```bash
sudo apt install openjdk-8-jdk
```

2- Install Eclipse ide for JAVA from the official Eclipse website. We have tested the SecDATAVIEW with (Oxygen.3a Release (4.7.3a)

3- Install git on your master node. 
```bash
sudo apt install git
```

4- Clone the SecDATAVIEW project from git on your master node. 
```bash
git clone https://github.com/shiyonglu/SecDATAVIEW.git
```

5- Download and copy the binaries of modified SGX-LKL in your home folder. The current release uses the modified version of SGX-LKL for Intel SGX attestation purposes.  You may find the binaries and source codes in https://www.dropbox.com/sh/hywa7du0sr70nec/AABAyHqD_4tPYXVUNdw2bQAEa?dl=0

6- Install Wire Guard VPN on the master node
``` bash
sudo add-apt-repository ppa:wireguard/wireguard
sudo apt update
sudo apt install wireguard
#From the /machineScript/SGX/WireGuard_VPN_Setup/ folder run wg-sgxlkl-client.sh to setup master node wire guard endpoint.
#wgclient.priv and wgclient.pub key files in /WireGuard_VPN_Setup/ folder should be available to wg-sgxlkl-client.sh.
#Make sure the wire guard is functioning and correctly by calling into following status command and get a result similar to the below image.
sudo wg
```
![](machineScript/SGX/WireGuard_VPN_Setup/wg-image.png)

7- Create a new workspace on your Eclipse IDE

8- Import the SecDATAVIEW project into your Eclipse workspace. Subfolder ```DATAVIEW``` should be imported into the Eclipse workspace.
9- Update your workflow; consult Tutorials in https://github.com/shiyonglu/DATAVIEW to learn about creating a workflow. Also, this repository contains two pre-created workflow for the test purpose.

10- Update the ```config.txt``` file in ```confidentialIinfo``` folder by enlisting all the confidential tasks in the following format
```json
{
  ""confidentialTasks"":
    [
      {
        ""taskName"" : ""A""
      },
      {
        ""taskName"" : ""B""
      }
    ]
}
```
11- Encrypt the workflow input dataset (for confidential tasks) with the provided ```CryptoTools``` application.
 Update ```secretkey``` and ```associatedData ``` values in the ```TaskExecutor.java``` with your cryptography setting. 
 ```java
 public static String secretKey = ""abc"";
 public static String associatedData = ""abc"";
 ```
12- Put the encrypted workflow input files in ```workflowDataDir```folder.

13- Update the IP addresses for SEV and SGX in ```IPPool.txt```, that is located in ```confidentialIinfo``` in the following format. No duplicated IP is allowed in this list.
```json
{
  ""IPPool"":
    [
      {
        ""AMD"" : ""172.30.18.183""
      },
      {
        ""AMD"" : ""172.30.18.184""
      },
      {
        ""SGX"" : ""172.30.18.185""
      }
    ]
}
```
14- Create all the tasks in the following format. (or follow tutorial link for more information)

14.1- Define the size of the InputPort, OutPort, and type of dataset in the constructor of the class.
  
14.2- Overwrite your task in run() method. 
  
14.3- Put all the workflow tasks in the default package.

14. Export the runnable jar ```TaskExecutor.jar``` to ```confidentialInfo``` folder by selecting ```TaskExecutor.java``` file. This executable jar file should be created with Eclipse IDE.

15. Run the corresponding driver class to start the WorkflowExecutor.  

16- You also may export the runnable jar ```YourWorkflowDriver.jar``` into ```DATAVIEW``` folder by selecting ```<YourWorkflowDriver>.java``` file. This executable jar file should be created with Eclipse IDE and should include all the neccessary libraries in the jar. In this way you can use your jar file to execute the workflow with the below command.

```bash
java -jar <YourWorkflowDriver>.jar
```
When using the runaable jar file to execute the workflow tree structure bellow must be available for the jar file otherwise it cannot find neccessary files.

```console 
> machineScript
> DATAVIEW
>         ├── <YourWorkflow>.jar
>         │   ├── confidentialInfo
>         │   ├── workflowDataDir
>         │   └── workflowLibDir
```

Setting up SecDATAVIEW master node
----------------------------------
1- Update the values of the variables below in the ```WorkflowExecutor_Alfa.java``` based on the following information

Update the value of ```SGX_IMG_SRC_FILE```  with the path for sgx-lkl disk image  
```java
public static final String SGX_IMG_SRC_FILE = ""/home/mofrad-s/SecDATAVIEW-v3-img.enc"";
```

Update the value for ```SGX_SCRIPT_SRC_FILE``` relative to your project path
```java
public static final String SGX_SCRIPT_SRC_FILE = ""/home/mofrad-s/SecDATAVIEW/machineScript/SGX/sgx-lkl-server-remote-launch.sh"";
```

Update the value of ```AMD_IMG_SRC_FILE```  with the path for AMD SEV disk image 
```java
public static final String AMD_IMG_SRC_FILE = ""/home/mofrad-s/sev-image.qcow2"";
```

Update the value of ```AMD_IMG_DST_FOLDER```  with the path for home folder of your account on the AMD host server  
```java
public static final String AMD_IMG_DST_FOLDER = ""/home/mofrad-s/"";
```

Update the value for ```AMD_SCRIPT_SRC_FILE``` relative to your project path
```java
public static final String AMD_SCRIPT_SRC_FILE = ""/home/mofrad-s/SecDATAVIEW/machineScript/AMD/vm1-launch-dataview-sev.sh"";
```

Update the value of ```AMD_SCRIPT_DST_FOLDER```  with the path for the home folder of your account on the AMD host server  

```java
public static final String AMD_SCRIPT_DST_FOLDER = ""/home/mofrad-s/"";
```

Update the value of ```AMD_SERVER_IP```  with the IP address of your AMD host server  

```java 
public static final String AMD_SERVER_IP = ""172.30.18.202"";
```
keep the value of ```AMD_SERVER_USER_NAME```  as root. SecDATAVIEW requires root access to the machine to launch SEV VMs on the AMD server

```java
public static final String AMD_SERVER_USER_NAME = ""root"";
```
Update the value of ```AMD_SERVER_PASSWORD```  for 'root' account in the AMD server   

```java
public static final String AMD_SERVER_PASSWORD = ""acsac19"";
```
Update the value of ```SGX_SERVER_USER_NAME```  with the user name of your SGX host server  
```java
public static final String SGX_SERVER_USER_NAME = ""ubuntu"";
```
Update the value of ```SGX_SERVER_PASSWORD```  with the password of your SGX host server  
```java
public static final String SGX_SERVER_PASSWORD = ""dataview"";
```
Update the value of ```SGX_MRENCLVE```  with the expected value 
```java
public static final String SGX_MRENCLVE = ""9b6cb5e8e67c7ae5bfce1ec9d60363e259261c041f9a46cf284ac1fe714405c1""; 
```
Update the value of ```SGX_MRSIGNER```  with the expected value 
```java
public static final String SGX_MRSIGNER = ""f3b24a591ba692e90bd8a02ca4241a2a73b2128e90041d048fc84cf5fba5d7f6"";
 ```
You may learn expected SGX_MRENCLVE and SGX_MRSIGNER values by manually running sgx-lkl-server-remote-launch.sh on your SGX hardware similar to below image.

![](machineScript/SGX/mrenclave-signervalues.png)

Update the value of ```IAS_SPID ```  with the expected value 
```java
public static final String IAS_SPID = ""use your Intel SPID here""; //use the same values in the SGX machine script file
 ```
Update the value of ```IAS_SKEY  ```  with the expected value 
```java
public static final String IAS_SKEY = ""use one of your Intel Primary or Secondary key here""; 
 ```
Update the path of ```bashCommand``` and ```env``` strings for ```executeSGXremoteControl``` and  ```executeSGXremoteAttestation``` methods in the VMProvisionerSGX class and with the path to SGX-LKL folder on the master. Also, update the path to ```secdataview.app.conf``` file in the ```executeSGXremoteControl``` method. The file is located in ```/SecDATAVIEW/machineScript/SGX/secdataview.app.conf``` in this repository. 
 
Update all worker's script files path values in the ```/machineScript/AMD/vm1-launch-dataview-sev.sh```  and  ```/machineScript/SGX/sgx-lkl-server-remote-launch.sh```  based on your SGX and AMD folders and user credentials settings. For SGX script update the SPID value in the script similar to your IAS_SPID

CryptoTools instructions
------------------------
Put all the workflow input files in ```source_folder_location``` and then run the ```CryptoTool``` by the following format
```
java -jar CryptoTool.jar ""enc"" ""associated_data"" ""secret_key"" ""source_folder_location"" ""destination_folder_location""
```
The parementer order are as follows:``` ""mode"" ""secret_Key"" ""associated_data"" ""source_folder_location"" ""destination_folder_location""```. The ""mode"" is consisted with two types: a) ""enc"" b) ""dec""


Sample Workflows
----------------

1- Running Diagnosis Recommendation Workflow: (Example of the hybrid workflow; SGX and SEV workers)
---------------------------------------------------------------------------------------------------
Import the SecDATAVIEW project into Eclipse IDE and execute the ```DriverDiagnosisNew.java``` file as a driver class for Diagnosis Recommendation Workflow. This driver class is invoked with an SGX and a SEV machines. All necessary inputs and files have been provided in this repository. Since Diagnosis Recommendation Workflow involves six tasks, the first four tasks are assigned to SGX machines, and the rest of the tasks are allocated to a SEV machine. The output file for this workflow will be assigned to the machine that is associated with the last tasks ""Evaluation.""


2- Running Diagnosis Remommendation Workflow: (Example of SGX only workflows)
-----------------------------------------------------------------------------
1- Import the SecDATAVIEW project into Eclipse IDE and execute the ```DriverDiagnosisSGXonly.java``` file as the driver class for running the workflow with only one SGX worker. All the tasks associated with this workflow is configured to assign to only one SGX machine. Use ```DriverDiagnosis.java``` file as the driver class to execute the workflow with five SGX workers; all the tasks associated with this workflow is configured to assign between five SGX machines. 

3- Journal Exprimental Source Codes, the Scenarios and Parameter Setup:
---------------------------------
Source code for experimental Workflows and their data generator that have been used to produce results in the Journal version of the SecDATAVIEW system is available in the ""JournalReproduce-SourceCode"" folder.

Journal paper section 4.1.1. The Diagnosis Recommendation workflow:
The raw textual dataset can be generated through the dataset generator programm. For each of the scenarios we used 75% random dataset for training and the rest of them are for testing. The corresponding parameters for various training and testing data size are enlisted in Figure 5 (a) legends in the paper.  

Journal paper section 4.1.2. The Word Count (Map-Reduce) workflow:
The dataset for various size can be generated through the source code that we provided. In this experiment the experiment conductor needs to select the word length and the total number of words in the provided code and then execute the workflow. The corresponding parameters for various size are enlisted in Figure 5 (b) legends in the paper. 

Journal paper section 4.1.3. The Distributed K-means workflow:
In order to conduct the experiment, we generated various points (2-dimensional) dataset through the source code. To initiate the execution, we need to put on various points. When we obtain the corresponding dataset, we can feed this to our publicly available distributed k-means workflow. The corresponding parameters are enlisted in Figure 5 (c) legends in the paper. 

Journal paper section 4.1.4. The MONTAGE workflow:
In order to conduct the experiment, we randomly generate integers and the input should be the total number of integers that we need to generate. The corresponding parameters are enlisted in Figure 6 (a) legends in the paper. 

Journal paper section 4.1.5. The Neural Network (NN) workflow:
To replicate the experiment result, the source code for NN network workflow has been given. In order to vary the result, we need to specify the total number of nodes and the rest of the steps are fully automated through the workflow source code. The corresponding parameters are enlisted in Figure 6 (b) legends in the paper. 



",2023-07-07 18:48:18+00:00
seldon,seldon-core,SeldonIO/seldon-core,"An MLOps framework to package, deploy, monitor and manage thousands of production machine learning models",https://www.seldon.io/tech/products/core/,False,3803,2023-07-06 09:09:51+00:00,2017-12-20 14:51:54+00:00,765,83,161,58,v1.16.0,2023-04-19 15:31:46+00:00,Apache License 2.0,7286,v-,116,2019-10-27 14:51:16+00:00,2023-07-07 17:02:06+00:00,2023-07-07 16:53:13+00:00,"# Seldon Core: Blazing Fast, Industry-Ready ML
An open source platform to deploy your machine learning models on Kubernetes at massive scale.

## Seldon Core V2 Now Available

[![scv2_image](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/_static/scv2_banner.png)](https://docs.seldon.io/projects/seldon-core/en/v2/index.html)

[Seldon Core V2](https://docs.seldon.io/projects/seldon-core/en/v2/index.html) **is now available**. If you're new to Seldon Core we recommend you [start here](https://docs.seldon.io/projects/seldon-core/en/v2/contents/getting-started/index.html). Check out the [docs here](https://docs.seldon.io/projects/seldon-core/en/v2/index.html) and make sure to leave feedback on [our slack community](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg) and [submit bugs or feature requests on the repo](https://github.com/SeldonIO/seldon-core/issues/new/choose). The codebase can be found [in this branch](https://github.com/SeldonIO/seldon-core/tree/v2).

Continue reading for info on Seldon Core V1...

[![video_play_icon](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/core-play-logo.png)](https://www.youtube.com/watch?v=5Q-03We8aDE)

## Overview

Seldon core converts your ML models (Tensorflow, Pytorch, H2o, etc.) or language wrappers (Python, Java, etc.) into production REST/GRPC microservices.

Seldon handles scaling to thousands of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more.

* Read the [Seldon Core Documentation](https://docs.seldon.io/projects/seldon-core/en/latest/)
* Join our [community Slack](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg) to ask any questions
* Get started with [Seldon Core Notebook Examples](https://docs.seldon.io/projects/seldon-core/en/latest/examples/notebooks.html)
* Join our fortnightly [online working group calls](https://docs.seldon.io/projects/seldon-core/en/latest/developer/community.html) : [Google Calendar](https://calendar.google.com/event?action=TEMPLATE&tmeid=MXBtNzI1cjk0dG9kczhsZTRkcWlmcm1kdjVfMjAyMDA3MDlUMTUwMDAwWiBzZWxkb24uaW9fbTRuMnZtcmZubDI3M3FsczVnYjlwNjVpMHNAZw&tmsrc=seldon.io_m4n2vmrfnl273qls5gb9p65i0s%40group.calendar.google.com&scp=ALL)
* Learn how you can [start contributing](https://docs.seldon.io/projects/seldon-core/en/latest/developer/contributing.html)
* Check out [Blogs](https://docs.seldon.io/projects/seldon-core/en/latest/tutorials/blogs.html) that dive into Seldon Core components
* Watch some of the [Videos and Talks](https://docs.seldon.io/projects/seldon-core/en/latest/tutorials/videos.html) using Seldon Core

![](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/seldon-core-high-level.jpg)

### High Level Features

With over 2M installs, Seldon Core is used across organisations to manage large scale deployment of machine learning models, and key benefits include:

 * Easy way to containerise ML models using our [pre-packaged inference servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html), [custom servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html), or [language wrappers](https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/language_wrappers.html).
 * Out of the box endpoints which can be tested through [Swagger UI](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/openapi.html?highlight=swagger), [Seldon Python Client or Curl / GRPCurl](https://docs.seldon.io/projects/seldon-core/en/latest/python/python_module.html#seldon-core-python-api-client).
 * Cloud agnostic and tested on [AWS EKS, Azure AKS, Google GKE, Alicloud, Digital Ocean and Openshift](https://docs.seldon.io/projects/seldon-core/en/latest/examples/notebooks.html#cloud-specific-examples).
 * Powerful and rich inference graphs made out of [predictors, transformers, routers, combiners, and more](https://docs.seldon.io/projects/seldon-core/en/latest/examples/graph-metadata.html).
 * Metadata provenance to ensure each model can be traced back to its respective [training system, data and metrics](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/metadata.html).
 * Advanced and customisable metrics with integration [to Prometheus and Grafana](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html).
 * Full auditability through model input-output request [logging integration with Elasticsearch](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/log_level.html).
 * Microservice distributed tracing through [integration to Jaeger](https://docs.seldon.io/projects/seldon-core/en/latest/graph/distributed-tracing.html) for insights on latency across microservice hops.
 * Secure, reliable and robust system maintained through a consistent [security & updates policy](https://github.com/SeldonIO/seldon-core/blob/master/SECURITY.md).


## Getting Started

Deploying your models using Seldon Core is simplified through our pre-packaged inference servers and language wrappers. Below you can see how you can deploy our ""hello world Iris"" example. You can see more details on these workflows in our [Documentation Quickstart](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/quickstart.html).

### Install Seldon Core

Quick install using Helm 3 (you can also use Kustomize):

```bash
kubectl create namespace seldon-system

helm install seldon-core seldon-core-operator \
    --repo https://storage.googleapis.com/seldon-charts \
    --set usageMetrics.enabled=true \
    --namespace seldon-system \
    --set istio.enabled=true
    # You can set ambassador instead with --set ambassador.enabled=true
```

### Deploy your model using pre-packaged model servers

We provide optimized model servers for some of the most popular Deep Learning and Machine Learning frameworks that allow you to deploy your trained model binaries/weights without having to containerize or modify them.

You only have to upload your model binaries into your preferred object store, in this case we have a trained scikit-learn iris model in a Google bucket:

```console
gs://seldon-models/v1.17.0-dev/sklearn/iris/model.joblib
```

Create a namespace to run your model in:

```bash
kubectl create namespace seldon
```

We then can deploy this model with Seldon Core to our Kubernetes cluster using the pre-packaged model server for scikit-learn (SKLEARN_SERVER) by running the `kubectl apply` command below:

```yaml
$ kubectl apply -f - << END
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: iris-model
  namespace: seldon
spec:
  name: iris
  predictors:
  - graph:
      implementation: SKLEARN_SERVER
      modelUri: gs://seldon-models/v1.17.0-dev/sklearn/iris
      name: classifier
    name: default
    replicas: 1
END
```

#### Send API requests to your deployed model

Every model deployed exposes a standardised User Interface to send requests using our OpenAPI schema.

This can be accessed through the endpoint `http://<ingress_url>/seldon/<namespace>/<model-name>/api/v1.0/doc/` which will allow you to send requests directly through your browser.

![](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/rest-openapi.jpg)

Or alternatively you can send requests programmatically using our [Seldon Python Client](https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html) or another Linux CLI:

```console
$ curl -X POST http://<ingress>/seldon/seldon/iris-model/api/v1.0/predictions \
    -H 'Content-Type: application/json' \
    -d '{ ""data"": { ""ndarray"": [[1,2,3,4]] } }'

{
   ""meta"" : {},
   ""data"" : {
      ""names"" : [
         ""t:0"",
         ""t:1"",
         ""t:2""
      ],
      ""ndarray"" : [
         [
            0.000698519453116284,
            0.00366803903943576,
            0.995633441507448
         ]
      ]
   }
}
```

### Deploy your custom model using language wrappers

For more custom deep learning and machine learning use-cases which have custom dependencies (such as 3rd party libraries, operating system binaries or even external systems), we can use any of the Seldon Core language wrappers.

You only have to write a class wrapper that exposes the logic of your model; for example in Python we can create a file `Model.py`:

```python
import pickle
class Model:
    def __init__(self):
        self._model = pickle.loads( open(""model.pickle"", ""rb"") )

    def predict(self, X):
        output = self._model(X)
        return output
```

We can now containerize our class file using the [Seldon Core s2i utils](https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/s2i.html) to produce the `sklearn_iris` image:

```console
s2i build . seldonio/seldon-core-s2i-python3:0.18 sklearn_iris:0.1
```

And we now deploy it to our Seldon Core Kubernetes Cluster:

```yaml
$ kubectl apply -f - << END
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: iris-model
  namespace: model-namespace
spec:
  name: iris
  predictors:
  - componentSpecs:
    - spec:
        containers:
        - name: classifier
          image: sklearn_iris:0.1
    graph:
      name: classifier
    name: default
    replicas: 1
END
```

#### Send API requests to your deployed model

Every model deployed exposes a standardised User Interface to send requests using our OpenAPI schema.

This can be accessed through the endpoint `http://<ingress_url>/seldon/<namespace>/<model-name>/api/v1.0/doc/` which will allow you to send requests directly through your browser.

![](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/rest-openapi.jpg)

Or alternatively you can send requests programmatically using our [Seldon Python Client](https://docs.seldon.io/projects/seldon-core/en/latest/python/seldon_client.html) or another Linux CLI:

```console
$ curl -X POST http://<ingress>/seldon/model-namespace/iris-model/api/v1.0/predictions \
    -H 'Content-Type: application/json' \
    -d '{ ""data"": { ""ndarray"": [1,2,3,4] } }' | json_pp

{
   ""meta"" : {},
   ""data"" : {
      ""names"" : [
         ""t:0"",
         ""t:1"",
         ""t:2""
      ],
      ""ndarray"" : [
         [
            0.000698519453116284,
            0.00366803903943576,
            0.995633441507448
         ]
      ]
   }
}
```

### Dive into the Advanced Production ML Integrations

Any model that is deployed and orchestrated with Seldon Core provides out of the box machine learning insights for monitoring, managing, scaling and debugging.

Below are some of the core components together with link to the logs that provide further insights on how to set them up.

<table>
  <tr valign=""top"">
    <td width=""50%"" >
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html"">
            <br>
            <b>Standard and custom metrics with prometheus</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/analytics/dashboard.png"">
        </a>
    </td>
    <td width=""50%"">
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/analytics/logging.html"">
            <br>
            <b>Full audit trails with ELK request logging</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/kibana-custom-search.png"">
        </a>
    </td>
  </tr>
  <tr valign=""top"">
    <td width=""50%"">
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/analytics/explainers.html"">
            <br>
            <b>Explainers for Machine Learning Interpretability</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/anchors.jpg"">
        </a>
    </td>
    <td width=""50%"">
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/analytics/outlier_detection.html"">
            <br>
            <b>Outlier and Adversarial Detectors for Monitoring</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/adversarial-attack.png"">
        </a>
    </td>
  </tr>
  <tr valign=""top"">
    <td width=""50%"">
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/analytics/cicd-mlops.html"">
            <br>
            <b>CI/CD for MLOps at Massive Scale</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/cicd-seldon.jpg"">
        </a>
    </td>
    <td width=""50%"">
        <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/graph/distributed-tracing.html"">
            <br>
            <b>Distributed tracing for performance monitoring</b>
            <br>
            <br>
            <img src=""https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/graph/jaeger-ui-rest-example.png"">
        </a>
    </td>
  </tr>
</table>


## Where to go from here

### Getting Started

* [Quickstart Guide ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/github-readme.html)
* [Overview of Components ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/overview.html)
* [Install Seldon Core on Kubernetes ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html)
* [Join the Community ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/community.html)

### Seldon Core Deep Dive

* [Detailed Installation Parameters ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/helm.html)
* [Pre-packaged Inference Servers ](https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html)
* [Language Wrappers for Custom Models ](https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/language_wrappers.html)
* [Create your Inference Graph ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/inference-graph.html)
* [Deploy your Model  ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/deploying.html)
* [Testing your Model Endpoints  ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/serving.html)
* [Troubleshooting guide ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/troubleshooting.html)
* [Usage reporting ](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/usage-reporting.html)
* [Upgrading ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/upgrading.html)
* [Changelog ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/changelog.html)

### Pre-Packaged Inference Servers

* [MLflow Server ](https://docs.seldon.io/projects/seldon-core/en/latest/servers/mlflow.html)
* [SKLearn server ](https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html)
* [Tensorflow Serving ](https://docs.seldon.io/projects/seldon-core/en/latest/servers/tensorflow.html)
* [XGBoost server ](https://docs.seldon.io/projects/seldon-core/en/latest/servers/xgboost.html)

### Language Wrappers (Production)

* [Python Language Wrapper [Production] ](https://docs.seldon.io/projects/seldon-core/en/latest/python/index.html)

### Language Wrappers (Incubating)

* [Java Language Wrapper [Incubating] ](https://docs.seldon.io/projects/seldon-core/en/latest/java/README.html)
* [R Language Wrapper [ALPHA] ](https://docs.seldon.io/projects/seldon-core/en/latest/R/README.html)
* [NodeJS Language Wrapper [ALPHA] ](https://docs.seldon.io/projects/seldon-core/en/latest/nodejs/README.html)
* [Go Language Wrapper [ALPHA] ](https://docs.seldon.io/projects/seldon-core/en/latest/go/go_wrapper_link.html)

### Ingress

* [Ambassador Ingress ](https://docs.seldon.io/projects/seldon-core/en/latest/ingress/ambassador.html)
* [Istio Ingress ](https://docs.seldon.io/projects/seldon-core/en/latest/ingress/istio.html)

### Production

* [Supported API Protocols ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html)
* [CI/CD MLOps at Scale ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/cicd-mlops.html)
* [Metrics with Prometheus ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html)
* [Payload Logging with ELK ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/logging.html)
* [Distributed Tracing with Jaeger ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/distributed-tracing.html)
* [Replica Scaling ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/scaling.html)
* [Budgeting Disruptions](https://docs.seldon.io/projects/seldon-core/en/latest/graph/disruption-budgets.html)
* [Custom Inference Servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html)

### Advanced Inference

* [Model Explanations ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/explainers.html)
* [Outlier Detection ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/outlier_detection.html)
* [Routers (incl. Multi Armed Bandits)  ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/routers.html)

### Examples

* [Notebooks ](https://docs.seldon.io/projects/seldon-core/en/latest/examples/notebooks.html)
* [Articles/Blogs ](https://docs.seldon.io/projects/seldon-core/en/latest/tutorials/blogs.html)
* [Videos ](https://docs.seldon.io/projects/seldon-core/en/latest/tutorials/videos.html)

### Reference

* [Annotation-based Configuration ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/annotations.html)
* [Benchmarking ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/benchmarking.html)
* [General Availability ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/ga.html)
* [Helm Charts ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/helm_charts.html)
* [Images ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/images.html)
* [Logging & Log Level ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/log_level.html)
* [Private Docker Registry ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/private_registries.html)
* [Prediction APIs ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/index.html)
* [Python API reference ](https://docs.seldon.io/projects/seldon-core/en/latest/python/api/modules.html)
* [Release Highlights ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/release-highlights.html)
* [Seldon Deployment CRD ](https://docs.seldon.io/projects/seldon-core/en/latest/reference/seldon-deployment.html)
* [Service Orchestrator ](https://docs.seldon.io/projects/seldon-core/en/latest/graph/svcorch.html)
* [Kubeflow ](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/kubeflow.html)

### Developer

* [Overview ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/readme.html)
* [Contributing to Seldon Core ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/contributing.html)
* [End to End Tests ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/e2e.html)
* [Roadmap ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/roadmap.html)
* [Build using private repo ](https://docs.seldon.io/projects/seldon-core/en/latest/developer/build-using-private-repo.html)



## About the name ""Seldon Core""

The name Seldon (ˈSɛldən) Core was inspired from [the Foundation Series (Scifi Novel)](https://en.wikipedia.org/wiki/Foundation_series) where it's premise consists of a mathematician called ""Hari Seldon"" who spends his life developing a theory of Psychohistory, a new and effective mathematical sociology which allows for the future to be predicted extremely accurate through long periods of time (across hundreds of thousands of years).

## Commercial Support

![](https://raw.githubusercontent.com/SeldonIO/seldon-core/master/doc/source/images/deploy-logo.png)

We offer commercial support via our enterprise product Seldon Deploy. Please visit [https://www.seldon.io/](https://www.seldon.io/) for details and a trial.

",2023-07-07 18:48:22+00:00
seqpig,SeqPig,HadoopGenomics/SeqPig,"SeqPig is a library for Apache Pig for the distributed analysis of large sequencing datasets. It provides import and export functions for file formats commonly used for sequencing data, as well as a collection of Pig user-defined-functions (UDF’s) to help process aligned and unaligned sequence data.",,False,9,2016-08-11 14:51:29+00:00,2014-04-25 12:25:52+00:00,8,8,4,1,,,,287,v0.5,5,2014-07-02 15:40:44+00:00,,2016-08-11 14:21:46+00:00,"
SeqPig is a library of import and export functions for file formats
commonly used in bioinformatics for Apache Pig. Additionally, it
provides a collection of Pig user-defined functions (UDF's) that allow
for processing of aligned and unaligned sequence data. Currently
SeqPig supports BAM/SAM, FastQ and Qseq input and output and FASTA
input. It is built on top of the Hadoop-BAM library. Fore more
information see

http://seqpig.sourceforge.net/

and the documentation that comes with the release.

Releases of SeqPig come bundled with Picard/Samtools, which were developed at
the Wellcome Trust Sanger Institute, and Biodoop/Seal, which were developed
at the Center for Advanced Studies, Research and Development in Sardinia. See

http://samtools.sourceforge.net/
http://biodoop-seal.sourceforge.net/

Installation with precompiled Seal library
  > mvn install:install-file -Dfile=lib/seal-0.4.0-with-hadoop-bam-7.4.0.jar -DgroupId=it.crs4 -DartifactId=seal -Dversion=0.4.0-with-hadoop-bam-7.4.0 -Dpackaging=jar -DgeneratePom=true
  > mvn package -DskipTests


",2023-07-07 18:48:27+00:00
seqtools,SeqTools,nlgranger/SeqTools,"A python library to manipulate and transform indexable data (lists, arrays, ...)",https://seqtools-doc.readthedocs.io,False,42,2023-02-09 01:00:13+00:00,2017-06-21 21:45:31+00:00,4,1,1,5,v1.2.0,2021-09-08 11:11:47+00:00,Mozilla Public License 2.0,106,v1.2.0,16,2021-09-08 11:03:13+00:00,,2021-09-08 11:03:13+00:00,".. image:: https://badge.fury.io/py/SeqTools.svg
   :target: https://github.com/nlgranger/SeqTools
   :alt: PyPi package
.. image:: https://circleci.com/gh/nlgranger/SeqTools.svg?style=shield
   :target: https://circleci.com/gh/nlgranger/SeqTools
   :alt: CircleCI Continuous integration
.. image:: https://readthedocs.org/projects/seqtools-doc/badge
   :target: http://seqtools-doc.readthedocs.io
   :alt: Documentation
.. image:: https://api.codacy.com/project/badge/Grade/f5324dc1e36d46f7ae1cabaaf6bce263
   :target: https://www.codacy.com/app/nlgranger/SeqTools?utm_source=github.com&utm_medium=referral&utm_content=nlgranger/SeqTools&utm_campaign=Badge_Grade
   :alt: Code quality analysis
.. image:: https://codecov.io/gh/nlgranger/SeqTools/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/nlgranger/SeqTools
   :alt: Tests coverage
.. image:: http://joss.theoj.org/papers/527a3c6e78ef0b31f93bbd29235d5a0b/status.svg
   :target: http://joss.theoj.org/papers/527a3c6e78ef0b31f93bbd29235d5a0b
   :alt: Citable paper

SeqTools
========

SeqTools extends the functionalities of itertools to indexable (list-like)
objects. Some of the provided functionalities include: element-wise function
mapping, reordering, reindexing, concatenation, joining, slicing, minibatching,
`etc <https://seqtools-doc.readthedocs.io/en/stable/reference.html>`_.

SeqTools functions implement **on-demand evaluation** under the hood:
operations and transformations are only applied to individual items when they
are actually accessed. A simple but powerful prefetch function is also provided
to quickly evaluate elements.

SeqTools originally targets data science, more precisely the data preprocessing
stages. Being aware of the experimental nature of this usage,
on-demand execution is made as transparent as possible by providing
**fault-tolerant functions and insightful error message**.

Example
-------

Example
-------

>>> def f1(x):
...     return x + 1
...
>>> def f2(x):  # slow and memory heavy transformation
...     time.sleep(.01)
...     return [x for _ in range(500)]
...
>>> def f3(x):
...     return sum(x) / len(x)
...
>>> data = list(range(1000))

Without seqtools, defining the pipeline and reading values looks like
so:

>>> tmp1 = [f1(x) for x in data]
>>> tmp2 = [f2(x) for x in tmp1]  # takes 10 seconds and a lot of memory
>>> res = [f3(x) for x in tmp2]
>>> print(res[2])
3.0
>>> print(max(tmp2[2]))  # requires to store 499 500 useless values along
3

With seqtools:

>>> tmp1 = seqtools.smap(f1, data)
>>> tmp2 = seqtools.smap(f2, tmp1)
>>> res = seqtools.smap(f3, tmp2)  # no computations so far
>>> print(res[2])  # takes 0.01 seconds
3.0
>>> print(max(tmp2[2]))  # easy access to intermediate results
3


Batteries included!
-------------------

The library comes with a set of functions to manipulate sequences:

.. |concatenate| image:: docs/_static/concatenate.png

.. _concatenation: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.concatenate

.. |batch| image:: docs/_static/batch.png

.. _batching: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.batch

.. |gather| image:: docs/_static/gather.png

.. _reindexing: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.gather

.. |prefetch| image:: docs/_static/prefetch.png

.. _prefetching: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.prefetch

.. |interleaving| image:: docs/_static/interleaving.png

.. _interleaving: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.interleave

.. |uniter| image:: docs/_static/uniter.png

.. _uniter: https://seqtools-doc.readthedocs.io/en/latest/reference.html#seqtools.uniter

==================== ================= ===============
| `concatenation`_   | `batching`_     | `reindexing`_
| |concatenate|      | |batch|         | |gather|
| `prefetching`_     | `interleaving`_ | `uniter`_
| |prefetch|         | |interleaving|  | |uniter|
==================== ================= ===============

and others (suggestions are also welcome).

Installation
------------

.. code-block:: bash

   pip install seqtools

Documentation
-------------

The documentation is hosted at `https://seqtools-doc.readthedocs.io
<https://seqtools-doc.readthedocs.io>`_.

Contributing and Support
------------------------

Use the `issue tracker <https://github.com/nlgranger/SeqTools/issues>`_
to request features, propose improvements or report issues. For questions
regarding usage, please send an `email
<mailto:3764009+nlgranger@users.noreply.github.com>`_.

Related libraries
-----------------

`Joblib <https://joblib.readthedocs.io>`_, proposes low-level functions with
many optimization settings to optimize pipelined transformations. This library
notably provides advanced caching mechanisms which are not the primary concern
of SeqTool. SeqTool uses a simpler container-oriented interface with multiple
utility functions in order to assist fast prototyping. On-demand evaluation is
its default behaviour and applies at all layers of a transformation pipeline. In
particular, parallel evaluation can be inserted in the middle of the
transformation pipeline and won't block the execution to wait for the
computation of all elements from the dataset.

SeqTools is conceived to connect nicely to the data loading pipeline of Machine
Learning libraries such as PyTorch's `torch.utils.data
<http://pytorch.org/docs/master/data.html>`_ and `torchvision.transforms
<http://pytorch.org/docs/master/torchvision/transforms.html>`_ or Tensorflow's
`tf.data <https://www.tensorflow.org/guide/datasets>`_. The interface of these
libraries focuses on `iterators
<https://docs.python.org/3/library/stdtypes.html#iterator-types>`_ to access
transformed elements, contrary to SeqTools which also provides arbitrary reads
via indexing.",2023-07-07 18:48:31+00:00
seqware,seqware,SeqWare/seqware,This is the SeqWare Project's main repo.,http://seqware.io/,False,28,2022-09-23 09:54:02+00:00,2012-05-11 15:51:55+00:00,17,26,18,29,1.1.1,2015-05-01 15:12:55+00:00,GNU General Public License v3.0,3206,release/seqware-0.12.5.1,74,2012-10-11 19:49:10+00:00,,2016-09-28 20:40:48+00:00,"[![Build Status](https://travis-ci.org/SeqWare/seqware.svg?branch=develop)](https://travis-ci.org/SeqWare/seqware)
[![Join the chat at https://gitter.im/SeqWare/seqware](https://badges.gitter.im/SeqWare/seqware.svg)](https://gitter.im/SeqWare/seqware?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Coverity Scan](https://img.shields.io/coverity/scan/9681.svg?maxAge=2592000)](https://scan.coverity.com/projects/seqware-seqware)

[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.33952.svg)](http://dx.doi.org/10.5281/zenodo.33952)


## Introduction 

This README is just a quick overview of building SeqWare. See our
[project homepage](http://seqware.github.com) for much more documentation.

This is top level of the [SeqWare Project](http://seqware.github.com).
This contains the 6 major components of the SeqWare project along with
documentation and testing:

* seqware-meta-db
* seqware-webservice
* seqware-admin-webservice
* seqware-portal
* seqware-pipeline
* seqware-queryengine
* seqware-common
* the http://seqware.github.com website and manual
* seqware-ext-testing
* seqware-ext-admin-testing

The seqware-common sub-project provides a location for common code
and most of the other sub-projects have this as a dependency.

## Prerequisites 

###A Recent Linux Distribution

This pretty much goes without saying but the SeqWare project is targeted at
Linux.  You may be able to compile and use the software on MacOS X but, in all
honesty, we recommend you use a recent Linux distribution such as Debian
(Ubuntu, Linux Mint, etc) or RedHat (RedHat Enterprise, Fedora, etc).  This
software, although written in Java mostly, was never intended to work on
Windows. If you need to use Windows for development or deployment we recommend
you simply use our VirtualBox VM for both activities, see our extensive documentation
on http://seqware.github.com for more information. You can also use this same
approach on MacOS (or even another version of Linux).

###Java

SeqWare requires Oracle JDK 1.8 or greater, we primarily write and test with JDK 1.8.x.
An example of instructions on how to update your Linux installation can be found [here](https://ccp.cloudera.com/display/CDH4DOC/Before+You+Install+CDH4+on+a+Single+Node#BeforeYouInstallCDH4onaSingleNode-InstalltheOracleJavaDevelopmentKit). You will need to use the method appropriate to your distribution to install this.

## Building 

### Getting the Source Code 

Our source code is available from [GitHub](https://github.com/SeqWare/seqware) or the ""Fork me on GitHub"" banner at the upper right of our website

To get a copy of of our source code you will first need to install Git (<code>sudo apt-get install git</code> in Ubuntu) and then clone our repository.

<pre title=""Cloning the git repository"">
<span class=""prompt"">~$</span> <kbd>git clone git://github.com/SeqWare/seqware.git</kbd>
Cloning into 'seqware'...
remote: Counting objects: 8984, done.
remote: Compressing objects: 100% (2908/2908), done.
remote: Total 8984 (delta 4308), reused 8940 (delta 4265)
Receiving objects: 100% (8984/8984), 33.57 MiB | 392 KiB/s, done.
Resolving deltas: 100% (4308/4308), done.
</pre>

By default, this will land you on the default branch. You will want to check-out the latest release. 

For example:

	~$ cd seqware_github/
	~/seqware_github$ git checkout 0.13.6.5
	HEAD is now at f8698e9... Merge branch 'hotfix/0.13.6.5'

### Building and Automated Testing 

We're moving to Maven for our builds, this is currently how
you build without running any tests in the trunk directory:

    mvn clean install -DskipTests

Maven now runs unit tests as follows (unit tests in the SeqWare context are quick tests that do not require the embedded HBase or Tomcat instance):

    mvn clean install  

In order to run the integration tests on the entire project, please ensure that you have followed the steps in each of the integration testing guides for our sub-projects. This includes [MetaDB](http://seqware.github.com/docs/github_readme/3-metadb/) , [Web Service](http://seqware.github.com/docs/github_readme/4-webservice/) , and [Query Engine](http://seqware.github.com/docs/github_readme/2-queryengine/). 

**WARNING:  While integration and extended tests are running, launching workflows and workflow status checking MUST not occur.  If you have a cronjob performing these tasks it MUST be disabled prior to integration and extended testing.**

When this is complete: 

    export MAVEN_OPTS=""-Xmx1024m -XX:MaxPermSize=512m"" 
(This ensures that enough memory is allocated for integration tests)
    mvn clean install -DskipITs=false
(This runs all unit tests and integration tests that only require postgres as a prerequisite)
    mvn clean install -DskipITs=false -P extITs,embeddedTomcat
(runs all unit tests and all integration tests including those that require Oozie)
    mvn clean install -DskipITs=false -P longITs,embeddedTomcat
(runs all unit tests and just the long integration tests that take longer than can run on Travis-CI)

In the last case, the extended integration tests profile is used to trigger integration tests that run our command line utilities. 
In order to point your command-line tools at the web service brought up by the integration tests, you will need to comment out your crontab and modify your SeqWare ~/.seqware/settings to include:

    SW_REST_URL=http://localhost:8889/seqware-webservice

It is possible to disable our embedded tomcat instance and run against both a remote postgres and Tomcat instance. Set the following variables in your .seqware/settings to override these settings for basic integration tests and extended integration tests respectively:

    BASIC_TEST_DB_HOST=otherserver.ca
    BASIC_TEST_DB_NAME=test_seqware_meta_db
    BASIC_TEST_DB_USER=seqware
    BASIC_TEST_DB_PASSWORD=seqware

    EXTENDED_TEST_DB_HOST=otherserver.ca
    EXTENDED_TEST_DB_NAME=test_seqware_meta_db
    EXTENDED_TEST_DB_USER=seqware
    EXTENDED_TEST_DB_PASSWORD=seqware

Then set your SW\_REST\_URL to the web service that uses the above database and invoke the following command. Note that you will need to deploy the seqware-webservice war yourself. 

    mvn clean install -DskipITs=false -P 'extITs,!embeddedTomcat'

Alternatively, if you wish to still use an embedded tomcat instance for testing, modify the properties at the beginning of your seqware-webservice/pom.xml to match the above databases and invoke the integration tests with your SW\_REST\_URL set to http://localhost:8889/seqware-webservice

    mvn clean install -DskipITs=false -P extITs,embeddedTomcat

You can also run the integration tests by using a locally installed tomcat instance. Make sure the BASIC_TEST_* and EXTENDED_TEST_* are defined to get this to work:

    mvn clean install -DskipITs=false -P extITs

You can also build individual components such as the new query engine with: 

    cd seqware-queryengine
    mvn clean install

### Coding Standards

Please refer to SeqWare's [Coding Standards](https://seqware.github.io/docs/100-coding-standards/). 

### Building Our Site

In order to publish to seqware.io , checkout our current master and publish our site:

    git checkout master 
    mvn site-deploy

Then you would look at the site at: http://seqware.io

In order to publish our docs leading up to 1.1.x , checkout from develop and publish:

    git checkout develop 
    mvn site-deploy

Then you would look at the site at: http://seqware.github.io/unstable.seqware.io/

###Problems with Maven

Sometimes we run into problems when building, strange missing dependency issues
and broken packages. A lot of the time this is an issue with Maven, try
deleting your ~/.m2 directory and running the build process again.

## Citations

Browse DOI codes for SeqWare and related software for software engineering at OICR at [Software Engineering at Zenodo](https://zenodo.org/collection/user-softeng-at-oicr)


## Installing

See our [Installation Guide](http://seqware.github.com/docs/2-installation/) for detailed installation instructions
including links to a pre-configured virtual machine that can be used for
testing, development, and deployment.

## seqware-docker
This organization as a whole also documents the various docker distributions used by the SeqWare and Pancancer projects.
Prerequisite containers can be resolved from Docker Hub which also runs continuous integration (except for seqware\_full which does not work in the docker hub environment). 

Install Docker using the following script. This will automatically setup AUFS which is recommended for performance reasons. 

        curl -sSL https://get.docker.com/ | sudo sh
        sudo usermod -aG docker ubuntu

When using Ubuntu, we recommend 14.04. 
After setting up, remember to exit your shell and log back in to refresh your environment.

Currently, most of these containers (including all of the SeqWare ones) are available and served as [automated builds](https://registry.hub.docker.com/repos/seqware/) on Docker Hub. We are also working on [quay.io](https://quay.io/repository/?namespace=seqware). 

### Java

You will need Java 7 to extract the workflow .zip bundles.  Please install that version for your system.

### SeqWare WhiteStar 

This version of SeqWare uses the WhiteStar workflow engine to quickly run workflows without any dependencies on SGE, Oozie, Hadoop, or even the SeqWare webservice. These containers start quickly and with no running services or overhead. The trade-off is that running workflows is less robust and access to features such as throttling based on memory (SGE), retrying workflows (Oozie), or querying metadata (webservice) are not available.

Go to [seqware\_whitestar](https://github.com/SeqWare/seqware_whitestar) for setup instructions

#### Documentation Builder 

Pre-requisite: SeqWare WhiteStar

Used internally for the SeqWare project to build documentation via jenkins when changes are pushed to GitHub. 

Go to [documentation\_builder](https://github.com/SeqWare/documentation_builder) for setup instructions

### SeqWare Oozie-SGE 

This version of SeqWare uses the Oozie-SGE workflow engine to run workflows. This requires SGE, Oozie, Hadoop, and the SeqWare webservice and thus containers are started with a script which spins up these services. These containers should be functionally very similar to full VMs spun up using [Bindle](https://github.com/CloudBindle/Bindle) and ansible-playbooks from [seqware-bag](https://github.com/SeqWare/seqware-bag).

Go to [seqware\_full](https://github.com/SeqWare/seqware_full) for setup instructions


## Copyright

Copyright 2008-2015 Brian D O'Connor, OICR, UNC, and Nimbus Informatics, LLC

## Contributors

Please see our [partners and contributors](http://seqware.github.com/partners/)

## License

SeqWare is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

SeqWare is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with SeqWare.  If not, see <http://www.gnu.org/licenses/>.


",2023-07-07 18:48:35+00:00
signac,signac,glotzerlab/signac,Manage large and heterogeneous data spaces on the file system.,https://signac.io/,False,123,2023-07-04 19:35:44+00:00,2019-01-30 23:37:23+00:00,33,14,31,12,v2.0.0,2023-03-30 19:20:31+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",2500,v2.0.0,50,2023-03-30 19:20:31+00:00,2023-07-06 13:57:45+00:00,2023-07-06 13:57:41+00:00,"# <img src=""https://raw.githubusercontent.com/glotzerlab/signac/main/doc/images/palette-header.png"" width=""75"" height=""58""> signac - simple data management

[![Affiliated with NumFOCUS](https://img.shields.io/badge/NumFOCUS-affiliated%20project-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/sponsored-projects/affiliated-projects)
[![PyPI](https://img.shields.io/pypi/v/signac.svg)](https://pypi.org/project/signac/)
[![conda-forge](https://img.shields.io/conda/vn/conda-forge/signac.svg?style=flat)](https://anaconda.org/conda-forge/signac)
[![GitHub Actions](https://github.com/glotzerlab/signac/actions/workflows/run-pytest.yml/badge.svg)](https://github.com/glotzerlab/signac/actions)
[![RTD](https://img.shields.io/readthedocs/signac.svg?style=flat)](https://docs.signac.io)
[![License](https://img.shields.io/github/license/glotzerlab/signac.svg)](https://github.com/glotzerlab/signac/blob/main/LICENSE.txt)
[![PyPI-downloads](https://img.shields.io/pypi/dm/signac.svg?style=flat)](https://pypistats.org/packages/signac)
[![Slack](https://img.shields.io/badge/Slack-chat%20support-brightgreen.svg?style=flat&logo=slack)](https://signac.io/slack-invite/)
[![Twitter](https://img.shields.io/twitter/follow/signacdata?style=social)](https://twitter.com/signacdata)
[![GitHub Stars](https://img.shields.io/github/stars/glotzerlab/signac?style=social)](https://github.com/glotzerlab/signac/)

The [**signac** framework](https://signac.io) helps users manage and scale file-based workflows, facilitating data reuse, sharing, and reproducibility.

It provides a simple and robust data model to create a well-defined indexable storage layout for data and metadata.
This makes it easier to operate on large data spaces, streamlines post-processing and analysis and makes data collectively accessible.

## Resources

- [Framework documentation](https://docs.signac.io/):
  Examples, tutorials, topic guides, and package Python APIs.
- [Package documentation](https://docs.signac.io/projects/core/):
  API reference for the **signac** package.
- [Slack Chat Support](https://signac.io/slack-invite/):
  Get help and ask questions on the **signac** Slack workspace.
- [**signac** website](https://signac.io/):
  Framework overview and news.

## Installation

The recommended installation method for **signac** is through **conda** or **pip**.
The software is tested for Python 3.8+ and is built for all major platforms.

To install **signac** *via* the [conda-forge](https://conda-forge.github.io/) channel, execute:

```bash
conda install -c conda-forge signac
```

To install **signac** *via* **pip**, execute:

```bash
pip install signac
```

**Detailed information about alternative installation methods can be found in the [documentation](https://docs.signac.io/en/latest/installation.html).**

## Quickstart

The framework facilitates a project-based workflow.
Set up a new project:

```bash
$ mkdir my_project
$ cd my_project
$ signac init
```

and access the project handle:

```python
>>> project = signac.get_project()
```

## Testing

You can test this package by executing:

```bash
$ python -m pytest tests/
```

## Acknowledgment

When using **signac** as part of your work towards a publication, we would really appreciate that you acknowledge **signac** appropriately.
We have prepared examples on how to do that [here](https://docs.signac.io/en/latest/acknowledge.html).
**Thank you very much!**

The signac framework is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects).
",2023-07-07 18:48:39+00:00
simtool,simtool,hubzero/simtool,Simulation Tool Library,,False,0,2021-10-04 19:19:17+00:00,2019-06-17 18:34:00+00:00,4,7,2,0,,,MIT License,115,0.4.1,7,2022-12-14 01:02:59+00:00,,2022-12-14 01:02:59+00:00,"===============================
SimTool
===============================


.. image:: https://img.shields.io/pypi/v/simtool.svg
        :target: https://pypi.python.org/pypi/simtool

.. image:: https://img.shields.io/travis/hubzero/simtool.svg
        :target: https://travis-ci.org/hubzero/simtool

.. image:: https://readthedocs.org/projects/simtool/badge/?version=latest
        :target: https://simtool.readthedocs.io/en/latest/?badge=latest
        :alt: Documentation Status

Functions for creating and running Simulation Tools on the HUBzero_ platform lead by nanoHUB_

* Free software: MIT license
* Documentation: https://simtool.readthedocs.io.


Features
--------

* Easily declare and validate inputs and outputs of a simulation using Python and Jupyter notebooks. The entire simulation code can run inside a notebook or the notebook can be a wrapper that invokes complex external codes.
 
* Uses papermill_ to run parameterized notebooks, saving a new copy for each run.

* Results saved in a datastore (filesystem or web service based).  The datastore can be used for machine learning and statistical analysis.  Additionally, it functions as a cache.

* Can be containerized for remote execution.



Credits
---------

This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.

.. _HUBzero: https://help.hubzero.org
.. _nanoHUB: https://nanohub.org
.. _Cookiecutter: https://github.com/audreyr/cookiecutter
.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage
.. _papermill: https://github.com/nteract/papermill
",2023-07-07 18:48:44+00:00
skyport2,Skyport2,MG-RAST/Skyport2, framework for large scale data management and reproducible multi-cloud workflow execution,,False,5,2020-06-10 06:56:47+00:00,2017-09-27 18:18:00+00:00,6,4,5,1,,,"BSD 2-Clause ""Simplified"" License",498,0.1,1,2017-10-13 15:01:58+00:00,,2019-04-09 18:36:40+00:00,"
Skyport2
========


![skyport logo](data/pictures/skyportlogo.small.jpg) 

Skyport2 is a RESTful framework for large scale data management and reproducible multi-cloud workflow execution. 

Scientists and engineers are accustomed to using a different computer systems to accomplish scientific workflow execution. Skyport2 handles data management and execution of CWL workflows across. Data is stored in the RESTful SHOCK object store that handles indexing, subsetting and format conversions. SHOCK is programmable and can be customized to perform additional functions e.g. convert image formats. AWE worker nodes connect to the AWE resource manager and check execute workflows described in the common workflow language. 


## Quick start

# Install docker 

Install docker, see Docker website for platform specific instructions:

https://www.docker.com/get-docker


E.g. on a recent ubuntu version you can install docker with this command:

```bash
sudo apt-get update && sudo apt-get -y install docker.io
```

Linux: Add user to docker group
```bash
sudo groupadd docker
sudo gpasswd -a $USER docker
newgrp docker
```

# Install docker-compose

Install docker-compose (note that the docker-compose version in the ubuntu repository might be too old) 


https://docs.docker.com/compose/install/

For ubuntu:

```bash
sudo curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose 
```

Note that with a newer version of docker you might need a newer version of docker-compose. (Note to self: Find mechanism to determine correct version) 

# Install Skyport2

Clone the Skyport repository:

```bash
git clone https://github.com/MG-RAST/Skyport2.git
cd Skyport2
```


```bash
sudo ./scripts/add_etc_hosts_entry.sh
source ./init.sh
docker-compose up
```

Open Skyport in your browser:

http://skyport.local:8001




## Updates


To get the latest code and docker images you can run the ```update.sh``` script.

```
source ./init.sh
./scripts/update.sh
```


## Finding services

If you need a listing of all skyport services run ```skyport2-overview.sh```: 


```bash
> ./skyport2-overview.sh 

------- Skyport2 -----------------------------
Skyport2 main URL: http://skyport.local:8001

AWE monitor:       http://skyport.local:8001/awe/monitor/

AWE server API:    http://skyport.local:8001/awe/api/
Shock server API:  http://skyport.local:8001/shock/api/

Auth server:       http://skyport.local:8001/auth/

----------------------------------------------
```

You can also use ```skyport2.env``` to see the full configuration (i.e. URLs) via environment variables:

```bash
> cat skyport2.env

export TAG=demo
export CONFIGDIR=/Users/you/git/Skyport2/Config/
export SHOCKDIR=/Users/you/git/Skyport2/live-data/shock/
export DATADIR=/Users/you/git/Skyport2/live-data
export LOGDIR=/Users/you/git/Skyport2/live-data/log/

export SKYPORT_HOST=skyport.local
export NGINX_PORT=8001
export SKYPORT_URL=http://skyport.local:8001
export AWE_SERVER_URL=http://skyport.local:8001/awe/api/
export SHOCK_SERVER_URL=http://skyport.local:8001/shock/api/
export AUTH_URL=http://skyport.local:8001/auth/

export SKYPORT_DOCKER_GATEWAY=172.21.0.1
export DOCKER_VERSION=18.09.0
export DOCKER_BINARY=/Users/you/git/Skyport2/live-data/docker-18.09.0

```

and you can source this file to update your environment variables:
```bash
source skyport2.env 
```

",2023-07-07 18:48:47+00:00
snakemake,snakemake,snakemake/snakemake,"This is the development home of the workflow management system Snakemake. For general information, see",https://snakemake.readthedocs.io,False,1749,2023-07-07 03:48:23+00:00,2019-10-04 14:58:11+00:00,451,20,267,173,v7.30.1,2023-06-28 17:14:32+00:00,MIT License,4767,v7.30.1,251,2023-06-28 17:14:32+00:00,2023-07-07 11:27:20+00:00,2023-07-07 10:33:48+00:00,"[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/snakemake/snakemake)
[![test status](https://github.com/snakemake/snakemake/workflows/CI/badge.svg?branch=main)](https://github.com/snakemake/snakemake/actions?query=branch%3Amain+workflow%3ACI)
[![Sonarcloud Status](https://sonarcloud.io/api/project_badges/measure?project=snakemake_snakemake&metric=alert_status)](https://sonarcloud.io/dashboard?id=snakemake_snakemake)
[![Bioconda](https://img.shields.io/conda/dn/bioconda/snakemake.svg?label=Bioconda)](https://bioconda.github.io/recipes/snakemake/README.html)
[![Pypi](https://img.shields.io/pypi/pyversions/snakemake.svg)](https://pypi.org/project/snakemake)
[![docker container status](https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/docker-publish.yml?color=blue&label=docker%20container)](https://hub.docker.com/r/snakemake/snakemake)
[![Stack Overflow](https://img.shields.io/badge/stack-overflow-orange.svg)](https://stackoverflow.com/questions/tagged/snakemake)
[![Twitter](https://img.shields.io/twitter/follow/johanneskoester.svg?style=social&label=Follow)](https://twitter.com/search?l=&q=%23snakemake%20from%3Ajohanneskoester)
[![Discord](https://img.shields.io/discord/753690260830945390?label=discord%20chat)](https://discord.gg/NUdMtmr)
[![Github stars](https://img.shields.io/github/stars/snakemake/snakemake?style=social)](https://github.com/snakemake/snakemake/stargazers)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md) 

# Snakemake

The Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses.
Snakemake is highly popular, with on average more than 7 new citations per week in 2021, and almost 400k downloads.
Workflows are described via a human readable, Python based language.
They can be seamlessly scaled to server, cluster, grid and cloud environments without the need to modify the workflow definition.
Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.

**Homepage: https://snakemake.github.io**

Copyright (c) 2012-2022 Johannes Köster <johannes.koester@uni-due.com> (see LICENSE)
",2023-07-07 18:48:51+00:00
snorkel,snorkel,snorkel-team/snorkel,A system for quickly generating training data with weak supervision,https://snorkel.org,False,5522,2023-07-06 11:27:19+00:00,2016-02-26 05:52:45+00:00,850,173,67,15,v0.9.9,2022-07-29 03:48:32+00:00,Apache License 2.0,2685,v0.9.9,19,2022-07-29 03:47:31+00:00,2023-07-06 11:27:19+00:00,2023-02-17 21:58:39+00:00,"<img src=""figs/logo_01.png"" width=""150""/>

![PyPI - Python Version](https://img.shields.io/pypi/pyversions/snorkel)
![PyPI](https://img.shields.io/pypi/v/snorkel)
![Conda](https://img.shields.io/conda/v/conda-forge/snorkel)
[![CircleCI](https://circleci.com/gh/snorkel-team/snorkel/tree/master.svg?style=svg)](https://circleci.com/gh/snorkel-team/snorkel/tree/master)
[![docs](https://readthedocs.org/projects/snorkel/badge/?version=master)](https://snorkel.readthedocs.io/en/master)
[![coverage](https://codecov.io/gh/snorkel-team/snorkel/branch/master/graph/badge.svg)](https://codecov.io/gh/snorkel-team/snorkel/branch/master)
[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Join the community on Spectrum](https://withspectrum.github.io/badge/badge.svg)](https://spectrum.chat/snorkel)

***Programmatically Build and Manage Training Data***

## Announcement

**The Snorkel team is now focusing their efforts on Snorkel Flow, an end-to-end AI application development platform based on the core ideas behind Snorkel—you can check it out [here](https://snorkel.ai) or [join us](www.snorkel.ai/careers) in building it!**

The [Snorkel project](https://snorkel.ai/how-to-use-snorkel-to-build-ai-applications/) started at Stanford in 2015 with a simple technical bet: that it would increasingly be the **training data**, not the models, algorithms, or infrastructure, that decided whether a machine learning project succeeded or failed. Given this premise, we set out to explore the radical idea that you could bring mathematical and systems structure to the messy and often entirely manual process of training data creation and management, starting by empowering users to **programmatically label, build, and manage** training data.

To say that the Snorkel project succeeded and expanded beyond what we had ever expected would be an understatement. The basic goals of a research repo like Snorkel are to provide a minimum viable framework for testing and validating hypotheses. Four years later, we’ve been fortunate to do not just this, but to develop and deploy early versions of Snorkel in partnership with some of the world’s leading organizations like [Google](https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html), [Intel](https://dl.acm.org/doi/abs/10.1145/3329486.3329492), [Stanford Medicine](https://www.cell.com/patterns/fulltext/S2666-3899(20)30019-2), and many more; author over [thirty-six peer-reviewed publications](https://snorkel.ai/technology.html) on our findings around Snorkel and related innovations in weak supervision modeling, data augmentation, multi-task learning, and more; be included in courses at top-tier universities; support production deployments in systems that you’ve likely used in the last few hours; and work with an amazing community of researchers and practitioners from industry, medicine, government, academia, and beyond.

However, we realized increasingly–from conversations with users in weekly office hours, workshops, online discussions, and industry partners–that the Snorkel project was just the very first step. The ideas behind Snorkel change not just how you label training data, but so much of the entire lifecycle and pipeline of building, deploying, and managing ML: how users inject their knowledge; how models are constructed, trained, inspected, versioned, and monitored; how entire pipelines are developed iteratively; and how the full set of stakeholders in any ML deployment, from subject matter experts to ML engineers, are incorporated into the process.

Over the last year, we have been building the platform to support this broader vision: [Snorkel Flow](https://snorkel.ai/snorkel-flow-platform/), an end-to-end machine learning platform for developing and deploying AI applications. Snorkel Flow incorporates many of the concepts of the Snorkel project with a range of newer techniques around weak supervision modeling, data augmentation, multi-task learning, data slicing and structuring, monitoring and analysis, and more, all of which integrate in a way that is greater than the sum of its parts–and that we believe makes ML truly faster, more flexible, and more practical than ever before.

Moving forward, we will be focusing our efforts on Snorkel Flow. We are extremely grateful for all of you that have contributed to the Snorkel project, and are excited for you to check out our next chapter [here](https://snorkel.ai).


# Quick Links
* [Snorkel website](https://snorkel.org)
* [Snorkel tutorials](https://github.com/snorkel-team/snorkel-tutorials)
* [Snorkel documentation](https://snorkel.readthedocs.io/)
* [Snorkel community forum](https://spectrum.chat/snorkel)
* [Snorkel mailing list](https://groups.google.com/forum/#!forum/snorkel-ml)
* [Snorkel Twitter](https://twitter.com/SnorkelAI)

# Getting Started
The quickest way to familiarize yourself with the Snorkel library is to walk through the [Get Started](https://snorkel.org/get-started/) page on the Snorkel website, followed by the full-length tutorials in the [Snorkel tutorials](https://github.com/snorkel-team/snorkel-tutorials) repository.
These tutorials demonstrate a variety of tasks, domains, labeling techniques, and integrations that can serve as templates as you apply Snorkel to your own applications.


# Installation

Snorkel requires Python 3.6 or later. To install Snorkel, we recommend using `pip`:

```bash
pip install snorkel
```

or `conda`:

```bash
conda install snorkel -c conda-forge
```

For information on installing from source and contributing to Snorkel, see our
[contributing guidelines](./CONTRIBUTING.md).

<details><summary><b>Details on installing with <tt>conda</tt></b></summary>
<p>

The following example commands give some more color on installing with `conda`.
These commands assume that your `conda` installation is Python 3.6,
and that you want to use a virtual environment called `snorkel-env`.

```bash
# [OPTIONAL] Activate a virtual environment called ""snorkel""
conda create --yes -n snorkel-env python=3.6
conda activate snorkel-env

# We specify PyTorch here to ensure compatibility, but it may not be necessary.
conda install pytorch==1.1.0 -c pytorch
conda install snorkel==0.9.0 -c conda-forge
```

</p>
</details>

<details><summary><b>A quick note for Windows users</b></summary>
<p>

If you're using Windows, we highly recommend using Docker
(you can find an example in our
[tutorials repo](https://github.com/snorkel-team/snorkel-tutorials/blob/master/Dockerfile))
or the [Linux subsystem](https://docs.microsoft.com/en-us/windows/wsl/faq).
We've done limited testing on Windows, so if you want to contribute instructions
or improvements, feel free to open a PR!

</p>
</details>

# Discussion

## Issues
We use [GitHub Issues](https://github.com/snorkel-team/snorkel/issues) for posting bugs and feature requests — anything code-related.
Just make sure you search for related issues first and use our Issues templates.
We may ask for contributions if a prompt fix doesn't fit into the immediate roadmap of the core development team.

## Contributions
We welcome contributions from the Snorkel community! 
This is likely the fastest way to get a change you'd like to see into the library.

Small contributions can be made directly in a pull request (PR).
If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. 
For ideas about what to work on, we've labeled specific issues as [`help wanted`](https://github.com/snorkel-team/snorkel/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+).

To set up a development environment for contributing back to Snorkel, see our [contributing guidelines](./CONTRIBUTING.md).
All PRs must pass the continuous integration tests and receive approval from a member of the Snorkel development team before they will be merged.

## Community Forum
For broader Q&A, discussions about using Snorkel, tutorial requests, etc., use the [Snorkel community forum](https://spectrum.chat/snorkel) hosted on Spectrum.
We hope this will be a venue for you to interact with other Snorkel users — please don't be shy about posting!

## Announcements
To stay up-to-date on Snorkel-related announcements (e.g. version releases, upcoming workshops), subscribe to the [Snorkel mailing list](https://groups.google.com/forum/#!forum/snorkel-ml). We promise to respect your inboxes — communication will be sparse!

## Twitter
Follow us on Twitter [@SnorkelAI](https://twitter.com/SnorkelAI).
",2023-07-07 18:48:56+00:00
sosworkflow,sos,vatlab/sos,SoS workflow system for daily data analysis,http://vatlab.github.io/sos-docs,False,253,2023-06-17 04:16:09+00:00,2016-02-13 00:46:46+00:00,35,8,15,51,0.9.11,2019-05-21 01:02:12+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",7588,v0.9.9.1-beta.3,51,2017-09-13 20:30:31+00:00,2023-06-17 04:16:08+00:00,2023-06-15 19:17:00+00:00,"[![Anaconda-Server Badge](https://anaconda.org/conda-forge/sos/badges/version.svg)](https://anaconda.org/conda-forge/sos)
[![PyPI version](https://badge.fury.io/py/sos.svg)](https://badge.fury.io/py/sos)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1291524.svg)](https://doi.org/10.5281/zenodo.1291524)
[![Join the chat at https://gitter.im/vatlab/SoS](https://badges.gitter.im/vatlab/SoS.svg)](https://gitter.im/vatlab/SoS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Build Status](https://travis-ci.org/vatlab/SoS.svg?branch=master)](https://travis-ci.org/vatlab/SoS)
[![Build Status](https://ci.appveyor.com/api/projects/status/x092eusa0tta3msw?svg=true
)](https://ci.appveyor.com/project/BoPeng/sos)


**Script of Scripts (SoS)** consists of [`SoS Notebook`](https://github.com/vatlab/sos-notebook), a Jupyter-based polyglot notebook that allows the use of multiple Jupyter kernels in one notebook, and
[`SoS Workflow`](https://github.com/vatlab/sos), a workflow system for the execution of workflows in both process- and outcome-oriented styles. It is designed for data scientists and bioinformatics who routinely work with scripts in different languages such as bash, Python, R, and SAS. This repository contains the `SoS Workflow` workflow engine.

Please refer to relevant publications [SoS Notebook: An Interactive Multi-Language Data Analysis Environment](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty405/5001386
) in Bioinformatics, [Script of Scripts: a pragmatic workflow system for daily computational research](https://doi.org/10.1371/journal.pcbi.1006843) in PLoS Computational Biology, and the [SoS homepage](http://vatlab.github.io/sos-docs) for more information on SoS Notebook and SoS Workflow.

We welcome and value community contributions: please [post issues](https://github.com/vatlab/SoS/issues)
to provide us feedback or get our support; please [send pull requests](https://github.com/vatlab/SoS/pulls)
if you have helped fixing bugs or making improvements to the source code. In addition, if you are interesting in extending SoS, for example adding a new target type or task engine, please have a look at our tutorial on extending SoS [here](https://vatlab.github.io/sos-docs/doc/user_guide/extending_sos.html).

# Installing SoS Workflow and Notebook

* If you are using conda, you can install sos using command

  ```bash
  conda install sos -c conda-forge
  ```
  You can also install a majority of the SoS suite of tools using commands such as
  ```bash
  conda install sos sos-pbs sos-notebook jupyterlab-sos sos-bash sos-python sos-r -c conda-forge
  ```
  This will install SoS Workflow, SoS Notebook and its JupyterLab extension, language modules for
  `Bash`, `Python2`, `Python3`, `R`, `R` itself (`r-base` and `r-irkernel` etc) and needed
  libraries (e.g. `feather`) if needed.

* If you are not using conda, you will have to install all the pieces one by one, but you will also have the freedom to use non-conda installation of `R`, `Julia` etc. With a working Python 3.6 installation, you can install SoS Workflow with command

  ```bash
  pip install sos
  ```
  and also
  ```bash
  pip install sos-pbs
  ```
  if you would like to use SoS with a remote batch system such as LSF or Slurm.

  You can install SoS Notebook, and register the sos kernel with Jupyter using the following commands

  ```bash
  pip install sos-notebook
  python -m sos_notebook.install
  ```

  To exchange data between live kernels in SoS Notebook, you will need to install individual kernels (e.g. `irkernel` for `R`), make sure they work under Jupyter, and install SoS language modules using commands such as

  ```bash
  pip install sos-r sos-matlab sos-python sos-bash
  ```
  Different modules might be needed to assist data exchange among kernels. Please refer to [the installation page of sos website](https://vatlab.github.io/sos-docs/#runningsos) for details.

### Change Log of SoS Workflow and SoS Notebook

SoS 0.22.5:
* [sos#1420](https://github.com/vatlab/sos/issues/1420): Allow checking the status, killing, and purge of remote workflows.

SoS Notebook 0.22.3
* [sos-notebook#303](https://github.com/vatlab/sos-notebook/issues/303): Make task execution non-blocking in sos notebook, so that the buttons to check status and remove tasks actually operatable.

SoS Notebook 0.22.2
* [sos-notebook#307](https://github.com/vatlab/sos-notebook/issues/307): Use a new template organization method for nbconvert > 0.6.0.

SoS 0.21.12
* [sos#1392](https://github.com/vatlab/sos/issues/1392): Allow the use of `pem_file` to authenticate to remote hosts.

SoS 0.20.9
* [sos#1319](https://github.com/vatlab/SoS/issues/1319): Extend option `-r host` to allow workflow to be executed with templates and on PBS
* [sos#1322](https://github.com/vatlab/SoS/issues/1322): Deprecate option `-b BIN_DIR` due to [sos#1319](https://github.com/vatlab/SoS/issues/1319)

SoS 0.19.15
* [sos#1273](https://github.com/vatlab/SoS/issues/1273): Allow workflows to be executed on cluster systems.
* [sos#1277](https://github.com/vatlab/SoS/issues/1277): Allow master tasks to be executed on cluster systems with extended runtime option `trunk_workers`.
* [sos#1279](https://github.com/vatlab/SoS/issues/1279): Extend option `-j` of commands `sos run` and `sos execute` to use remote workers.
* [sos#1288](https://github.com/vatlab/SoS/issues/1288): Change the default value of option `-q` from `localhost` to `None` (no task).

SoS 0.19.0
* [sos#1056](https://github.com/vatlab/SoS/issues/1056), [sos#1218](https://github.com/vatlab/SoS/issues/1218): Use a new shared execution model to enforce optio `-j`.
* [sos#1219](https://github.com/vatlab/SoS/issues/1219): Make sure global sections are executed only once

SoS Notebook 0.18.5
* [sos-notebook#192](https://github.com/vatlab/sos-notebook/issues/192): Allow language modules to support pattern matched kernel names such as julia-?.?

SoS 0.18.6
* [sos#1206](https://github.com/vatlab/SoS/issues/1206): Allow `output_from(step)` to obtain output from a workflow.

SoS 0.18.5
* [sos#1186](https://github.com/vatlab/SoS/issues/1186): Add command line option `-T` to enfore dependency tracing.
* [sos#1197](https://github.com/vatlab/SoS/issues/1197): Introduce function `traced` to make dependencies that will always be traced
* [sos#1201](https://github.com/vatlab/SoS/issues/1201): Introduce dependencies between compounded workflows

SoS 0.18.4
* [sos#1185](https://github.com/vatlab/SoS/issues/1185): Disallow the use of parameters in tasks.
* [sos#1186](https://github.com/vatlab/SoS/issues/1186): (reverted) Enforcing the concept that targets in `depends` statement always try to build dependency.

SoS Notebook 0.18.1
* [sos-notebook#178](https://github.com/vatlab/sos-notebook/issues/178): Allow adding `&` to the end of magics `%run`, `%sosrun`, and `%runfile` to execute workflow in background.
* [sos-notebook#179](https://github.com/vatlab/sos-notebook/issues/179): Remove magic `%rerun` and add magic `%runfile`
* [sos-notebook#180](https://github.com/vatlab/sos-notebook/issues/180): Add option `-r` to `%save` to execute the cell after saving it.

SoS 0.18.0
* [sos#1115](https://github.com/vatlab/SoS/issues/1115): Function `output_from` and `named_output` to support named inputs and outputs, among other new features summarized in this ticket.
* [sos#1120](https://github.com/vatlab/SoS/issues/1120): Allow paremeters `group_by`, `paired_with` etc for functions `output_from` and `named_output`.
* [sos#1125](https://github.com/vatlab/SoS/issues/1125): Set `concurrent=True` as default for substep execution.
* [sos#1132](https://github.com/vatlab/SoS/issues/1132): Deprecate action `stop_if` and replace it with `done_if` and `skip_if`
* [sos#1175](https://github.com/vatlab/SoS/issues/1175): Enforce the use of `sos_variable` to import shared variable in a step

SoS Notebook 0.18.0
* [sos-notebook#150](https://github.com/vatlab/sos-notebook/issues/150): A new side panel that works in the same fashion as JupyterLab's console window.
* [sos-notebook#154](https://github.com/vatlab/sos-notebook/issues/154): New task status table that allows reporting status, killing, and purging multiple tasks with the same tags.

SoS 0.9.16.10
* [sos#786](https://github.com/vatlab/SoS/issues/786): Support singularity. See [SoS Singularity Guide](https://vatlab.github.io/sos-docs/doc/tutorials/Singularity.html) for details.

SoS 0.9.16.0, SoS Notebook 0.9.16.0
* [sos#991](https://github.com/vatlab/SoS/issues/991): Use a new task file format to replace multiple files for each task. **This change is not backward compatible so please upgrade only after you completed and removed all existing tasks.**

SoS 0.9.15.1
* [sos-notebook#89](https://github.com/vatlab/sos-notebook/issues/89): Added templates to highlight source code using codemirror, with optional auto-generated table of contents.

SoS 0.9.14.10
* [sos#983](https://github.com/vatlab/SoS/issues/983): Allow depending on an entire process-oriented workflow using a `sos_step()` target that matches multiple steps.

SoS 0.9.14.3:
* [sos#975](https://github.com/vatlab/SoS/issues/975): Add option `-p` to generate a summary report after the completion of workflow.
* [sos#976](https://github.com/vatlab/SoS/issues/976): Much improved workflow help message (`sos run script -h`).

SoS Notebook 0.9.14.4:
* [sos-notebook#79](https://github.com/vatlab/sos-notebook/issues/79): Allow auto-completion and inspection in subkernel cells.

SoS Notebook 0.9.14.1
* [sos-notebook#74](https://github.com/vatlab/sos-notebook/issues/74): Add a `%revisions` magic to display revision history of the current document.

SoS 0.9.14.1
* [sos#925](https://github.com/vatlab/SoS/issues/924): Output summary of executed and ignored step, substeps, and tasks after the execution of workflows.

SoS Notebook 0.9.13.4
* [jupyterlab-sos#11](https://github.com/vatlab/jupyterlab-sos/issues/11): Magic `%cd` now changes directory of all subkernels

SoS Notebook 0.9.12.12
* [sos-notebook#52](https://github.com/vatlab/sos-notebook/issues/52): All new syntax highlighter that highlights expanded expressions
* [sos-notebook#58](https://github.com/vatlab/sos-notebook/issues/58): Stop removing leading comments from cells.

SoS 0.9.12.11
* [sos#922](https://github.com/vatlab/SoS/issues/922): Use user-id for docker execution (-u)
* [sos#926](https://github.com/vatlab/SoS/issues/926): Add function `zap()` to SoS path classes `path`, `paths`, `file_target`, and `sos_targets`


SoS Notebook 0.9.12.11
* [sos-notebook#44](https://github.com/vatlab/sos-notebook/issues/44): Allow sending text in markdown cells to side panel for execution.
* [sos-notebook#47](https://github.com/vatlab/sos-notebook/issues/47): Allow clear any HTML element with magic `%clear --class`
* [sos-notebook#50](https://github.com/vatlab/sos-notebook/issues/50): Re-design logo for SoS Notebook.

SoS 0.9.12.9
* [sos#914](https://github.com/vatlab/SoS/issues/914): Allow option `active` of actions and tasks to accept conditions.
* [sos#915](https://github.com/vatlab/SoS/issues/915): Automatically expand user (`~`) for SoS path types `path`, `paths` and `file_targets`.
* [sos#916](https://github.com/vatlab/SoS/issues/916): Use hashlib instead of faster xxhash under windows

SoS Notebook 0.9.12.9
* [sos-notebook#41](https://github.com/vatlab/sos-notebook/issues/41): Stop saving unused kernels in sos notebook.

SoS 0.9.12.3
* [sos#859](https://github.com/vatlab/SoS/issues/859): Introduce automatic auxiliary steps to simplify the use of makefile steps.

SoS 0.9.11.3
* [sos#879](https://github.com/vatlab/SoS/issues/879): Add action options `stdout` and `stderr` to redict output from script-executing actions.
* [sos-notebook#42](https://github.com/vatlab/sos-notebook/issues/42): Add option `--append` to magic `%capture` .

SoS 0.9.11.2
* [sos-notebook#39](https://github.com/vatlab/sos-notebook/issues/39): Separation installation and deployment and use command `python -m sos_notebook.install` to install `sos` kernel to Jupyter.

SoS 0.9.10.19

* [sos#874](https://github.com/vatlab/SoS/issues/874): Add input option `concurrent=True` to allow parallel execution of input groups.
* [sos#874](https://github.com/vatlab/SoS/issues/874): Optimize task submission of task engines to reduce status checking

SoS Notebook 0.9.10.17

* [sos-notebook#32](https://github.com/vatlab/sos-notebook/issues/32): Add magic `%capture` to capture output of cells
",2023-07-07 18:49:00+00:00
spark,spark,apache/spark,Apache Spark - A unified analytics engine for large-scale data processing,https://spark.apache.org/,False,36179,2023-07-07 15:42:27+00:00,2014-02-25 08:00:08+00:00,27102,2040,329,0,,,Apache License 2.0,37282,v3.4.1,203,2023-06-19 22:17:28+00:00,2023-07-07 18:44:32+00:00,2023-07-07 17:38:39+00:00,"# Apache Spark

Spark is a unified analytics engine for large-scale data processing. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis. It also supports a
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,
and Structured Streaming for stream processing.

<https://spark.apache.org/>

[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)
[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)
[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)
[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)


## Online Documentation

You can find the latest Spark documentation, including a programming
guide, on the [project web page](https://spark.apache.org/documentation.html).
This README file only contains basic setup instructions.

## Building Spark

Spark is built using [Apache Maven](https://maven.apache.org/).
To build Spark and its example programs, run:

```bash
./build/mvn -DskipTests clean package
```

(You do not need to do this if you downloaded a pre-built package.)

More detailed documentation is available from the project site, at
[""Building Spark""](https://spark.apache.org/docs/latest/building-spark.html).

For general development tips, including info on developing Spark using an IDE, see [""Useful Developer Tools""](https://spark.apache.org/developer-tools.html).

## Interactive Scala Shell

The easiest way to start using Spark is through the Scala shell:

```bash
./bin/spark-shell
```

Try the following command, which should return 1,000,000,000:

```scala
scala> spark.range(1000 * 1000 * 1000).count()
```

## Interactive Python Shell

Alternatively, if you prefer Python, you can use the Python shell:

```bash
./bin/pyspark
```

And run the following command, which should also return 1,000,000,000:

```python
>>> spark.range(1000 * 1000 * 1000).count()
```

## Example Programs

Spark also comes with several sample programs in the `examples` directory.
To run one of them, use `./bin/run-example <class> [params]`. For example:

```bash
./bin/run-example SparkPi
```

will run the Pi example locally.

You can set the MASTER environment variable when running examples to submit
examples to a cluster. This can be a mesos:// or spark:// URL,
""yarn"" to run on YARN, and ""local"" to run
locally with one thread, or ""local[N]"" to run locally with N threads. You
can also use an abbreviated class name if the class is in the `examples`
package. For instance:

```bash
MASTER=spark://host:7077 ./bin/run-example SparkPi
```

Many of the example programs print usage help if no params are given.

## Running Tests

Testing first requires [building Spark](#building-spark). Once Spark is built, tests
can be run using:

```bash
./dev/run-tests
```

Please see the guidance on how to
[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).

There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md

## A Note About Hadoop Versions

Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported
storage systems. Because the protocols have changed in different versions of
Hadoop, you must build Spark against the same version that your cluster runs.

Please refer to the build documentation at
[""Specifying the Hadoop Version and Enabling YARN""](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)
for detailed guidance on building for a particular distribution of Hadoop, including
building for particular Hive and Hive Thriftserver distributions.

## Configuration

Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)
in the online documentation for an overview on how to configure Spark.

## Contributing

Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)
for information on how to get started contributing to the project.
",2023-07-07 18:49:05+00:00
spiff,SpiffWorkflow,sartography/SpiffWorkflow,A powerful workflow engine implemented in pure Python,,False,1455,2023-07-06 17:11:17+00:00,2009-12-25 09:11:22+00:00,301,85,41,13,v2.0.1,2023-06-16 19:35:03+00:00,GNU Lesser General Public License v3.0,1918,v2.0.1,15,2023-06-16 19:35:03+00:00,2023-07-06 17:11:18+00:00,2023-06-22 12:15:58+00:00,"## SpiffWorkflow
![Logo](./graphics/logo_med.png)

Spiff Workflow is a workflow engine implemented in pure Python. It is based on
the excellent work of the Workflow Patterns initiative. In 2020 and 2021,
extensive support was added for BPMN / DMN processing.

## Motivation
We created SpiffWorkflow to support the development of low-code business
applications in Python.  Using BPMN will allow non-developers to describe
complex workflow processes in a visual diagram, coupled with a powerful python
script engine that works seamlessly within the diagrams.  SpiffWorkflow can parse
these diagrams and execute them.  The ability for businesses to create
clear, coherent diagrams that drive an application has far reaching potential.
While multiple tools exist for doing this in Java, we believe that wide
adoption of the Python Language, and it's ease of use, create a winning
strategy for building Low-Code applications.


## Build status
[![Build Status](https://travis-ci.com/sartography/SpiffWorkflow.svg?branch=master)](https://travis-ci.org/sartography/SpiffWorkflow)
[![SpiffWorkflow](https://github.com/sartography/SpiffWorkflow/actions/workflows/tests.yaml/badge.svg)](https://github.com/sartography/SpiffWorkflow/actions/workflows/tests.yaml)
[![Documentation Status](https://readthedocs.org/projects/spiffworkflow/badge/?version=latest)](http://spiffworkflow.readthedocs.io/en/latest/?badge=latest)
[![Issues](https://img.shields.io/github/issues/sartography/spiffworkflow)](https://github.com/sartography/SpiffWorkflow/issues)
[![Pull Requests](https://img.shields.io/github/issues-pr/sartography/spiffworkflow)](https://github.com/sartography/SpiffWorkflow/pulls)

## Code style

[![PEP8](https://img.shields.io/badge/code%20style-pep8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)


## Dependencies
We've worked to minimize external dependencies.  We rely on lxml for parsing
XML Documents, and there is some legacy support for Celery, but it is not
core to the implementation, it is just a way to interconnect these systems.
<b>Built with</b>
- [lxml](https://lxml.de/)
- [celery](https://docs.celeryproject.org/en/stable/)

## Features
* __BPMN__ - support for parsing BPMN diagrams, including the more complex
components, like pools and lanes, multi-instance tasks, sub-workflows, timer
events, signals, messages, boudary events and looping.
* __DMN__ - We have a baseline implementation of DMN that is well integrated
with our Python Execution Engine.
* __Forms__ - forms, including text fields, selection lists, and most every other
thing you can be extracted from the Camunda xml extension, and returned as
json data that can be used to generate forms on the command line, or in web
applications (we've used Formly to good success)
* __Python Workflows__ - We've retained support for building workflows directly
in code, or running workflows based on a internal json data structure.

_A complete list of the latest features is available with our [release notes](https://github.com/sartography/SpiffWorkflow/releases/tag/1.0) for
version 1.0._

## Code Examples and Documentation
Detailed documentation is available on [ReadTheDocs](https://spiffworkflow.readthedocs.io/en/latest/)
Also, checkout our [example application](https://github.com/sartography/spiff-example-cli), which we
reference extensively from the Documentation.

## Installation
```
pip install spiffworkflow
```

## Tests
```
cd tests/SpiffWorkflow
coverage run --source=SpiffWorkflow -m unittest discover -v . ""*Test.py""
```

## Support
You can find us on Discord at https://discord.gg/BYHcc7PpUC

Commercial support for SpiffWorkflow is available from
[Sartography](https://sartography.com)

## Contribute
Pull Requests are and always will be welcome!

Please check your formatting, assure that all tests are passing, and include
any additional tests that can demonstrate the new code you created is working
as expected.  If applicable, please reference the issue number in your pull
request.

## Credits and Thanks

Samuel Abels (@knipknap) for creating SpiffWorkflow and maintaining it for over
a decade.

Matthew Hampton (@matthewhampton) for his initial contributions around BPMN
parsing and execution.

The University of Virginia for allowing us to take on the mammoth task of
building a general-purpose workflow system for BPMN, and allowing us to
contribute that back to the open source community. In particular, we would like
to thank [Ron Hutchins](https://www.linkedin.com/in/ron-hutchins-b19603123/),
for his trust and support.  Without him our efforts would not be possible.

Bruce Silver, the author of BPMN Quick and Easy Using Method and Style, whose
work we referenced extensively as we made implementation decisions and
educated ourselves on the BPMN and DMN standards.

The BPMN.js library, without which we would not have the tools to effectively
build out our models, embed an editor in our application, and pull this mad
mess together.

Kelly McDonald (@w4kpm) who dove deeper into the core of SpiffWorkflow than
anyone else, and was instrumental in helping us get some of these major
enhancements working correctly.

Thanks also to the many contributions from our community.  Large and small.
From Ziad (@ziadsawalha) in the early days to Elizabeth (@essweine) more
recently.  It is good to be a part of this long lived and strong
community.


## License
GNU LESSER GENERAL PUBLIC LICENSE
",2023-07-07 18:49:10+00:00
springclouddataflow,spring-cloud-dataflow,spring-cloud/spring-cloud-dataflow,A microservices-based Streaming and Batch data processing in Cloud Foundry and Kubernetes,https://dataflow.spring.io,False,996,2023-06-25 14:08:07+00:00,2015-07-21 20:56:11+00:00,554,102,83,99,v2.10.3,2023-05-04 23:24:21+00:00,Apache License 2.0,4496,v2.10.3,129,2023-05-04 23:24:21+00:00,2023-07-07 07:54:48+00:00,2023-06-28 12:16:54+00:00,"<p align=""center"">
  <a href=""https://dataflow.spring.io/"">
    <img alt=""Spring Data Flow Dashboard"" title=""Spring Data Flow"" src=""https://i.imgur.com/hpeKaRk.png"" width=""450"" />
  </a>
</p>

<p align=""center"">
  <a href=""https://dataflow.spring.io/getting-started/"">
    <img src=""https://spring.io/badges/spring-cloud-dataflow/ga.svg""
         alt=""Latest Release Version"" />
  </a>
  <a href=""https://dataflow.spring.io/getting-started/"">
    <img src=""https://spring.io/badges/spring-cloud-dataflow/snapshot.svg""
         alt=""Latest Snapshot Version"" />
  </a>
  <br>
  <a href=""https://build.spring.io/browse/SCD-BMASTER"">
    <img src=""https://build.spring.io/plugins/servlet/wittified/build-status/SCD-BMASTER""
         alt=""Build Status"" />
  </a>
</p>

*Spring Cloud Data Flow* is a microservices-based toolkit for building streaming and batch data processing pipelines in
Cloud Foundry and Kubernetes.

Data processing pipelines consist of Spring Boot apps, built using the [Spring Cloud Stream](https://github.com/spring-cloud/spring-cloud-stream)
or [Spring Cloud Task](https://github.com/spring-cloud/spring-cloud-task) microservice frameworks. 

This makes Spring Cloud Data Flow ideal for a range of data processing use cases, from import/export to event streaming
and predictive analytics.

----

## Components

**Architecture**: The Spring Cloud Data Flow Server is a Spring Boot application that provides RESTful API and REST clients
(Shell, Dashboard, Java DSL).
A single Spring Cloud Data Flow installation can support orchestrating the deployment of streams and tasks to Local,
Cloud Foundry, and Kubernetes.

Familiarize yourself with the Spring Cloud Data Flow [architecture](https://dataflow.spring.io/docs/concepts/architecture/)
and [feature capabilities](https://dataflow.spring.io/features/).

**Deployer SPI**: A Service Provider Interface (SPI) is defined in the [Spring Cloud Deployer](https://github.com/spring-cloud/spring-cloud-deployer)
project. The Deployer SPI provides an abstraction layer for deploying the apps for a given streaming or batch data pipeline
and managing the application lifecycle.

Spring Cloud Deployer Implementations:

* [Local](https://github.com/spring-cloud/spring-cloud-deployer-local)
* [Cloud Foundry](https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry)
* [Kubernetes](https://github.com/spring-cloud/spring-cloud-deployer-kubernetes)

**Domain Model**: The Spring Cloud Data Flow [domain module](https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-core)
includes the concept of a *stream* that is a composition of Spring Cloud Stream applications in a linear data pipeline
from a *source* to a *sink*, optionally including *processor* application(s) in between. The domain also includes the
concept of a *task*, which may be any process that does not run indefinitely, including [Spring Batch](https://github.com/spring-projects/spring-batch)
jobs.

**Application Registry**: The [App Registry](https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-registry)
maintains the metadata of the catalog of reusable applications.
For example, if relying on Maven coordinates, an application URI would be of the format:
`maven://<groupId>:<artifactId>:<version>`.

**Shell/CLI**: The [Shell](https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-shell)
connects to the Spring Cloud Data Flow Server's REST API and supports a DSL that simplifies the process of defining a
stream or task and managing its lifecycle.

----

## Building

Clone the repo and type 

    $ ./mvnw clean install 

Looking for more information? Follow this [link](https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/spring-cloud-dataflow-docs/src/main/asciidoc/appendix-building.adoc).

### Building on Windows

When using Git on Windows to check out the project, it is important to handle line-endings correctly during checkouts.
By default Git will change the line-endings during checkout to `CRLF`. This is, however, not desired for _Spring Cloud Data Flow_
as this may lead to test failures under Windows.

Therefore, please ensure that you set Git property `core.autocrlf` to `false`, e.g. using: `$ git config core.autocrlf false`.
For more information please refer to the [Git documentation, Formatting and Whitespace](https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration).

----

## Contributing

We welcome contributions! See the [CONTRIBUTING](./CONTRIBUTING.adoc) guide for details.

----

## Code formatting guidelines

* The directory ./src/eclipse has two files for use with code formatting, `eclipse-code-formatter.xml` for the majority of the code formatting rules and `eclipse.importorder` to order the import statements.

* In eclipse you import these files by navigating `Windows -> Preferences` and then the menu items `Preferences > Java > Code Style > Formatter` and `Preferences > Java > Code Style > Organize Imports` respectfully.

* In `IntelliJ`, install the plugin `Eclipse Code Formatter`.  You can find it by searching the ""Browse Repositories"" under the plugin option within `IntelliJ` (Once installed you will need to reboot Intellij for it to take effect).
Then navigate to `Intellij IDEA > Preferences` and select the Eclipse Code Formatter.  Select the `eclipse-code-formatter.xml` file for the field `Eclipse Java Formatter config file` and the file `eclipse.importorder` for the field `Import order`.
Enable the `Eclipse code formatter` by clicking `Use the Eclipse code formatter` then click the *OK* button.
** NOTE: If you configure the `Eclipse Code Formatter` from `File > Other Settings > Default Settings` it will set this policy across all of your Intellij projects.

## License

Spring Cloud Data Flow is Open Source software released under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0.html).
",2023-07-07 18:49:15+00:00
squonk,squonk,InformaticsMatters/squonk,Squonk platform and computational notebook,,False,7,2022-02-16 05:32:30+00:00,2017-06-15 08:40:39+00:00,3,6,6,34,0.5.8,2021-03-08 18:09:42+00:00,Apache License 2.0,1999,redhat-sso-7.2-A,47,2019-05-16 13:47:47+00:00,,2021-03-08 18:09:42+00:00,"# Squonk main repository

![build](https://github.com/InformaticsMatters/squonk/workflows/build/badge.svg)
![publish latest](https://github.com/InformaticsMatters/squonk/workflows/publish%20latest/badge.svg)
![publish tag](https://github.com/InformaticsMatters/squonk/workflows/publish%20tag/badge.svg)

![GitHub tag (latest SemVer)](https://img.shields.io/github/tag/informaticsmatters/squonk)

![GitHub](https://img.shields.io/github/license/informaticsmatters/squonk)
![Codacy grade](https://img.shields.io/codacy/grade/d7ff748f71f04962b4131975a14864d3)

This is the main repository for [Squonk], both the **Squonk
Platform** and the **Squonk Computational Notebook**.

Currently the portal application part of the Squonk Computational Notebook
is NOT included as that is in a separate repository that needs to be merged
into this one. Also part of Squonk are components from the [Pipelines repository].

The main source code is in the components directory. To build:

Checkout the [Pipelines repository] project into the same directory that 
contains the squonk project. Then copy the current set of service descriptors
to Squonk (this is needed for some of the tests and for those services to be
present at runtime):

    cd pipelines
    ./copy.dirs.sh

Next you need to setup access to the ChemAxon Maven repository. 
Instructions for this can be found in the [ChemAxon docs],
though they are slightly garbled.

Create or edit the gradle.properties file in your `$HOME/.gradle` directory
and add these 3 properties:

    cxnMavenUser=YOUR_EMAIL
    cxnMavenPassword=YOUR_ACCESS_KEY
    cxnMavenRepositoryUrl=https://hub.chemaxon.com/artifactory/libs-release

Replace `YOUR_EMAIL` and `YOUR_ACCESS_KEY` with the appropriate values.
Instructions for generating these can be found [ChemAxon docs].

Now build squonk by moving into the squonk/components directory and use the 
Gradle build file there. e.g.

    cd ../squonk/components
    ./gradlew build

## RDKit services
For some of the tests and some of the RDKit services you will need RDKit with
Java bindings to be present. See the [RDKit docs] for how to do this. If you
don't have this then make sure the RDBASE environment variable is not set.

## ChemAxon services
Data for the ChemAxon services and tests is located in the project's
`encrypted` directory.

    export SQUONK_DECRYPTION_KEY=""GetTheKey""
    cd components
    ./gradlew installChemaxonLicenseToHome
    ./gradlew installChemaxonLibrary

>   The `installChemaxonLicenseToHome` installs the license file to your
    `$HOME/.chemaxon` directory. If you want to put the file elsewhere you
    can, as long as you set `CHEMAXON_HOME` to the directory you use.

Once you've done this you can benefit from the full set of unit tests.

## Contributing
Currently it is not expected that third party developers will find it
easy to contribute to this codebase, but that will change. If you are
interested in using or contributing contact Tim Dudgeon
(tdudgeon \_at\_ informaticsmatters \_dot\_ com).

---

[ChemAxon docs]: https://docs.chemaxon.com/display/docs/Public+Repository#PublicRepository-HowtoCongfigureYourProject
[Pipelines repository]: https://github.com/InformaticsMatters/pipelines
[RDKit docs]: http://rdkit.org/docs/Install.html#building-from-source
[Squonk]: (http://squonk.it)
",2023-07-07 18:49:19+00:00
steppy,steppy,minerva-ml/steppy,"Lightweight, Python library for fast and reproducible experimentation :microscope:",,True,135,2023-04-17 23:27:06+00:00,2018-01-15 09:40:49+00:00,32,14,4,16,v0.1.16,2018-11-23 09:47:34+00:00,MIT License,69,v0.1.16,16,2018-11-23 09:47:34+00:00,2023-04-17 23:09:02+00:00,2018-11-23 09:47:34+00:00,"# Steppy
[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/minerva-ml/steppy/blob/master/LICENSE)

### What is Steppy?
1. Steppy is a lightweight, open-source, Python 3 library for fast and reproducible experimentation.
1. Steppy lets data scientist focus on data science, not on software development issues.
1. Steppy's minimal interface does not impose constraints, however, enables clean machine learning pipeline design.

### What problem steppy solves?
#### Problems
In the course of the project, data scientist faces two problems:
1. Difficulties with reproducibility in data science / machine learning projects.
1. Lack of the ability to prepare or extend experiments quickly.

#### Solution
Steppy address both problems by introducing two simple abstractions: `Step` and `Tranformer`. We consider it minimal interface for building machine learning pipelines.
1. `Step` is a wrapper over the transformer and handles multiple aspects of the execution of the pipeline, such as saving intermediate results (if needed), checkpointing the model during training and much more.
1. `Tranformer` in turn, is purely computational, data scientist-defined piece that takes an input data and produces some output data. Typical Transformers are neural network, machine learning algorithms and pre- or post-processing routines.

# Start using steppy
### Installation
Steppy requires `python3.5` or above.
```bash
pip3 install steppy
```
_(you probably want to install it in your [virtualenv](https://virtualenv.pypa.io/en/stable))_

### Resources
1. :ledger: [Documentation](https://steppy.readthedocs.io/en/latest)
1. :computer: [Source](https://github.com/minerva-ml/steppy)
1. :name_badge: [Bugs reports](https://github.com/minerva-ml/steppy/issues)
1. :rocket: [Feature requests](https://github.com/minerva-ml/steppy/issues)
1. :star2: Tutorial notebooks ([their repository](https://github.com/minerva-ml/steppy-examples)):
    - :arrow_forward: [Getting started](https://github.com/minerva-ml/steppy-examples/blob/master/tutorials/1-getting-started.ipynb)
    -  :arrow_forward:[Steps with multiple inputs](https://github.com/minerva-ml/steppy-examples/blob/master/tutorials/2-multi-step.ipynb)
    - :arrow_forward: [Advanced adapters](https://github.com/minerva-ml/steppy-examples/blob/master/tutorials/3-adapter_advanced.ipynb)
    - :arrow_forward: [Caching and persistance](https://github.com/minerva-ml/steppy-examples/blob/master/tutorials/4-caching-persistence.ipynb)
    - :arrow_forward: [Steppy with Keras](https://github.com/minerva-ml/steppy-examples/blob/master/tutorials/5-steps-with-keras.ipynb)

### Feature Requests
Please send us your ideas on how to improve steppy library! We are looking for your comments here: [Feature requests](https://github.com/minerva-ml/steppy/issues).

### Roadmap
:fast_forward: At this point steppy is early-stage library heavily tested on multiple machine learning challenges ([data-science-bowl](https://github.com/minerva-ml/open-solution-data-science-bowl-2018 ""Kaggle's data science bowl 2018""), [toxic-comment-classification-challenge](https://github.com/minerva-ml/open-solution-toxic-comments ""Kaggle's Toxic Comment Classification Challenge""), [mapping-challenge](https://github.com/minerva-ml/open-solution-mapping-challenge ""CrowdAI's Mapping Challenge"")) and educational projects ([minerva-advanced-data-scientific-training](https://github.com/minerva-ml/minerva-training-materials ""minerva.ml -> advanced data scientific training"")).

:fast_forward: We are developing steppy towards practical tool for data scientists who can run their experiments easily and change their pipelines with just few manipulations in the code.

### Related projects
We are also building [steppy-toolkit](https://github.com/minerva-ml/steppy-toolkit ""steppy toolkit""), a collection of high quality implementations of the top deep learning architectures -> all of them with the same, intuitive interface.

### Contributing
You are welcome to contribute to the Steppy library. Please check [CONTRIBUTING](https://github.com/minerva-ml/steppy/blob/master/CONTRIBUTING.md) for more information.

### Terms of use
Steppy is [MIT-licensed](https://github.com/minerva-ml/steppy/blob/master/LICENSE).
",2023-07-07 18:49:23+00:00
stimela,Stimela,ratt-ru/Stimela,A containerized radio interferometry scripting framework,,False,27,2022-05-21 10:24:40+00:00,2015-09-28 15:04:49+00:00,15,17,18,71,1.7.8,2023-06-15 09:25:45+00:00,GNU General Public License v2.0,2431,v1.1.3,73,2019-07-23 09:51:42+00:00,2023-07-05 10:22:23+00:00,2023-07-05 10:17:43+00:00,"
=======
Stimela
=======


|Pypi Version|
|Build Version|  

To reference *stimela* in a scholary work, please use this citation:

.. code-block:: latex

 @phdthesis{makhathini2018,
  author       = {Makhathini, S.},
  title        = {Advanced radio interferometric simulation and data reduction techniques},
  school       = {Rhodes University},
  year         = 2018,
  address      = {Drosty Rd, Grahamstown, 6139, Eastern Cape, South Africa},
  month        = 4,
  note         = {Available via \url{http://hdl.handle.net/10962/57348}}}


`Documentation <https://github.com/SpheMakh/Stimela/wiki>`_  
===========================================================

`Installation <https://github.com/SpheMakh/Stimela/wiki/Installation>`_
=======================================================================

`Tutorials <https://github.com/SpheMakh/Stimela/wiki/Examples>`_
================================================================


This project was initially funded by the `SKA/AWS AstroCompute In The Cloud <https://www.skatelescope.org/ska-aws-astrocompute-call-for-proposals>`_ initiative.


.. |Pypi Version| image:: https://img.shields.io/pypi/v/stimela.svg
                  :target: https://pypi.python.org/pypi/stimela
                  :alt:
.. |Build Version| image:: https://travis-ci.org/SpheMakh/Stimela.svg?branch=master
                  :target: https://travis-ci.com/Sphemakh/Stimela/
                  :alt:

.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/stimela.svg
                     :target: https://pypi.python.org/pypi/stimela
                     :alt:
",2023-07-07 18:49:28+00:00
stoa,Stoa,petehague/Stoa,A workflow management system for astronomy,,False,0,2021-11-02 14:22:02+00:00,2018-03-27 20:15:06+00:00,1,2,1,0,,,Apache License 2.0,107,,0,,,2021-11-02 14:21:52+00:00,"# Stoa

STOA stands for *Script Tracking for Observational Astronomy* and is a workflow management system primarily designed for large scale production of interferometry data. 

This software is still in development; users are encouraged to contact prh44 (AT) cam.ac.uk for assistance, and to follow updates carefully.

# How to Install

STOA requires Python 3 and some Python libraries, all available through `pip` - we recommend that you use a virtual environment when trying this out. When you have one set up, install `numpy`, `astropy`,`cwltool`,`grpcio-tools` and `tornado` and you should be good to go

You may also need to run `sudo pip install --upgrade protobuf`

In order to prepare STOA to run, type

`./ready.sh`

this only need be done once per STOA install. Then type

`./start.sh example 9000`

to run the demo. Go to your browser and visit `localhost:9000` to try it out

# How to Use

Log in as `guest` (no password is required) and try to create a worktable to implement the 'find' command. Use the `find.cwl` that is provided. Once this table is present, go into it and add a new row with 'product' as its input, and then run that row.

For more tutorial information, take a look at the user guide - `manual.pdf`

",2023-07-07 18:49:32+00:00
stolos,stolos,sailthru/stolos,A Directed Acyclic Graph task dependency scheduler designed to simplify complex distributed pipelines,,False,124,2023-06-24 02:06:31+00:00,2014-07-02 19:55:29+00:00,18,25,1,0,,,Apache License 2.0,408,v1.0.6-test1,10,2015-01-30 17:35:33+00:00,2023-06-24 02:06:32+00:00,2015-11-24 20:33:42+00:00,"
Summary:
==============

Stolos is a task dependency scheduler that helps build distributed pipelines.
It shares similarities with [Chronos](https://github.com/mesos/chronos),
[Luigi](http://luigi.readthedocs.org/en/latest/), and
[Azkaban](http://azkaban.github.io/azkaban/docs/2.5/), yet remains
fundamentally different from all three.

The goals of Stolos are the following:

  - Manage the order of execution of interdependent applications, where each
    application may run many times with different input parameters.
  - Provide an elegant way to define and reason about job dependencies.
  - Built for fault tolerance and scalability.
  - Applications are completely decoupled from and know nothing about Stolos.

![](/screenshot.png?raw=true)


How does Stolos work?
==============

Stolos consists of three primary components:

- a Queue (stores job state)
- a Configuration (defines task dependencies.  this is a JSON file by default)
- the Runner (ie. runs code via bash command or a plugin)

**Stolos manages a queuing system** to decide, at any given point in time, if
the current application has any jobs, if the current job is runnable, whether
to queue child or parent jobs, or whether to requeue the current job if it
failed.

**Stolos ""wraps"" jobs.**  This is an important concept for three reasons.
First, before the job starts and after the job finishes, Stolos updates the
job's state in the queueing system.  Second, rather than run a job directly (ie
from command-line), Stolos runs directly from the command-line, where it will
check the application's queue and run a queued job.  If no job is queued,
Stolos will wait for one or exit.  Third, Stolos must run once for every job in
the queue.  Stolos is like a queue consumer, and an external process must
maintain a healthy number of queue consumers.  This can be done with crontab
(meh), [Relay.Mesos](https://github.com/sailthru/relay.mesos), or any auto
scaler program.

Stolos lets its **users define deterministic dependency
relationships between applications**.  The documentation explains this in
detail.  In the future, we may let users define non-deterministic dependency
relationships, but we don't see the benefits yet.

**Applications are completely decoupled from Stolos.** This means applications
can run independently of Stolos and can also integrate directly with it without
any changes to the application's code.  Stolos identifies an application via a
Configuration, defined in the documentation.

Unlike many other dependency schedulers, **Stolos is decentralized**.  There is
no central server that ""runs"" things.  Decentralization here means a few
things.  First, Stolos does not care where or how jobs run.  Second, it doesn't
care about which queuing or configuration backends are used, provided that
Stolos is able to communicate with these backends.  Third, and perhaps most
importantly, the mission-critical questions about Consistency vs Availability
vs Partition Tolerance (as defined by the CAP theorem) are delegated to the
queue backend (and to some extent, the configuration backend).



Stolos, in summary:
==============

  - manages job state, queues future work, and starts your applications.
  - language agnostic (but written in Python).
  - ""at least once"" semantics (a guarantee that a job will successfully
    complete or fail after n retries)
  - designed for apps of various sizes: from large hadoop jobs to
    jobs that take a second to complete

What this is project not:
--------------

  - not aware of machines, nodes, network topologies and infrastructure
  - does not (and should not) auto-scale workers
  - not (necessarily) meant for ""real-time"" computation
  - This is not a grid scheduler (ie this does not solve a bin packing problem)
  - not a crontab.  (in certain cases this is not entirely true)
  - not meant to manage long-running services or servers (unless order
    in which they start is important)


Similar tools out there:
--------------

  - [Chronos](https://github.com/airbnb/chronos)
  - [Luigi](http://luigi.readthedocs.org/en/latest/)
  - [Azkaban](http://azkaban.github.io/azkaban/docs/2.5/)
  - [Dagobah](https://github.com/thieman/dagobah)
  - [Quartz](http://quartz-scheduler.org/)
  - [Cascading](http://www.cascading.org/documentation/)


Requirements:
--------------

  - A Queue backend (Redis or ZooKeeper)
  - A Configuration backend (JSON file, Redis, ...)
  - Some Python libraries (Kazoo, Networkx, Argparse, ...)

Optional requirements:
--------------

  - Apache Spark (for Spark plugin)
  - GraphViz (for visualizing the dependency graph)


Background: Inspiration
==============


The inspiration for this project comes from the notion that the way we
manage dependencies in our system defines how we characterize the work
that exists in our system.

This project arose from the needs of Sailthru's Data Science team to manage
execution of pipeline applications.  The team has a complex data pipeline
(build models, algorithms and applications that support many clients), and this
leads to a wide variety of work we have to perform within our system.  Some
work is very specific.  For instance, we need to train the same predictive
model once per (client, date).  Other tasks might be more complex: a
cross-client analysis across various groups of clients and ranges of dates.  In
either case, we cannot return results without having previously identified data
sets we need, transformed them, created some extra features on the data, and
built the model or analysis.

Since we have hundreds or thousands of instances of any particular application,
we cannot afford to manually verify that work gets completed. Therefore,
we need a system to manage execution of applications.


Concept: Application Dependencies as a Directed Graph
==============


We can model dependencies between applications as a directed graph, where nodes
are apps and edges are dependency requirements.  The following section
explains how Stolos uses a directed graph to define application
dependencies.

We start with an assumption that our applications depend on each other:

           Scenario 1:               Scenario 2:

              App_A                     App_A
                |                        /     \
                v                       v       v
              App_B                   App_B    App_C
                                        |       |
                                        |      App_D
                                        |       |
                                        v       v
                                          App_E


In Scenario 1, `App_B` cannot run until `App_A` completes.  In
Scenario 2, `App_B` and `App_C` cannot run until `App_A` completes,
but `App_B` and `App_C` can run in any order.  Also, `App_D` requires
`App_C` to complete, but doesn't care if `App_B` has run yet.
`App_E` requires `App_D` and `App_B` to have completed.

By design, we also support the scenario where one application expands into
multiple subtasks, or jobs.  The reason for this is that if we run a hundred or
thousand variations of the one app, the results of each job (ie subtask) may
bubble down through the dependency graph independently of other jobs.

There are several ways jobs may depend on other jobs, and this
system captures all deterministic dependency relationships (as far as we can
tell).


Imagine the scenario where `App_A` --> `App_B`


            Scenario 1:

               App_A
                 |
                 v
               App_B


Let's say `App_A` becomes multiple jobs, or subtasks, `App_A_i`.  And `App_B`
also becomes multiple jobs, `App_Bi`.  Scenario 1 may transform
into one of the following:


###### Scenario1, Situation I

     becomes     App_A1  App_A2  App_A3  App_An
     ------->      |          |      |         |
                   +----------+------+---------+
                   |          |      |         |
                   v          v      v         v
                 App_B1  App_B2  App_B3  App_Bn

###### Scenario1, Situation II

                 App_A1  App_A2  App_A3  App_An
     or becomes    |          |      |         |
     ------->      |          |      |         |
                   v          v      v         v
                App_B1  App_B2  App_B3  App_Bn


In Situation 1, each job, `App_Bi`, depends on completion of all of `App_A`'s
jobs before it can run.  For instance, `App_B1` cannot run until all `App_A`
jobs (1 to n) have completed.  From Stolos's point of view, this is not
different than the simple case where `App_A(1 to n)` --> `App_Bi`.  In this
case, we create a dependency graph for each `App_Bi`.  See below:

###### Scenario1, Situation I (view 2)

     becomes     App_A1  App_A2  App_A3  App_An
     ------->      |          |      |         |
                   +----------+------+---------+
                                 |
                                 v
                               App_Bi


In Situation 2, each job, `App_Bi`, depends only on completion of
its related job in `App_A`, or `App_Ai`.  For instance,
`App_B1` depends on completion of `App_A1`, but it doesn't have any
dependency on `App_A2`'s completion.  In this case, we create n
dependency graphs, as shown in Scenario 1, Situation II.

As we have just seen, dependencies can be modeled as directed acyclic
multi-graphs.  (acyclic means no cycles - ie no loops.  multi-graph
contains many separate graphs).  Situation 2 is the
default in Stolos (`App_Bi` depends only on `App_Ai`).


Concept: Job IDs
==============

*For details on how to use and configure `job_id`s, see the section, [Job
ID Configuration](/README.md#configuration-job-ids)*  This section
explains what `job_id`s are.

Stolos recognizes apps (ie `App_A` or `App_B`) and jobs (`App_A1`,
`App_A2`, ...).  An application, or app, represents a group of jobs.  A
`job_id` identifies jobs, and it is made up of ""identifiers"" that we mash
together via a `job_id` template.  A `job_id` identifies all possible
variations of some application that Stolos is aware of.  To give some
context for how `job_id` templates characterize apps, see below:


               App_A    ""{date}_{client_id}_{dataset}""
                 |
                 v
               App_B    ""{date}_{your_custom_identifier}""


Some example `job_id`s of `App_A` and `App_B`, respectively, might be:

    App_A:  ""20140614_client1_dataset1""  <--->  ""{date}_{client_id}_{dataset}""
    App_B:  ""20140601_analysis1""  <--->  ""{date}_{your_custom_identifier}""


A `job_id` represents the smallest piece of work that Stolos can
recognize, and good choices in `job_id` structure identify how work is
changing from app to app.  For instance, assume the second `job_id`
above, `20140601_analysis1`, depends on all `job_id`s from `20140601`
that matched a specific subset of clients and datasets.  We chose to
identify this subset of clients and datasets with the name `analysis1`.
But our `job_id` template also includes a `date` because we wish to run
`analysis1` on different days.  Note how the choice of `job_id`
clarifies what the first and second apps have in common.

Here's some general advice for choosing a `job_id` template:

  - What results does this app generate?  The words that differentiate this
    app's results from other apps' results are great candidates for identifers
    in a `job_id`.
  - What parameters does this app expect?  The command-line arguments
    to a piece of code can be great `job_id` identiers.
  - How many different variations of this app exist?
  - How do I expect to use this app in my system?
  - How complex is my data pipeline?  Do I have any branches in my
    dependency tree?  If you have a very simple pipeline, you may simply
    wish to have all `job_id` templates be the same across apps.

It is important to note that the way(s) in which `App_B` depends on
`App_A` have not been explained in this section.  A `job_id` does not
explain how apps depend on each other, but rather, it characterizes how
we choose to identify a app's jobs in context of the parent and child
apps.


Concept: Bubble Up and Bubble Down
==============

""Bubble Up"" and ""Bubble Down"" refer to the direction in which work and
app state move through the dependency graph.

Recall Scenario 1, which defines two apps.  `App_B` depends on
`App_A`.  The following picture is a dependency tree:

           Scenario 1:

              App_A
                |
                v
              App_B

""Bubble Down""
--------------

By analogy, the ""Bubble Down"" approach is like ""pushing"" work through a
pipe.

Assume that `App_A` and `App_B` each had their own `job_id` queue.  A
`job_id`, `job_id_123` is submitted to `App_A`'s queue, some worker
fetches that job, completes required work, and then marks the
`(App_A, job_id_123)` pair as completed.

The ""Bubble Down"" process happens when, just before `(App_A,
job_id_123)` is marked complete, we queue `(App_B, f(job_id_123))`
where `f()` is a magic function that translates `App_A`'s `job_id` to
the equivalent `job_id`(s) for `App_B`.

In other words, the completion of `App_A` work triggers the completion
of `App_B` work.  A more semantically correct version is the following:
the completion of `(App_A, job_id_123)` depends on both the successful
execution of `App_A` code and then successfully queuing some `App_B` work.


""Bubble Up""
--------------

The ""Bubble Up"" approach is the concept of ""pulling"" work through a
pipe.

In contrast to ""Bubble Down"", where we executed `App_A` first, ""Bubble
Up"" executes `App_B` first.  ""Bubble Up"" is a process of starting at
some child (or descendant job), queuing the furthest uncompleted and
unqueued ancestor, and removing the child from the queue.  When
ancestors complete, they will queue their children via ""Bubble Down"" and
re-queue the original child job.

For instance, we can attempt to execute `(App_B, job_id_B)` first.
When `(App_B, job_id_B)` runs, it checks to see if its parent,
`(App_A, g(job_id_B))` has completed.  Since `(App_A, g(job_id_B))`
has not completed, it queues this job and then removes `job_id_B` from
the `App_B` queue.  Finally, `App_A` executes and via ""Bubble Down"",
`App_B` also completes.


Magic functions f() and g()
--------------

Note that `g()`, mentioned in the ""Bubble Up"" subsection, is the inverse of
`f()`, mentioned in the ""Bubble Down"" subsection.  If `f()` is a magic function
that translates `App_A`'s `job_id` to one or more `job_id`s for `App_B`, then
`g()` is a similar magic function that transforms a `App_B` `job_id` to one or
more equivalent ones for `App_A`.  In reality, `g()` and `f()` receive one
`job_id` as input and return at least one `job_id` as output.

These two functions can be quite complex:

  - If the parent and child app have the same `job_id` template, then `f()
    == g()`.  In other words, `f()` and `g()` return the same `job_id`.
  - If they have different templates, the functions will attempt to use
    the metadata available from configuration metadata (ie in TASKS_JSON)
  - If a parent has many children, `f(parent_job_id)` returns a `job_id`
    for each child and `g(child_id)` returns at least 1 `job_id` for
    that parent app.  This may involve calculating the crossproduct of
    `job_id` identifier metadata listed in dependency configuration for
    that app.
    - If a child has many parents, `g` and `f` perform similar
      operations.


Why perform a ""Bubble Up"" operation at all?
--------------

If Stolos was purely a ""Bubble Down"" system (like many task dependency
schedulers), executing `App_B` first means we
would have to wait indefinitely until `App_A` successfully completed
and submitted a `job_id` to the queue.  This can pose many problems: we
don't know if `App_A` will ever run, so we sit and wait; waiting
processes take up resources and become non-deterministic (we have no
idea if the process will hang indefinitely); we can create locking
scenarios where there aren't enough resources to execute `App_A`;
`App_B`'s queue size can become excessively high; we suddenly need a
queue prioritization scheme and other complex algorithms to manage
scaling and resource contention.

If a task dependency system, such as Stolos, supports a ""Bubble Up"" approach,
we can simply pick and run any app in a dependency graph and expect that it
will be queued to execute as soon as possible to do so.  This avoids the above
mentioned problems.

Additionally, if Stolos is used properly, ""Bubble up"" will never queue
particular jobs that would otherwise be ignored.

The tradeoff to this approach is that if you request to run a leaf node of a
dependency graph that has never run before, due to bubble up, you will
eventually queue all tasks in that tree that have never run before, which may
result in quite a bit of computation for one simple request.


Do I need to choose between ""Bubble Up"" and ""Bubble Down"" modes?
--------------

Stolos performs ""Bubble Up"" and ""Bubble Down"" operations simultaneously, and as
a user, you do not need to choose whether to set Stolos into ""Bubble Up"" mode
or ""Bubble Down"" mode, as both work by default.  Currently, Stolos does not let
users disable ""Bubble Up.""

The key question you do need to answer is how do you want to start the jobs in
your dependency graph.  You can start by queueing jobs at the very top of a
tree and then ""bubble down"" to all of your child jobs.  You could also start by
queueing the last node in your tree, or you can start jobs from somewhere in
the middle.  In the latter two cases, jobs would ""Bubble Up"" and then ""Bubble
Down"" as described earlier in this section.

You can even start jobs from the top and the bottom of a tree at the same time,
though there is not much point to that.  If you have multiple dependency trees
defined in the graph, you can start some from the top (and bubble down) and
others from the bottom (and bubble up).  It really doesn't matter how you queue
jobs because whater you choose to do, Stolos will eventually run all parent and
then child jobs from your chosen starting point.


Concept: Job State
==============

There are 4 recognized job states.  A job_id should be in any one of these
states at any given time.

- `completed`  --  When a job has successfully completed the work defined by a
  `job_id`, and children have been queued, the `job_id` is marked as completed.
- `pending`  --  A `job_id` is pending when it is queued for work or otherwise
    waiting to be queued on completion of a parent job.
- `failed`  --  Failed `job_id`s have failed more than the maximum allowed
  number of times.  Children of failed jobs will never be executed.
- `skipped`  --  A `job_id` is skipped if it does not pass valid_if_or criteria
    defined for that app.  A skipped job is treated like a ""failed"" job.


Concept: Dependency Graph
==============

An underlying graph representation defines how apps depend on each other.  This
information is stored in the [Configuration
Backend](README.md#setup-configuration-backends).

You should think about the graph stored in the configuration backend as an
abstract, generic template that that may define multiple independent DAGs
(Directed Acyclic Graphs).  Because it's just a template, the graph typically
represents an infinite number of DAGS.  Here's what we mean:

If you recall the `Task_Ai` --> `Task_Bi` relationship, for each ith `job_id`,
there is a 2 node DAG.  If we think about the dependency graph as defined
in the Configuration Backend as a template, we can imagine just saying that
`Task_A` --> `Task_B`.  Dropping the `i` term, and assuming that `i` could be
infinitely large,  we have now created an overly simplistic representation of
an infinite number of 2 node DAGs.

Stolos extends this concept quite a further by using components of the
`job_id` template to define how more intricate dependencies that may exist
between two apps.  For instance, if `Task_B` depends on ""all"" of `Task_A`
job_ids, we must be able to enumerate all possible `Task_A` `job_id`s. Further
details on the configuration of this graph are in [Setup: Configuration
Backends](README.md#setup-configuration-backends).

Another key assumption of this graph is that it is fully deterministic.  This
means that, given an `app_name` and `job_id`, we can compute the full DAG
for that `job_id`.  An example of a non-deterministic graph is one where the
parents (or children) of an app change randomly.


Concept: Queue
==============

Every app known to Stolos has a queue.  In fact, Stolos can be thought of as an
ordered queueing system.  When jobs are consumed from the first queue, they
queue further work in child queues.  Taking this idea one step further, the
queue is an area where the pending work for a given App may come from many
different DAGs (as defined by the Dependency Graph in Configuration Backend).

The queue means a few things.  First, that Stolos will not consume from an
app's queue unless explicitly told to do so.  Second, it does not manage the
queues or solve the auto-scaling problem.  There are many other ways to solve
this.  At Sailthru, we use a tool called
[Relay.Mesos](github.com/sailthru/relay.mesos) to autoscale our Stolos queues.
Third, Stolos is only as reliable, fault tolerant and scalable as the queuing
system used.


Usage: Quick Start
==============

This will get you started playing around with the pre-configured version of
Stolos that we use for testing.

```
git clone git@github.com:sailthru/stolos.git
cd ./stolos
python setup.py develop

export $(cat conf/stolos-env.sh |grep -v \# )
```

Submit a `job_id` to `app1`'s queue

```
stolos-submit -a app1 --job_id 20140101_123_profile
```

Consume the job

```
stolos -a app1
```

**Take a look at some [examples](stolos/examples/tasks/)**


Usage: Installation and Setup (Detailed):
==============

1. The first thing you'll want to do is install Stolos

    ```
    pip install stolos

    # If you prefer a portable Python egg, clone the repo and then type:
    # python setup.py bdist_egg
    ```

2. Next, define environment vars.  You decide here which configuration backend and
queue backend you will use.  Use this [sample environment
configuration](conf/stolos-env.sh) as a guide.

    ```
    export STOLOS_JOB_ID_DEFAULT_TEMPLATE=""{date}_{client_id}_{collection_name}""
    export STOLOS_JOB_ID_VALIDATIONS=""my_python_codebase.job_id_validations""

    # assuming you choose the default JSON configuration backend:
    export STOLOS_TASKS_JSON=""/path/to/a/file/called/tasks.json""

    # and assuming you use the default Redis queue backend:
    export STOLOS_QUEUE_BACKEND=""redis""
    export STOLOS_QB_REDIS_HOST=""localhost""
    ```

3. Then, you need to define the dependency graph that informs Stolos how your
applications depend on each other.  This graph is stored in the ""Configuration
Backend."" In the environment vars defined above, we assume you're using the
default configuration backend, a JSON file.  See the links below to define the
dependency graph within your configuration backend.

  - Learn about [Job Ids](README.md#configuration-job-ids)

  - Help me define app configuration: [Configuration: Apps, Dependencies and
    Configuration](README.md#configuration-apps-dependencies-and-configuration)

4. Last, create a `job_id_validations` python file that should look like this:

  - See [example `job_id` validations file](
      stolos/examples/job_id_validations.py)


5. [Use Stolos to run my applications](README.md#usage)

    ```
    $ stolos-submit <args>
    $ stolos <args>
    ```

For help configuring non-default options:
- Non-default configuration backend: [Setup: Configuration
  Backends](README.md#setup-configuration-backends)

- Non-default queue backend: [Setup: Queue
  Backends](README.md#setup-queue-backends)


Usage: Command-line and API
==============

Stolos wraps your application code so it can track job state just before and
just after the application runs.  Therefore, you should think of running an
application via Stolos like running the application itself.  This is
fundamentally different than many projects because there is no centralized
server or ""master"" node from you can launch tasks or control Stolos.  Stolos is
simply a thin wrapper for your application.  It is ignorant of your
infrastructure and network topology.  You can generally execute Stolos
applications the same way you would treat your application without Stolos.
Keep in mind that your job will not run unless it is queued.


This is how you can manually queue a job from command-line:

    # to use the demo example, run first:
    export $(cat conf/stolos-env.sh |grep -v \# )

    stolos-submit -a app1 -j 20150101_123_profile


In order to run a job, you have to queue it and then execute it.  You
can get a job from the application's queue and execute code via:


    stolos -a app1 -h



We also provide a way to bypass Stolos and execute a job
directly.  This isn't useful unless you are testing an app that may be
dependent on Stolos plugins, such as the pyspark plugin.


    stolos --bypass_scheduler -a my_app --job_id my_job_id


Finally, there's also an API that exposes to users of Stolos the key features.
The API is visible [in code at this link](stolos/api.py).  To use the API, try
something like this:

    $ export $(cat conf/stolos-env.sh |grep -v \# )
    $ ipython
    In [1]: from stolos import api ; api.initialize()
    In [2]: api.<TAB>

    # Get details on a function by using the question mark
    In [4]: api.maybe_add_subtask?

    # If you have GraphViz installed, try this out
    In [5]: api.visualize_dag()


Setup: Configuration Backends
==============

The configuration backend identifies where you store the dependency graph.  By
default, and in our examples, Stolos expects to use a a simple json file
to store the dependency graph.  However, you can choose to store this data in
other formats or databases.  The choice of configuration backend defines how
you store configuration.  Also, keep in mind that every time a Stolos app
initializes, it queries the configuration.

Currently, the only supported configuration backends are a JSON file or a Redis
database.  However, it is also simple to extend Stolos with your own
configuration backend.  If you do implement your own configuration backend,
please consider submitting a pull request to us!

These are the steps you need to take to use a non-default backend:

1. First, let Stolos know which backend to load.  (You could also specify
   your own configuration backend, if you are so inclined).

    ```
    export STOLOS_CONFIGURATION_BACKEND=""json""  # default
    ```

    OR

    ```
    export STOLOS_CONFIGURATION_BACKEND=""redis""
    ```

    OR (to roll your own configuration backend)

    ```
    export STOLOS_CONFIGURATION_BACKEND=""mypython.module.mybackend.MyMapping""
    ```

2. Second, each backend has its own options.
    - For the JSON backend, you must define:

        ```
        export STOLOS_TASKS_JSON=""$DIR/stolos/examples/tasks.json""
        ```

    - For the Redis backend, it is optional to overide these defaults:

        ```
        export STOLOS_REDIS_DB=0  # which redis db is Stolos using?
        export STOLOS_REDIS_PORT=6379
        export STOLOS_REDIS_HOST='localhost'
        ```

3. The specific backend you use should have a way of storing and representing
   data as Mappings (key:value dictionaries) and Sequences (lists)

    - If using the JSON representation, you simply modify a JSON file when you which to change the graph.
    - The Redis configuration backend (not to be confused with queue backend)
      may require some creativity on your part as to how to initialize and
      modify the graph.  We suggest creating a json file and uploading it
      to redis using your favorite tools.


For examples, see the file, [conf/stolos-env.sh](conf/stolos-env.sh)


Setup: Queue Backends
==============

The queue backend identifies where (and how) you store job state.

Currently, the only supported queue backends are Redis and Zookeeper.  By
default, Stolos uses the Redis backend, as we have found the
Redis backend much more scalable and suitable to our needs.  Both of these
databases have strong consistency guarantees.  If using the Redis backend with
replication, be careful to follow the Redis documentation about
[Replication](http://redis.io/topics/replication).  Subtle configuration errors
in any distributed database can result in data consistency and corruption
issues that may cause Stolos to running tasks multiple times or in the worst
case run tasks infinitely until the database is manually cleaned up.

These are the steps you need to take to choose a backend:

1. First, let Stolos know which backend to load.

```
export STOLOS_QUEUE_BACKEND=""redis""  # default
```

OR

```
export STOLOS_QUEUE_BACKEND=""zookeeper""
```

2. Second, each backend has its own options.
    - For the Redis backend, you may define the following:

        export STOLOS_QB_REDIS_PORT=6379
        export STOLOS_QB_REDIS_HOST='localhost'
        export STOLOS_QB_REDIS_DB=0  # which redis db is Stolos using?
        export STOLOS_QB_REDIS_LOCK_TIMEOUT=60
        export STOLOS_QB_REDIS_MAX_NETWORK_DELAY=30
        export STOLOS_QB_REDIS_SOCKET_TIMEOUT=15

    - For the Zookeeper backend, you can define:

        export STOLOS_QB_ZOOKEEPER_HOSTS=""localhost:2181""  # or appropriate uri
        export STOLOS_QB_ZOOKEEPER_TIMEOUT=30



For examples, see the file, [conf/stolos-env.sh](conf/stolos-env.sh)


Configuration: Job IDs
==============

This section explains what configuration for `job_id`s must exist.

These environment variables must be available to Stolos:

    export STOLOS_JOB_ID_DEFAULT_TEMPLATE=""{date}_{client_id}_{collection_name}""
    export STOLOS_JOB_ID_VALIDATIONS=""my_python_codebase.job_id_validations""

- `STOLOS_JOB_ID_VALIDATIONS` points to a python module containing code to
  verify that the identifiers in a `job_id` are correct.
  - See
    [stolos/examples/job_id_validations.py](stolos/examples/job_id_validations.py)
    for the expected code structure
  - These validations specify exactly which identifiers can be used in
    job_id templates and what format they take (ie is `date` a datetime
    instance, an int or string?).
  - They are optional to implement, but you will see several warning
    messages for each unvalidated `job_id` identifier.
- `STOLOS_JOB_ID_DEFAULT_TEMPLATE` - defines the default `job_id` for an app if
  the `job_id` template isn't explicitly defined in the app's config.  You
  should have `job_id` validation code for each identifier in your default
  template.

In addition to these defaults, each app in the app configuration may also
contain a custom `job_id` template.  See section: [Configuration: Apps,
Dependencies and Configuration
](README.md#configuration-apps-dependencies-and-configuration) for details.


Configuration: Apps, Dependencies and Configuration
==============

This section will show you how to define the dependency graph.  This graph
answers questions like: ""What are the parents or children for this (`app_name`,
`job_id`) pair?"" and ""What general key:value configuration is defined for this
app?"".  It does not store the state of `job_id`s and it does not contain
queuing logic.  This section exposes how we define the apps and their
relationships to other apps.

Configuration can be defined in different configuration backends:
a json file, Redis, a key-value database, etc.  For instructions on how to
setup different or custom configuration backends, see section ""Setup:
Configuration Backends.""

Each `app_name` in the graph may define a few different options:

- *`job_type`* - (optional) Select which plugin this particular app uses
  to execute your code.  The default `job_type` is `bash`.
- *`depends_on`* - (optional) A designation that a (`app_name`, `job_id`)
  can only be queued to run if certain parent `job_id`s have completed.
- *`job_id`* - (optional) A template describing what identifiers compose
  the `job_id`s for your app.  If not given, assumes the default `job_id`
  template.  `job_id` templates determine how work changes through your
  pipeline
- *`valid_if_or`* - (optional) Criteria that `job_id`s are matched against.
  If a `job_id` for an app does not match the given
  `valid_if_or` criteria, then the job is immediately marked as ""skipped""


Here is a minimum viable configuration for an app:

    {
        ""app_name"": {
          ""bash_cmd"": ""echo 123""
        }
    }

As you can see, there's not much to it.

Next up is an example of a simple `App_Ai` --> `App_Bi` relationship.
Also notice that the `bash_cmd` performs string interpolation so
applications can receive dynamically determined command-line parameters.

    {
        ""App1"": {
            ""bash_cmd"": ""echo {app_name} is Running App 1 with {job_id}""
        },
        ""App2"": {
            ""bash_cmd"": ""echo Running App 2. job_id contains date={date}""
            ""depends_on"": {""app_name"": [""App1""]}
        }
    }

Next, we will see a slightly more complex variant of a `App_Ai` --> `App_Bi`
relationship.  Notice that the `job_id` of the child app has changed, meaning a
`preprocess` job identified by `{date}_{client_id}` would kick off a
`modelBuild` job identified by `{date}_{client_id}_purchaseRevenue`.  The
""autofill_values"" section informs Stolos that if ever there is a scenario where
the `modelBuild`'s `target` is undefined, we can fill in the missing
information using values defined in ""autofill_values"".  For instance, when
`preprocess` `20150101_123` completes, it should queue `modelBuild`
`20150101_123_purchaseRevenue`.  If autofill_values was not defined, Stolos
would raise an exeption.

    {
        ""preprocess"": {
            ""job_id"": ""{date}_{client_id}""
        },
        ""modelBuild"": {
            ""job_id"": ""{date}_{client_id}_{target}""
            ""autofill_values"": {
              ""target"": [""purchaseRevenue""]
            },
            ""depends_on"": {
                ""app_name"": [""preprocess""]
            }
        }
    }

Expanding on the above example, we see a dependency graph demonstrating how
`App_A` queues up multiple `App_Bi`.  In this example, the completion of a
`preprocess` job identified by `20140101_1234` enqueues two `modelBuild` jobs:
`20140101_1234_purchaseRevenue` and `20140101_1234_numberOfPageviews`.

    {
        ""preprocess"": {
            ""job_id"": ""{date}_{client_id}""
        },
        ""modelBuild"""": {
            ""autofill_values"": {
              ""target"": [""purchaseRevenue"", ""numberOfPageviews""]
            },
            ""job_id"": ""{date}_{client_id}_{target}""
            ""depends_on"": {
                ""app_name"": [""preprocess""]
            }
        }
    }

The below configuration demonstrates how multiple `App_Ai` reduce to `App_Bj`.
In other words, the `modelBuild2` job, `client1_purchaseRevenue`, cannot run
(or be queued to run) until `preprocess` has completed these `job_ids`:
`20140601_client1`, `20140501_client1`, and `20140401_client1`.  The same
applies to `client1_numberOfPageviews`.  However, it does not matter whether
`client1_numberOfPageviews` or `client1_purchaseRevenue` runs first.  Looking
at this from the other way around, the children of `preprocess`
`20140501_client1` are these two `modelBuild2` jobs: `client1_purchaseRevenue`
and `client1_numberOfPageViews`.  Also, notice that these dates are hardcoded.
If you would like to implement more complex logic around dates, you should
refer to the section, [Configuration: Defining Dependencies with Two-Way
Functions](README.md#configuration-defining-dependencies-with-a-two-way-functions).

    {
        ""preprocess"": {
            ""job_id"": ""{date}_{client_id}""
        },
        ""modelBuild2"": {
            ""job_id"": ""{client_id}_{target}""
            ""autofill_values"": {
              ""target"": [""purchaseRevenue"", ""numberOfPageviews""]
            }
            ""depends_on"": {
                ""app_name"": [""preprocess""],
                ""date"": [20140601, 20140501, 20140401]
            }
        }
    }

We also enable boolean logic in dependency structures.  We will introduce a
concept of a dependency group and then say that the use of a list specifies AND
logic, while the declaration of different dependency groups specifies OR logic.
An app can depend on `dependency_group_1` OR another dependency group.  Within
a dependency group, you can also specify that the dependencies come from one
different set of `job_ids` AND another set.  The AND and OR logic can also be
combined in one example, and this can result in surprisingly complex
relationships.

Take a look at the below example.  In this example, there are several things
happening.  Firstly, note that in order for any of `modelBuild3`'s jobs to be
queued to run, either the dependencies in `dependency_group_1` OR those in
`dependency_group_2` must be met.  Looking more closely at
`dependency_group_1`, we can see that it defines a list of key-value objects
ANDed together using a list.  `dependency_group_1` will not be satisfied unless
all of the following is true: the three listed dates for `preprocess` have
completed AND the two dates for `otherPreprocess`  have completed.  In summary,
the value of `dependency_group_1` is a list.  The use of a list specifies AND
logic, while the declaration of different dependency groups specifies OR logic.

Also take note of how `target` is defined here.  The `autofill_values` for
`target` are different depending on which dependency group we are dealing with.
If you find that putting target in the depends_on is confusing, we agree!  Open
a GH issue if you have suggestions!

    {
        ""preprocess"": {
            ""job_id"": ""{date}_{client_id}""
        },
        ""modelBuild"": {
            ""job_id"": ""{client_id}_{target}""
            ""autofill_values"": {
                     ""target"": []
            },
            ""depends_on"": {
                ""dependency_group_1"": [
                    {""app_name"": [""preprocess""],
                     ""date"": [20140601, 20140501, 20140401],
                      ""target"": [""purchaseRevenue"", ""purchaseQuantity""]
                    },
                    {""app_name"": [""otherPreprocess""],
                     ""target"": [""purchaseRevenue"", ""purchaseQuantity""],
                     ""date"": [20120101, 20130101]
                    }
                  ],
                ""dependency_group_2"": {
                    ""app_name"": [""preprocess""],
                    ""target"": [""numberOfPageviews""],
                    ""date"": [20140615]
                }
            }

Finally, the last two examples worth exploring are the use of depends_on ""all""
and also the use of ranges in autofill_values.  Here's an example where App2
jobs are identified by day.  Each `date`, App2 cannot run until all of that
`date`'s App1 jobs run.  That includes the cross product of 10 `stage_id`s with
all even numbered `response_id`s greater than or equal to 10 and less than 50.
This means job_id like `App2` `20150101` depends on 200 (10 * 20) App1 jobs.
You can see how complexity builds fairly quickly.

    {
        ""App1"": {
            ""job_id_template"": ""{date}_{stage_id}_{response_id}"",
            ""autofill_values"": {
              ""stage_id"": ""0:10"",
               ""response_id"": ""10:50:2""
            }
        },
        ""App2"": {
            ""bash_cmd"": ""echo Running App 2. job_id contains date={date}""
            ""job_id_template"": ""{date}"",
            ""depends_on"": {
              ""app_name"": [""App1""],
              ""response_id"": ""all"",
              ""stage_id"": ""all""
            }
        }
    }

There are many structures that `depends_on` can take, and some are better than
others.  We've given you enough building blocks to express many
deterministic batch processing pipelines.  That said, there are several things
Stolos does not currently support:

- Complex logic in depends_on (Two-Way functions directly solve this problem)
- Dependency on failure conditions (Two-way functions might be able to solve)

Finally, for more examples, consult
[stolos/examples/tasks.json](stolos/examples/tasks.json)


Configuration: Defining Dependencies with a Two-Way Functions
==============

 TODO  This isn't implemented yet.

Two way functions *WILL* allow users of Stolos to define arbitrarily complex
dependency relationships between jobs.  The general idea of a two-way function
is to define how the job, `App_Ai`, can spawn one or more children, `App_Bj`.
Being ""two-way"", this function must be able to identify child and parent
jobs.  One risk of introducing two-way functions is that users can define
non-deterministic dependencies. Do this at your own risk. It's not recommended!
A second risk, or perhaps a feature, of these functions is that one could
define dependency structures that are one directional.  For instance, given a
parent app_name and parent job_id components, the function should return
children.  It could be crafted such that given children, the function returns
no parents.  The details still need to be ironed out.

    ```
    depends_on:
        {_func: ""python.import.path.to.package.module.func"", app_name: ...}
    ```


Configuration: Job Types
==============

The `job_type` specifier in the config defines how your application code
should run.  For example, should your code be treated as a bash job (and
executed in its own shell), or should it be an Apache Spark (python) job
that receives elements of a stream or a textFile instance?  The
following table defines different `job_type` options available.  Each
`job_type` has its own set of configuration options, and these are
available at the commandline and (possibly) in the app configuration.

For most use-cases, we recommend ""bash"" job type.  However, if a plugin seems
particularly useful, remember that running the application without Stolos may
require some extra code on your part.

 - `job_type`=""bash""
    - `bash_cmd`
 - `job_type`=""pyspark""
    - `pymodule` - a python import path to python application code.  ie.
      `stolos.examples.tasks.test_task`,
    - `spark_conf` - a dict of Spark config keys and values
    - `env` - a dict of environment variables and values
    - `env_from_os` - a list if os environment variables that should exist on
      the Spark driver
    - `uris` - a list of Spark files and pyFiles

(Developer note) Different `job_type`s correspond to specific ""plugins""
recognized by Stolos.  One can extend Stolos to support custom `job_type`s.
You may wish to do this if you determine that it is more convenient have
similar apps re-use the same start-up and tear-down logic.  Keep in mind that
plugins generally violate Stolos's rule that it is ignorant of your runtime
environment, network topology, infrastructure, etc.  As a best practice,
try to make your plugin completely isolated from the rest of Stolos's codebase.
Refer to the developer documentation for writing custom plugins.


Developer's Guide
===============

Submitting a Pull Request
---------------

We love that you're interested in contributing to this project!  Hopefully,
this section will help you make a successful pull request to the project.

If you'd like to make a change, you should:

1. Create an issue and form a plan with maintainers on the issue tracker
1. Fork this repo, clone it to you machine, make changes in your fork,
   and then submit a Pull Request.  Google ""How to submit a pull request"" or
   [follow this guide](https://help.github.com/articles/using-pull-requests).
  - Before submitting the PR, run tests to verify your code is clean:

    ./bin/test_stolos_in_docker  # run the tests

1. Get code reviewed and iterate until PR is closed


Creating a plugin
---------------

Plugins define how Stolos should execute an Application's jobs.  By default,
Stolos supports executing bash applications and Python Spark (pyspark)
applications.  In general, just using the bash plugin is fine for most
scenarios.  However, we expose a plugin api so that you may more tightly couple
your running application to Stolos's runtime environment.  It might make sense
to do this if you want your application to share the same process as Stolos, or
perhaps you wish to standardize the way different applications are initialized.

If you wish to add another plugin to Stolos or use your own, please follow
these instructions.  If you wish to create a custom plugin, create a
python file that defines exactly two things:

  - It must define a `main(ns)` function.  `ns` is an `argparse.Namespace`
    instance.  This function should use the values defined by variables
    `ns.app_name` and `ns.job_id` (and whatever other ns variables) to execute
    some specific piece of code that exists somewhere.
  - It must define a `build_arg_parser` object that will populate the `ns`.
    Keep in mind that Stolos will also populate this ns and it will force you
    to avoid naming conflicts in argument options.

Boilerplate for a Stolos plugin is this:

```
from stolos.plugins import at, api

def main(ns):
    pass

build_arg_parser = at.build_arg_parser([...])
```

To use your custom plugin, in your application's configuration, set the
`job_type` to `python.import.path.to.your.module`.


Roadmap:
===============

Here are some improvements we are considering in the future:

- Support additional configuration backends
  - Some of these backends may support a web UI for creating, viewing and
    managing app config and dependencies
  - We currently support storing configuration in json file xor in Redis.
- A web UI showing various things like:
  - Interactive dependency graph
  - Current job status
",2023-07-07 18:49:35+00:00
apachestorm,storm,apache/storm,Mirror of Apache Storm,,False,6467,2023-07-06 19:25:10+00:00,2013-11-05 08:00:14+00:00,4104,583,279,0,,,Apache License 2.0,10569,v2.5.0,49,2023-07-06 20:43:46+00:00,2023-07-06 20:44:05+00:00,2023-07-06 20:44:01+00:00,"Master Branch:  
[![Java CI with Maven](https://github.com/apache/storm/actions/workflows/maven.yaml/badge.svg)](https://github.com/apache/storm/actions/workflows/maven.yaml)
[![Maven Version](https://maven-badges.herokuapp.com/maven-central/org.apache.storm/storm-core/badge.svg)](http://search.maven.org/#search|gav|1|g:""org.apache.storm""%20AND%20a:""storm-core"")
 
Storm is a distributed realtime computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing realtime computation. Storm is simple, can be used with any programming language, [is used by many companies](http://storm.apache.org/Powered-By.html), and is a lot of fun to use!

The [Rationale page](http://storm.apache.org/documentation/Rationale.html) explains what Storm is and why it was built. [This presentation](http://vimeo.com/40972420) is also a good introduction to the project.

Storm has a website at [storm.apache.org](http://storm.apache.org). Follow [@stormprocessor](https://twitter.com/stormprocessor) on Twitter for updates on the project.

## Documentation

Documentation and tutorials can be found on the [Storm website](http://storm.apache.org/documentation/Home.html).

Developers and contributors should also take a look at our [Developer documentation](DEVELOPER.md).
 

## Getting help

__NOTE:__ The google groups account storm-user@googlegroups.com is now officially deprecated in favor of the Apache-hosted user/dev mailing lists.

### Storm Users
Storm users should send messages and subscribe to [user@storm.apache.org](mailto:user@storm.apache.org).

You can subscribe to this list by sending an email to [user-subscribe@storm.apache.org](mailto:user-subscribe@storm.apache.org). Likewise, you can cancel a subscription by sending an email to [user-unsubscribe@storm.apache.org](mailto:user-unsubscribe@storm.apache.org).

You can also [browse the archives of the storm-user mailing list](http://mail-archives.apache.org/mod_mbox/storm-user/).

### Storm Developers
Storm developers should send messages and subscribe to [dev@storm.apache.org](mailto:dev@storm.apache.org).

You can subscribe to this list by sending an email to [dev-subscribe@storm.apache.org](mailto:dev-subscribe@storm.apache.org). Likewise, you can cancel a subscription by sending an email to [dev-unsubscribe@storm.apache.org](mailto:dev-unsubscribe@storm.apache.org).

You can also [browse the archives of the storm-dev mailing list](http://mail-archives.apache.org/mod_mbox/storm-dev/).

Storm developers who would want to track the JIRA issues should subscribe to [issues@storm.apache.org](mailto:issues@storm.apache.org).

You can subscribe to this list by sending an email to [issues-subscribe@storm.apache.org](mailto:issues-subscribe@storm.apache.org). Likewise, you can cancel a subscription by sending an email to [issues-unsubscribe@storm.apache.org](mailto:issues-unsubscribe@storm.apache.org).

You can view the archives of the mailing list [here](http://mail-archives.apache.org/mod_mbox/storm-issues/).

### Issue tracker
In case you want to raise a bug/feature or propose an idea, please use [Apache Jira](https://issues.apache.org/jira/projects/STORM)

### Which list should I send/subscribe to?
If you are using a pre-built binary distribution of Storm, then chances are you should send questions, comments, storm-related announcements, etc. to [user@storm.apache.org](mailto:user@storm.apache.org).

If you are building storm from source, developing new features, or otherwise hacking storm source code, then [dev@storm.apache.org](mailto:dev@storm.apache.org) is more appropriate.

If you are committers and/or PMCs, or contributors looking for following up and participating development of Storm, then you would want to also subscribe [issues@storm.apache.org](issues@storm.apache.org) in addition to [dev@storm.apache.org](dev@storm.apache.org).

### What will happen with storm-user@googlegroups.com?
All existing messages will remain archived there, and can be accessed/searched [here](https://groups.google.com/forum/#!forum/storm-user).

New messages sent to storm-user@googlegroups.com will either be rejected/bounced or replied to with a message to direct the email to the appropriate Apache-hosted group.

## License

Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.

The LICENSE and NOTICE files cover the source distributions. The LICENSE-binary and NOTICE-binary files cover the binary distributions. The DEPENDENCY-LICENSES file lists the licenses of all dependencies of Storm, including those not packaged in the source or binary distributions, such as dependencies of optional connector modules.


## Project lead

* Nathan Marz ([@nathanmarz](http://twitter.com/nathanmarz))

## Committers

* James Xu ([@xumingming](https://github.com/xumingming))
* Jason Jackson ([@jason_j](http://twitter.com/jason_j))
* Andy Feng ([@anfeng](https://github.com/anfeng))
* Flip Kromer ([@mrflip](https://github.com/mrflip))
* David Lao ([@davidlao2k](https://github.com/davidlao2k))
* P. Taylor Goetz ([@ptgoetz](https://github.com/ptgoetz))
* Derek Dagit ([@d2r](https://github.com/d2r))
* Robert Evans ([@revans2](https://github.com/revans2))
* Michael G. Noll ([@miguno](https://github.com/miguno))
* Kishor Patil ([@kishorvpatil](https://github.com/kishorvpatil))
* Sriharsha Chintalapani([@harshach](https://github.com/harshach))
* Sean Zhong ([@clockfly](http://github.com/clockfly))
* Kyle Nusbaum ([@knusbaum](https://github.com/knusbaum))
* Parth Brahmbhatt ([@Parth-Brahmbhatt](https://github.com/Parth-Brahmbhatt))
* Jungtaek Lim ([@HeartSaVioR](https://github.com/HeartSaVioR))
* Aaron Dossett ([@dossett](https://github.com/dossett))
* Matthias J. Sax ([@mjsax](https://github.com/mjsax))
* Arun Mahadevan ([@arunmahadevan](https://github.com/arunmahadevan))
* Boyang Jerry Peng ([@jerrypeng](https://github.com/jerrypeng))
* Zhuo Liu ([@zhuoliu](https://github.com/zhuoliu))
* Haohui Mai ([@haohui](https://github.com/haohui))
* Sanket Chintapalli ([@redsanket](https://github.com/redsanket))
* Longda Feng ([@longda](https://github.com/longdafeng))
* John Fang ([@hustfxj](https://github.com/hustfxj))
* Abhishek Agarwal ([@abhishekagarwal87](https://github.com/abhishekagarwal87))
* Satish Duggana ([@satishd](https://github.com/satishd))
* Xin Wang ([@vesense](https://github.com/vesense))
* Hugo da Cruz Louro ([@hmcl](https://github.com/hmcl))
* Stig Rohde Døssing ([@srdo](https://github.com/srdo/))
* Roshan Naik ([@roshannaik](http://github.com/roshannaik))
* Ethan Li ([@Ethanlm](https://github.com/Ethanlm))
* Govind Menon ([@govind](https://github.com/govind-menon))
* Aaron Gresch ([@agresch](https://github.com/agresch))
* Rui Li ([@ruili](https://github.com/RuiLi8080))
* Bipin Prasad ([@bipinprasad](https://github.com/bipinprasad))

## Acknowledgements

YourKit is kindly supporting open source projects with its full-featured Java Profiler. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications. Take a look at YourKit's leading software products: [YourKit Java Profiler](http://www.yourkit.com/java/profiler/index.jsp) and [YourKit .NET Profiler](http://www.yourkit.com/.net/profiler/index.jsp).
",2023-07-07 18:49:40+00:00
streamflow,streamflow,alpha-unito/streamflow,StreamFlow Workflow Manager,https://streamflow.di.unito.it,False,38,2023-06-24 15:15:26+00:00,2019-11-18 11:39:15+00:00,12,8,7,26,0.2.0.dev6,2023-06-15 00:14:07+00:00,GNU Lesser General Public License v3.0,317,0.2.0.dev6,29,2023-06-15 00:14:07+00:00,2023-07-07 12:26:59+00:00,2023-07-06 13:47:51+00:00,"# StreamFlow

[![CI Tests](https://github.com/alpha-unito/streamflow/actions/workflows/ci-tests.yaml/badge.svg?branch=master)](https://github.com/alpha-unito/streamflow/actions/workflows/ci-tests.yaml)

The [StreamFlow](https://streamflow.di.unito.it/) framework is a container-native *Workflow Management System (WMS)* written in Python 3.
It has been designed around two main principles:
* Allow the execution of tasks in **multi-container environments**, in order to support concurrent execution
of multiple communicating tasks in a multi-agent ecosystem.
* Relax the requirement of a single shared data space, in order to allow for **hybrid workflow** executions on top of
multi-cloud or hybrid cloud/HPC infrastructures.

## Use StreamFlow

#### PyPI
 
The StreamFlow module is available on [PyPI](https://pypi.org/project/streamflow/), so you can install it using pip.

```bash
pip install streamflow
```

Please note that StreamFlow requires `python >= 3.8`. Then you can execute it directly from the CLI

```bash
streamflow run /path/to/streamflow.yml
```

#### Docker

StreamFlow Docker images are available on [Docker Hub](https://hub.docker.com/r/alphaunito/streamflow). In order to run
a workflow inside the StreamFlow image
 - A StreamFlow project, containing a `streamflow.yml` file and all the other relevant dependencies (e.g. a CWL
   description of the workflow steps and a Helm description of the execution environment) needs to be mounted as a volume
   inside the container, for example in the `/streamflow/project` folder
 - Workflow outputs, if any, will be stored in the `/streamflow/results` folder. Therefore, it is necessary to mount
   such location as a volume in order to persist the results
 - StreamFlow will save all its temporary files inside the `/tmp/streamflow` location. For debugging purposes, or in
   order to improve I/O performances in case of huge files, it could be useful to mount also such location as a volume
 - The path of the `streamflow.yml` file **inside the container** (e.g. `/streamflow/project/streamflow.yml`) must be
   passed as an argument to the Docker container

The script below gives an example of StreamFlow execution in a Docker container

```bash
docker run -d \
    --mount type=bind,source=""$(pwd)""/my-project,target=/streamflow/project \
    --mount type=bind,source=""$(pwd)""/results,target=/streamflow/results \
    --mount type=bind,source=""$(pwd)""/tmp,target=/tmp/streamflow \
    alphaunito/streamflow run /streamflow/project/streamflow.yml
```

#### Kubernetes

It is also possible to execute the StreamFlow container as a `Job` in [Kubernetes](https://kubernetes.io/).
In this case, StreamFlow is able to deploy `Helm` charts directly on the parent cluster through the
`ServiceAccount` credentials. In order to do that, the `inCluster` option must be set to `true` for each
involved module on the `streamflow.yml` file

```yaml
deployments:
  helm-deployment:
    type: helm
    config:
      inCluster: true
      ...
```

A `Helm` template of a StreamFlow `Job` can be found in the `helm/chart` folder.

Please note that, in case [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) is active on the
Kubernetes cluster, a proper `RoleBinding` must be attached to the `ServiceAccount` object, in order to give
StreamFlow the permissions to manage deployments of pods and executions of tasks.

## CWL Compatibility

StreamFlow relies on the [Common Workflow Language](https://www.commonwl.org/) (CWL) standard to design workflow models. CWL conformance badges for StreamFlow are reported below.

### CWL v1.0

#### Classes

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/command_line_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/expression_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/workflow.json)

#### Required features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/required.json)

#### Optional features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/docker.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/env_var.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/initial_work_dir.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/inline_javascript.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/multiple_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/resource.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/scatter.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/schema_def.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/shell_command.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/step_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.0/subworkflow.json)

### CWL v1.1

#### Classes

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/command_line_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/expression_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/workflow.json)

#### Required features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/required.json)

#### Optional features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/docker.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/env_var.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/format_checking.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/initial_work_dir.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/inline_javascript.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/inplace_update.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/input_object_requirements.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/multiple_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/networkaccess.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/resource.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/scatter.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/schema_def.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/shell_command.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/step_input_expression.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/step_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/subworkflow.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.1/timelimit.json)

### CWL v1.2

#### Classes

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/command_line_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/expression_tool.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/workflow.json)

#### Required features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/required.json)

#### Optional features

![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/conditional.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/docker.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/env_var.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/format_checking.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/initial_work_dir.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/inline_javascript.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/inplace_update.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/input_object_requirements.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/load_listing.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/multiple_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/multiple.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/networkaccess.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/resource.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/scatter.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/schema_def.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/shell_command.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/step_input.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/subworkflow.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/timelimit.json)
![](https://badgen.net/https/streamflow.di.unito.it/cwl-conformance/v1.2/work_reuse.json)

## Contribute to StreamFlow

As a first step, get StreamFlow from [GitHub](https://github.com/alpha-unito/streamflow) 
```bash
git clone git@github.com:alpha-unito/streamflow.git
```

Then you can install all the required packages using the `pip install` command

```bash
cd streamflow
pip install -r requirements.txt
```

StreamFlow relies on [GitHub Actions](https://github.com/features/actions) for PyPI and Docker Hub distributions. Therefore, in order to publish a
new version of the software, you only have to augment the version number in `version.py` file.

## StreamFlow Team

Iacopo Colonnelli <iacopo.colonnelli@unito.it> (creator and maintainer)  
Barbara Cantalupo <barbara.cantalupo@unito.it> (maintainer)  
Marco Aldinucci <aldinuc@di.unito.it> (maintainer)

Gaetano Saitta <gaetano.saitta@edu.unito.it> (contributor)  
Alberto Mulone <alberto.mulone@edu.unito.it> (contributor)
",2023-07-07 18:49:44+00:00
streampipes,streampipes,apache/streampipes,"Apache StreamPipes - A self-service (Industrial) IoT toolbox to enable non-technical users to connect, analyze and explore IoT data streams.",https://streampipes.apache.org,False,427,2023-07-07 09:22:36+00:00,2018-04-22 20:06:55+00:00,129,28,52,15,release/0.92.0,2023-06-09 06:17:14+00:00,Apache License 2.0,10233,version-0.55.2,17,2018-05-08 21:46:05+00:00,2023-07-07 17:58:13+00:00,2023-07-07 16:51:03+00:00,"<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the ""License""); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an ""AS IS"" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  ~
  -->

[![Github Actions](https://img.shields.io/github/actions/workflow/status/apache/streampipes/build.yml)](https://github.com/apache/streampipes/actions/)
[![Docker pulls](https://img.shields.io/docker/pulls/apachestreampipes/backend.svg)](https://hub.docker.com/r/apachestreampipes/backend/)
![](https://img.shields.io/badge/java--version-17-blue.svg)
[![Maven central](https://img.shields.io/maven-central/v/org.apache.streampipes/streampipes-service-core.svg)](https://img.shields.io/maven-central/v/org.apache.streampipes/streampipes-service-core.svg)
[![License](https://img.shields.io/github/license/apache/streampipes.svg)](http://www.apache.org/licenses/LICENSE-2.0)
[![Last commit](https://img.shields.io/github/last-commit/apache/streampipes.svg)]()
[![Apache StreamPipes](https://img.shields.io/endpoint?url=https://dashboard.cypress.io/badge/detailed/q1jdu2&style=flat&logo=cypress)](https://dashboard.cypress.io/projects/q1jdu2/runs)
[![Contributors](https://img.shields.io/github/contributors/apache/streampipes)](https://github.com/apache/streampipes/graphs/contributors)
![GitHub commit activity](https://img.shields.io/github/commit-activity/y/apache/streampipes)
[![GitHub issues by-label](https://img.shields.io/github/issues/apache/streampipes/good%20first%20issue)](https://github.com/apache/streampipes/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
<br>
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/company/apache-streampipes)
[![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/StreamPipes)

<h1 align=""center"">
  <br>
   <img src=""https://streampipes.apache.org/img/sp-logo-color.png"" 
   alt=""StreamPipes Logo"" title=""Apache StreamPipes Logo"" width=""50%""/>
  <br>
</h1>
<h3 align=""center"">Self-Service Data Analytics for the (Industrial) IoT</h3>
<h4 align=""center"">StreamPipes is a self-service (Industrial) IoT toolbox to enable non-technical users to connect
, analyze and explore IoT data streams. </h4>
<p align=""center"">  
    <img src=""https://raw.githubusercontent.com/apache/streampipes/dev/images/streampipes-overview.png"" alt=""StreamPipes Overview""/>
</p>


***

## Table of contents

  * [About Apache StreamPipes](#about-apache-streampipes)
  * [User interface](#userinterface)
  * [Installation](#installation)
  * [Documentation](#documentation)
  * [Building StreamPipes](#building-streampipes)
  * [Pipeline Elements](#pipeline-elements)  
  * [Extending StreamPipes](#extending-streampipes)
  * [Bugs and Feature Requests](#bugs-and-feature-requests)
  * [Get help](#get-help)
  * [Contribute](#contribute)
  * [Feedback](#feedback)
  * [License](#license)

***

## About Apache StreamPipes

Apache StreamPipes makes industrial data analytics easy!

StreamPipes is an end-to-end toolbox for the industrial IoT. 
It comes with a rich graphical user interface targeted at non-technical users and provides the following features:


* Quickly connect >20 industrial protocols such as OPC-UA, PLCs, MQTT, REST, Pulsar, Kafka and others.
* Create data harmonization and analytics pipelines using > 100 algorithms and data sinks to forward data to third-party systems.
* Use the data explorer to visually explore historical data with many widgets tailored for time-series data.
* A live dashboard to display real-time data from data sources and pipelines, e.g., for shopfloor monitoring.


StreamPipes is highly extensible and includes a Java SDK to create new 
pipeline elements and adapters. Python support is available in an early development stage - stay tuned!  
Pipeline elements are standalone microservices that can run anywhere -
centrally on your server or close at the edge.
You want to employ your own machine learning model on live data?
Just write your own data processor and make it reusable as a pipeline element.

Besides that, StreamPipes includes features for production deployments:

* Assign resources such as pipelines, data streams and dashboards to assets for better organization
* Monitoring & metrics of pipelines and adapters
* Built-in user and access rights management
* Export and import resources

## User interface

* Connect data from an OPC-UA server following a three-step configuration process:

![StreamPipes Connect](https://raw.githubusercontent.com/apache/streampipes/dev/images/streampipes-connect.gif)

* Create a pipeline to detect a continuous decrease using a trend detection data processor and a ``Notification``sink:

![StreamPipes Pipeline Editor](https://raw.githubusercontent.com/apache/streampipes/dev/images/streampipes-pipelines.gif)

* Visually analyze data using the data explorer:

![StreamPipes Data Explorer](https://raw.githubusercontent.com/apache/streampipes/dev/images/streampipes-data-explorer.gif)


## Installation

The quickest way to run StreamPipes including the latest extensions (adapters, pipeline elements) is by using our Docker-based [installation & operation options](installer), namely: 

* **[StreamPipes Compose](installer/compose)** - The User's Choice
* **[StreamPipes CLI](installer/cli)** - The Developer's Favorite
* **[StreamPipes k8s](installer/k8s)** - The Operator's Dream

> **NOTE**: StreamPipes CLI & k8s are highly recommended for developers or operators. Standard users should stick to StreamPipes Compose.

Please follow the instructions provided in the corresponding `README.md` to get started.

For a more in-depth manual, read the [installation guide](https://streampipes.apache.org/docs/docs/try-installation.html).

> TL;DR: Download the latest release, switch to the ``installer/compose`` directory and run ``docker-compose up -d``.

## Documentation

The full documentation is available [here](https://streampipes.apache.org/docs/index.html).

Quick Links:

* [Installation](https://streampipes.apache.org/docs/docs/try-installation.html)
* [Create adapters](https://streampipes.apache.org/docs/docs/use-connect.html)
* [Create pipelines](https://streampipes.apache.org/docs/docs/use-pipeline-editor.html)
* [Pipeline elements](https://streampipes.apache.org/pipeline-elements.html)
* [Write you own pipeline elements](https://streampipes.apache.org/docs/docs/extend-archetypes.html)

## Building StreamPipes

To properly build the StreamPipes core, the following tools should be installed:

### Prerequisites
* Java 17 JDK (We officially only support Java 17, JDKs above 17 might work as well, but we don't provide any guarantee)
* Maven (tested with 3.8)
* NodeJS + NPM (tested with v12+/ v6+)
* Docker + Docker-Compose

### Building

To build the core project, do the following:

```
    mvn clean package
```

To build the ui, switch to the ``ui`` folder and perform the following steps:

```
    npm install
    npm run build
```

### Starting

To start StreamPipes, run ``docker-compose up --build -d`` from the root directory.

You can also use the installer or CLI as described in the ``Installation`` section.

## Pipeline Elements
StreamPipes includes a repository of extensions for adapters and pipeline elements:
* **Connect adapters** for a variety of IoT data sources as well as 
* **Data Processors** and **Data Sinks** as ready-to-use pipeline elements. 

The source code of all included pipeline elements and adapters can be found [here](https://github.com/apache/streampipes/tree/dev/streampipes-extensions).

## Extending StreamPipes

You can easily add your own data streams, processors or sinks. A [Java-based SDK](https://streampipes.apache.org/docs/docs/extend-tutorial-data-processors.html) can be used to integrate your existing processing logic into StreamPipes. 
Pipeline elements are packaged as Docker images and can be installed at runtime, whenever your requirements change.

👉 Check our [developer guide](https://streampipes.apache.org/docs/docs/extend-setup.html).

## Bugs and Feature Requests

If you've found a bug or have a feature that you'd love to see in StreamPipes, feel free to create an issue on GitHub:

👉 [Bugs](https://github.com/apache/streampipes/issues)
👉 [Feature requests](https://github.com/apache/streampipes/discussions/categories/ideas)

## Get help

If you have any problems during the installation or questions around StreamPipes, you'll get help through one of our 
community channels:

👉 [Mailing Lists](https://streampipes.apache.org/mailinglists.html)

Or directly subscribe to [users-subscribe@streampipes.apache.org](mailto:users-subscribe@streampipes.apache.org)!

👉 And don't forget to follow us on [Twitter](https://twitter.com/streampipes)!

## Contribute

We welcome all kinds of contributions to StreamPipes. If you are interested in contributing, let us know! You'll
 get to know an open-minded and motivated team working together to build the next IIoT analytics toolbox.

Here are some first steps in case you want to contribute:
* Subscribe to our dev mailing list [dev-subscribe@streampipes.apache.org](mailto:dev-subscribe@streampipes.apache.org)
* Send an email, tell us about your interests and which parts of StreamPipes you'd like to contribute (e.g., core or UI)!
* Ask for a mentor who helps you to understand the code base and guides you through the first setup steps
* Find an issue on [GitHub](https://github.com/apache/streampipes/issues) which is tagged with a _good first issue_ tag
* Have a look at our developer [wiki](https://cwiki.apache.org/confluence/display/STREAMPIPES) to learn more about StreamPipes development.

Have fun!

## Feedback

We'd love to hear your feedback! Subscribe to [users@streampipes.apache.org](mailto:users@streampipes.apache.org)

## License

[Apache License 2.0](LICENSE)
",2023-07-07 18:49:49+00:00
substation,substation,brexhq/substation,"Substation is a cloud-native, event-driven data pipeline toolkit designed for security and observability teams.",https://substation.readme.io,False,189,2023-07-07 00:24:04+00:00,2022-04-15 14:23:32+00:00,9,8,7,16,v0.9.1,2023-05-09 19:39:45+00:00,MIT License,92,v0.9.1,16,2023-05-09 19:39:45+00:00,2023-07-07 00:24:05+00:00,2023-06-30 13:04:12+00:00,"# Substation

<p align=""center"">
<img alt=""substation logo"" width=""200""
src=""https://github.com/brexhq/substation/blob/main/substation_logo.png"" />
</p>

<p align=""center"">Substation is a cloud-native, event-driven data pipeline toolkit designed for security and observability teams.</p>

## Resources

* [Documentation](https://substation.readme.io/docs)
* [Announcements](https://github.com/brexhq/substation/discussions/categories/announcements)
* [Announcement Post](https://medium.com/brexeng/announcing-substation-188d049d979b)

## Features

Substation provides three unique capabilities:

* Deploy modular, serverless data pipelines in minutes
  * Design pipelines based on your unique use cases and requirements
  * Autoscale beyond 100,000 events per second with almost zero maintenance
  * Route data to SIEMs, data lakes, and other log management platforms
* Inspect, normalize, and enrich event logs in real-time
  * Inspect data before applying transformation functions and routing decisions
  * Normalize data to a common schema for easy analysis and correlation
  * Enrich data with threat, infrastructure, and business context
* Create custom data processing applications written in Go
  * Build Substation applications that run in any cloud environment or on-prem
  * Use Substation's Go packages to inspect and transform data in your own applications
  
## Getting Started

* [Download, build, and test Substation in under 1 minute](https://substation.readme.io/recipes/1-minute-quickstart)
* [Deploy Substation to your AWS account](https://substation.readme.io/recipes/deploying-aws-pipelines)
* [Learn about Substation's data transformation features in your browser](https://substation.run/)

## Substation Explained

Substation transforms event logs like this ...

```json
{
  ""ts"": 1591367999.305988,
  ""uid"": ""CMdzit1AMNsmfAIiQc"",
  ""id.orig_h"": ""192.168.4.76"",
  ""id.orig_p"": 36844,
  ""id.resp_h"": ""192.168.4.1"",
  ""id.resp_p"": 53,
  ""proto"": ""udp"",
  ""service"": ""dns"",
  ""duration"": 0.06685185432434082,
  ""orig_bytes"": 62,
  ""resp_bytes"": 141,
  ""conn_state"": ""SF"",
  ""missed_bytes"": 0,
  ""history"": ""Dd"",
  ""orig_pkts"": 2,
  ""orig_ip_bytes"": 118,
  ""resp_pkts"": 2,
  ""resp_ip_bytes"": 197
}
{
  ""ts"": 1591367999.430166,
  ""uid"": ""C5bLoe2Mvxqhawzqqd"",
  ""id.orig_h"": ""192.168.4.76"",
  ""id.orig_p"": 46378,
  ""id.resp_h"": ""31.3.245.133"",
  ""id.resp_p"": 80,
  ""proto"": ""tcp"",
  ""service"": ""http"",
  ""duration"": 0.25411510467529297,
  ""orig_bytes"": 77,
  ""resp_bytes"": 295,
  ""conn_state"": ""SF"",
  ""missed_bytes"": 0,
  ""history"": ""ShADadFf"",
  ""orig_pkts"": 6,
  ""orig_ip_bytes"": 397,
  ""resp_pkts"": 4,
  ""resp_ip_bytes"": 511
}
```

... into this ...

```json
{
  ""event"": {
    ""original"": {
      ""ts"": 1591367999.305988,
      ""uid"": ""CMdzit1AMNsmfAIiQc"",
      ""id.orig_h"": ""192.168.4.76"",
      ""id.orig_p"": 36844,
      ""id.resp_h"": ""192.168.4.1"",
      ""id.resp_p"": 53,
      ""proto"": ""udp"",
      ""service"": ""dns"",
      ""duration"": 0.06685185432434082,
      ""orig_bytes"": 62,
      ""resp_bytes"": 141,
      ""conn_state"": ""SF"",
      ""missed_bytes"": 0,
      ""history"": ""Dd"",
      ""orig_pkts"": 2,
      ""orig_ip_bytes"": 118,
      ""resp_pkts"": 2,
      ""resp_ip_bytes"": 197
    },
    ""hash"": ""7ed38f773271e700e2d55984a2ba7902be9ec8c2922e52fc7558aeade425c3de"",
    ""created"": ""2022-12-30T17:20:41.027457Z"",
    ""id"": ""CMdzit1AMNsmfAIiQc"",
    ""kind"": ""event"",
    ""category"": [
      ""network""
    ],
    ""action"": ""network-connection"",
    ""outcome"": ""success"",
    ""duration"": 66851854.32434082
  },
  ""@timestamp"": ""2020-06-05T14:39:59.305988Z"",
  ""client"": {
    ""address"": ""192.168.4.76"",
    ""ip"": ""192.168.4.76"",
    ""port"": 36844,
    ""packets"": 2,
    ""bytes"": 62
  },
  ""server"": {
    ""address"": ""192.168.4.1"",
    ""ip"": ""192.168.4.1"",
    ""port"": 53,
    ""packets"": 2,
    ""bytes"": 141
  },
  ""network"": {
    ""protocol"": ""udp"",
    ""bytes"": 203,
    ""packets"": 4,
    ""direction"": ""internal""
  }
}
{
  ""event"": {
    ""original"": {
      ""ts"": 1591367999.430166,
      ""uid"": ""C5bLoe2Mvxqhawzqqd"",
      ""id.orig_h"": ""192.168.4.76"",
      ""id.orig_p"": 46378,
      ""id.resp_h"": ""31.3.245.133"",
      ""id.resp_p"": 80,
      ""proto"": ""tcp"",
      ""service"": ""http"",
      ""duration"": 0.25411510467529297,
      ""orig_bytes"": 77,
      ""resp_bytes"": 295,
      ""conn_state"": ""SF"",
      ""missed_bytes"": 0,
      ""history"": ""ShADadFf"",
      ""orig_pkts"": 6,
      ""orig_ip_bytes"": 397,
      ""resp_pkts"": 4,
      ""resp_ip_bytes"": 511
    },
    ""hash"": ""af70ea0b38e1fb529e230d3eca6badd54cd6a080d7fcb909cac4ee0191bb788f"",
    ""created"": ""2022-12-30T17:20:41.027505Z"",
    ""id"": ""C5bLoe2Mvxqhawzqqd"",
    ""kind"": ""event"",
    ""category"": [
      ""network""
    ],
    ""action"": ""network-connection"",
    ""outcome"": ""success"",
    ""duration"": 254115104.67529297
  },
  ""@timestamp"": ""2020-06-05T14:39:59.430166Z"",
  ""client"": {
    ""address"": ""192.168.4.76"",
    ""ip"": ""192.168.4.76"",
    ""port"": 46378,
    ""packets"": 6,
    ""bytes"": 77
  },
  ""server"": {
    ""address"": ""31.3.245.133"",
    ""ip"": ""31.3.245.133"",
    ""port"": 80,
    ""packets"": 4,
    ""bytes"": 295,
    ""domain"": ""h31-3-245-133.host.redstation.co.uk"",
    ""top_level_domain"": ""co.uk"",
    ""subdomain"": ""h31-3-245-133.host"",
    ""registered_domain"": ""redstation.co.uk"",
    ""as"": {
      ""number"": 20860,
      ""organization"": {
        ""name"": ""Iomart Cloud Services Limited""
      }
    },
    ""geo"": {
      ""continent_name"": ""Europe"",
      ""country_name"": ""United Kingdom"",
      ""city_name"": ""Manchester"",
      ""location"": {
        ""latitude"": 53.5039,
        ""longitude"": -2.1959
      },
      ""accuracy"": 1000
    }
  },
  ""network"": {
    ""protocol"": ""tcp"",
    ""bytes"": 372,
    ""packets"": 10,
    ""direction"": ""outbound""
  }
}
```

... using this ...

```jsonnet
local sub = import 'substation.libsonnet';

local event = import 'event.libsonnet';
local client = import 'client.libsonnet';
local server = import 'server.libsonnet';
local network = import 'network.libsonnet';

{
  sink: sub.interfaces.sink.stdout,
  transform: {
    type: 'batch',
    settings: {
      processors:
        event.processors
        + client.processors
        + server.processors
        + network.processors
    },
  },
}
```

... running in any data pipeline like these ...

![alt text](substation_architecture.png)

## Licensing

Substation and its associated code is released under the terms of the [MIT License](LICENSE).

<!-- Keywords: go, golang, aws, kinesis, lambda, dynamodb, kafka, siem, data lake, cribl, cribl.io, tarsal, tarsal.co, datadog, datadoghq.com, confluent, confluent.io, elastic, logstash, filebeat, elastic.co, fluent, fluentd, fluentd.org, mezmo, mezmo.com -->
",2023-07-07 18:49:55+00:00
sumatra,sumatra,open-research/sumatra,,http://neuralensemble.org/sumatra/,False,125,2023-06-29 11:36:36+00:00,2015-01-28 21:23:43+00:00,50,19,19,0,,,"BSD 2-Clause ""Simplified"" License",1279,Release___0.1,15,2010-03-30 15:03:15+00:00,2023-06-14 02:03:29+00:00,2020-10-21 13:18:29+00:00,"=============
About Sumatra
=============

Sumatra is a tool for managing and tracking projects based on numerical
simulation and/or analysis, with the aim of supporting reproducible research.
It can be thought of as an automated electronic lab notebook for computational
projects.

It consists of:

* a command-line interface, smt, for launching simulations/analyses with
  automatic recording of information about the experiment, annotating these
  records, linking to data files, etc.
* a web interface with a built-in web-server, smtweb, for browsing and
  annotating simulation/analysis results.
* a Python API, on which smt and smtweb are based, that can be used in your own
  scripts in place of using smt, or could be integrated into a GUI-based
  application.

Sumatra is currently beta code, and should be used with caution and frequent
backups of your records.

For documentation, see http://packages.python.org/Sumatra/ and http://neuralensemble.org/sumatra/


Functionality:

    * launch simulations and analyses, and record various pieces of information,
      including:

        - the executable (identity, version)
        - the script (identity, version)
        - the parameters
        - the duration (execution time)
        - console output
        - links to all data (whether in files, in a database, etc.) produced by
          the simulation/analysis
        - the reason for doing the simulation/analysis
        - the outcome of the simulation/analysis

    * allow browsing/searching/visualising the results of previous experiments
    * allow the re-running of previous simulations/analyses with automatic
      verification that the results are unchanged
    * launch single or batch experiments, serial or parallel


============
Requirements
============

Sumatra requires Python versions 2.7, 3.4, 3.5 or 3.6. The web interface requires
Django (>= 1.8) and the django-tagging package.
Sumatra requires that you keep your own code in a version control
system (currently Subversion, Mercurial, Git and Bazaar are supported). If you
are already using Bazaar there is nothing else to install. If you
are using Subversion you will need to install the pysvn package. If you using
Git, you will need to install git-python bindings, and for Mercurial install hg-api.


============
Installation
============

These instructions are for Unix, Mac OS X. They may work on Windows as well, but
it hasn't been thoroughly tested.

If you have downloaded the source package, Sumatra-0.7.0.tar.gz::

    $ tar xzf Sumatra-0.7.0.tar.gz
    $ cd Sumatra-0.7.0
    # python setup.py install

The last step may have to be done as root.


Alternatively, you can install directly from the Python Package Index::

    $ pip install sumatra

or::

    $ easy_install sumatra

You will also need to install Python bindings for the version control system(s) you use, e.g.::

    $ pip install gitpython
    $ pip install mercurial hgapi


===========
Code status
===========

.. image:: https://travis-ci.org/open-research/sumatra.png?branch=master
   :target: https://travis-ci.org/open-research/sumatra
   :alt: Unit Test Status

.. image:: https://coveralls.io/repos/open-research/sumatra/badge.svg
   :target: https://coveralls.io/repos/open-research/r/sumatra
   :alt: Code coverage
",2023-07-07 18:49:59+00:00
suro,suro,Netflix/suro,Netflix's distributed Data Pipeline,,False,777,2023-07-01 12:38:48+00:00,2013-03-20 21:02:32+00:00,179,491,10,0,,,Apache License 2.0,554,v0.2.10-rc.4,38,2015-06-05 01:08:47+00:00,2023-07-01 12:38:48+00:00,2015-12-11 23:50:02+00:00,"# Suro: Netflix's Data Pipeline

Suro is a data pipeline service for collecting, aggregating, and dispatching large volume of application events including log data. It has the following features:

- It is distributed and can be horizontally scaled.
- It supports streaming data flow, large number of connections, and high throughput.
- It allows dynamically dispatching events to different locations with flexible dispatching rules.
- It has a simple and flexible architecture to allow users to add additional data destinations.
- It fits well into NetflixOSS ecosystem
- It is a best-effort data pipeline with support of flexible retries and store-and-forward to minimize message loss

Learn more about Suro on the <a href=""https://github.com/Netflix/suro/wiki"">Suro Wiki</a> and the <a href=""http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html"">Netflix TechBlog post</a> where Suro was introduced.

## Master Build Status

<a href='https://netflixoss.ci.cloudbees.com/job/suro-master/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-master/badge/icon'></a>

## Pull Request Build Status

<a href='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/badge/icon'></a></img></a></img></a>

Build
-----

NetflixGraph is built via Gradle (www.gradle.org). To build from the command line:

    ./gradlew build

See the `build.gradle` file for other gradle targets, like `distTar`, `distZip`, `installApp`, and `runServer`.

Running the server
------------------

You can run the server locally by just running `./gradlew runServer`.

More more advanced usage you may wish to run `./gradlew installApp` and then:

	cd suro-server
	java -cp ""build/install/suro-server/lib/*"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json

To enable basic logging you can downloaded `slf4j-simple-1.7.7.jar` and copy it into `suro-server` then run:

	cd suro-server
	java -cp ""build/install/suro-server/lib/*:slf4j-simple-1.7.7.jar"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json

Support
-----

We will use the Google Group, Suro Users, to discuss issues: https://groups.google.com/forum/#!forum/suro-users
",2023-07-07 18:50:03+00:00
sushi,sushi,uzh/sushi,SUSHI: Supporting User for SHell script Integration,,False,23,2022-12-14 22:33:24+00:00,2015-09-17 06:15:38+00:00,8,20,14,15,v5.0.2,2023-05-12 14:22:09+00:00,,5894,v5.0.2,23,2023-05-12 12:14:30+00:00,2023-07-04 11:29:56+00:00,2023-07-04 11:29:23+00:00,"== 1. Getting started

*Process your raw data without cooking it*

*SUSHI* can Support Users for SHell script Integration, defined as a recursive acronym as usual, but someone might say after using it, SUSHI is a Super Ultra Special Hyper Incredible system!! SUSHI is an agile and extensible data analysis framework that brings innovative concepts to Next Generation Sequencing bioinformatics. SUSHI lets bioinformaticians wrap open source tools like e.g. bowtie, STAR or GATK into apps and provides natively a commandline and a web interface to run these apps. Users can script their analysis as well as do an analysis-by-clicking in their browser. When running an SUSHI application SUSHI takes care of all aspects of documentation: Input data, parameters, software tools and versions are stored persistently. SUSHI accepts meta-information on samples and processing and lets the user define the meta-information she/he needs in a tabular format. Finally, all results, the associated logs, all parameters and all meta-information is stored in a single, self-contained directory that can be shared with collaborators. Altogether, SUSHI as a framework truly supports collaborative, reproducible data analysis and research. At the Functional Genomics Center, SUSHI is tightly integrated with B-Fabric(http://fgcz-bfabric.uzh.ch) and shares the authentication, sample information and the storage. The production version of SUSHI is currently being further integrated in the overall B-Fabric framework.

Getting started at the demo SUSHI server: http://fgcz-sushi-demo.uzh.ch, just go and play around with it, and feel what SUSHI tastes like. Go to *Todays Menu* and select your project number and then *DataSet* to start analyzing data or viewing analysis results. You can find the basic usage here: http://fgcz-sushi.uzh.ch/usage.html

== 2. Installation

First of all, let me introduce quickly main SUSHI components:

1. SUSHI server (Web-based front-end, Ruby on Rails)
2. SUSHI application (sushi_fabric.gem)
3. Workflow manager (workflow_manager.gem)

SUSHI server is the user interface and it send some requests to SUSHI application, and the SUSHI application calls an job submission command of the Worlflow manager that manages and observes each submitted job. All the components will be installed at once by using bundle command (see the next section). Note that the workflow manager is working as an independent process from SUSHI server (Ruby on Rails) process. Both processes do not have to run on a same computer node (they communicate each other using dRuby).

=== 2.1. Requirements

* Ruby (>= v2.6.x)
* Git (>= v2.10.2)
* bundler (RubyGem, >= v1.10.6)
* (Ruby on Rails (>= v5) this wll be automatically installed through bundler)

Just confirm first to check if the required applications are installed in your system as follows:

  $ ruby -v
  $ git --version
  $ bundle -v

If you get 'Command not found'-like message, you have to install the applications before going on to the next step.

Note
* Ruby on Rails will be installed in the following steps by bundler
* Please refer to the following sites for the other application installation
* Ruby https://www.ruby-lang.org/en
* Git https://git-scm.com
* To instal 'bundler', just type the following command after you install Ruby:

  $ gem install bundler

=== 2.2. Download and configration

  $ git clone https://github.com/uzh/sushi.git
  $ cd sushi/master
  $ bundle install --path vendor/bundle
  $ bundle exec rake secret

If you get an error during gem library installation, please try 1) delete Gemfile.lock and 2) bundle install again. This may adjust the library version to your environment.

'rake secret' command gives you the secret key and paste it in /config/initilizer/devise.rb as follows:

    config.secret_key = 'xxxxx'

# xxxxx is your secret key. Next, 

  $ bundle exec rake db:migrate RAILS_ENV=production

This command initializes the database appropriately.

== 3. Start SUSHI server

=== 3.1. Start workflow_manager

  $ mkdir workflow_manager
  $ cd workflow_manager/
  $ bundle exec workflow_manager
  mode = development
  ruby = xxx
  GEM_PATH = xxx
  DB = PStore
  Cluster = LocalComputer
  druby://localhost:12345

Note
* The final line shows the dRuby address of the workflow manager
* If KyotoCabinet (gem) is installed in your system, KyotoCabinet is automatically selected as a DB system rather than PSotre
* KyotoCabinet: http://fallabs.com/kyotocabinet/pkg

=== 3.2. Start SUSHI server

  $ cd master/
  $ bundle exec rails server -e production

Note
* SUSHI (Ruby on Rails) server should run in a different folder from the workflow manager folder
* The rails server command starts a WEBRick webserver daemon, which is implemented in Ruby
* SUSHI server uses SQLite3 DBMS as a default
* You can replace the DBMS to MySQL and the webserver application to Apache (+passenger module) if you need, then SUSHI will be much faster and more stable
* Please search google and refer to a lot of Rails documents on the web for it

== 4. Set your first NGS Data and DataSet file

You need to import a DataSet file (.tsv) to SUSHI database.

A SUSHI application takes a DataSet, which is meta-information set of actual data, as an input, and a SUSHI application produces another DataSet as a result. The DataSet is identified as a .tsv (Tab-Separated-Value) text file, and it includes all input/output actual data file location (path). All DataSet file (.tsv) should be located in your project directory, the default project path becomes $RAILS_ROOT/public/gstore/projects/p1001, which directory will be automatically generated at the first running of the SUSHI server. 

Sample DataSet
* http://localhost:3000/ventricles_100k.tgz

You can download it from your SUSHI server, or you can copy it from $RAIL_ROOT/public/ventricles_100k.tgz. After you decompress the archive file, move the folder in the project directory, $RAILS_ROOT/public/gstore/projects/p1001/

# Currently, SUSHI does not have a function to upload the data itself. It is the SUSHI framework concept, SUSHI does not manage data itself, but SUSHI does manage only the meta-information, called DataSet.

then you can find it from the SUSHI server by clicking 'gStore' of the main menu, and you can find 'ventricles_100k' directory. Double click the directory and you will find 'dataset.tsv' file in the ventricles_100k directory. By clickking the ""+""(plus) button next to 'dataset.tsv' file under the ventricles_100k directory imports it as a SUSHI DataSet to SUSHI database, and a new DataSet, ventricles_100k, will show up in 'DataSet' view (automatically jump to 'DataSet' view and also you can select 'DataSet' menu from the top menu).
 
== 5. How to run a SUSHI application

You can run a SUSHI application after selecting a DataSet:

1. Select a DataSet
2. Click a SUSHI application button
3. Set parameters
4. Click 'submit' button

Note
* Initially, only 'WordCountApp' is available, though other SUSHI application buttons appear in DataSet view. 
* After installing ezRun package and installing corresponding applications in your system, the other SUSHI applications will work. See the following steps.

== 6. Install ezRun package

Please refer to https://github.com/uzh/ezRun in detail.

Simply to say, just run the following command in R environement:

  > source(""http://bioconductor.org/biocLite.R"")
  > biocLite(""devtools"")
  > biocLite(""uzh/ezRun"")

And set the following constants in lib/global_variables.rb appropriately:

  EZ_GLOBAL_VARIABLES = '/usr/local/ngseq/opt/EZ_GLOBAL_VARIABLES.txt'
  R_COMMAND = '/usr/local/ngseq/stow/R-3.2.0/bin/R'

You can find the EZ_GLOBAL_VARIABLES.txt location in your system by loading ezRun package 

  > library(ezRun)

in the R environment console, and R path for

  $ which R

in shell command terminal.

== 7. Install third party applications and configure EZ_GLOBAL_VARIABLES.txt

ezRun package bundles a sort of wrapper applications to call a thrid party application software. For example, FastQCApp (SUSHI application) calles EzAppFastqc method in ezRun package, and subsequently it calls 'fastqc' command defined in EZ_GLOBAL_VARIABLES.txt. In other words, FastQCApp does not work without the installation of FastQC application software. All the application paths are defined in EZ_GLOBAL_VARIABLES.txt explicitly. Please check the application paths defined in EZ_GLOBAL_VARIABLES.txt one by one and set the correct path installed in your environement. If an appropriate application software is not found, the corresponding SUSHI application will stop with an error, though the SUSHI application button in DataSet view appears. SUSHI does not check if the third party application software is installed in your computer.

== 8. Install iGenome reference

In the case of using alignment tools, such as STARApp and BowtieApp, you need reference genome sequence(s). We use iGenome format for the reference data. If you can find your target species in the site of Illumina/iGenome, you can download it in your system as a reference of SUSHI application. The current all SUSHI applciations refer to the following environment variable 

* GENOME_REF_DIR = '/srv/GT/reference'

in lib/global_variables.rb and

* GENOMES_ROOT = '/srv/GT/reference'

in EZ_GLOBAL_VARIABLES.txt. Please set the GENOME_REF_DIR and GENOMES_ROOT path accordingly to the local iGenome folders in your system.

Reference
* iGenome: https://support.illumina.com/sequencing/sequencing_software/igenome.html

== 9. Final small configuration

Please make a symbolic link to gstore directory in public directory, namely just run the following command in public/ directory:

  $ ln -s gstore/projects

and set the following environment variable 

* PROJECT_BASE_URL=""http://localhost:3000/projects"" 

in EZ_GLOBAL_VARIABLES.txt

== 10. Advanced SUSHI configuration

=== 10.1. SUSHI configuration

Usually Ruby on Railes uses either configuration file depending on the Rails mode (development or production):

* config/environments/development.rb
* config/environments/production.rb

in these files, the following four properties are used in SUSHI:

1. config.workflow_manager = 'druby://localhost:12345'
2. config.gstore_dir = '/sushi/public/gstore/projects'
3. config.sushi_app_dir = '/sushi'
4. config.scratch_dir = '/tmp/scratch'

config.workflow_manager is the hostname and port number of which the workflow manager is working. config.gstore_dir is the path to the actual project folder where input and result data is stored. config.sushi_app_dir is the location of SUSHI server (Ruby on Rails) installation. config.scratch_dir is used by actuall shell script as a temporary directory. After a job finished, all the data is copied to config.gstore_dir from config.scratch_dir. config.scratch_dir should be enough disk space for every job calculation, otherwise a job will stop due to no disk space.

=== 10.2. How to make/add a new SUSHI application

All SUSHI applications (classes) inherit SushiApp class that is defined in sushi_fabric.gem. At the moment, we must make a SUSHI application class file manually and import it in *lib* directory. You can refer to many existing application files in lib directory. lib/otherApps/WordCountApp.rb is so smallest application that you can refer to for the first time when you create a new SUSHI Application.

Template of SUSHI application

  #!/usr/bin/env ruby
  # encoding: utf-8

  require 'sushiApp'

  class YourSushiApplication < SushiApp
    def initialize
      super
      @name = 'Application_Name'      # No Space
      @analysis_category = 'Category' # No Space
      @required_columns = ['Name', 'Read1']
      @required_params = []
    end
    def next_dataset
      {'Name'=>@dataset['Name'],
       'Stats [File]'=>File.join(@result_dir, @dataset['Name'].to_s + '.stats')
      }
      # [File] tag value (as a file) will be exported in gStore directory
    end
    def preprocess
    end
    def commands
      'echo hoge'
    end
  end

Note
* The important methods required to define are *initialize*, *next_dataset*, and *commands* 
* The *preprocess* method is not required but it will be executed before calling *commands* method
* The *next_dataset* method must return Hash type data, the key will be column name and the value will be corresponding value to the column name
* The *commands* method must return String type data that will become actual script commands
* \@name and \@analysis_category instance variable is required and no space must be inserted

After you make your new SUSHI application code, put it in lib/ directory and restart the SUSHI server. The SUSHI application button will appear in DataSet view if the DataSet has the required columns defined in your SUSHI applicaiton.

=== 10.3. Workflow Manager configuration

As well as SUSHI configuration, there are two configuration files depending on running mode (development/production):

* config/environments/development.rb
* config/environments/production.rb

These files are generated automatically with default setting at the first run of Workflow Manager.

  WorkflowManager::Server.configure do |config|
    config.log_dir = '/home/workflow_manager/logs'
    config.db_dir = '/home/workflow_manager/dbs'
    config.interval = 30
    config.resubmit = 0
    config.cluster = WorkflowManager::LocalComputer.new('local_computer')
  end

config.log_dir is the log directory path, and config.db_dir is the database directory path. config.interval means the interval time (seconds) to check a running job whether it is running, finished, or failed. When config.resubmit > 0, a failed job is resubmitted again. config.cluster sets a cluster class which has several method definitions depending on the cluster on which a job will actually run. For more details to set/customize/add a cluster, refer to the next section.

=== 10.4. How to add a new cluster

For example, WorkflowManager::LocalComputer class, inheriting WorkflowManager::Cluster class, defines (overwrites) the following methods:

1. submit_job
2. job_running?
3. job_ends?
4. copy_commands
5. kill_command
6. delete_command
7. cluster_nodes

When you add or customize your cluster system (which has calculation node(s)), you need to add a new cluser class and define (overwrite) these methods. Even though you have only one laptop computer, it is regonized as a 'cluster'. These methods are called from SUSHI server via WorkflowManager instance and SUSHI manages the job submission. The Workflow Manager instance should run in the cluster system, but it is not required if the submission and job managing commands are available from the node where the Workflow Manager instance is running on.

Actually, although the WorkflowManager::LocalComputer class file is located in 

  {workflow_manager gem installed directory}/lib/workflow_manager/cluster.rb

, you can make a new class definition in the Workflow Manager configuration file (the following Ruby source code file):

* config/environments/development.rb
* config/environments/production.rb

An example to add a new cluster node is shown below:

  #!/usr/bin/env ruby
  # encoding: utf-8

  class WorkflowManager::FGCZCluster
    alias :cluster_nodes_orig :cluster_nodes
    def cluster_nodes
      nodes = cluster_nodes_orig
      nodes.merge!({
        'fgcz-h-007: cpu 16,mem 61GB,scr 350GB' => 'fgcz-h-007',
        'fgcz-h-008: cpu 16,mem 61GB,scr 350GB' => 'fgcz-h-008',
      })
      Hash[*nodes.sort.flatten]
    end
  end

  WorkflowManager::Server.configure do |config|
    config.log_dir = '/srv/GT/analysis/masaomi/workflow_manager/run_workflow_manager/logs'
    config.db_dir = '/srv/GT/analysis/masaomi/workflow_manager/run_workflow_manager/dbs'
    config.interval = 30
    config.resubmit = 0
    config.cluster = WorkflowManager::FGCZCluster.new('FGCZ Cluster')
  end

The other methods can be updated as well accordingly to your environment. Please refer to the WorkflowManager::LocalComputer class source code for more details.

=== 10.5. Local user authentification

As default, there is no user authentification. Please apply the patch, local_devise.20160415.patch, before executing DB migration (rake db:migrate) as follows:

  $ patch -p0 < master/local_devise.patch

== Release information

Ver. 5.0.0, 2023.02.19
* Rails v6.1.7.2, Ruby v3.1.3, ezRun v3.16.1 (Bioconductor v3.16), sushi_fabric v1.1.8, workflow_manager v0.8.0

Ver. 2.1.2, 2021.03.11
* Rails v5.2.4.3, Ruby v2.6.6, ezRun v3.12.1 (Bioconductor v3.12.1, R v4.0.3), sushi_fabric v1.0.8, workflow_manager v0.6.0
* all source files are concatenated in g-req copy case of job footer, sushi_fabric.gem ver.1.0.8
* remove non-existing igv files in MpileupApp
* extend EdgeR memory options
* add SCMultipleSamplesApp in replacement of the other merging samples apps
* Fixed SBATCH job dependency option, sushi_fabric.gem ver.1.0.7
* Fixed EAGLERCApp

Ver. 2.1.0, 2020.11.04
* Rails v5.2.4.3, Ruby v2.6.6, ezRun v3.12.1, sushi_fabric v1.0.0, workflow_manager v0.5.9

Ver. 2.0.2
* fastq bug fix
* Workflow Manager v0.5.6, added a new Cluster class for Debian10 (with slurm)

Ver. 2.0.0
* Rails v5 + Ruby v2.6 + ezRun v3.11.1 (BioConductor v3.11.1)

Ver. 2.2.0
* Rails v5 + Ruby v2.6 + ezRun v3.13 (BioConductor v3.13)

",2023-07-07 18:50:07+00:00
swift,swift-k,swift-lang/swift-k,,http://swift-lang.org,False,79,2022-04-15 12:10:17+00:00,2014-07-12 02:30:02+00:00,23,23,14,0,,,Apache License 2.0,6751,swift-0.95-RC6-swift,17,2014-06-02 21:11:10+00:00,2023-04-11 23:16:50+00:00,2018-06-28 17:48:11+00:00,"Swift Parallel Scripting Language
=================================
See http://swift-lang.org for documentation, downloads, and general
information about the Swift parallel scripting language.

Official releases are available at http://swift-lang.org/downloads.

This is the source code for the Swift/K implementation of the language.
Swift/K focuses on robust distributed execution of tasks, particularly
command-line programs operating on files, on varied compute resources
including clouds and clusters.  The sibling Swift/T implementation
(https://github.com/swift-lang/swift-t) focuses on high-performance
computation on clusters and supercomputers.

Building
========
Run `ant redist` to build Swift.  The compiled Swift distribution will
be created under `dist/swift-<version>` (dist/swift-svn for the
development version).  Copy this directory to your preferred
location to install it. If you are unfamiliar with Swift, we suggest
you start with the Swift tutorials to get yourself familiarized:
http://swift-lang.org/docs/

Repository Layout
=================
At the root of the repository are a number of subdirectories that contain
the source code and supporting files for the Swift language.  The main
directories are:

* **bin**: Swift executables
* **build**: working directory for temporary build files
* **cogkit**: modules based on the Java CoG kit
* **dist**: destination for final compiled distribution of Swift
* **docs**: Swift documentation
* **etc**: miscellaneous supporting files
* **lib**: libraries required for Swift, particularly Java jars.
* **libexec**: other libraries required for Swift, particularly Karajan
      libraries and shell scripts
* **resources**: miscellaneous resources used in building Swift.
* **src**: Java source code for Swift compiler and runtime.
* **tests**: test suite for Swift

Swift depends on a number of additional modules that reside in their own
directories under `cogkit/modules`, for example `cogkit/modules/karajan` or
`cogkit/modules/provider-coaster`.

Some of the more interesting modules are:

* **karajan**: the Karajan language, used as a compilation target for Swift.
* **provider-coaster**: the Coaster service and Java client for lightweight
    execution of jobs on remote resources
* **provider-coaster-c-client**: a C/C++ client for the Coaster service
* **provider-...**: other providers that support remote job execution on
    various computing resources

Java CoG Kit
============
This distribution of Swift incorporates code originally developed by the
CoG (Commodity Grid) project.  For information about the Java CoG Kit
see README.txt, CHANGES.txt and LICENSE.txt in `cogkit`, and
http://www.cogkit.org/.  Thanks to Gregor von Laszweski and the rest
of the CoG developers.
",2023-07-07 18:50:11+00:00
apachesystemml,systemds,apache/systemds,An open source ML system for the end-to-end data science lifecycle,https://systemds.apache.org/,False,975,2023-07-06 15:47:21+00:00,2015-11-10 08:00:06+00:00,443,94,144,8,3.1.0-rc1,2023-03-02 20:14:48+00:00,Apache License 2.0,8247,v2.0.0-rc3,40,2020-10-05 14:39:33+00:00,2023-07-07 13:50:07+00:00,2023-07-06 08:58:48+00:00,"<!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the ""License""); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% end comment %}
-->

# Apache SystemDS

**Overview:** SystemDS is an open source ML system for the end-to-end data science lifecycle from data integration, cleaning,
and feature engineering, over efficient, local and distributed ML model training, to deployment and serving. To this
end, we aim to provide a stack of declarative languages with R-like syntax for (1) the different tasks of the data-science
lifecycle, and (2) users with different expertise. These high-level scripts are compiled into hybrid execution plans of
local, in-memory CPU and GPU operations, as well as distributed operations on Apache Spark. In contrast to existing
systems - that either provide homogeneous tensors or 2D Datasets - and in order to serve the entire data science lifecycle,
the underlying data model are DataTensors, i.e., tensors (multi-dimensional arrays) whose first dimension may have a
heterogeneous and nested schema.

**Quick Start** [Install, Quick Start and Hello World](https://apache.github.io/systemds/site/install.html)

**Documentation:** [SystemDS Documentation](https://apache.github.io/systemds/)

**Python Documentation** [Python SystemDS Documentation](https://apache.github.io/systemds/api/python/index.html)

**Issue Tracker** [Jira Dashboard](https://issues.apache.org/jira/secure/Dashboard.jspa?selectPageId=12335852)

**Status and Build:** SystemDS is renamed from SystemML which is an **Apache Top Level Project**.
To build from source visit [SystemDS Install from source](https://apache.github.io/systemds/site/install.html)
  
[![Build](https://github.com/apache/systemds/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/apache/systemds/actions/workflows/build.yml)
[![Documentation](https://github.com/apache/systemds/actions/workflows/documentation.yml/badge.svg?branch=main)](https://github.com/apache/systemds/actions/workflows/documentation.yml)
[![LicenseCheck](https://github.com/apache/systemds/actions/workflows/license.yml/badge.svg?branch=main)](https://github.com/apache/systemds/actions/workflows/license.yml)
[![Java Tests](https://github.com/apache/systemds/actions/workflows/javaTests.yml/badge.svg?branch=main)](https://github.com/apache/systemds/actions/workflows/javaTests.yml)
[![Python Test](https://github.com/apache/systemds/actions/workflows/python.yml/badge.svg?branch=main)](https://github.com/apache/systemds/actions/workflows/python.yml)
",2023-07-07 18:50:15+00:00
targets,targets,ropensci/targets,Function-oriented Make-like declarative workflows for R,https://docs.ropensci.org/targets/,False,766,2023-07-06 22:52:32+00:00,2019-08-01 17:33:25+00:00,63,17,19,37,1.2.0,2023-06-26 15:11:41+00:00,Other,2682,1.2.0,37,2023-06-26 15:11:41+00:00,2023-07-06 22:52:36+00:00,2023-06-26 16:10:34+00:00,"
# targets <img src='man/figures/logo.png' align=""right"" height=""139""/>

[![ropensci](https://badges.ropensci.org/401_status.svg)](https://github.com/ropensci/software-review/issues/401)
[![JOSS](https://joss.theoj.org/papers/10.21105/joss.02959/status.svg)](https://doi.org/10.21105/joss.02959)
[![zenodo](https://zenodo.org/badge/200093430.svg)](https://zenodo.org/badge/latestdoi/200093430)
[![R
Targetopia](https://img.shields.io/badge/R_Targetopia-member-blue?style=flat&labelColor=gray)](https://wlandau.github.io/targetopia/)
[![CRAN](https://www.r-pkg.org/badges/version/targets)](https://CRAN.R-project.org/package=targets)
[![status](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![check](https://github.com/ropensci/targets/workflows/check/badge.svg)](https://github.com/ropensci/targets/actions?query=workflow%3Acheck)
[![codecov](https://codecov.io/gh/ropensci/targets/branch/main/graph/badge.svg?token=3T5DlLwUVl)](https://app.codecov.io/gh/ropensci/targets)
[![lint](https://github.com/ropensci/targets/workflows/lint/badge.svg)](https://github.com/ropensci/targets/actions?query=workflow%3Alint)

Pipeline tools coordinate the pieces of computationally demanding
analysis projects. The `targets` package is a
[Make](https://www.gnu.org/software/make/)-like pipeline tool for
statistics and data science in R. The package skips costly runtime for
tasks that are already up to date, orchestrates the necessary
computation with implicit parallel computing, and abstracts files as R
objects. If all the current output matches the current upstream code and
data, then the whole pipeline is up to date, and the results are more
trustworthy than otherwise.

## Philosophy

A pipeline is a computational workflow that does statistics, analytics,
or data science. Examples include forecasting customer behavior,
simulating a clinical trial, and detecting differential expression from
genomics data. A pipeline contains tasks to prepare datasets, run
models, and summarize results for a business deliverable or research
paper. The methods behind these tasks are user-defined R functions that
live in R scripts, ideally in a folder called `""R/""` in the project. The
tasks themselves are called “targets”, and they run the functions and
return R objects. The `targets` package orchestrates the targets and
stores the output objects to make your pipeline efficient, painless, and
reproducible.

## Prerequisites

1.  Familiarity with the [R programming
    language](https://www.r-project.org/), covered in [R for Data
    Science](https://r4ds.had.co.nz/).
2.  [Data science workflow management
    techniques](https://rstats.wtf/index.html).
3.  [How to write functions](https://r4ds.had.co.nz/functions.html) to
    prepare data, analyze data, and summarize results in a data analysis
    project.

## Installation

If you are using `targets` [with `crew` for distributed
computing](https://books.ropensci.org/targets/crew.html), it is
recommended to use `nanonext` version `0.8.3.9010` or higher, `mirai`
version `0.8.7.9013` or higher, and `crew` version `0.2.0` or higher. If
the latest CRAN releases are older, then you can install the development
versions from R-universe.

``` r
install.packages(""nanonext"", repos = ""https://shikokuchuo.r-universe.dev"")
install.packages(""mirai"", repos = ""https://shikokuchuo.r-universe.dev"")
install.packages(""crew"", repos = ""https://wlandau.r-universe.dev"")
```

There are multiple ways to install the `targets` package itself, and
both the latest release and the development version are available.

| Type        | Source   | Command                                                           |
|-------------|----------|-------------------------------------------------------------------|
| Release     | CRAN     | `install.packages(""targets"")`                                     |
| Development | GitHub   | `remotes::install_github(""ropensci/targets"")`                     |
| Development | rOpenSci | `install.packages(""targets"", repos = ""https://dev.ropensci.org"")` |

## Get started in 4 minutes

The 4-minute video at <https://vimeo.com/700982360> demonstrates the
example pipeline used in the
[walkthrough](https://books.ropensci.org/targets/walkthrough.html) and
[functions](https://books.ropensci.org/targets/functions.html) chapters
of the [user manual](https://books.ropensci.org/targets/). Visit
<https://github.com/wlandau/targets-four-minutes> for the code and
<https://rstudio.cloud/project/3946303> to try out the code in a browser
(no download or installation required).

[![](./man/figures/video.png)](https://vimeo.com/700982360)

## Usage

To create a pipeline of your own:

1.  [Write R
    functions](https://books.ropensci.org/targets/functions.html) for a
    pipeline and save them to R scripts (ideally in the `""R/""` folder of
    your project).
2.  Call
    [`use_targets()`](https://docs.ropensci.org/targets/reference/use_targets.html)
    to write key files, including the vital `_targets.R` file which
    configures and defines the pipeline.
3.  Follow the comments in `_targets.R` to fill in the details of your
    specific pipeline.
4.  Check the pipeline with
    [`tar_visnetwork()`](https://docs.ropensci.org/targets/reference/tar_visnetwork.html),
    run it with
    [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html),
    and read output with
    [`tar_read()`](https://docs.ropensci.org/targets/reference/tar_read.html).
    [More
    functions](https://docs.ropensci.org/targets/reference/index.html)
    are available.

## Documentation

- [User manual](https://books.ropensci.org/targets/): in-depth
  discussion about how to use `targets`. The most important chapters are
  the
  [walkthrough](https://books.ropensci.org/targets/walkthrough.html),
  [help guide](https://books.ropensci.org/targets/help.html), and
  [debugging guide](https://books.ropensci.org/targets/debugging.html).
- [Reference website](https://docs.ropensci.org/targets/): formal
  documentation of all user-side functions, the statement of need, and
  multiple design documents of the internal architecture.
- [Developer documentation](https://books.ropensci.org/targets-design/):
  software design documents for developers contributing to the deep
  internal architecture of `targets`.

## Help

Please read the [help
guide](https://books.ropensci.org/targets/help.html) to learn how best
to ask for help using `targets`.

## Courses

- [Official half-day interactive
  tutorial](https://github.com/wlandau/targets-tutorial).

## Recorded talks

### English

- [Get started with `targets` in 4 minutes
  (4:08)](https://vimeo.com/700982360)
- [{targets} in Action](https://ropensci.org/commcalls/jan2023-targets/)
- [R/Medicine 2021 (15.33)](https://youtu.be/HJI5mQJRGpY)
- [R/Pharma 2020
  (9:24)](https://www.youtube.com/watch?v=GRqKJBaC5g4&list=PLMtxz1fUYA5C0YflXsR8EEAQXfjntlV1H&index=6)
- [LA R Users Meetup, October 2020
  (1:14:40)](https://www.youtube.com/watch?v=Qq25BUxpJu4)
- [New York Open Statistical Programming Meetup, December 2020
  (1:54:28)](https://youtu.be/Gqn7Xn4d5NI)
- [ds-incubator series,
  2021](https://www.youtube.com/playlist?list=PLvgdJdJDL-APJqHy5CXs6m4N7hUVp5rb4)
- [Lille R User Group, June 2021 (45:54)](https://youtu.be/FODSavXGjYg)

### Español

- [R-Ladies Barcelona, 2021-05-25
  (1:25:12)](https://www.youtube.com/watch?v=Vj312AfdpBo).

### 日本語

- [Bio”Pack”athon, 2022-03-31
  (1:04:10)](https://togotv.dbcls.jp/20220331.html)

## Example projects

- [Four-minute example](https://github.com/wlandau/targets-four-minutes)
- [Minimal example](https://github.com/wlandau/targets-minimal)
- [Machine learning with
  Keras](https://github.com/wlandau/targets-keras)
- [Validate a minimal Stan
  model](https://github.com/wlandau/targets-stan)
- [Using Target Markdown and `stantargets` to validate a Bayesian
  longitudinal model for clinical trial data
  analysis](https://github.com/wlandau/rmedicine2021-pipeline)
- [Shiny app that runs a
  pipeline](https://github.com/wlandau/targets-shiny)
- [Deploy a pipeline to RStudio
  Connect](https://github.com/sol-eng/targets-deployment-rsc)

## Apps

- [`tar_watch()`](https://docs.ropensci.org/targets/reference/tar_watch.html):
  a built-in Shiny app to visualize progress while a pipeline is
  running. Available as a Shiny module via
  [`tar_watch_ui()`](https://docs.ropensci.org/targets/reference/tar_watch_ui.html)
  and
  [`tar_watch_server()`](https://docs.ropensci.org/targets/reference/tar_watch_server.html).
- [`targetsketch`](https://wlandau.shinyapps.io/targetsketch): a Shiny
  app to help sketch pipelines
  ([app](https://wlandau.shinyapps.io/targetsketch),
  [source](https://github.com/wlandau/targetsketch)).

## Deployment

- <https://solutions.rstudio.com/r/workflows/> explains how to deploy a
  pipeline to RStudio Connect ([example
  code](https://github.com/sol-eng/targets-deployment-rsc)).
- [`tar_github_actions()`](https://docs.ropensci.org/targets/reference/tar_github_actions.html)
  sets up a pipeline to run on GitHub Actions. The [minimal
  example](https://github.com/wlandau/targets-minimal) demonstrates this
  approach.

## Extending and customizing targets

- [R Targetopia](https://wlandau.github.io/targetopia/): a collection of
  [R packages](https://wlandau.github.io/targetopia/packages.html) that
  extend `targets`. [These
  packages](https://wlandau.github.io/targetopia/packages.html) simplify
  pipeline construction for specific fields of Statistics and data
  science.
- [Target
  factories](https://wlandau.github.io/targetopia/contributing.html#target-factories):
  a programming technique to write specialized interfaces for custom
  pipelines. Posts [here](https://ropensci.org/blog/2021/02/03/targets/)
  and [here](https://wlandau.github.io/targetopia/contributing.html)
  describe how.

## Code of conduct

Please note that this package is released with a [Contributor Code of
Conduct](https://ropensci.org/code-of-conduct/).

## Citation

``` r
citation(""targets"")
To cite targets in publications use:

  Landau, W. M., (2021). The targets R package: a dynamic Make-like
  function-oriented pipeline toolkit for reproducibility and
  high-performance computing. Journal of Open Source Software, 6(57),
  2959, https://doi.org/10.21105/joss.02959

A BibTeX entry for LaTeX users is

  @Article{,
    title = {The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing},
    author = {William Michael Landau},
    journal = {Journal of Open Source Software},
    year = {2021},
    volume = {6},
    number = {57},
    pages = {2959},
    url = {https://doi.org/10.21105/joss.02959},
  }
```
",2023-07-07 18:50:21+00:00
task,task,go-task/task,A task runner / simpler Make alternative written in Go,https://taskfile.dev,False,7959,2023-07-07 14:49:24+00:00,2017-02-27 00:46:04+00:00,467,71,135,73,v3.27.1,2023-06-30 12:36:44+00:00,MIT License,1636,v3.27.1,76,2023-06-30 12:36:33+00:00,2023-07-07 15:08:33+00:00,2023-07-06 20:31:26+00:00,"<div align=""center"">
  <a href=""https://taskfile.dev"">
    <img src=""docs/static/img/logo.svg"" width=""200px"" height=""200px"" />
  </a>

  <h1>Task</h1>

  <p>
    Task is a task runner / build tool that aims to be simpler and easier to use than, for example, <a href=""https://www.gnu.org/software/make/"">GNU Make<a>.
  </p>

  <p>
    <a href=""https://taskfile.dev/installation/"">Installation</a> | <a href=""https://taskfile.dev/usage/"">Documentation</a> | <a href=""https://twitter.com/taskfiledev"">Twitter</a> | <a href=""https://fosstodon.org/@task"">Mastodon</a> | <a href=""https://discord.gg/6TY36E39UK"">Discord</a>
  </p>
</div>

## Gold Sponsors

<div align=""center"">

| [Appwrite][appwrite]                                   |
| ------------------------------------------------------ |
| [![Appwrite](/docs/static/img/appwrite.svg)][appwrite] |

</div>

<!-- prettier-ignore-start -->
[appwrite]: https://appwrite.io/?utm_source=task_github&utm_medium=social&utm_campaign=task_oss_fund
<!-- prettier-ignore-end -->
",2023-07-07 18:50:27+00:00
taskgraph,taskgraph,natcap/taskgraph,,,False,18,2023-07-07 05:51:47+00:00,2019-10-25 18:52:58+00:00,6,5,6,7,0.11.0,2021-10-12 16:35:27+00:00,Other,823,0.11.0,34,2021-10-12 16:35:27+00:00,2023-07-07 05:51:48+00:00,2023-04-19 19:02:51+00:00,"===============
About TaskGraph
===============

``TaskGraph`` is a library that was developed to help manage complicated
computational software pipelines consisting of long running individual tasks.
Many of these tasks could be executed in parallel, almost all of them wrote
results to disk, and many times results could be reused from part of the
pipeline. TaskGraph manages all of this for you. With it you can schedule
tasks with dependencies, avoid recomputing results that have already been
computed, and allot multiple CPU cores to execute tasks in parallel if
desired.

TaskGraph Dependencies
----------------------

Task Graph is written in pure Python, but if the ``psutils`` package is
installed the distributed multiprocessing processes will be ``nice``\d.

Example Use
-----------

Install ``TaskGraph`` with

``pip install taskgraph``

Then

.. code-block:: python

  import os
  import pickle
  import logging

  import taskgraph

  logging.basicConfig(level=logging.DEBUG)

  def _create_list_on_disk(value, length, target_path):
      """"""Create a numpy array on disk filled with value of `size`.""""""
      target_list = [value] * length
      pickle.dump(target_list, open(target_path, 'wb'))


  def _sum_lists_from_disk(list_a_path, list_b_path, target_path):
      """"""Read two lists, add them and save result.""""""
      list_a = pickle.load(open(list_a_path, 'rb'))
      list_b = pickle.load(open(list_b_path, 'rb'))
      target_list = []
      for a, b in zip(list_a, list_b):
          target_list.append(a+b)
      pickle.dump(target_list, open(target_path, 'wb'))

  # create a taskgraph that uses 4 multiprocessing subprocesses when possible
  if __name__ == '__main__':
      workspace_dir = 'workspace'
      task_graph = taskgraph.TaskGraph(workspace_dir, 4)
      target_a_path = os.path.join(workspace_dir, 'a.dat')
      target_b_path = os.path.join(workspace_dir, 'b.dat')
      result_path = os.path.join(workspace_dir, 'result.dat')
      result_2_path = os.path.join(workspace_dir, 'result2.dat')
      value_a = 5
      value_b = 10
      list_len = 10
      task_a = task_graph.add_task(
          func=_create_list_on_disk,
          args=(value_a, list_len, target_a_path),
          target_path_list=[target_a_path])
      task_b = task_graph.add_task(
          func=_create_list_on_disk,
          args=(value_b, list_len, target_b_path),
          target_path_list=[target_b_path])
      sum_task = task_graph.add_task(
          func=_sum_lists_from_disk,
          args=(target_a_path, target_b_path, result_path),
          target_path_list=[result_path],
          dependent_task_list=[task_a, task_b])

      task_graph.close()
      task_graph.join()

      # expect that result is a list `list_len` long with `value_a+value_b` in it
      result = pickle.load(open(result_path, 'rb'))


Caveats
-------

* Taskgraph's default method of checking whether a file has changed
  (``hash_algorithm='sizetimestamp'``) uses the filesystem's modification
  timestamp, interpreted in integer nanoseconds.  This check is only as
  accurate as the filesystem's timestamp.  For example:

  * FAT and FAT32 timestamps have a 2-second modification timestamp resolution
  * exFAT has a 10 millisecond timestamp resolution
  * NTFS has a 100 nanosecond timestamp resolution
  * HFS+ has a 1 second timestamp resolution
  * APFS has a 1 nanosecond timestamp resolution
  * ext3 has a 1 second timestamp resolution
  * ext4 has a 1 nanosecond timestamp resolution

  If you suspect timestamp resolution to be an issue on your filesystem, you
  may wish to store your files on a filesystem with more accurate timestamps or
  else consider using a different ``hash_algorithm``.


Running Tests
-------------

Taskgraph includes a ``tox`` configuration for automating builds across
multiple python versions and whether ``psutil`` is installed.  To execute all
tests on all platforms, run:

    $ tox

Alternatively, if you're only trying to run tests on a single configuration
(say, python 3.7 without ``psutil``), you'd run::

    $ tox -e py37

Or if you'd like to run the tests for the combination of Python 3.7 with
``psutil``, you'd run::

    $ tox -e py37-psutil

If you don't have multiple python installations already available on your system,
an easy way to accomplish this is to use ``tox-conda``
(https://github.com/tox-dev/tox-conda) which will use conda environments to manage
the versions of python available::

    $ pip install tox-conda
    $ tox
",2023-07-07 18:50:31+00:00
tektonpipelines,pipeline,tektoncd/pipeline,A cloud-native Pipeline resource.,https://tekton.dev,False,7919,2023-07-07 02:55:41+00:00,2018-08-29 18:21:55+00:00,1683,133,283,130,v0.49.0,2023-06-20 15:02:36+00:00,Apache License 2.0,4021,v0.49.0,131,2023-06-20 15:02:36+00:00,2023-07-07 18:36:04+00:00,2023-07-07 17:28:19+00:00,"# ![pipe](./pipe.png) Tekton Pipelines

[![Go Report Card](https://goreportcard.com/badge/tektoncd/pipeline)](https://goreportcard.com/report/tektoncd/pipeline)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4020/badge)](https://bestpractices.coreinfrastructure.org/projects/4020)

The Tekton Pipelines project provides k8s-style resources for declaring
CI/CD-style pipelines.

Tekton Pipelines are **Cloud Native**:

- Run on Kubernetes
- Have Kubernetes clusters as a first class type
- Use containers as their building blocks

Tekton Pipelines are **Decoupled**:

- One Pipeline can be used to deploy to any k8s cluster
- The Tasks which make up a Pipeline can easily be run in isolation
- Resources such as git repos can easily be swapped between runs

Tekton Pipelines are **Typed**:

- The concept of typed resources means that for a resource such as an `Image`,
  implementations can easily be swapped out (e.g. building with
  [kaniko](https://github.com/GoogleContainerTools/kaniko) v.s.
  [buildkit](https://github.com/moby/buildkit))

## Want to start using Pipelines

- [Installing Tekton Pipelines](docs/install.md)
- Jump in with [the ""Getting started"" tutorial!](https://tekton.dev/docs/getting-started/tasks/)
- Take a look at our [roadmap](roadmap.md)
- Discover our [releases](releases.md)

### Required Kubernetes Version

- Starting from the v0.24.x release of Tekton: **Kubernetes version 1.18 or later**
- Starting from the v0.27.x release of Tekton: **Kubernetes version 1.19 or later**
- Starting from the v0.30.x release of Tekton: **Kubernetes version 1.20 or later**
- Starting from the v0.33.x release of Tekton: **Kubernetes version 1.21 or later**
- Starting from the v0.39.x release of Tekton: **Kubernetes version 1.22 or later**
- Starting from the v0.41.x release of Tekton: **Kubernetes version 1.23 or later**
- Starting from the v0.45.x release of Tekton: **Kubernetes version 1.24 or later**

### Read the docs

The latest version of our docs is available at:

- [Installation Guide @ HEAD](DEVELOPMENT.md#install-pipeline)
- [Docs @ HEAD](/docs/README.md)
- [Examples @ HEAD](/examples)

Version specific links are available in the [releases](releases.md) page and on the
[Tekton website](https://tekton.dev/docs).

_See [our API compatibility policy](api_compatibility_policy.md) for info on the
stability level of the API._

_See [our Deprecations table](docs/deprecations.md) for features that have been
deprecated and the earliest date they'll be removed._

## Migrating

### v1alpha1 to v1beta1

In the move from v1alpha1 to v1beta1 several spec fields and Tekton
CRDs were updated or removed .

For users migrating their Tasks and Pipelines from v1alpha1 to v1beta1, check
out [the spec changes and migration paths](./docs/migrating-v1alpha1-to-v1beta1.md).

## Want to contribute

We are so excited to have you!

- See [CONTRIBUTING.md](CONTRIBUTING.md) for an overview of our processes
- See [DEVELOPMENT.md](DEVELOPMENT.md) for how to get started
- [Deep dive](./docs/developers/README.md) into demystifying the inner workings
  (advanced reading material)
- Look at our
  [good first issues](https://github.com/tektoncd/pipeline/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
  and our
  [help wanted issues](https://github.com/tektoncd/pipeline/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
",2023-07-07 18:50:35+00:00
temporal,temporal,temporalio/temporal,Temporal service,https://docs.temporal.io,False,7532,2023-07-07 17:44:12+00:00,2019-10-16 22:15:35+00:00,587,90,163,114,v1.21.1,2023-07-01 00:27:11+00:00,MIT License,5196,v1.21.1,174,2023-07-01 00:27:11+00:00,2023-07-07 18:50:36+00:00,2023-07-07 14:06:44+00:00,"[![Build status](https://badge.buildkite.com/98c157ed502d55722ef7f28e6470aa20702c76d6989a0a5a89.svg?branch=master)](https://buildkite.com/temporal/temporal-public)
[![Coverage Status](https://coveralls.io/repos/github/temporalio/temporal/badge.svg?branch=master)](https://coveralls.io/github/temporalio/temporal?branch=master)
[![Discourse](https://img.shields.io/static/v1?label=Discourse&message=Get%20Help&color=informational)](https://community.temporal.io)
[![Go Report Card][go-report-image]][go-report-url]

[go-report-image]: https://goreportcard.com/badge/github.com/temporalio/temporal
[go-report-url]: https://goreportcard.com/report/github.com/temporalio/temporal

# Temporal  

Temporal is a microservice orchestration platform which enables developers to build scalable applications without sacrificing productivity or reliability.
Temporal server executes units of application logic, Workflows, in a resilient manner that automatically handles intermittent failures, and retries failed operations.

Temporal is a mature technology, a fork of Uber's Cadence.
Temporal is being developed by [Temporal Technologies](https://temporal.io/), a startup by the creators of Cadence.

[![image](https://user-images.githubusercontent.com/12602502/136433917-98abe0d7-4f81-4f97-9b11-62b331c76608.png)](http://www.youtube.com/watch?v=f-18XztyN6c ""Temporal"")

Learn more about Temporal at [docs.temporal.io](https://docs.temporal.io).

## Getting Started

### Download and Start Temporal Server Locally

Execute the following commands to start a pre-built image along with all the dependencies.

```bash
git clone https://github.com/temporalio/docker-compose.git
cd docker-compose
docker-compose up
```

Refer to Temporal [docker-compose](https://github.com/temporalio/docker-compose) repo for more advanced options.

For more details on Docker images refer to [docker-builds](https://github.com/temporalio/docker-builds) repo.

### Run the Samples

Clone or download samples for [Go](https://github.com/temporalio/samples-go) or [Java](https://github.com/temporalio/samples-java) and run them with the local Temporal server.
We have a number of [HelloWorld type scenarios](https://github.com/temporalio/samples-java#helloworld) available, as well as more advanced ones. Note that the sets of samples are currently different between Go and Java.

### Use CLI

Use [Temporal's command line tool](https://docs.temporal.io/tctl-v1) `tctl` to interact with the local Temporal server.

```bash
alias tctl=""docker exec temporal-admin-tools tctl""
tctl namespace list
tctl workflow list
```

### Use Temporal Web UI

Try [Temporal Web UI](https://github.com/temporalio/ui) by opening [http://localhost:8080](http://localhost:8080) for viewing your sample workflows executing on Temporal.

## Repository

This repository contains the source code of the Temporal server. To implement Workflows, Activities and Workers, use [Go SDK](https://github.com/temporalio/sdk-go) or [Java SDK](https://github.com/temporalio/sdk-java).

## Contributing

We'd love your help in making Temporal great. Please review our [contribution guide](CONTRIBUTING.md).

If you'd like to work on or propose a new feature, first peruse [feature requests](https://community.temporal.io/c/feature-requests/6) and our [proposals repo](https://github.com/temporalio/proposals) to discover existing active and accepted proposals.

Feel free to join the Temporal [community](https://community.temporal.io) or [Slack channel](https://temporalio.slack.com/join/shared_invite/zt-169fymtfr-ADaoVXop6fJ~xn2oAo8csg#/shared-invite/email) to start a discussion or check if a feature has already been discussed.
Once you're sure the proposal is not covered elsewhere, please follow our [proposal instructions](https://github.com/temporalio/proposals#creating-a-new-proposal) or submit a [feature request](https://community.temporal.io/c/feature-requests/6).

## License

[MIT License](https://github.com/temporalio/temporal/blob/master/LICENSE)
",2023-07-07 18:50:40+00:00
texera,texera,Texera/texera,Collaborative Machine-Learning-Centric Data Analytics Using Workflows,https://texera.github.io,False,131,2023-07-05 12:17:47+00:00,2016-03-15 20:38:46+00:00,55,29,90,1,,,Apache License 2.0,5749,0.1.0,1,2017-09-23 01:26:05+00:00,2023-07-07 11:15:41+00:00,2023-07-07 09:22:12+00:00,"<h1 align=""center"">Texera - Collaborative Data Analytics Using Workflows.</h1>

<p align=""center"">
  <img src=""core/new-gui/src/assets/logos/full_logo_small.png"" alt=""texera-logo"" width=""192px"" height=""109px""/>
  <br>
  <i>Texera supports scalable computation and enables advanced AI/ML techniques.</i>
  <br>
  <i>""Collaboration"" is a key focus, and we enable an experience similar to Google Docs, but for data analytics. </i>
  <br>
  
  <h4 align=""center"">
    <a href=""https://youtu.be/2gfPUZNsoBs"">Demo Video</a>
    |
    <a href=""https://texera.github.io/blog/"">Blogs</a>
    |
    <a href=""https://github.com/Texera/texera/wiki/Getting-Started"">Getting Started</a>
    <br>
  </h4>
  
</p>
</p>

<!---
(Orignal intro paragraph, commented out)
## Texera

Texera is a system to support collaborative, ML-centric data analytics as a cloud-based service using GUI-based workflows. It supports scalable computation with a parallel backend engine, and enables advanced AI/ML techniques. ""Collaboration"" is a key focus, and we want to enable an experience similar to existing services such as Google Docs, but for data analytics, especially for people with different backgrounds, including IT developers and domain scientists with limited programming background.
-->

## Motivation

* Many data analysts need to spend a significant amount of effort on low-level computation to do data wrangling and preparation, and want to use latest AI/ML techniques. These tasks are especially tough for non-IT users. 

* Many workflow-based analysis systems are not parallel, making them not capable of dealing with big data sets. 

* Cloud-based services and technologies have emerged and advanced significantly in the past decade. Emerging browser-based techniques make it possible to develop powerful browser-based interfaces, which also benefit from high-speed networks.

* Existing big data systems support little interaction during the execution of a long running job, making them hard to manage once they are started.

## Goals

* Provide data analytics as cloud services;
* Provide a browser-based GUI to form a workflow without writing code;
* Allow non-IT people to do data analytics;
* Support collaborative data analytics;
* Allow users to interact with the execution of a job;
* Support huge volumes of data efficiently.

## Sample Workflow

The following is a workflow formulated using the Texera GUI in a Web browser, which consists of operators such as regex search, sentiment analysis, user-defined function (UDF) in Python, and visualization.

![Sample Texera Workflow](https://user-images.githubusercontent.com/12926365/171459157-1792971d-a31f-49e7-ab98-6f3b9ead9f5b.png)

## Publications (Computer Science):

* (4/2017) A Demonstration of TextDB: Declarative and Scalable Text Analytics on Large Data Sets, Zuozhi Wang, Flavio Bayer, Seungjin Lee, Kishore Narendran, Xuxi Pan, Qing Tang, Jimmy Wang, Chen Li, [ICDE 2017](http://icde2017.sdsc.edu/), **Best Demo award**, [PDF](https://chenli.ics.uci.edu/files/icde2017-textdb-demo.pdf), [Video](https://github.com/Texera/texera/wiki/Video).
* (1/2020) Amber: A Debuggable Dataflow system based on the Actor Model, Avinash Kumar, Zuozhi Wang, Shengquan Ni, Chen Li, VLDB 2020 [PDF](http://www.vldb.org/pvldb/vol13/p740-kumar.pdf), [Video](https://www.youtube.com/watch?v=T5ShFRfHmgI), [Slides](https://docs.google.com/presentation/d/1v8G9lDmfv4Ff2YWyrGfo_9iMQVF4N8a-4gO4H-K6rCk/edit?usp=sharing)
* (7/2020) Demonstration of Interactive Runtime Debugging of
Distributed Dataflows in Texera, Zuozhi Wang, Avinash Kumar, Shengquan Ni, Chen Li, VLDB 2020 [PDF](http://www.vldb.org/pvldb/vol13/p2953-wang.pdf), [Video](https://www.youtube.com/watch?v=SP-XiDADbw0), [Slides](https://docs.google.com/presentation/d/14U6RPZfeb8Ho0aO2HsCSc8lRs6ul6AxEIm5gpjeVUYA/edit?usp=sharing)
* (4/2022) Optimizing Machine Learning Inference Queries with Correlative Proxy Models, Zhihui Yang, Zuozhi Wang, Yicong Huang, Yao Lu, Chen Li, X. Sean Wang, VLDB 2022 [PDF](https://www.vldb.org/pvldb/vol15/p2032-yang.pdf).
* (6/2022) Demonstration of Collaborative and Interactive Workflow-Based Data Analytics in Texera, Xiaozhen Liu, Zuozhi Wang, Shengquan Ni, Sadeem Alsudais, Yicong Huang, Avinash Kumar, Chen Li, in VLDB 2022 [PDF](https://www.vldb.org/pvldb/vol15/p3738-liu.pdf), [Demo Video](https://youtu.be/2gfPUZNsoBs).
* (6/2022) Demonstration of Accelerating Machine Learning Inference Queries with Correlative Proxy Models, Zhihui Yang, Yicong Huang, Zuozhi Wang, Feng Gao, Yao Lu, Chen Li, X. Sean Wang, in VLDB 2022 [PDF](https://www.vldb.org/pvldb/vol15/p3734-yang.pdf) .
* (7/2022) Drove: Tracking Execution Results of Workflows on Large Datasets, Sadeem Alsudais, in the PhD workshop at VLDB 2022 [PDF](http://ceur-ws.org/Vol-3186/paper_10.pdf).
* (9/2022) Fries: Fast and Consistent Runtime Reconfiguration in Dataflow Systems with Transactional Guarantees, Zuozhi Wang, Shengquan Ni, Avinash Kumar, Chen Li, VLDB 2023 [PDF](https://www.vldb.org/pvldb/vol16/p256-wang.pdf). 
* (12/2022) Towards Interactive, Adaptive and Result-aware Big Data Analytics, Avinash Kumar, Phd Thesis [PDF](https://arxiv.org/abs/2212.07096). 

## Publications (Interdisciplinary):

* (4/2021) Why Do People Oppose Mask Wearing? A Comprehensive Analysis of US Tweets During the COVID-19 Pandemic, Lu He, Changyang He, Tera Leigh Reynolds, Qiushi Bai, Yicong Huang, Chen Li, Kai Zheng, and Yunan Chen in JAMIA 2021 [PDF](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7989302/pdf/ocab047.pdf).
* (9/2021) The Social Amplification and Attenuation of COVID-19 Risk Perception Shaping Mask Wearing Behavior: A Longitudinal Twitter Analysis, Suellen Hopfer, Emilia J. Fields, Yuwen Lu, Ganesh Ramakrishnan, Ted Grover, Quishi Bai, Yicong Huang, Chen Li, and Gloria Mark in PLOS ONE, 2021 [PDF](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257428).
* (3/2023) Understanding underlying moral values and language use of COVID-19 vaccine attitudes on twitter, Judith Borghouts, Yicong Huang, Sydney Gibbs, Suellen Hopfer, Chen Li and Gloria Mark in PNAS Nexus, 2023 [PDF](https://academic.oup.com/pnasnexus/article-pdf/2/3/pgad013/49435858/pgad013.pdf).
* (5/2023) Public Opinions toward COVID-19 Vaccine Mandates: A Machine Learning-based Analysis of U.S. Tweets, Yawen Guo, Jun Zhu, Yicong Huang, Lu He, Changyang He, Chen Li and Kai Zheng in AMIA, 2022 [PDF](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10148373/pdf/1066.pdf).

## Videos

* [Texera demo in VLDB 2020](https://www.youtube.com/watch?v=SP-XiDADbw0)
* [Amber engine presentation in VLDB 2020](https://www.youtube.com/watch?v=T5ShFRfHmgI)
* See [Texera in action](https://www.youtube.com/watch?v=NXfynBUwdVg). 

## Getting Started

* For users, visit [Guide to Use Texera](https://github.com/Texera/texera/wiki/Getting-Started).
* For developers, visit [Guide to Develop Texera](https://github.com/Texera/texera/wiki/Guide-for-Developers).

Texera was formally known as ""TextDB"" before August 28, 2017.

## Instructions for VLDB 2022 Demo Paper

To try our collaborative data analytics in _Demonstration of Collaborative and Interactive Workflow-Based Data Analytics in Texera_, visit [https://github.com/Texera/texera/wiki/Instructions-for-VLDB-2022-Demo](https://github.com/Texera/texera/wiki/Instructions-for-VLDB-2022-Demo).

## Acknowledgements

This project is supported by the <a href=""http://www.nsf.gov"">National Science Foundation</a> under the awards [III 1745673](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1745673), [III 2107150](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2107150), AWS Research Credits, and Google Cloud Platform Education Programs.

* <a href=""http://www.yourkit.com""><img src=""https://www.yourkit.com/images/yklogo.png"" alt=""Yourkit"" height=""30""/></a>  [Yourkit](https://www.yourkit.com/) has given an open source license to use their profiler in this project. 
",2023-07-07 18:50:45+00:00
tez,tez,apache/tez,Apache Tez,https://tez.apache.org/,False,425,2023-06-15 07:11:38+00:00,2013-04-08 07:20:23+00:00,395,35,65,0,,,Apache License 2.0,2953,release-0.10.2-rc0,67,2022-07-04 09:10:15+00:00,2023-07-07 14:21:03+00:00,2023-06-29 19:54:00+00:00,"<!--
  Licensed under the Apache License, Version 2.0 (the ""License"");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

Apache Tez
==========

Apache Tez is a generic data-processing pipeline engine envisioned as a low-level engine for higher abstractions
such as Apache Hadoop Map-Reduce, Apache Pig, Apache Hive etc.

At its heart, tez is very simple and has just two components:

*   The data-processing pipeline engine where-in one can plug-in input, processing and output implementations to 
    perform arbitrary data-processing. Every 'task' in tez has the following:
   -   Input to consume key/value pairs from.
   -   Processor to process them.
   -   Output to collect the processed key/value pairs.


*  A master for the data-processing application, where-by one can put together arbitrary data-processing 'tasks' 
   described above into a task-DAG to process data as desired. 
   The generic master is implemented as a Apache Hadoop YARN ApplicationMaster.
",2023-07-07 18:50:49+00:00
thegenomemodelingsystem,gms,genome/gms,The Genome Modeling System installer,https://github.com/genome/gms/wiki,True,76,2023-01-28 15:57:01+00:00,2013-08-28 21:53:31+00:00,25,55,5,0,,,GNU Lesser General Public License v3.0,468,gms-stable-2014.01.26,1,2014-01-26 06:46:35+00:00,2023-05-10 06:25:32+00:00,2015-07-10 14:28:36+00:00,"The Genome Modeling System (GMS)
===

A paper describing the GMS has been published here: <a href=""http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004274"">Genome Modeling System: A Knowledge Management Platform for Genomics</a>.

For a brief demonstration of the GMS please start with the: 
<a href=""https://github.com/genome/gms/wiki/Quick-VM-Tour"">Quick Tour in a Pre-configured Virtual Machine</a>.

More detailed documentation and tutorials are available in the home page: <big><a href=""https://github.com/genome/gms/wiki"">GMS Home</a></big>

This documentation includes:
the <a href=""https://github.com/genome/gms/wiki/Install"">Installation Guide</a>, 
the <a href=""https://github.com/genome/gms/wiki/HCC1395-WGS-Exome-RNA-Seq-Data"">Location and Description of the HCC1395 Data</a>, 
the <a href=""https://github.com/genome/gms/wiki/FAQ"">FAQ page</a>,
the <a href=""https://github.com/genome/gms/wiki/Beginner%27s-Guide-to-the-Demonstration-Analysis"">Guide to Importing your own Data</a>, 
the <a href=""https://github.com/genome/gms/wiki/Useful-GMS-Commands"">Reference Manual for useful Genome Commands</a>, 
the <a href=""https://github.com/genome/gms/wiki/Beginner%27s-Guide-to-the-Demonstration-Analysis"">Beginners Guide to Demonstration analysis</a>, and much more.

Installation of the GMS is possible by many avenues.  The following approaches have been tested and have associated tutorials and other documentation:
On dedicated hardware, as a preconfigured VirtualBox virtual machine, using Vagrant to create a virtual machine, using Docker, on an Amazon (AWS) EC2 instance, using an OpenStack instance.  Refer to the <a href=""https://github.com/genome/gms/wiki/Installation-Types-Overview"">installation types overview</a> for more details.   

Some of the tools made available through the GMS can be downloaded individually from <a href=""http://gmt.genome.wustl.edu/"">Genome Modeling Tools</a>.

The raw data and reference files needed for the GMS tutorial are made available through our <a href=""https://xfer.genome.wustl.edu/gxfer1/project/gms/testdata/"">FTP</a> and as an <a href=""https://gmsdata.s3.amazonaws.com/"">Amazon Public Dataset</a>.


Installation of the GMS involves installation of many commonly used bioinformatics tools, a postgres database, other services and use of the following git repositories: <a href=""https://github.com/genome/genome"">genome</a>, <a href=""https://github.com/genome/UR"">UR</a>, <a href=""https://github.com/genome/gms-webviews"">gms-webviews</a>, and <a href=""https://github.com/genome/tgi-workflow"">tgi-workflow</a>. Additional related projects developed at the Genome Institute can be found at <a href=""https://github.com/genome"">github genome</a> and external software packaged for use in the GMS can be found at <a href=""https://github.com/genome-vendor"">github genome-vendor</a>.


#### Quick navigation:

| [Install] (https://github.com/genome/gms/wiki/Install) | [Docs] (https://github.com/genome/gms/wiki/Docs) | [Tutorials] (https://github.com/genome/gms/wiki/Tutorials) | [FAQ] (https://github.com/genome/gms/wiki/FAQ) |
|----------------------------|----------------------------|----------------------------|----------------------------|
| [<img src=""https://github.com/genome/gms/wiki/Images/Gnome-system-software-installer-small.png"">](https://github.com/genome/gms/wiki/Install) | [<img src=""https://github.com/genome/gms/wiki/Images/Gnome-emblem-documents-small.png"">](https://github.com/genome/gms/wiki/Docs) | [<img src=""https://github.com/genome/gms/wiki/Images/DNA_sequence-small.png"">](https://github.com/genome/gms/wiki/Tutorials) | [<img src=""https://github.com/genome/gms/wiki/Images/Faq-icon-small.png"">](https://github.com/genome/gms/wiki/FAQ) |
| Step-by-step instructions for installing the sGMS | Technical documentation about the internals of the sGMS | Tutorials for running different analyses using the sGMS | Frequently asked questions about the sGMS |



| [Home] (https://github.com/genome/gms/wiki/Home) | [Install] (https://github.com/genome/gms/wiki/Install) | [Docs]  (https://github.com/genome/gms/wiki/Docs) | [Tutorials] (https://github.com/genome/gms/wiki/Tutorials) | [FAQ] (https://github.com/genome/gms/wiki/FAQ) |
|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|
",2023-07-07 18:50:55+00:00
tibanna,tibanna,4dn-dcic/tibanna," Tibanna helps you run your genomic pipelines on Amazon cloud (AWS). It is used by the 4DN DCIC (4D Nucleome Data Coordination and Integration Center) to process data. Tibanna supports CWL/WDL (w/ docker), Snakemake (w/ conda) and custom Docker/shell command.",,False,68,2023-02-24 08:49:32+00:00,2016-07-14 15:42:51+00:00,22,16,16,100,v1.8.1,2021-11-03 13:39:40+00:00,MIT License,3940,v1.8.1,102,2021-11-03 13:39:40+00:00,2023-05-30 15:37:45+00:00,2023-04-12 16:24:25+00:00,"# Tibanna

[![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380/) [![Build Status](https://travis-ci.org/4dn-dcic/tibanna.svg?branch=master)](https://travis-ci.org/4dn-dcic/tibanna) [![Code Quality](https://api.codacy.com/project/badge/Grade/d2946b5bc0704e5c9a4893426a7e0314)](https://www.codacy.com/app/4dn/tibanna?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=4dn-dcic/tibanna&amp;utm_campaign=Badge_Grade) [![Test Coverage](https://api.codacy.com/project/badge/Coverage/d2946b5bc0704e5c9a4893426a7e0314)](https://www.codacy.com/app/4dn/tibanna?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=4dn-dcic/tibanna&amp;utm_campaign=Badge_Coverage) [![Documentation Status](https://readthedocs.org/projects/tibanna/badge/?version=latest)](https://tibanna.readthedocs.io/en/latest/?badge=latest)

***

Tibanna runs portable pipelines (in CWL/WDL/Snakemake/shell) on the AWS Cloud.

<br>

Install Tibanna.
```bash
pip install tibanna
```

<br>

Use CLI to set up the cloud component and run workflow.
```bash
# Deploy Unicorn to the Cloud (Unicorn = serverless scheduler/resource allocator).
tibanna deploy_unicorn --usergroup=mygroup

# Run CWL/WDL workflow on the Cloud.
tibanna run_workflow --input-json=myrun.json
```

<br>

Alternatively, use Python API.

```python
from tibanna.core import API

# Deploy Unicorn to the Cloud.
API().deploy_unicorn(usergroup='mygroup')

# Run CWL/WDL workflow on the Cloud.
API().run_workflow(input_json='myrun.json')
```

<br>

---
Note: Starting `0.8.2`, Tibanna supports local CWL/WDL files as well as shell commands and Snakemake workflows.

Note 2: As of Tibanna version `2.0.0`, Python 3.6 is no longer supported. Please switch to Python 3.8! Python 3.7 is also supported as a fallback, but please prefer 3.8 if you can.

Note 3: Starting `0.8.0`, one no longer needs to `git clone` the Tibanna repo. 
* Please switch from `invoke <command>` to `tibanna <command>`! 
* We also renovated the Python API as an inheritable class to allow development around tibanna.


For more details, see Tibanna [**Documentation**](http://tibanna.readthedocs.io/en/latest).
* Also check out our [**paper in _Bioinformatics_**](https://doi.org/10.1093/bioinformatics/btz379).
* A preprint can also be found on [**biorxiv**](https://www.biorxiv.org/content/10.1101/440974v3).

",2023-07-07 18:50:58+00:00
toil,toil,DataBiosphere/toil,"A scalable, efficient, cross-platform (Linux/macOS) and easy-to-use workflow engine in pure Python.",http://toil.ucsc-cgl.org/.,False,843,2023-07-01 17:36:51+00:00,2015-03-30 21:08:58+00:00,238,57,111,49,releases/5.11.0,2023-06-15 15:06:35+00:00,Apache License 2.0,5849,releases/5.11.0,68,2023-06-15 15:02:06+00:00,2023-07-07 14:29:15+00:00,2023-07-06 15:37:41+00:00,".. image:: https://badges.gitter.im/bd2k-genomics-toil/Lobby.svg
   :alt: Join the chat at https://gitter.im/bd2k-genomics-toil/Lobby
   :target: https://gitter.im/bd2k-genomics-toil/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

Toil is a scalable, efficient, cross-platform (Linux & macOS) pipeline management system,
written entirely in Python, and designed around the principles of functional
programming.

* Check the `website`_ for a description of Toil and its features.
* Full documentation for the latest stable release can be found at
  `Read the Docs`_.
* Please subscribe to low-volume `announce`_ mailing list so we keep you informed
* Google Groups discussion `forum`_
* See our occasional `blog`_ for tutorials. 
* Use `biostars`_ channel for discussion.

.. _website: http://toil.ucsc-cgl.org/
.. _Read the Docs: https://toil.readthedocs.io/en/latest
.. _announce: https://groups.google.com/forum/#!forum/toil-announce
.. _forum: https://groups.google.com/forum/#!forum/toil-community
.. _blog: https://toilpipelines.wordpress.com/
.. _biostars: https://www.biostars.org/t/toil/

Notes:

* Toil moved from https://github.com/BD2KGenomics/toil to https://github.com/DataBiosphere/toil on July 5th, 2018.
* Toil dropped python 2.7 support on February 13, 2020 (last working py2.7 version is 3.24.0).
",2023-07-07 18:51:03+00:00
tpot,tpot,EpistasisLab/tpot,A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.,http://epistasislab.github.io/tpot/,False,9142,2023-07-07 15:48:37+00:00,2015-11-03 21:08:40+00:00,1536,290,76,28,v0.12.0,2023-05-25 22:11:55+00:00,GNU Lesser General Public License v3.0,2418,v0.12.0,30,2023-05-25 22:11:55+00:00,2023-07-07 15:48:37+00:00,2023-06-01 22:41:40+00:00,"Master status: [![Master Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=master)](https://travis-ci.com/EpistasisLab/tpot)
[![Master Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)
[![Master Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=master)](https://coveralls.io/github/EpistasisLab/tpot?branch=master)

Development status: [![Development Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=development)](https://travis-ci.com/EpistasisLab/tpot/branches)
[![Development Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)
[![Development Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=development)](https://coveralls.io/github/EpistasisLab/tpot?branch=development)

Package information: [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)
[![License: LGPL v3](https://img.shields.io/badge/license-LGPL%20v3-blue.svg)](http://www.gnu.org/licenses/lgpl-3.0)
[![PyPI version](https://badge.fury.io/py/TPOT.svg)](https://badge.fury.io/py/TPOT)

<p align=""center"">
<img src=""https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-logo.jpg"" width=300 />
</p>

---
To try the ![NEW!](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/NEW-small.gif ""NEW!"") TPOT2 (*alpha*) please go [here](https://github.com/EpistasisLab/tpot2)!

- - - -

**TPOT** stands for **T**ree-based **P**ipeline **O**ptimization **T**ool. Consider TPOT your **Data Science Assistant**. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.

![TPOT Demo](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-demo.gif ""TPOT Demo"")

TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.

![An example Machine Learning pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-ml-pipeline.png ""An example Machine Learning pipeline"")

<p align=""center""><strong>An example Machine Learning pipeline</strong></p>

Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.

![An example TPOT pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-pipeline-example.png ""An example TPOT pipeline"")

TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.

**TPOT is still under active development** and we encourage you to check back on this repository regularly for updates.

For further information about TPOT, please see the [project documentation](http://epistasislab.github.io/tpot/).

## License

Please see the [repository license](https://github.com/EpistasisLab/tpot/blob/master/LICENSE) for the licensing and usage information for TPOT.

Generally, we have licensed TPOT to make it as widely usable as possible.

## Installation

We maintain the [TPOT installation instructions](http://epistasislab.github.io/tpot/installing/) in the documentation. TPOT requires a working installation of Python.

## Usage

TPOT can be used [on the command line](http://epistasislab.github.io/tpot/using/#tpot-on-the-command-line) or [with Python code](http://epistasislab.github.io/tpot/using/#tpot-with-code).

Click on the corresponding links to find more information on TPOT usage in the documentation.

## Examples

### Classification

Below is a minimal working example with the optical recognition of handwritten digits dataset.

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_digits_pipeline.py')
```

Running this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the `tpot_digits_pipeline.py` file and look similar to the following:

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import PolynomialFeatures
from tpot.builtins import StackingEstimator
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# Average CV score on the training set was: 0.9799428471757372
exported_pipeline = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),
    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=""l1"")),
    RandomForestClassifier(bootstrap=True, criterion=""entropy"", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
```

### Regression

Similarly, TPOT can optimize pipelines for regression problems. Below is a minimal working example with the practice Boston housing prices data set.

```python
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_boston_pipeline.py')
```

which should result in a pipeline that achieves about 12.77 mean squared error (MSE), and the Python code in `tpot_boston_pipeline.py` should look similar to:

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# Average CV score on the training set was: -10.812040755234403
exported_pipeline = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),
    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
```

Check the documentation for [more examples and tutorials](http://epistasislab.github.io/tpot/examples/).

## Contributing to TPOT

We welcome you to [check the existing issues](https://github.com/EpistasisLab/tpot/issues/) for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) so we can discuss it.

Before submitting any contributions, please review our [contribution guidelines](http://epistasislab.github.io/tpot/contributing/).

## Having problems or have questions about TPOT?

Please [check the existing open and closed issues](https://github.com/EpistasisLab/tpot/issues?utf8=%E2%9C%93&q=is%3Aissue) to see if your issue has already been attended to. If it hasn't, [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) on this repository so we can review your issue.

## Citing TPOT

If you use TPOT in a scientific publication, please consider citing at least one of the following papers:

Trang T. Le, Weixuan Fu and Jason H. Moore (2020). [Scaling tree-based automated machine learning to biomedical big data with a feature set selector](https://academic.oup.com/bioinformatics/article/36/1/250/5511404). *Bioinformatics*.36(1): 250-256.

BibTeX entry:

```bibtex
@article{le2020scaling,
  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},
  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},
  journal={Bioinformatics},
  volume={36},
  number={1},
  pages={250--256},
  year={2020},
  publisher={Oxford University Press}
}
```


Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). [Automating biomedical data science through tree-based pipeline optimization](http://link.springer.com/chapter/10.1007/978-3-319-31204-0_9). *Applications of Evolutionary Computation*, pages 123-137.

BibTeX entry:

```bibtex
@inbook{Olson2016EvoBio,
    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},
    editor={Squillero, Giovanni and Burelli, Paolo},
    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},
    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},
    year={2016},
    publisher={Springer International Publishing},
    pages={123--137},
    isbn={978-3-319-31204-0},
    doi={10.1007/978-3-319-31204-0_9},
    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}
}
```

Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). [Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science](http://dl.acm.org/citation.cfm?id=2908918). *Proceedings of GECCO 2016*, pages 485-492.

BibTeX entry:

```bibtex
@inproceedings{OlsonGECCO2016,
    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},
    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
    series = {GECCO '16},
    year = {2016},
    isbn = {978-1-4503-4206-3},
    location = {Denver, Colorado, USA},
    pages = {485--492},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/2908812.2908918},
    doi = {10.1145/2908812.2908918},
    acmid = {2908918},
    publisher = {ACM},
    address = {New York, NY, USA},
}
```

Alternatively, you can cite the repository directly with the following DOI:

[![DOI](https://zenodo.org/badge/20747/rhiever/tpot.svg)](https://zenodo.org/badge/latestdoi/20747/rhiever/tpot)

## Support for TPOT

TPOT was developed in the [Computational Genetics Lab](http://epistasis.org/) at the [University of Pennsylvania](https://www.upenn.edu/) with funding from the [NIH](http://www.nih.gov/) under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.

The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.
",2023-07-07 18:51:07+00:00
trains,clearml,allegroai/clearml,"ClearML - Auto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management",https://clear.ml/docs,False,4526,2023-07-07 15:21:06+00:00,2019-06-10 08:18:32+00:00,594,84,75,84,v1.11.1,2023-06-21 15:33:44+00:00,Apache License 2.0,2140,v1.11.1,145,2023-06-21 15:33:44+00:00,2023-07-07 15:21:07+00:00,2023-07-06 09:23:06+00:00,"<div align=""center"" style=""text-align: center"">

<p style=""text-align: center"">
  <img align=""center"" src=""docs/clearml-logo.svg#gh-light-mode-only"" alt=""Clear|ML""><img align=""center"" src=""docs/clearml-logo-dark.svg#gh-dark-mode-only"" alt=""Clear|ML"">
</p>

**[ClearML](https://clear.ml) - Auto-Magical Suite of tools to streamline your ML workflow
</br>Experiment Manager, MLOps and Data-Management**

[![GitHub license](https://img.shields.io/github/license/allegroai/clearml.svg)](https://img.shields.io/github/license/allegroai/clearml.svg) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/clearml.svg)](https://img.shields.io/pypi/pyversions/clearml.svg) [![PyPI version shields.io](https://img.shields.io/pypi/v/clearml.svg)](https://pypi.org/project/clearml/) [![Conda version shields.io](https://img.shields.io/conda/v/clearml/clearml)](https://anaconda.org/clearml/clearml) [![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)<br>
[![PyPI Downloads](https://pepy.tech/badge/clearml/month)](https://pypi.org/project/clearml/) [![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/allegroai)](https://artifacthub.io/packages/search?repo=allegroai) [![Youtube](https://img.shields.io/badge/ClearML-DD0000?logo=youtube&logoColor=white)](https://www.youtube.com/c/clearml) [![Slack Channel](https://img.shields.io/badge/slack-%23clearml--community-blueviolet?logo=slack)](https://joinslack.clear.ml) [![Signup](https://img.shields.io/badge/Clear%7CML-Signup-brightgreen)](https://app.clear.ml)

</div>

---
### ClearML
<sup>*Formerly known as Allegro Trains*<sup>

ClearML is a ML/DL development and production suite, it contains FIVE main modules:

- [Experiment Manager](#clearml-experiment-manager) - Automagical experiment tracking, environments and results
- [MLOps](https://github.com/allegroai/clearml-agent) - Orchestration, Automation & Pipelines solution for ML/DL jobs (K8s / Cloud / bare-metal)  
- [Data-Management](https://github.com/allegroai/clearml/blob/master/docs/datasets.md) - Fully differentiable data management & version control solution on top of object-storage 
  (S3 / GS / Azure / NAS)  
- [Model-Serving](https://github.com/allegroai/clearml-serving) - *cloud-ready* Scalable model serving solution! 
  - **Deploy new model endpoints in under 5 minutes** 
  - Includes optimized GPU serving support backed by Nvidia-Triton 
  - **with out-of-the-box  Model Monitoring** 
- **NEW** :fire: [Reports](https://clear.ml/docs/latest/docs/webapp/webapp_reports) - Create and share rich MarkDown documents supporting embeddable online content 
  

Instrumenting these components is the **ClearML-server**, see [Self-Hosting](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server) & [Free tier Hosting](https://app.clear.ml)  


---
<div align=""center"">

**[Sign up](https://app.clear.ml)  &  [Start using](https://clear.ml/docs/) in under 2 minutes**

---
**Friendly tutorials to get you started**

<table>
<tbody>
  <tr>
    <td><a href=""https://github.com/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_1_Experiment_Management.ipynb""><b>Step 1</b></a> - Experiment Management</td>
    <td><a target=""_blank"" href=""https://colab.research.google.com/github/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_1_Experiment_Management.ipynb"">
  <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a></td>
  </tr>
  <tr>
    <td><a href=""https://github.com/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_2_Setting_Up_Agent.ipynb""><b>Step 2</b></a> - Remote Execution Agent Setup</td>
    <td><a target=""_blank"" href=""https://colab.research.google.com/github/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_2_Setting_Up_Agent.ipynb"">
  <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a></td>
  </tr>
  <tr>
    <td><a href=""https://github.com/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_3_Remote_Execution.ipynb""><b>Step 3</b></a> - Remotely Execute Tasks</td>
    <td><a target=""_blank"" href=""https://colab.research.google.com/github/allegroai/clearml/blob/master/docs/tutorials/Getting_Started_3_Remote_Execution.ipynb"">
  <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a></td>
  </tr>
</tbody>
</table>

</div>

---
<a href=""https://app.clear.ml""><img src=""https://github.com/allegroai/clearml/blob/master/docs/webapp_screenshots.gif?raw=true"" width=""100%""></a>

## ClearML Experiment Manager

**Adding only 2 lines to your code gets you the following**

* Complete experiment setup log
    * Full source control info including non-committed local changes
    * Execution environment (including specific packages & versions)
    * Hyper-parameters
        * [`argparse`](https://docs.python.org/3/library/argparse.html)/[Click](https://github.com/pallets/click/)/[PythonFire](https://github.com/google/python-fire) for command line parameters with currently used values
        * Explicit parameters dictionary
        * Tensorflow Defines (absl-py)
        * [Hydra](https://github.com/facebookresearch/hydra) configuration and overrides
    * Initial model weights file
* Full experiment output automatic capture
    * stdout and stderr
    * Resource Monitoring (CPU/GPU utilization, temperature, IO, network, etc.)
    * Model snapshots (With optional automatic upload to central storage: Shared folder, S3, GS, Azure, Http)
    * Artifacts log & store (Shared folder, S3, GS, Azure, Http)
    * Tensorboard/[TensorboardX](https://github.com/allegroai/clearml/tree/master/examples/frameworks/tensorboardx) scalars, metrics, histograms, **images, audio and video samples**
    * [Matplotlib & Seaborn](https://github.com/allegroai/clearml/tree/master/examples/frameworks/matplotlib)
    * [ClearML Logger](https://clear.ml/docs/latest/docs/fundamentals/logger) interface for complete flexibility.
* Extensive platform support and integrations
    * Supported ML/DL frameworks: [PyTorch](https://github.com/allegroai/clearml/tree/master/examples/frameworks/pytorch) (incl' [ignite](https://github.com/allegroai/clearml/tree/master/examples/frameworks/ignite) / [lightning](https://github.com/allegroai/clearml/tree/master/examples/frameworks/pytorch-lightning)), [Tensorflow](https://github.com/allegroai/clearml/tree/master/examples/frameworks/tensorflow), [Keras](https://github.com/allegroai/clearml/tree/master/examples/frameworks/keras), [AutoKeras](https://github.com/allegroai/clearml/tree/master/examples/frameworks/autokeras), [FastAI](https://github.com/allegroai/clearml/tree/master/examples/frameworks/fastai), [XGBoost](https://github.com/allegroai/clearml/tree/master/examples/frameworks/xgboost), [LightGBM](https://github.com/allegroai/clearml/tree/master/examples/frameworks/lightgbm), [MegEngine](https://github.com/allegroai/clearml/tree/master/examples/frameworks/megengine) and [Scikit-Learn](https://github.com/allegroai/clearml/tree/master/examples/frameworks/scikit-learn)
    * Seamless integration (including version control) with [**Jupyter Notebook**](https://jupyter.org/)
    and [*PyCharm* remote debugging](https://github.com/allegroai/trains-pycharm-plugin)
      
#### [Start using ClearML](https://clear.ml/docs/latest/docs/getting_started/ds/ds_first_steps) 


1. Sign up for free to the [ClearML Hosted Service](https://app.clear.ml) (alternatively, you can set up your own server, see [here](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server)).

    > **_ClearML Demo Server:_** ClearML no longer uses the demo server by default. To enable the demo server, set the `CLEARML_NO_DEFAULT_SERVER=0`
    > environment variable. Credentials aren't needed, but experiments launched to the demo server are public, so make sure not 
    > to launch sensitive experiments if using the demo server.

1. Install the `clearml` python package:

    ```bash
    pip install clearml
    ```

1. Connect the ClearML SDK to the server by [creating credentials](https://app.clear.ml/settings/workspace-configuration), then execute the command
below and follow the instructions: 

    ```bash
    clearml-init
    ```

1. Add two lines to your code:
    ```python
    from clearml import Task
    task = Task.init(project_name='examples', task_name='hello world')
    ```

You are done, everything your process outputs is now automagically logged into ClearML.

Next step, automation! **Learn more about ClearML's two-click automation [here](https://clear.ml/docs/latest/docs/getting_started/mlops/mlops_first_steps)**. 

## ClearML Architecture

The ClearML run-time components:

* The ClearML Python Package for integrating ClearML into your existing scripts by adding just two lines of code, and optionally extending your experiments and other workflows with ClearML's powerful and versatile set of classes and methods.
* The ClearML Server for storing experiment, model, and workflow data, and supporting the Web UI experiment manager, and MLOps automation for reproducibility and tuning. It is available as a hosted service and open source for you to deploy your own ClearML Server.
* The ClearML Agent for MLOps orchestration, experiment and workflow reproducibility, and scalability.

<img src=""https://raw.githubusercontent.com/allegroai/clearml-docs/main/docs/img/clearml_architecture.png"" width=""100%"" alt=""clearml-architecture"">

## Additional Modules 

- [clearml-session](https://github.com/allegroai/clearml-session) - **Launch remote JupyterLab / VSCode-server inside any docker, on Cloud/On-Prem machines**
- [clearml-task](https://github.com/allegroai/clearml/blob/master/docs/clearml-task.md) - Run any codebase on remote machines with full remote logging of Tensorboard, Matplotlib & Console outputs 
- [clearml-data](https://github.com/allegroai/clearml/blob/master/docs/datasets.md) - **CLI for managing and versioning your datasets, including creating / uploading / downloading of data from S3/GS/Azure/NAS** 
- [AWS Auto-Scaler](https://clear.ml/docs/latest/docs/guides/services/aws_autoscaler) - Automatically spin EC2 instances based on your workloads with preconfigured budget! No need for K8s!
- [Hyper-Parameter Optimization](https://clear.ml/docs/latest/docs/guides/optimization/hyper-parameter-optimization/examples_hyperparam_opt) - Optimize any code with black-box approach and state of the art Bayesian optimization algorithms 
- [Automation Pipeline](https://clear.ml/docs/latest/docs/guides/pipeline/pipeline_controller) - Build pipelines based on existing experiments / jobs, supports building pipelines of pipelines!  
- [Slack Integration](https://clear.ml/docs/latest/docs/guides/services/slack_alerts) - Report experiments progress / failure directly to Slack (fully customizable!)  

## Why ClearML?

ClearML is our solution to a problem we share with countless other researchers and developers in the machine
learning/deep learning universe: Training production-grade deep learning models is a glorious but messy process.
ClearML tracks and controls the process by associating code version control, research projects,
performance metrics, and model provenance.

We designed ClearML specifically to require effortless integration so that teams can preserve their existing methods
and practices. 

  - Use it on a daily basis to boost collaboration and visibility in your team 
  - Create a remote job from any experiment with a click of a button
  - Automate processes and create pipelines to collect your experimentation logs, outputs, and data
  - Store all you data on any object-storage solution, with the simplest interface possible
  - Make you data transparent by cataloging it all on the ClearML platform    

We believe ClearML is ground-breaking. We wish to establish new standards of true seamless integration between
experiment management, MLOps and data management. 

## Who We Are

ClearML is supported by you and the [clear.ml](https://clear.ml) team, which helps enterprise companies build scalable MLOps. 

We built ClearML to track and control the glorious but messy process of training production-grade deep learning models.
We are committed to vigorously supporting and expanding the capabilities of ClearML.

We promise to always be backwardly compatible, making sure all your logs, data and pipelines 
will always upgrade with you.

## License

Apache License, Version 2.0 (see the [LICENSE](https://www.apache.org/licenses/LICENSE-2.0.html) for more information)

If ClearML is part of your development process / project / publication, please cite us :heart: : 
```
@misc{clearml,
title = {ClearML - Your entire MLOps stack in one open-source tool},
year = {2019},
note = {Software available from http://github.com/allegroai/clearml},
url={https://clear.ml/},
author = {ClearML},
}
```

## Documentation, Community & Support

More information in the [official documentation](https://clear.ml/docs) and [on YouTube](https://www.youtube.com/c/ClearML).

For examples and use cases, check the [examples folder](https://github.com/allegroai/clearml/tree/master/examples) and [corresponding documentation](https://clear.ml/docs/latest/docs/guides).

If you have any questions: post on our [Slack Channel](https://joinslack.clear.ml), or tag your questions on [stackoverflow](https://stackoverflow.com/questions/tagged/clearml) with '**[clearml](https://stackoverflow.com/questions/tagged/clearml)**' tag (*previously [trains](https://stackoverflow.com/questions/tagged/trains) tag*).

For feature requests or bug reports, please use [GitHub issues](https://github.com/allegroai/clearml/issues).

Additionally, you can always find us at *info@clear.ml*

## Contributing

**PRs are always welcome** :heart: See more details in the ClearML [Guidelines for Contributing](https://github.com/allegroai/clearml/blob/master/docs/contributing.md).


_May the force (and the goddess of learning rates) be with you!_
",2023-07-07 18:51:12+00:00
triggerflow,triggerflow,triggerflow/triggerflow,Event-based Orchestration of Serverless Workflows,,True,42,2023-06-06 09:02:22+00:00,2020-04-09 01:27:32+00:00,11,4,4,0,,,Apache License 2.0,339,,0,,2023-06-06 09:02:22+00:00,2021-12-14 09:57:36+00:00,"# TriggerFlow: Event-based Orchestration of Serverless Workflows

Triggerflow is a scalable, extensible and serverless in design platform for event-based orchestration of
serverless workflows.

![triggerflow_architecture](https://user-images.githubusercontent.com/33722759/85291482-d46b8180-b49a-11ea-973f-3995b21425ad.png)

Triggerflow follows an Event-Condition-Action architecture with stateful triggers that can aggregate, filter,
process and route incoming events from a variety of event sources in a consistent and fault tolerant way.

Thanks to Triggerflow's extensibility provided by its fully programmable trigger condition and action functions, and 
combining and chaining multiple triggers, we can orchestrate different serverless workflow abstractions such as
DAGs (Apache Airflow), State Machines (Amazon Step Functions), and Workflow as Code like (Azure Durable Functions),
among other specialized workflows.

Triggerflow has been implemented using Open-Source Cloud Native projects like CloudEvents and KEDA or Knative.
When Triggerflow is deployed using KEDA or Knative, the trigger processing service runs only when there are incoming
events so that it can be scaled down to zero when it is not used, which results in a pay-per-use serverless model.

You can read more about Triggerflow architecture and features in the
[Triggerflow: Trigger-based Orchestration of Serverless Workflows](https://arxiv.org/abs/2006.08654) article, presented 
and accepted at the [ACM Distributed and Event Based Systems 2020 conference](https://2020.debs.org/accepted-papers/).

## Documentation

### Installation and deployment
- [Triggerflow Client Installation](docs/CLIENT_INSTALL.md)

- [Standalone Deployment for testing guide](deploy/standalone/README.md)

- [Knative on Kubernetes Deployment guide](deploy/knative/README.md)

- [KEDA on Kubernetes Deployment guide](deploy/keda/README.md)

### Examples
- [DAG Interface example](examples/dag-example/count_words.ipynb)

- [ASL State Machines example](docs/STATEMACHINES.md)

- [Workflow As Code example](docs/WORKFLOWASCODE.md)

- [Triggerflow Experiments instructions for replication](docs/EXPERIMENTS.md)

## Triggerflow Client Example
```python
from triggerflow import Triggerflow, CloudEvent, DefaultConditions
from triggerflow.functions import PythonCallable
from triggerflow.eventsources.rabbit import RabbitMQEventSource

# Instantiate Triggerflow client
tf_client = Triggerflow()

# Create a workspace and add a RabbitMQ event source to it
rabbitmq_source = RabbitMQEventSource(amqp_url='amqp://guest:guest@172.17.0.3/', queue='My-Queue')
tf_client.create_workspace(workspace_name='test', event_source=rabbitmq_source)


def my_action(context, event):
    context['message'] += 'World!'

# Create the trigger activation event 
activation_event = CloudEvent().SetEventType('test.event.type').SetSubject('Test')

# Create a trigger with a custom Python callable action and a Join condition that joins 10 events
tf_client.add_trigger(trigger_id='MyTrigger',
                      event=activation_event,
                      condition=DefaultConditions.JOIN,
                      action=PythonCallable(my_action),
                      context={'message': 'Hello ', 'join': 10})

# Publish 10 activation events, the action will only be executed on the 10th event
for _ in range(10):
    rabbitmq_source.publish_cloudevent(activation_event)

# Retrieve the trigger's context
trg = tf_client.get_trigger('MyTrigger')
print(trg['context']['message'])  # Prints 'Hello World!'
```   
",2023-07-07 18:51:16+00:00
triquetrum,triquetrum,eclipse-archived/triquetrum,Triquetrum project,,True,13,2023-01-28 06:06:25+00:00,2015-12-04 21:28:07+00:00,13,11,4,12,v0.4.0,2020-10-02 00:20:03+00:00,Other,647,v0.4.0,12,2020-10-02 00:20:03+00:00,,2021-10-06 20:22:59+00:00,"##THIS PROJECT HAS BEEN ARCHIVED

# Triquetrum


Triquetrum delivers an open platform for managing and executing scientific workflows. The goal of Triquetrum is to support a wide range of use cases, ranging from automated processes based on predefined models, to replaying ad-hoc research workflows recorded from a user's actions in a scientific workbench UI. It will allow to define and execute models from personal pipelines with a few steps to massive models with thousands of elements.

Besides delivering a generic workflow environment, Triquetrum also delivers extensions with a focus on scientific software. There is no a-priori limitation on target scientific domains, but the current interested organizations are big research institutions in materials research (synchrotrons), physics and engineering.

## Resources
* [Downloads](https://wiki.eclipse.org/Triquetrum/Downloads)
* [Wiki](https://wiki.eclipse.org/Triquetrum)
",2023-07-07 18:51:20+00:00
uap,uap,yigbt/uap,Universal Analysis Pipeline - A python tool for controlled and coordinated data analysis ,,False,3,2021-12-13 09:09:33+00:00,2016-11-07 13:50:24+00:00,5,4,9,0,,,,3353,v1.0,1,2016-12-02 12:32:33+00:00,,2021-12-13 09:09:26+00:00,"# uap -- Universal Analysis Pipeline

## Authors

Christoph Kämpf, Michael Specht, Alexander Scholz, Sven-Holger Puppel, Gero Doose, Kristin Reiche, Jana Schor, Jörg Hackermüller

[uap: reproducible and robust HTS data analysis. BMC Bioinformatics 20, 664 (2019)](https://doi.org/10.1186/s12859-019-3219-1)

## Introduction

The **uap** package is a framework to configure, run, and control
large data multi-step analyses.
Its main focus is on the analysis of high-throughput sequencing data.

The aim of this data processing pipeline is to enable robust and straightforward
bioinformatics data evaluation.
It is implemented in Python, runs under GNU/Linux and can be controlled from the
command-line interface.
Although the primary focus is the evaluation of sequencing data, its design
allows for a variety of other applications.

## About this Repository

This repository contains the development status of **uap** at Fraunhofer IZI.
It is based on the **uap** repository as published in [Kämpf, C., Specht, M.,Scholz, A. et al. uap: reproducible and robust HTS data analysis. BMC Bioinformatics 20, 664 (2019)](https://doi.org/10.1186/s12859-019-3219-1), which is located [here](https://github.com/yigbt/uap).
 
This version [v2.0.0rc2](https://github.com/fraunhofer-izi/uap/releases/tag/v2.0.0rc2) contains the following changes (for a complete list see the [CHANGELOG](CHANGELOG.md)):

* code conversion from Python2 to Python3
* improved user interaction
* enhanced error detection for configuration
* validation of existing results by using annotation as configuration and recalculation of SHA256
* `status --details` completely lists errors or changes caused by adaptation of the configuration 
* enhanced detection of changes (software version, output files, sha256 of results (optional))
* improved error-management
* removed checksum suffix in output directories
* extended backward-compatible connection-management
* Source_controller step to check input data
* no need to configure `uap` internal scripts, GNU coreutils and `lmod`
* improved job-execution (signal handling, array jobs, enhanced logging, changes to configuration do not impact running jobs)
* processes are executed in temporary directories
* error fixing and code improvement

Please note, the version [v2.0.0rc2](https://github.com/fraunhofer-izi/uap/releases/tag/v2.0.0rc2) of **uap** requires Python >= 3.5 and is only tested on [SLURM](https://slurm.schedmd.com/documentation.html).


## Singularity Container

A singularity container that encapsulates **uap** can be found in our [Sylabs registry](https://cloud.sylabs.io/library/bioinf_ufz/uap).
Within this container, we provide **uap** with support for SLURM and UGE grid engines.

* uap.sif:latest mirrors this commit: [f0d2cc2](https://github.com/yigbt/uap/commit/f0d2cc25834d7136fd3cad8c9300e17aaad73938)


## Contacts

* Christoph Kämpf, [christoph.kaempf@izi.fraunhofer.de](mailto:christoph.kaempf@izi.fraunhofer.de)
* Kristin Reiche, [kristin.reiche@izi.fraunhofer.de](mailto:kristin.reiche@izi.fraunhofer.de)
* Jana Schor, [jana.schor@ufz.de](mailto:jana.schor@ufz.de)
* Jörg Hackermüller, [joerg.hackermueller@ufz.de](mailto:joerg.hackermueller@ufz.de)
* Sebastian Canzler, [sebastian.canzler@ufz.de](mailto:sebastian.canzler@ufz.de)

Helmholtz Centre for Environmental Research - UFZ</br>
Permoserstr. 15, 04318 Leipzig, Germany

Fraunhofer Institute for Cell Therapy and Immunology (IZI)</br>
Perlickstraße 1, 04103 Leipzig, Germany

## Main contributors:

* Christoph Kämpf
* Dominik Otto
* Michael Specht
* Alexander Scholz
* Sven-Holger Puppel
* Gero Doose
* Kristin Reiche
* Sebastian Canzler
* Julienne Lehmann
* Jana Schor
* Jörg Hackermüller

## License

Copyright (C) 2011 - 2020 Helmholtz Centre for Environmental Research - UFZ and Fraunhofer Gesellschaft zur Foerderung der angewandten Forschung e.V. acting on behalf of its Fraunhofer Institute for Cell Therapy and Immunology (IZI).

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [UFZ-IZI](https://github.com/yigbt/uap/blob/master/UFZ-IZI-LICENSE) License document for more details.
",2023-07-07 18:51:24+00:00
unicore,uftp,UNICORE-EU/uftp,The UFTP repository contains source code for the server and client components of the UNICORE FTP high-performance file transfer toolkit,,False,3,2022-12-12 17:49:32+00:00,2020-05-18 10:35:17+00:00,3,2,2,0,,,Other,183,uftpd-3.3.0,35,2023-06-30 06:54:21+00:00,2023-07-03 10:29:10+00:00,2023-07-03 10:29:00+00:00,"# UNICORE File transfer (UFTP)

This repository contains the source code for various
server and client components for the UNICORE FTP
high-performance file transfer toolkit.

WORKING BINARIES can be
[downloaded from SourceForge](https://sourceforge.net/projects/unicore/files)

DOCUMENTATION for users and administrators can be found at
[UFTP-Docs](https://uftp-docs.readthedocs.io)

 * core - contains the core Java library, as well as a
   reference implementation of the 'uftpd' server, which
   can be used in tests of other Java components.

 * pyuftpd - contains the production version of the uftpd server,
   written in Python

 * authserver -  a set of services providing authentication for UFTP as well
   as data sharing features

 * uftp-client - the standalone ""uftp"" client application

 * datashare - library containing support code for the data sharing feature

The [PyUFTP commandline client](https://github.com/UNICORE-EU/pyuftp) has its own repo.
",2023-07-07 18:51:27+00:00
uniprougene,ugene,ugeneunipro/ugene,UGENE is free open-source cross-platform bioinformatics software,http://ugene.net,False,173,2023-06-30 08:42:11+00:00,2016-03-11 13:06:48+00:00,54,21,28,31,47.0,2023-05-20 08:38:16+00:00,GNU General Public License v2.0,16667,47.0,48,2023-05-20 08:38:16+00:00,2023-07-07 18:05:14+00:00,2023-07-07 18:05:05+00:00,"# UGENE
Download UGENE: [https://ugeneunipro.github.io/ugene/](https://ugeneunipro.github.io/ugene/)

## Building UGENE

### Prerequisites

Qt (>= 5.12.0 and <= 5.15.x) with the following components installed with Qt installer:
* Desktop
* QtScript

### For Windows users:

To build with devenv (Visual Studio):

1. `qmake -r -tp vc ugene.pro`
2. open ugene.sln from Visual Studio and build or run `devenv.exe ugene.sln /Build` from MSVC command line

To build with nmake:

1. `qmake -r ugene.pro`
2. run `nmake`, `nmake debug` or `nmake release` to build UGENE

### For *nix users:

To build and install UGENE on *nix:

1. `qmake -r PREFIX=/opt/ugene-${VERSION}`.
2. `make -j 4`
3. `sudo make install`
4. `sudo ln -s /opt/ugene-${VERSION}/ugene /usr/bin`
5. `ugene -ui`

> Note: you do not need 'sudo' if you select your own folder for the installation.

",2023-07-07 18:51:32+00:00
vistrails,VisTrails,VisTrails/VisTrails,"VisTrails is an open-source data analysis and visualization tool. It provides a comprehensive provenance infrastructure that maintains detailed history information about the steps followed and data derived in the course of an exploratory task: VisTrails maintains provenance of data products, of the computational processes that derive these products and their executions. ",http://www.vistrails.org,False,98,2022-12-08 01:50:21+00:00,2012-08-08 19:37:18+00:00,43,16,21,8,v2.2.4,2016-05-02 16:56:54+00:00,"BSD 3-Clause ""New"" or ""Revised"" License",6696,v2.2.4,17,2016-05-02 16:56:08+00:00,,2017-11-20 19:57:36+00:00,"Homepage: <http://www.vistrails.org>

VisTrails is an open-source data analysis and visualization tool. It provides a comprehensive provenance infrastructure that maintains detailed history information about the steps followed and data derived in the course of an exploratory task: VisTrails maintains provenance of data products, of the computational processes that derive these products and their executions.

For more information, take a look at the [documentation](http://www.vistrails.org/index.php/Documentation), the [users guide](http://www.vistrails.org/usersguide/v2.1/html/), or our [publications](http://www.vistrails.org/index.php/Publications,_Tutorials_and_Presentations).

Binary releases are available on our [download](http://www.vistrails.org/index.php/Downloads) page. Anaconda packages are available in the [vida-nyu channel](https://anaconda.org/vida-nyu) (use `-c vida-nyu`). To report bugs, please use the github [issue](https://github.com/VisTrails/VisTrails/issues) tracker, after checking our [FAQ](http://www.vistrails.org/index.php/FAQ) for known issues.

Who we are: <http://www.vistrails.org/index.php/People>
",2023-07-07 18:51:36+00:00
volcano,volcano,volcano-sh/volcano,A Cloud Native Batch System (Project under CNCF),https://volcano.sh,False,3140,2023-07-06 15:13:15+00:00,2019-03-14 09:47:29+00:00,724,87,180,23,v1.7.0,2023-01-04 09:31:39+00:00,Apache License 2.0,4776,v1.7.0,29,2022-12-30 15:54:42+00:00,2023-07-07 10:22:08+00:00,2023-06-30 00:32:16+00:00,"<a href=""https://volcano.sh/"">
    <img src=""https://raw.githubusercontent.com/volcano-sh/volcano/master/docs/images/volcano-horizontal-color.png""/>
</a>

-------

[![Build Status](https://travis-ci.org/volcano-sh/volcano.svg?branch=master)](https://travis-ci.org/volcano-sh/volcano)
[![Go Report Card](https://goreportcard.com/badge/github.com/volcano-sh/volcano)](https://goreportcard.com/report/github.com/volcano-sh/volcano)
[![RepoSize](https://img.shields.io/github/repo-size/volcano-sh/volcano.svg)](https://github.com/volcano-sh/volcano)
[![Release](https://img.shields.io/github/release/volcano-sh/volcano.svg)](https://github.com/volcano-sh/volcano/releases)
[![LICENSE](https://img.shields.io/github/license/volcano-sh/volcano.svg)](https://github.com/volcano-sh/volcano/blob/master/LICENSE)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3012/badge)](https://bestpractices.coreinfrastructure.org/projects/3012)


[Volcano](https://volcano.sh/) is a batch system built on Kubernetes. It provides a suite of mechanisms that are commonly required by
many classes of batch & elastic workload including: machine learning/deep learning, bioinformatics/genomics and
other ""big data"" applications. These types of applications typically run on generalized domain frameworks like
TensorFlow, Spark, Ray, PyTorch, MPI, etc, which Volcano integrates with.

Volcano builds upon a decade and a half of experience running a wide
variety of high performance workloads at scale using several systems
and platforms, combined with best-of-breed ideas and practices from
the open source community.

Until June 2021, Volcano has been widely used around the world at a variety of industries such as Internet/Cloud/Finance/
Manufacturing/Medical. More than 20 companies or institutions are not only end users but also active contributors. Hundreds
of contributors are taking active part in the code commit/PR review/issue discussion/docs update and design provision. We
are looking forward to your participation.

**NOTE**: the scheduler is built based on [kube-batch](https://github.com/kubernetes-sigs/kube-batch);
refer to [#241](https://github.com/volcano-sh/volcano/issues/241) and [#288](https://github.com/volcano-sh/volcano/pull/288) for more detail.

![cncf_logo](docs/images/cncf-logo.png)

Volcano is an incubating project of the [Cloud Native Computing Foundation](https://cncf.io/) (CNCF). Please consider joining the CNCF if you are an organization that wants to take an active role in supporting the growth and evolution of the cloud native ecosystem. 

## Overall Architecture

![volcano](docs/images/volcano-architecture.png)

## Talks

- [Intro: Kubernetes Batch Scheduling @ KubeCon 2019 EU](https://sched.co/MPi7)
- [Volcano 在 Kubernetes 中运行高性能作业实践 @ ArchSummit 2019](https://archsummit.infoq.cn/2019/shenzhen/presentation/1817)
- [Volcano：基于云原生的高密计算解决方案 @ Huawei Connection 2019](https://agenda.events.huawei.com/2019/cn/minisite/agenda.html#dayTab=day7&tagName=%7B%22language%22%3A%22Cn%22%7D&seminarId=1743)
- [Improving Performance of Deep Learning Workloads With Volcano @ KubeCon 2019 NA](https://sched.co/UaZi)
- [Batch Capability of Kubernetes Intro @ KubeCon 2019 NA](https://sched.co/Uajv)
- [Intro: Kubernetes Batch Scheduling @ KubeCon 2019 EU](https://sched.co/MPi7)


## Ecosystem

- [spark-operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/volcano-integration.md)
- [kubeflow/tf-operator](https://www.kubeflow.org/docs/use-cases/job-scheduling/)
- [kubeflow/arena](https://github.com/kubeflow/arena/blob/master/docs/training/volcanojob/volcanojob.md)
- [Horovod/MPI](https://github.com/volcano-sh/volcano/tree/master/example/integrations/mpi)
- [paddlepaddle](https://github.com/volcano-sh/volcano/tree/master/example/integrations/paddlepaddle)
- [cromwell](https://github.com/broadinstitute/cromwell/blob/develop/docs/backends/Volcano.md)
- [KubeRay](https://ray-project.github.io/kuberay/guidance/volcano-integration)

## Quick Start Guide

### Prerequisites

- Kubernetes 1.12+ with CRD support


You can try Volcano by one of the following two ways.

Note: 
* For Kubernetes v1.17+ use CRDs under config/crd/bases (recommended)
* For Kubernetes versions < v1.16 use CRDs under config/crd/v1beta1 (deprecated)

### Install with YAML files

Install Volcano on an existing Kubernetes cluster. This way is both available for x86_64 and arm64 architecture.

```
kubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/master/installer/volcano-development.yaml
```

Enjoy! Volcano will create the following resources in `volcano-system` namespace.


```
NAME                                       READY   STATUS      RESTARTS   AGE
pod/volcano-admission-5bd5756f79-dnr4l     1/1     Running     0          96s
pod/volcano-admission-init-4hjpx           0/1     Completed   0          96s
pod/volcano-controllers-687948d9c8-nw4b4   1/1     Running     0          96s
pod/volcano-scheduler-94998fc64-4z8kh      1/1     Running     0          96s

NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/volcano-admission-service   ClusterIP   10.98.152.108   <none>        443/TCP   96s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/volcano-admission     1/1     1            1           96s
deployment.apps/volcano-controllers   1/1     1            1           96s
deployment.apps/volcano-scheduler     1/1     1            1           96s

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/volcano-admission-5bd5756f79     1         1         1       96s
replicaset.apps/volcano-controllers-687948d9c8   1         1         1       96s
replicaset.apps/volcano-scheduler-94998fc64      1         1         1       96s

NAME                               COMPLETIONS   DURATION   AGE
job.batch/volcano-admission-init   1/1           48s        96s

```

### Install from code

If you don't have a kubernetes cluster, try one-click install from code base:

```bash
./hack/local-up-volcano.sh
```

This way is only available for x86_64 temporarily.

### Install monitoring system

If you want to get prometheus and grafana volcano dashboard after volcano installed, try following commands:

```bash
make TAG=latest generate-yaml
kubectl create -f _output/release/volcano-monitoring-latest.yaml
```

## Kubernetes compatibility

|                        | Kubernetes 1.17 | Kubernetes 1.18 | Kubernetes 1.19 | Kubernetes 1.20 | Kubernetes 1.21 | Kubernetes 1.22 | Kubernetes 1.23 | Kubernetes 1.24 | Kubernetes 1.25 |
|------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Volcano v1.6          | ✓               | ✓               | ✓               | ✓               | ✓               | ✓               | ✓               | -               | -               |
| Volcano v1.7          | -               | -               | ✓               | ✓               | ✓               | ✓               | ✓               |✓               |✓               |
| Volcano HEAD (master) | -               | -               | ✓               | ✓               | ✓               | ✓               | ✓               |✓               |✓               |

Key:
* `✓` Volcano and the Kubernetes version are exactly compatible.
* `+` Volcano has features or API objects that may not be present in the Kubernetes version.
* `-` The Kubernetes version has features or API objects that Volcano can't use.


## Meeting

Community weekly meeting for Asia: 15:00 - 16:00 (UTC+8) Friday. ([Convert to your timezone.](https://www.thetimezoneconverter.com/?t=10%3A00&tz=GMT%2B8&))

Community biweekly meeting for America: 08:30 - 09:30 (UTC-8) Thursday. ([Convert to your timezone.](https://www.thetimezoneconverter.com/?t=10%3A00&tz=GMT%2B8&))

Community meeting for Europe is ongoing on demand now. If you have some ideas or topics to discuss, please leave message
in the [slack](https://cloud-native.slack.com/archives/C011GJDQS0N). Maintainers will contact with you and book an open meeting for that.

Resources:
- [Meeting notes and agenda](https://docs.google.com/document/d/1YLbF8zjZBiR9PbXQPB22iuc_L0Oui5A1lddVfRnZrqs/edit)
- [Meeting link](https://zoom.us/j/91804791393)
- [Meeting Calendar](https://calendar.google.com/calendar/b/1/embed?src=volcano.sh.bot@gmail.com) | [Subscribe](https://calendar.google.com/calendar/b/1?cid=dm9sY2Fuby5zaC5ib3RAZ21haWwuY29t)

## Contact

If you have any question, feel free to reach out to us in the following ways:

[Volcano Slack Channel](https://volcano-sh.slack.com)

[Mailing List](https://groups.google.com/forum/#!forum/volcano-sh)
",2023-07-07 18:51:40+00:00
voltdb,voltdb,VoltDB/voltdb,Volt Active Data,http://voltactivedata.com/,False,2011,2023-07-03 18:27:32+00:00,2011-09-23 18:46:12+00:00,455,150,94,0,,,GNU Affero General Public License v3.0,28863,voltdb-11.0,297,2021-08-11 01:46:02+00:00,2023-07-03 18:27:32+00:00,2022-06-14 15:30:30+00:00,"What is VoltDB?
====================

Thank you for your interest in VoltDB!

VoltDB provides reliable data services for today's demanding applications. A distributed, horizontally-scalable, ACID-compliant database that provides streaming, storage, and real-time analytics for applications that benefit from strong consistency, high throughput and low, predictable latency.

VoltDB and Open Source
====================

VoltDB offers the fully open source, AGPL3-licensed Community Edition of VoltDB through GitHub here: 

https://github.com/voltdb/voltdb/

Trials of the enterprise edition of VoltDB are available from the VoltDB website at the following URL:

https://www.voltdb.com/

The Community Edition has full application compatibility and provides everything needed to run a real-time, in-memory SQL database with datacenter-local redundancy and snapshot-based disk persistence.

The commercial editions add operational features to support industrial strength durability and availability, including per-transaction disk-based persistence, multi-datacenter replication, elastic online expansion, live online upgrade, etc..

For more information, please visit the VoltDB website.

https://voltdb.com/

VoltDB Branches and Tags
====================

The latest development branch is _master_. We develop features on branches and merge to _master_ when stable. While _master_ is usually stable, it should not be considered production-ready and may also have partially implemented features.

Code that corresponds to released versions of VoltDB are tagged ""voltdb-X.X"" or ""voltdb-X.X.X"". To build corresponding OSS VoltDB versions, use these tags.


Building VoltDB
====================

Information on building VoltDB from this source repository is maintained in a GitHub wiki page available here:

https://github.com/VoltDB/voltdb/wiki/Building-VoltDB


First Steps
====================

From the directory where you installed VoltDB, you can either use bin/{command} or add the bin folder to your path so you can use the VoltDB commands anywhere. For example:

    PATH=""$PATH:$(pwd)/bin/""
    voltdb --version
    
Then, initialize a root directory and start a single-server database. By default the root directory is created in your current working directory. Or you can use the --dir option to specify a location:

    voltdb init [--dir ~/mydb]
    voltdb start [--dir ~/mydb] [--background]
    
To start a SQL console to enter SQL DDL, DML or DQL:

    sqlcmd
    
To launch the web-based VoltDB Management Console (VMC), open a web browser and connect to localhost on port 8080 (unless there is a port conflict): http://localhost:8080.
    
To stop the running VoltDB cluster, use the shutdown command. For commercial customers, the database contents are saved automatically by default. For open-source users, add the --save argument to manually save the contents of your database:

    voltadmin shutdown [--save]
    
Then you can simply use the start command to restart the database:

    voltdb start [--dir ~/mydb] [--background]
    
Further guidance can be found in the tutorial: https://docs.voltdb.com/tutorial/. For more on the CLI, see the documentation: https://docs.voltdb.com/UsingVoltDB/clivoltdb.php.


Next Steps
====================

### Examples

You can find application examples in the ""examples"" directory inside this VoltDB kit. The Voter app (""examples/voter"") is a great example to start with. See the README to learn what it does and how to get it running.

The ""examples"" directory provides additional examples and a README explaining how to run them.

### Tutorial

The VoltDB Tutorial walks you through building and running your first VoltDB application.

https://docs.voltdb.com/tutorial/

### Documentation

The _Using VoltDB_ guide and supporting documentation is comprehensive and easy to use. It's a great place for broad understanding or to look up something specific.

https://docs.voltdb.com/

### Go Full Cloud

For information on using VoltDB in the Cloud, see the _VoltDB Kubernetes Administrator's Guide_.

https://docs.voltdb.com/KubernetesAdmin/


What's Included
====================

If you have installed VoltDB from the distribution kit, you now have a directory containing this README file and several subdirectories, including:

- **bin** - Scripts for starting VoltDB, bulk loading data, as well as interacting with and managing the running database. Including:
  - bin/voltdb - Start a VoltDB process.
  - bin/voltadmin - CLI to manage a running cluster.
  - bin/sqlcmd - SQL console. 
- **doc** - Documentation, tutorials, and java-doc
- **examples** - Sample programs demonstrating the use of VoltDB
- **lib** - Third party libraries
- **tools** - XML schemas, monitoring plugins, and other tools
- **voltdb** - the VoltDB binary software itself including:
  - log4j files - Logging configuration.
  - voltdbclient-version.jar - Java/JVM client for connecting to VoltDB, including native VoltDB client and JDBC driver.
  - voltdb-version.jar - The full VoltDB binary, including platform-specific native libraries embedded within the jar. This is a superset of the client code and can be used as a native client driver or JDBC driver.


Commercial VoltDB Differences
====================

VoltDB offers sandboxes and a pre-built trial version of VoltDB for application developers who want to try out the product. See the VoltDB website for more information.

https://voltdb.com/

Getting Help & Providing Feedback
====================

If you have any questions or comments about VoltDB, we encourage you to reach out to the VoltDB team and community through our public Slack channel.

http://chat.voltdb.com.


Licensing
====================

This program is free software distributed under the terms of the 
GNU Affero General Public License Version 3. See the accompanying 
LICENSE file for details on your rights and responsibilities with 
regards to the use and redistribution of VoltDB software.
",2023-07-07 18:51:45+00:00
vowpalwabbit,vowpal_wabbit,VowpalWabbit/vowpal_wabbit,"Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.  ",https://vowpalwabbit.org,False,8241,2023-07-05 19:56:58+00:00,2009-07-31 19:36:58+00:00,1956,351,213,28,9.8.0,2023-03-14 17:28:03+00:00,Other,10384,wasm_v0.0.6,128,2023-05-31 15:09:46+00:00,2023-07-07 13:11:13+00:00,2023-06-22 19:57:32+00:00,"<img src=""/logo_assets/vowpal-wabbits-github-logo@3x.png"" height=""auto"" width=""100%"" alt=""Vowpal Wabbit"">

[![Linux build status](https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/23?label=Linux%20build&logo=Azure%20Devops)](https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=23&branchName=master)
[![Windows build status](https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/14?label=Windows%20build&logo=Azure%20Devops)](https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=14&branchName=master)

[![codecov](https://codecov.io/gh/VowpalWabbit/vowpal_wabbit/branch/master/graph/badge.svg)](https://codecov.io/gh/VowpalWabbit/vowpal_wabbit)
[![Total Alerts](https://img.shields.io/lgtm/alerts/g/JohnLangford/vowpal_wabbit.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/JohnLangford/vowpal_wabbit/alerts/)

This is the *Vowpal Wabbit* fast online learning code.

## Why Vowpal Wabbit?
Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning. There is a specific focus on reinforcement learning with several contextual bandit algorithms implemented and the online nature lending to the problem well. Vowpal Wabbit is a destination for implementing and maturing state of the art algorithms with performance in mind.

- **Input Format.** The input format for the learning algorithm is substantially more flexible than might be expected. Examples can have features consisting of free form text, which is interpreted in a bag-of-words way. There can even be multiple sets of free form text in different namespaces.
- **Speed.** The learning algorithm is fast -- similar to the few other online algorithm implementations out there. There are several optimization algorithms available with the baseline being sparse gradient descent (GD) on a loss function.
- **Scalability.** This is not the same as fast. Instead, the important characteristic here is that the memory footprint of the program is bounded independent of data. This means the training set is not loaded into main memory before learning starts. In addition, the size of the set of features is bounded independent of the amount of training data using the hashing trick.
- **Feature Interaction.** Subsets of features can be internally paired so that the algorithm is linear in the cross-product of the subsets. This is useful for ranking problems. The alternative of explicitly expanding the features before feeding them into the learning algorithm can be both computation and space intensive, depending on how it's handled.

[Visit the wiki to learn more.](https://github.com/VowpalWabbit/vowpal_wabbit/wiki)

## Getting Started
For the most up to date instructions for getting started on Windows, MacOS or Linux [please see the wiki](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Getting-started). This includes:

- [Installing with a package manager](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Getting-started)
- [Building](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Building)
- [Tutorial](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Tutorial)
",2023-07-07 18:51:50+00:00
walrus,walrus,fjukstad/walrus,Containerized data analysis pipelines,,False,12,2023-06-22 18:40:51+00:00,2016-10-25 15:10:05+00:00,2,3,1,0,,,MIT License,138,,0,,2023-06-22 18:40:52+00:00,2018-06-13 06:48:33+00:00,"![walrus](misc/walrus-200x200.png)

# walrus
walrus is a small tool for executing data analysis pipelines using Docker
containers. It is very simple: walrus reads a pipeline description from either a
JSON or YAML file and starts Docker containers as described in this file. We
have used walrus to develop analysis pipelines for analyzing whome-exome as well
as RNA sequencing datasets. 

# Pipeline 
A pipeline has a *name*, a list of *pipeline stages*, optional
*comments* and *variables*. See below for an example pipeline. 

## Pipeline stage
A pipeline stage has a *name*, a *Docker image* it is based on, a list of
pipeline stages that it depends on (e.g. if it relies on output from these), and
a *command* it runs on start up. 

## IO
Each pipeline stage should write any output data to the directory
`/walrus/STAGENAME` that is automatically mounted onside the docker container
on start-up. walrus automatically mounts input directories from its dependencies
on start-up at `/walrus/INPUT_STAGENAME`. The user specifies where this
`/walrus` directory is on the host OS by using the `-output` command line flag
(see Usage for more information).
On default it writes everything to a `walrus` directory in the current working
directory of where the user executes the walrus command. 

## Parallelism
Pipeline stages that could be run in parallel are run in parallel by default. 

## Variables
You can declare variables in the pipeline description as well. You declare these
as `{""Name"": ""variableName"", ""Value"": ""variableValue""}` and use them in the
pipeline description by wrapping them like this `{{variableName}}`. See
[pipeline.json](https://github.com/fjukstad/walrus/blob/master/example/fruit_stand_variables/pipeline.json)
for an example. 

# Reproducible pipelines 
## Tools
Since walrus requires that tools are packaged within Docker containers, it
provides a simple mechanism to ensure a reproducible execution envirionment. 

## Parameters
We reccommend that you use [git](https://git-scm.com/) to version control your
pipeline descriptions. This will ensure that you can keep track of the different 
parameters to the different tools as you develop your analysis pipeline.

## Data
walrus automatically tracks data in the pipeline with
[git-lfs](https://git-lfs.github.com/). When users start a pipeline walrus will
track any output data from any of the pipeline stages and commit them to the
repository versioning the pipeline description, If you do not have a repository
walrus will set one up for you.

Using git to version control your pipeline data is completely
optional, and  users can of course opt out of versioning data with `git-lfs` by
using the `walrus -version-control=false` parameter. 

git-lfs requires a server for hosting the large files, and while
[Github](https://help.github.com/articles/about-git-large-file-storage/),
[BitBucket](https://confluence.atlassian.com/bitbucket/git-large-file-storage-in-bitbucket-829078514.html)
provide hosting opportunities, we have added a `-lfs-server` flag that starts a
local [git-lfs-server](https://github.com/fjukstad/lfs-server) for use with
`git-lfs`. Users can use this server to store files with `git-lfs` or push them
to some other remote. 

### Performance
You may experience that `git-lfs` uses some time to start keeping
track of your data. Adding the NA12878 WGS (270GB) bam file takes roughly 1 hour
on our fat server (80 Intel xenon CPUs, 10 cores/CPU, ~1TB memory). Bear in mind
that `git-lfs` runs on a single CPU. Most of the time spent is simply copying
the data into the `.git/lfs` folder. Hopefully this will improve in later
versions of `git-lfs`. 

# Installation and usage
There are two options for installing and using walrus: install walrus and its
dependencies natively on your system, or use our walrus Docker image. It may
sound a bit silly to have a Docker container orchestrate other containers, but
by sharing the Docker socket (`/var/run/docker.sock`) with the walrus container
it works!  There are
[drawbacks](https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container.html)
to sharing the Docker socket and we only encourage this approach if you want to
try out walrus without thinking about setting up your own environment. 


## Docker 
There's only a single command needed to start analyzing data using the walrus
Docker container. Let's assume you have a `pipeline.json` pipeline description
in your working directory. You can analyze it by running

```
    docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):$(pwd) -t fjukstad/walrus -i $(pwd)/pipeline.json -o $(pwd)/output
```

and it will write the output to a directory `output/` in your current working
directory. 


While there's a single command you also have to take special care when
specifying the volumes in your pipeline description.  You must use the full
path, not just relative path, where your data is on your host. 

Below is a short example to analyze the [fruit_stand
example](example/fruit_stand), that assumes that you have downloaded walrus to
your `GOPATH`. Before you can run the pipeline you have to modify one line of
the first stage in [pipeline.json](example/fruit_stand/pipeline.json) from

```
    ""Volumes"": [""data:/data""],
```

to 

```
    ""Volumes"": [""GOPATH/src/github.com/fjukstad/walrus/example/fruit_stand/data:/data""],
```

where you have to substitute `GOPATH` with your actual GOPATH. If your data is
elsewhere you'll have to substitute the path with the full path on your system.
Once you have updated the path you can then run the pipeline using

```
    docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd):$(pwd) -t fjukstad/walrus -i $(pwd)/pipeline.json -o $(pwd)/output
```


## Native
### Prerequisites and dependencies 
We are working on simplifying the installation process. In short you need to
install [go](http://golang.org), [git-lfs](https://git-lfs.github.com/),
[libgit2](https://github.com/libgit2/libgit2),
[git2go](https://github.com/libgit2/git2go), and the Docker Go packages before
you can install walrus. You also need [cmake](https://cmake.org/) to compile
libgit2 (install it via your preferred package manager. In addition to the
instructions below you can also have a look at the [Dockerfile](Dockerfile)
which lists all the necessary commands. 

#### Go 
Follow the [instructions on golang.org](https://golang.org/doc/install) to
install Go. You also need to set up your
[GOPATH](https://github.com/golang/go/wiki/SettingGOPATH). 

#### Libgit2 and git2go
First install `libgit`, specifically version 26.
```
    wget https://github.com/libgit2/libgit2/archive/v0.26.0.zip
    unzip v0.26.0.zip 
    cd libgit2-0.26.0/

    mkdir build && cd build
    cmake ..
    cmake --build . --target install
```

Make sure that you have added the install directory to your `LD_LIBRARY_PATH`
before continuing. For example, like this: 

```
    echo ""export LD_LIBRARY_PATH=/usr/local/lib"" >> ~/.bash_profile
```

After `libgit2` is installed you can install version 26 of `git2go`

```
    go get gopkg.in/libgit2/git2go.v26
```

#### git-lfs
Install `git-lfs` following the instructions on the
[git-lfs](https://git-lfs.github.com/) homepage.

### Docker Go packages 
We need to do some wrangling of the Docker Go packages before we can install
walrus.  First download the packages, then remove the `vendor` directories
before continuing. 

```
    go get -u github.com/docker/docker github.com/docker/distribution
    rm -rf $GOPATH/src/github.com/docker/docker/vendor $GOPATH/src/github.com/docker/distribution/vendor
```

You also need to set your environment variable `DOCKER_API_VERSION=1.35`. 

### walrus 

```
    go get github.com/fjukstad/walrus
```

### Usage 
Once you have installed walrus you can start analyzing data with 

```
    walrus -i $PIPELINE_DESCRIPTION
```

where `$PIPELINE_DESCRIPTION` is the filename of a
pipeline description you've created. For more details run `$ walrus --help`. 

# Example pipeline
Here's a small example pipeline. It consists of two stages: the first writes all
filenames in the `/` directory to a file `/walrus/stage1/file`, the second writes
all filenames with `bin` in the name to a new file `/walrus/stage2/file2`. 

```
name: example
stages:
- name: stage1
  image: ubuntu:latest
  cmd:
  - sh
  - -c
  - ls / > /walrus/stage1/file
- name: stage2
  image: ubuntu:14.04
  cmd:
  - sh
  - -c
  - grep bin /walrus/stage1/file > /walrus/stage2/file2
  inputs:
  - stage1
comment: This is the first example pipeline!
```

# Name
Because every data analysis framework has to be named after a big animal.
Right? 

> There is something remarkably fantastic and prehistoric about these monsters. I could not help thinking of a merman, or something of the kind, as it lay there just under the surface of the water, blowing and snorting for quite a long while at a time, and glaring at us with its round glassy eyes. 
> - Fridtjof Nansen on walruses 
",2023-07-07 18:51:54+00:00
watchdog,watchdog,klugem/watchdog,Workflow management system for the automated and distributed analysis of large-scale experimental data.,,False,12,2021-12-16 14:54:42+00:00,2017-11-29 12:24:41+00:00,4,2,1,13,v2.0.8,2022-04-25 09:13:05+00:00,GNU General Public License v3.0,232,v2.0.8,13,2022-04-25 09:13:05+00:00,,2022-04-25 09:13:05+00:00,"![](https://img.shields.io/github/release/klugem/watchdog.svg) ![](https://img.shields.io/github/workflow/status/klugem/watchdog/mvn_build)

### MANUAL
The most recent version of the manual can be found online at https://www.bio.ifi.lmu.de/files/Software/Watchdog/Watchdog-manual.html and as HTML and PDF version in the documentation folder.

### REQUIREMENTS
Watchdog is written in Java and requires JDK 11 or higher. Oracle provides an installation guide for Windows, Linux and macOS at https://docs.oracle.com/en/java/javase/11/install/overview-jdk-installation.html.

The GUI of the workflow designer and the moduleMaker depend on the JavaFX SDK 11 or higher. The JavaFX SDK is also included with Watchdog and located in _jars/libs/modules/_. This version is used by default. Alternatively, it can be obtained from https://gluonhq.com/products/javafx/. An installation guide is provided here: https://openjfx.io/openjfx-docs/.

### INSTALLATION
- Manually:
    - Download a release from https://github.com/klugem/watchdog/releases
    - Extract the provided archive into a folder of your choice
    - Modules can be obtained from https://github.com/watchdog-wms/watchdog-wms-modules and moved manually into the modules directory or by calling `./modules/downloadCommunityModules.sh` from the Watchdog directory 

- Via conda:
    - `conda install -c bioconda -c conda-forge watchdog-wms`
- Via docker:
    - `docker pull klugem/watchdog-wms`

In case of conda or docker installation, the binaries are named watchdog-cmd and watchdog-gui while the rest of the files are located in _${PREFIX}/share/watchdog-wms-${VERSION}_.

### BUILD WATCHDOG
Watchdog can be build from the source files using Maven. The command `mvn` downloads all dependencies into _jars/libs_ and rebuilds the jar files.

### RUN WATCHDOG
The distributed jar files are build for Java 11 and are called internally by the bash scripts. In order to run a xml-file using the command line tool, call

`./watchdog.sh -x xml-file` # (or java -jar _jars/watchdog.jar_ -x xml-file)

To call the jar file directly is not recommended as CTRL+C signal will be forwarded to all child processes, which might cause errors on locally running tasks.

The workflow designer can be started using 
`./workflowDesigner.sh` # (or java -jar _jars/WatchdogDesigner.jar_)

The workflow designer depends on the Javafx SDK 11 or higher. The bash script will try to identify the installation location of the Javafx SDK automatically.

### JAR FILES
- watchdog.jar - command-line tool that executes Watchdog workflows
- watchdogDesigner.jar - graphical user interface for workflow design and execution
- docuTemplateExtractor.jar - generates templates for module documentation
- refBookGenerator.jar - creates a module reference book based on a set of modules
- reportGenerator.jar - basic reporting of steps performed during execution of a workflow
- moduleValidator.jar - command-line tool that can be used to verify integrity of modules
- workflowValidator.jar - command-line tool that can be used to verify integrity of workflows
- moduleMaker.jar - provides a graphical user interface for module creation

All jar files except moduleMaker.jar are available in the _jars/_ subdirectory of the Watchdog installation folder. The ModuleMaker is not shipped with Watchdog but can be obtained by running `./helper_scripts/downloadModuleMaker.sh` located in the Watchdog installation directory. See https://github.com/watchdog-wms/moduleMaker/blob/master/README.md for more information.

More information on how to use these programmes can be found in the [manual](https://klugem.github.io/watchdog/Watchdog-manual.html#JARs) in section 6.

### GETTING STARTED

Once Watchdog is correctly installed, you can run example workflows shipped with Watchdog. To configure them run `./helper_scripts/configureExamples.sh -i /path/to/install/folder/of/watchdog [-m your@mail-adress.com]`. Afterwards, the example workflows are located in _/path/to/install/folder/of/watchdog/examples_ and can be started using `./watchdog.sh -x path2/xml-file.xml`.

- example_basic_sleep.xml - basic example with one task to show XML workflow structure
- example_dependencies.xml - workflow with dependencies between tasks
- example_execution_environments.xml - workflow using different execution environments (requires modifications)
- example_process_blocks.xml - shows how to work with process sequences, folders and tables 
- example_task_actions.xml - introduces task actions using the example of a file copy action
- example_checkers.xml - shows how to use a custom success checker
- example_docker.xml - uses a module that internally uses a docker image containing bowtie2
- example_include.xml - shows how to use additional module folders
- example_simple_calculations.xml - usage of simple mathematical calculations within a workflow
- example_constant_replacement.xml - shows to to use constants in workflows
- example_environment_variables.xml - setting environment variables
- example_mail_notification.xml - example with mail notifications on completed subtasks and checkpoints
- example_streams.xml - rediction of stdout and stderr streams
- workflow1_basic_information_extraction.xml - simple workflow that extracts information from files using basic UNIX tools
- workflow2_differential_gene_expression.xml - workflow performing a differential gene expression analysis on an example data set (needs bwa and ContextMap2 to be installed and configured)

More information on these example workflows can be found in the [manual](https://klugem.github.io/watchdog/Watchdog-manual.html#getting_started) in section 3 and 4.

An introduction on how to [analyse replicate data](https://klugem.github.io/watchdog/ReplicateAnalysis_Overview.pdf) or how use the [workflow designer (GUI)](https://klugem.github.io/watchdog/WorkflowDesigner_Overview.pdf) can be found in the _documentation/_ folder.

### WATCHDOG COMMUNITY
Two repositories on Github under the watchdog-wms organization (https://github.com/watchdog-wms/) are dedicated for sharing modules and workflows by other users: 
- modules: https://github.com/watchdog-wms/watchdog-wms-modules
- workflows: https://github.com/watchdog-wms/watchdog-wms-workflows

#### CONTACT
If you have any questions or suggestions, please feel free to contact me: michael.kluge@bio.ifi.lmu.de
In case of bugs or feature requests, feel free to create an issues at https://github.com/klugem/watchdog/issues

### LICENCE
Watchdog is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

Watchdog is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with Watchdog.  If not, see <http://www.gnu.org/licenses/>.

Licenses of libraries Watchdog depends on can be found in _jars/libs/_.
",2023-07-07 18:51:57+00:00
wdl,wdl,openwdl/wdl,Workflow Description Language - Specification and Implementations,https://www.openwdl.org/,False,666,2023-07-06 21:14:41+00:00,2012-08-01 03:12:48+00:00,300,88,48,0,,,"BSD 3-Clause ""New"" or ""Revised"" License",606,release-1.1.0,6,2023-03-24 17:30:05+00:00,2023-06-28 15:55:55+00:00,2023-05-25 17:30:17+00:00,"Workflow Description Language (WDL)
========================================

The **Workflow Description Language (WDL)** is a way to specify data processing workflows with a human-readable and writeable syntax. WDL makes it straightforward to define complex analysis tasks, chain them together in workflows, and parallelize their execution. The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable.

# Language Specifications:

The current version of the WDL language is **1.1**. The [1.1 specification](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md) contains all relevant information for users, developers, and engine developers. Upcoming features which have previously been accepted can be viewed as part of the [development spec](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md). 

There are a number of draft versions (draft 1 - 3) which correspond to our initial efforts at creating WDL. While these are functional specifications, they should not be considered feature complete and contain many bugs and irregularities. Unless absolutely necessary, we would recommend that users should start with the current version of the language.


# Community and Support

There are a number of places to ask questions and get involved within the WDL community. Our community thrives the more you get involved and we encourage you to ask questions, provide answers, and make contributions.


- [Mailing list](https://groups.google.com/a/openwdl.org/forum/#!forum/community) - Joining our google group allows you to stay up to date with recent developments, be informed when new PR's are ready for voting, and participate in broader discussions about the language.
- [Issues](https://github.com/OpenWDL/wdl/issues) - Any bugs, ambiguity, or problems with the specification you encounter should be reported here. You can also create issues which are feature requests, however the most likely way to get a feature into the spec is by creating a PR yourself.
- [Slack Channel](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw) - Live chat with WDL users
- [Support Forum](https://bioinformatics.stackexchange.com/search?q=wdl) - View Previously answered questions about WDL or pose new questions. 
- [User Guide](https://support.terra.bio/hc/en-us/sections/360007274612-WDL-Documentation) (hosted by the Broad) View a general user guide and simple how-to for WDL

# Published Workflows 

There are many WDL's that have previously been published which provide a good starting point to extend or use as is to fit your workflow needs. While many of these workflows are scattered across the web and in many different repositories, you can find a great selection of high quality, published WDL's available at [Dockstore](https://dockstore.org/search?entryType=workflows&descriptorType=WDL&searchMode=files) as well as a large number of workflows and tasks at [BioWDL](https://github.com/biowdl).

Additionally, you can view and test out a number of different workflow's using [Terra](https://app.terra.bio). Please note, that you have to register with Terra in order to view the workflows.


# Software and Tools

### Execution Engines

WDL is not executable in and of itself, but requires an execution engine to run. Compliant executions engines should support the features of a specific version of the WDL specification. Please see the corresponding engine documentation for information on available execution options and support. 

- [Cromwell](https://github.com/broadinstitute/cromwell)
- [MiniWDL](https://github.com/chanzuckerberg/miniwdl)
- [dxWDL](https://github.com/dnanexus/dxWDL)


### Parsers and Language Support

- Basic parsers and their grammar definitions (based on hermes) can be found in the `parsers/` directory for each respective version. Currently there is support for java, python and javascript. We believe these parsers work, however have not validated these claims.
- [MiniWDL](https://github.com/chanzuckerberg/miniwdl) - MiniWDL provides python bindings for WDL as well as command line validation. It is light weight and easy to use.
- [WOMTool](https://cromwell.readthedocs.io/en/stable/WOMtool/) - a standalone tool for parsing, validating, linting, and generating a graph of a WDL.
- [wdl-aid](https://github.com/biowdl/wdl-aid) - generate documentation for the inputs of WDL workflows, based on the parameter_meta information defined in the WDL file.	
- [wdlTools](https://github.com/dnanexus-rnd/wdlTools) - provides 1) a parser library, based on the new [ANTLR4](https://github.com/openwdl/wdl/tree/main/versions/1.0/parsers/antlr4) grammars, for WDL draft-2, 1.0, and development, and 2) command-line tools for sytanx checking, type-checking, linting, code formatting (including upgrading from older to newer WDL versions), generating documentation, and executing WDL tasks locally.

### IDE Support

- Visual Studio Code: [WDL Syntax Highlighter](https://marketplace.visualstudio.com/items?itemName=broadinstitute.wdl)
- JetBrains IDE's: [Winstanly](https://plugins.jetbrains.com/plugin/8154-winstanley-wdl)
- Atom: [Language-WDL](https://atom.io/packages/language-wdl)
- Vim: [vim-wdl](https://github.com/broadinstitute/vim-wdl)

### Documentation

- [wdldoc](https://github.com/stjudecloud/wdldoc)

### Test tools

- [Pytest-workflow](https://github.com/LUMC/pytest-workflow) - workflow-engine agnostic workflow tester. Can be used with both Cromwell and MiniWDL. Tests are specified in YAML format. Uses pytest as underlying test framework. Allows for using python 
code tests in case the standard simple YAML tests are not sufficient.
- [Pytest-wdl](https://github.com/EliLillyCo/pytest-wdl) This package is a plugin for the pytest unit testing framework that enables testing of workflows written in Workflow Description Language.

### Packaging

- [wdl-packager](https://github.com/biowdl/wdl-packager). WDL packaging utility that uses miniwdl to find which paths are imported and packages these into a zip 
  together with the calling workflow. The zip can be used as an imports zip package for cromwell. The utility can add non-WDL files (such as the license) to the
  zip package and provides options to package the zip in a binary reproducible way.

# Contributing

WDL only advances through community contributions. While submitting an issue is a great way to report a bug in the spec, or create disscussion around current or new features, it will ultimately not translate into an actual change in the spec. The best way to make changes is by submitting a PR. For more information on how you can contribute, please see the [Contributing](CONTRIBUTING.md) readme. 

Additionally, once a PR has been submitted, it will be subjected to our [RFC Process](RFC.md).

# Governance

The WDL specification is entirely community driven, however it is overseen by a governance committee. For more information please see the [Governance](GOVERNANCE.md) documentation.

# RFC Process

Any changes submitted to the WDL Specification are subject to the [RFC Process](RFC.md). Please review and familiarize yourself with the process if you would like to see changes submitted to the specification.
",2023-07-07 18:52:01+00:00
whylogs,whylogs,whylabs/whylogs,The open standard for data logging,https://whylogs.readthedocs.io/,False,2300,2023-07-07 13:56:20+00:00,2020-08-14 23:25:32+00:00,98,32,19,117,v1.2.1,2023-07-04 19:34:03+00:00,Apache License 2.0,700,v1.2.1,122,2023-07-04 19:34:03+00:00,2023-07-07 08:23:13+00:00,2023-07-06 01:58:47+00:00,"<img src=""https://static.scarf.sh/a.png?x-pxid=bc3c57b0-9a65-49fe-b8ea-f711c4d35b82"" /><p align=""center"">
<img src=""https://i.imgur.com/nv33goV.png"" width=""35%""/>
</br>

<h1 align=""center"">The open standard for data logging

 </h1>
  <h3 align=""center"">
   <a href=""https://whylogs.readthedocs.io/""><b>Documentation</b></a> &bull;
   <a href=""https://bit.ly/whylogsslack""><b>Slack Community</b></a> &bull;
   <a href=""https://github.com/whylabs/whylogs#python-quickstart""><b>Python Quickstart</b></a> &bull;
   <a href=""https://whylogs.readthedocs.io/en/latest/examples/integrations/writers/Writing_to_WhyLabs.html""><b>WhyLabs Quickstart</b></a>
 </h3>

<p align=""center"">
<a href=""https://github.com/whylabs/whylogs-python/blob/mainline/LICENSE"" target=""_blank"">
    <img src=""http://img.shields.io/:license-Apache%202-blue.svg"" alt=""License"">
</a>
<a href=""https://badge.fury.io/py/whylogs"" target=""_blank"">
    <img src=""https://badge.fury.io/py/whylogs.svg"" alt=""PyPi Version"">
</a>
<a href=""https://github.com/python/black"" target=""_blank"">
    <img src=""https://img.shields.io/badge/code%20style-black-000000.svg"" alt=""Code style: black"">
</a>
<a href=""https://pepy.tech/project/whylogs"" target=""_blank"">
    <img src=""https://pepy.tech/badge/whylogs"" alt=""PyPi Downloads"">
</a>
<a href=""bit.ly/whylogs"" target=""_blank"">
    <img src=""https://github.com/whylabs/whylogs-python/workflows/whylogs%20CI/badge.svg"" alt=""CI"">
</a>
<a href=""https://codeclimate.com/github/whylabs/whylogs-python/maintainability"" target=""_blank"">
    <img src=""https://api.codeclimate.com/v1/badges/442f6ca3dca1e583a488/maintainability"" alt=""Maintainability"">
</a>
</p>

## What is whylogs

whylogs is an open source library for logging any kind of data. With whylogs, users are able to generate summaries of their datasets (called _whylogs profiles_) which they can use to:

1. Track changes in their dataset
2. Create _data constraints_ to know whether their data looks the way it should
3. Quickly visualize key summary statistics about their datasets

These three functionalities enable a variety of use cases for data scientists, machine learning engineers, and data engineers:

- Detect data drift in model input features
- Detect training-serving skew, concept drift, and model performance degradation
- Validate data quality in model inputs or in a data pipeline
- Perform exploratory data analysis of massive datasets
- Track data distributions & data quality for ML experiments
- Enable data auditing and governance across the organization
- Standardize data documentation practices across the organization
- And more

<a href=""https://hub.whylabsapp.com/signup"" target=""_blank"">
    <img src=""https://user-images.githubusercontent.com/7946482/193939278-66a36574-2f2c-482a-9811-ad4479f0aafe.png"" alt=""WhyLabs Signup"">
</a>

If you have any questions, comments, or just want to hang out with us, please join [our Slack Community](https://bit.ly/rsqrd-slack). In addition to joining the Slack Community, you can also help this project by giving us a ⭐ in the upper right corner of this page.

## Python Quickstart<a name=""python-quickstart"" />

Installing whylogs using the pip package manager is as easy as running `pip install whylogs` in your terminal.

From here, you can quickly log a dataset:

```python
import whylogs as why
import pandas as pd

#dataframe
df = pd.read_csv(""path/to/file.csv"")
results = why.log(df)
```

And voilà, you now have a whylogs profile. To learn more about what a whylogs profile is and what you can do with it, read on.

## Table of Contents

- [whylogs Profiles](#whylogs-profiles)
- [Data Constraints](#data-constraints)
- [Profile Visualization](#profile-visualization)
- [Integrations](#integrations)
- [Supported Data Types](#data-types)
- [Examples](#examples)
- [Usage Statistics](#usage-statistics)
- [Community](#community)
- [Contribute](#contribute)

## whylogs Profiles<a name=""whylogs-profiles"" />

### What are profiles

whylogs profiles are the core of the whylogs library. They capture key statistical properties of data, such as the distribution (far beyond simple mean, median, and standard deviation measures), the number of missing values, and a wide range of configurable custom metrics. By capturing these summary statistics, we are able to accurately represent the data and enable all of the use cases described in the introduction.

whylogs profiles have three properties that make them ideal for data logging: they are **efficient**, **customizable**, and **mergeable**.

<br />

<img align=""left"" src=""https://user-images.githubusercontent.com/7946482/171064257-26bf727e-3480-4ec3-9c9d-5d8a79567bca.png"">

**Efficient**: whylogs profiles efficiently describe the dataset that they represent. This high fidelity representation of datasets is what enables whylogs profiles to be effective snapshots of the data. They are better at capturing the characteristics of a dataset than a sample would be—as discussed in our [Data Logging: Sampling versus Profiling](https://whylabs.ai/blog/posts/data-logging-sampling-versus-profiling) blog post—and are very compact.

<br />

<img align=""left"" src=""https://user-images.githubusercontent.com/7946482/171064575-72ee0f76-7365-4fd1-9cab-4debb673baa8.png"">

**Customizable**: The statistics that whylogs profiles collect are easily configured and customizable. This is useful because different data types and use cases require different metrics, and whylogs users need to be able to easily define custom trackers for those metrics. It’s the customizability of whylogs that enables our text, image, and other complex data trackers.

<br />

<img align=""left"" src=""https://user-images.githubusercontent.com/7946482/171064525-2d314534-6cdb-4c07-9d9f-5c74d5c03029.png"">

**Mergeable**: One of the most powerful features of whylogs profiles is their mergeability. Mergeability means that whylogs profiles can be combined together to form new profiles which represent the aggregate of their constituent profiles. This enables logging for distributed and streaming systems, and allows users to view aggregated data across any time granularity.

<br />

### How do you generate profiles

Once whylogs is installed, it's easy to generate profiles in both Python and Java environments.

To generate a profile from a Pandas dataframe in Python, simply run:

```python
import whylogs as why
import pandas as pd

#dataframe
df = pd.read_csv(""path/to/file.csv"")
results = why.log(df)
```

<!---
For images, replace `df` with `image=""path/to/image.png""`. Similarly, you can profile Python dicts by replacing the dataframe within the `log()` function with a Python `dict` object.
--->

### What can you do with profiles

Once you’ve generated whylogs profiles, a few things can be done with them:

In your local Python environment, you can set data constraints or visualize your profiles. Setting data constraints on your profiles allows you to get notified when your data don’t match your expectations, allowing you to do data unit testing and some baseline data monitoring. With the Profile Visualizer, you can visually explore your data, allowing you to understand it and ensure that your ML models are ready for production.

In addition, you can send whylogs profiles to the SaaS ML monitoring and AI observability platform [WhyLabs](https://whylabs.ai). With WhyLabs, you can automatically set up monitoring for your machine learning models, getting notified on both data quality and data change issues (such as data drift). If you’re interested in trying out WhyLabs, check out the always free [Starter edition](https://whylabs.ai/free), which allows you to experience the entire platform’s capabilities with no credit card required.

## WhyLabs<a name=""whylabs"" />

WhyLabs is a managed service offering built for helping users make the most of their whylogs profiles. With WhyLabs, users can ingest profiles and set up automated monitoring as well as gain full observability into their data and ML systems. With WhyLabs, users can ensure the reliability of their data and models, and debug any problems that arise with them.

Ingesting whylogs profiles into WhyLabs is easy. After obtaining your access credentials from the platform, you’ll need to set them in your Python environment, log a dataset, and write it to WhyLabs, like so:

```python
import whylogs as why
import os

os.environ[""WHYLABS_DEFAULT_ORG_ID""] = ""org-0"" # ORG-ID is case-sensitive
os.environ[""WHYLABS_API_KEY""] = ""YOUR-API-KEY""
os.environ[""WHYLABS_DEFAULT_DATASET_ID""] = ""model-0"" # The selected model project ""MODEL-NAME"" is ""model-0""

results = why.log(df)

results.writer(""whylabs"").write()
```

![image](<https://github.com/whylabs/whylogs/raw/assets/images/chrome-capture-2022-9-4%20(1).gif>)

If you’re interested in trying out WhyLabs, check out the always free [Starter edition](https://hub.whylabsapp.com/signup), which allows you to experience the entire platform’s capabilities with no credit card required.

## Data Constraints<a name=""data-constraints"" />

Constraints are a powerful feature built on top of whylogs profiles that enable you to quickly and easily validate that your data looks the way that it should. There are numerous types of constraints that you can set on your data (that numerical data will always fall within a certain range, that text data will always be in a JSON format, etc) and, if your dataset fails to satisfy a constraint, you can fail your unit tests or your CI/CD pipeline.

A simple example of setting and testing a constraint is:

```python
import whylogs as why
from whylogs.core.constraints import Constraints, ConstraintsBuilder
from whylogs.core.constraints.factories import greater_than_number

profile_view = why.log(df).view()
builder = ConstraintsBuilder(profile_view)
builder.add_constraint(greater_than_number(column_name=""col_name"", number=0.15))

constraints = builder.build()
constraints.report()
```

To learn more about constraints, check out: the [Constraints Example](https://bit.ly/whylogsconstraintsexample).

## Profile Visualization<a name=""profile-visualization"" />

In addition to being able to automatically get notified about potential issues in data, it’s also useful to be able to inspect your data manually. With the profile visualizer, you can generate interactive reports about your profiles (either a single profile or comparing profiles against each other) directly in your Jupyter notebook environment. This enables exploratory data analysis, data drift detection, and data observability.

To access the profile visualizer, install the `[viz]` module of whylogs by running `pip install ""whylogs[viz]""` in your terminal. One type of profile visualization that we can create is a drift report; here's a simple example of how to analyze the drift between two profiles:

```python
import whylogs as why

from whylogs.viz import NotebookProfileVisualizer

result = why.log(pandas=df_target)
prof_view = result.view()

result_ref = why.log(pandas=df_reference)
prof_view_ref = result_ref.view()

visualization = NotebookProfileVisualizer()
visualization.set_profiles(target_profile_view=prof_view, reference_profile_view=prof_view_ref)

visualization.summary_drift_report()
```

![image](https://user-images.githubusercontent.com/7946482/169669536-a25cce95-acde-4637-b7b9-c2a685f0bc3f.png)

To learn more about visualizing your profiles, check out: the [Visualizer Example](https://bit.ly/whylogsvisualizerexample)

## Data Types<a name=""data-types"" />

whylogs supports both structured and unstructured data, specifically:

| Data type        | Features | Notebook Example                                                                                                                                                |
| ---------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Tabular Data     | ✅       | [Getting started with structured data](https://github.com/whylabs/whylogs/blob/mainline/python/examples/basic/Getting_Started.ipynb)                            |
| Image Data       | ✅       | [Getting started with images](https://github.com/whylabs/whylogs/blob/mainline/python/examples/advanced/Image_Logging.ipynb)                                    |
| Text Data        | ✅       | [String Features](https://github.com/whylabs/whylogs/blob/maintenance/0.7.x/examples/String_Features.ipynb)                                                     |
| Embeddings       | ✅       | [Embeddings Distance Logging](https://github.com/whylabs/whylogs/blob/mainline/python/examples/experimental/embeddings/Embeddings_Distance_Logging.ipynb)       |
| Other Data Types | ✋       | Do you have a request for a data type that you don’t see listed here? Raise an issue or join our Slack community and make a request! We’re always happy to help |

## Integrations

![current integration](https://user-images.githubusercontent.com/7946482/171062942-01c420f2-7768-4b7c-88b5-e3f291e1b7d8.png)

whylogs can seamslessly interact with different tooling along your Data and ML pipelines. We have currently built integrations with:

- AWS S3
- Apache Airflow
- Apache Spark
- Mlflow
- GCS

and much more!

If you want to check out our complete list, please refer to our [integrations examples](https://github.com/whylabs/whylogs/tree/mainline/python/examples/integrations) page.

## Examples

For a full set of our examples, please check out the [examples folder](https://github.com/whylabs/whylogs/tree/mainline/python/examples).

## Benchmarks of whylogs

By design, whylogs run directly in the data pipeline or in a sidecar container and use highly scalable streaming algorithms to compute statistics. Since data logging with whylogs happens in the same infrastructure where the raw data is being processed, it's important to think about the compute overhead. For the majority of use cases, the overhead is minimal, usually under 1%. For very large data volumes with thousands of features and 10M+ QPS it can add ~5% overhead. However, for large data volumes, customers are typically in a distributed environment, such as Ray or Apache Spark. This means they benefit from whylogs parallelization—and the map-reducible property of the whylogs profiles keeping the compute overhead to a minimum.
Below are benchmarks to demonstrate how efficient whylogs is at processing tabular data with default configurations (tracking distributions, missing values, counts, cardinality, and schema). Two important advantages of this approach are that parallelization speeds up the calculation and whylogs scales with the number of features, rather than the number of rows. Learn more about how whylogs scales here.

| DATA VOLUME                    |       TOTAL COST OF RUNNING WHYLOGS        |                        INSTANCE TYPE                         | CLUSTER SIZE |                                                  PROCESSING TIME |
| ------------------------------ | :----------------------------------------: | :----------------------------------------------------------: | -----------: | ---------------------------------------------------------------: |
| 10 GB ~34M rows x 43 columns   |    ~ $ 0.026 per 10 GB, or $2.45 per TB    | c5a.2xlarge, 8 CPU 16GB RAM, $0.308 on demand price per hour |  2 instances | 2.6 minutes of profiling time per instance (running in parallel) |
| 10 GB, ~34M rows x 43 columns  | ~ $0.016 per 10 GB, estimated $1.60 per TB | c6g.2xlarge, 8 CPU 16GB RAM, $0.272 on demand price per hour |  2 instances | 1.7 minutes of profiling time per instance (running in parallel) |
| 10 GB ~34M rows x 43 columns   |            ~ $ 0.045 per 10 GB             | c5a.2xlarge, 8 CPU 16GB RAM, $0.308 on demand price per hour | 16 instances |  33 seconds of profiling time per instance (running in parallel) |
| 80 GB, 83M rows x 119 columns  |             ~ $0.139 per 80 GB             | c5a.2xlarge, 8 CPU 16GB RAM, $0.308 on demand price per hour | 16 instances | 1.7 minutes of profiling time per instance (running in parallel) |
| 100 GB, 290M rows x 43 columns |            ~ $0.221 per 100 GB             | c5a.2xlarge, 8 CPU 16GB RAM, $0.308 on demand price per hour | 16 instances | 2.7 minutes of profiling time per instance (running in parallel) |

## Usage Statistics<a name=""whylogs-profiles"" />

Starting with whylogs v1.0.0, whylogs by default collects anonymous information about a user’s environment. These usage statistics do not include any information about the users or the data they are profiling, only the environment in which the user is running whylogs.

To read more about what usage statistics whylogs collects, check out the relevant [documentation](https://docs.whylabs.ai/docs/usage-statistics/).

To turn off Usage Statistics, simply set the `WHYLOGS_NO_ANALYTICS` environment variable to True, like so:

```python
import os
os.environ['WHYLOGS_NO_ANALYTICS']='True'
```

## Community

If you have any questions, comments, or just want to hang out with us, please join [our Slack channel](http://join.slack.whylabs.ai/).

## Contribute

### How to Contribute

We welcome contributions to whylogs. Please see our [contribution guide](https://github.com/whylabs/whylogs/blob/mainline/.github/CONTRIBUTING.md) and our [development guide](https://github.com/whylabs/whylogs/blob/mainline/.github/DEVELOPMENT.md) for details.

### Contributors

<a href=""https://github.com/whylabs/whylogs/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=whylabs/whylogs"" />
</a>

Made with [contrib.rocks](https://contrib.rocks).
",2023-07-07 18:52:05+00:00
wick,wick,candlecorp/wick,"Wick is a flow-like runtime for building secure, scalable software services out of WebAssembly components.",https://candle.dev,False,155,2023-07-04 12:47:42+00:00,2021-10-28 13:13:48+00:00,11,3,4,17,0.9.0,2023-06-12 18:51:23+00:00,Other,762,wick-xdg-v0.2.0,121,2023-04-18 17:37:26+00:00,2023-07-07 18:41:11+00:00,2023-07-07 14:34:12+00:00,"<a name=""readme-top""></a>

[![Contributors][contributors-shield]][contributors-url]
[![Issues][issues-shield]][issues-url]
[![Chat][discord-shield]][discord-url]
[![Build][build-shield]][build-url]
[![Version][version-shield]][version-url]
[![Twitter][twitter-shield]][twitter-url]
[![LinkedIn][linkedin-shield]][linkedin-url]


<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" width=""50%"" srcset=""https://github.com/candlecorp/.github/blob/main/assets/wick_logo_light@.5.png?raw=true"">
    <img alt=""wick logo"" width=""50%"" src=""https://github.com/candlecorp/.github/blob/main/assets/wick_logo.png@.5.png?raw=true"">
  </picture>

  <p align=""center"">
    A flow-based runtime for WebAssembly components.
    <br />
    <a href=""https://github.com/candlecorp/wick""><strong>Explore the docs »</strong></a>
    <br />
    <br />
    <a href=""#installation"">Install</a>
    ·
    <a href=""https://github.com/candlecorp/wick/issues"">Report Bug</a>
    ·
    <a href=""https://github.com/candlecorp/wick/issues"">Request Feature</a>
  </p>
</div>



<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href=""#about-the-project"">About The Project</a>
      <ul>
        <li><a href=""#built-with"">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href=""#getting-started"">Getting Started</a>
      <ul>
        <li><a href=""#installation"">Installation</a></li>
      </ul>
    </li>
    <li><a href=""#usage"">Usage</a></li>
    <li><a href=""#roadmap"">Roadmap</a></li>
    <li><a href=""#contributing"">Contributing</a></li>
    <li><a href=""#license"">License</a></li>
    <li><a href=""#contact"">Contact</a></li>
    <li><a href=""#acknowledgments"">Acknowledgments</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project

Wick is a low-code, flow-like runtime for stitching together WebAssembly components into full applications. Built With ❤️, Rust, and Wasm.

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- GETTING STARTED -->
## Getting Started

You'll need the `wick` binary to run Wick applications or invoke Wick components. You can install it with one of the following methods:

### Installation

Pick your installation preference:

1. [Cargo](#install-with-cargo)
2. [Homebrew](#install-with-homebrew)
3. [Pre-built binaries](#install-pre-built-binaries)
4. [Install from source](#install-from-source)

#### Install with Cargo

```
cargo install wick-cli
```

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

#### Install with Homebrew

```
brew install candlecorp/tap/wick
```

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

#### Install pre-built binaries

##### Mac/Linux

```
curl -sSL sh.wick.run | bash
```

##### Windows

```
curl https://ps.wick.run -UseBasicParsing | Invoke-Expression
```

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

#### Install from source

```sh
git clone https://github.com/candlecorp/wick.git && cd wick
just deps # install necessary dependencies
just install # or cargo install --path crates/bins/wick
```

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

<!-- USAGE EXAMPLES -->
## Usage

We're constantly adding examples to the [./examples](https://github.com/candlecorp/wick/tree/main/examples) directory which we also use as a base for our integration tests.

_For more information, please refer to the [Documentation](https://github.com/candlecorp/wick/wiki)_

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- ROADMAP -->
## Roadmap

- [X] HTTP Trigger
- [X] Cron Trigger
- [X] CLI Trigger
- [ ] WebAssembly Component-model support
- [ ] Expand automatic API generation
- [ ] WebSocket support
    - [ ] WebTransport as support improves

See the [open issues](https://github.com/candlecorp/wick/issues) for a full list of proposed features (and known issues).

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag ""enhancement"".
Don't forget to give the project a star! Thanks again!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- LICENSE -->
## License

Wick is distributed under the Elastic License 2.0 and Apache-2.0 licenses. See `LICENSE` for more information and individual crates for details.

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- CONTACT -->
## Contact

Your Name - [@candle_corp](https://twitter.com/@candle_corp) - jarrod@candle.dev

Project Link: [https://github.com/candlecorp/wick](https://github.com/candlecorp/wick)

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[tests-shield]: https://img.shields.io/github/actions/workflow/status/candlecorp/wick/mono_workflow.yaml
[contributors-shield]: https://img.shields.io/github/contributors/candlecorp/wick.svg
[contributors-url]: https://github.com/candlecorp/wick/graphs/contributors
[version-shield]: https://img.shields.io/crates/v/wick-cli
[version-url]: https://crates.io/crates/wick-cli
[build-shield]: https://img.shields.io/github/actions/workflow/status/candlecorp/wick/mono_workflow.yaml
[build-url]: https://github.com/candlecorp/wick/actions/workflows/mono_workflow.yaml
[discord-shield]: https://img.shields.io/discord/909866379904167947
[discord-url]: https://discord.gg/candle
[issues-shield]: https://img.shields.io/github/issues/candlecorp/wick.svg
[issues-url]: https://github.com/candlecorp/wick/issues
[license-shield]: https://img.shields.io/crates/l/wick-cli
[license-shield2]: https://img.shields.io/crates/l/wick-component
[license-url]: https://github.com/candlecorp/wick/blob/master/LICENSE
[twitter-shield]: https://img.shields.io/badge/-Twitter-black.svg?logo=twitter&colorB=555
[twitter-url]: https://www.twitter.com/@wickwasm
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?logo=linkedin&colorB=555
[linkedin-url]: https://www.linkedin.com/company/candlecorp
[product-screenshot]: images/screenshot.png
[Rust-badge]: https://img.shields.io/badge/rust-000000?style=for-the-badge&logo=rust&logoColor=white
[Rust-url]: https://www.rust-lang.org
[Wasm-badge]: https://img.shields.io/badge/wasm-000000?style=for-the-badge&logo=webassembly&logoColor=white
[Wasm-url]: https://webassembly.org
",2023-07-07 18:52:10+00:00
wings,wings,KnowledgeCaptureAndDiscovery/wings,Wings workflow system,http://www.wings-workflows.org,False,45,2023-05-08 08:37:56+00:00,2013-08-29 12:24:56+00:00,24,12,7,30,5.4.3,2023-06-20 15:47:45+00:00,Apache License 2.0,637,untagged-779dcb7fc9dc5bbd3a47,34,2019-11-07 19:17:10+00:00,2023-06-20 16:09:26+00:00,2023-06-20 15:47:45+00:00,"[![Test](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml/badge.svg)](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml)

# Wings

## Installation

### Docker 

You must install [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/install/).

Clone the repository

```bash
$ git clone https://github.com/KnowledgeCaptureAndDiscovery/wings.git
```

Deploy the container with the following command:

```bash
$ docker-compose up -d
```


Open the browser [http://localhost:8080/wings-portal](http://localhost:8080/wings-portal) to access the Wings portal.


Go to [README Docker](wings-docker/) for additional instructions on running the Docker image.


#### Images

Docker images are available at [DockerHub](https://hub.docker.com/repository/docker/ikcap/wings)

### Maven

Please follow the instructions in [README Maven](docs/maven.md) to install the Wings project.
",2023-07-07 18:52:14+00:00
wopmars,wopmars,aitgon/wopmars,,,False,1,2022-11-28 06:41:09+00:00,2017-09-26 13:28:07+00:00,2,1,2,6,0.1.5,2022-05-11 05:42:00+00:00,MIT License,698,0.1.5,19,2022-05-11 05:41:59+00:00,,2022-05-11 05:41:59+00:00,"WopMars: Workflow Python Manager for Reproducible Science
======================================================================

.. image:: https://img.shields.io/pypi/v/wopmars.svg
    :target: https://pypi.python.org/pypi/wopmars

.. image:: https://img.shields.io/pypi/pyversions/wopmars.svg
    :target: https://www.python.org

.. image:: https://readthedocs.org/projects/wopmars/badge/?version=latest
    :target: http://wopmars.readthedocs.io/en/latest/?badge=latest

.. image:: https://github.com/aitgon/wopmars/workflows/CI/badge.svg
    :target: https://github.com/aitgon/wopmars/actions?query=branch%3Amaster+workflow%3ACI

.. image:: https://codecov.io/gh/aitgon/wopmars/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/aitgon/wopmars

.. image:: https://static.pepy.tech/personalized-badge/wopmars?period=month&units=international_system&left_color=black&right_color=blue&left_text=Downloads
    :target: https://pepy.tech/project/wopmars

WopMars is a database-driven workflow manager written in python similar to GNU Makefile or Snakemake.
The difference is that the definition file of WopMars takes into account input/output SQLITE table defined as python
paths to SQLAlchemy models.

You can install the latest WopMars version via ""pypi"":

.. code-block:: bash

    pip install wopmars

Run a quick example:

.. code-block:: bash

    wopmars example
    cd example
    pip install -e .
    wopmars -D sqlite:///db.sqlite -w Wopfile.yml -v

If there were not errors, you can look at the content of the sqlite db.

.. code-block:: bash

    $ sqlite3 db.sqlite '.tables'

    Piece                            wom_TableInputOutputInformation
    PieceCar                         wom_TableModificationTime
    wom_Execution                    wom_ToolWrapper
    wom_FileInputOutputInformation   wom_TypeInputOrOutput
    wom_Option

    $ sqlite3 db.sqlite ""select * from Piece limit 5""

    1|UC8T9P7D0F|wheel|664.24
    2|2BPN653B9D|engine|550.49
    3|T808AHY3DS|engine|672.09
    4|977FPG7QJZ|bodywork|667.23
    5|KJ6WPB3N56|engine|678.83

The `WopMars documentation <http://wopmars.readthedocs.org/>`_ with user guide and
API reference is hosted at Read The Docs.

",2023-07-07 18:52:18+00:00
xbowflow,Project-Xbow,ChrisSuess/Project-Xbow,A computional chemistry cloud based project.,,False,7,2023-01-30 08:47:10+00:00,2017-09-11 14:00:12+00:00,2,3,3,0,,,,576,0.1,1,2017-11-30 13:52:43+00:00,,2020-02-10 11:18:53+00:00,"Crossbow
============

**Crossbow** is an EPSRC-funded project to develop software that makes it easier for Biomolecular Simulation scientists to use
Cloud computing resources.

This repository contains two linked packages: [xbow](https://github.com/ChrisSuess/Project-Xbow/tree/master/xbow) and [xbowflow](https://github.com/ChrisSuess/Project-Xbow/tree/master/xbowflow).

[**Xbow**](https://github.com/ChrisSuess/Project-Xbow/tree/master/xbow) provides tools that enable a computational scientist with little experience of cloud computing to launch a personal, 
tailored, compute cluster in the cloud, and to run simple jobs on it (e.g. molecular dynamics simulations) directly from their
local workstation/laptop.

[**Xbowflow**](https://github.com/ChrisSuess/Project-Xbow/tree/master/xbowflow) provides tools that allows more experienced biomolecular simulation scientists to create custom workflows that
run complex large-scale calculations across an **xbow** cluster.

Contacts:

 - Christian Suess (christian.suess@nottingham.ac.uk)
 - Charlie Laughton (charles.laughton@nottingham.ac.uk)

",2023-07-07 18:52:22+00:00
xnatpipelineengine,xnat-pipeline-engine,NrgXnat/xnat-pipeline-engine,XNAT Pipeline Engine and core pipelines,,False,4,2022-11-01 14:03:22+00:00,2017-03-15 15:14:38+00:00,7,8,8,2,1.8.3,2021-10-18 22:12:43+00:00,,147,1.8.8,15,2023-05-01 17:43:04+00:00,2023-05-08 21:57:30+00:00,2023-05-01 17:43:04+00:00,"XNAT Pipeline Engine
====================

Version 1.8.3
Neuroinformatics Research Group (https://nrg.wustl.edu), Washington University in Saint Louis

Developers
----------

* Mohana Ramaratnam, mohanakannan9@gmail.com
* Rick Herrick, rick.herrick@wustl.edu
* John Flavin, jflavin@wustl.edu

This repository was copied from a [previous repository on Bitbucket](https://bitbucket.org/xnatdev/xnat-pipeline), which was itself copied from a different [previous repository on Bitbucket](https://bitbucket.org/nrg/pipeline_1_7dev).

Installation
------------

This version of the XNAT pipeline engine is built and installed differently from earlier
versions. It uses a [Gradle-based](https://www.gradle.org) build script to manage the pipeline
resources and dependencies. More importantly, instead of building directly back into the
pipeline installation folder, it builds to another destination folder. You can specify the
build property destination with the path to the destination folder or just use the default
destination, which is the folder **build/pipeline** in the pipeline build folder.

You can build the pipeline engine by three different methods:

1. Call the **setup.sh** or **setup.bat** script, passing the administrator email, mail server,
and XNAT server address as parameters (you can also add the optional XNAT site name parameter):

        ./setup.sh <admin email> <SMTP server> <XNAT url> [XNAT site name] [destination] [modulePath1 modulePath2 modulePath3... modulePathn]

        ./setup.sh you@yourlab.org mail.yourlab.org https://xnat.yourlab.org YourXNAT

2. Call the **gradlew** or **gradlew.bat** script located in the root folder of the pipeline
build, along with values for all of the required and optional build parameters. Note that
Gradle build parameters are passed in the form **-P**_param_=_value_.

        ./gradlew -PadminEmail=you@yourlab.org -Pdestination=/pipeline/folder \
                  -PsiteName=YourXNAT -PxnatUrl=https://xnat.yourlab.org \
                  -PsmtpServer=mail.yourlab.org -PmodulePaths=/path1/to/modules,/path2/to/modules \
                  -PpluginsDir=/path/to/plugins -PpluginsDirProd=/path/to/plugins/on/prod

3. Lastly, you can call the **gradlew** or **gradlew.bat** script on its own, with all of the
values passed on the command line in the previous method now stored in the **gradle.properties**
file. This file just takes the standard properties file format of _param_=_value_. There's a
sample version of this file already in the build folder named **sample.gradle.properties**.
You can copy that file and fill in your own values as appropriate. This is very useful when
developing pipeline modules or other code that requires frequent redeployment.

Customization
-------------

The main point of converting the pipeline build to use Gradle is to support pipeline modules.
Pipeline modules are collections of code, scripts, and configuration files that can be integrated
directly into the pipeline build. The purpose of this README is _not_ to explain how to create
custom pipeline modules, however a little information on how modules can be deployed is in
order.

Pipeline modules can be placed in a number of locations:

* In the **modules** folder located inside the pipeline engine
* In any of the folders indicated by a path configured with the **modulePaths** build property

Pipeline modules can be structured in two ways:

* All folders and files directly under the module folder, using the same structure as the
pipeline build. This is a resource-only module, i.e. it has no scripts.
* All resource folders and files in a folder named **resources** located under the module
folder and all script folders and files in a folder named **scripts** located under the module
folder. Scripts are treated somewhat differently from resources, in that they are renamed
with the **.bat** extension when built on a Windows machine.


Plugins
-------------

If you have added plugins to your XNAT and these plugins add data types (have a `*beans*.jar`), 
then you need to include these in many of the pipeline executables. To do so, provide `-PpluginsDir=/path/to/plugins`
where `/path/to/plugins` is the location of all your plugin jars. If you are building the pipeline
engine on a machine that differs from the production environment, you can provide a mapping `-PpluginsDirProd=/path/to/plugins/on/prod`
which will replace the `/path/to/plugins` in the executables.

E.g., My plugins are in `/Users/me/XNATDEV/plugins` on the machine on which I'm building the pipeline engine:

		$ ls /Users/me/XNATDEV/plugins
		mydata-beans-1.0-SNAPSHOT.jar
		mydata-1.0-SNAPSHOT.jar


And I plan to copy them to production to `/data/xnat/home/plugins`. I should run:

        ./gradlew -PadminEmail=you@yourlab.org -Pdestination=/pipeline/folder \
                  -PsiteName=YourXNAT -PxnatUrl=https://xnat.yourlab.org \
                  -PsmtpServer=mail.yourlab.org -PpluginsDir=/Users/me/XNATDEV/plugins
                  -PpluginsDirProd=/data/xnat/home/plugins
",2023-07-07 18:52:26+00:00
yabi,yabi,muccg/yabi,Workflows made easy.,,True,11,2023-01-28 19:28:08+00:00,2015-06-25 08:05:10+00:00,6,9,3,1,9.9.5,2016-11-23 15:53:07+00:00,Other,7838,yabish-release-1.16,154,2011-08-22 07:02:41+00:00,,2018-03-23 10:40:54+00:00,"Yabi
====

About
-----

Yabi is a 3-tier application stack to provide users with an intuitive, easy to use, abstraction of compute and data environments. Developed at the Centre for Comparative Genomics (https://ccg.murdoch.edu.au/), Yabi has been deployed across a diverse set of scientific disciplines and high performance computing environments.

Yabi has a few key features:

- simplified web based access to High Performance Computing
- easy tool addition and maintenance
- handling of disparate compute and storage resouces ie. PBSPro, SGE, Torque, Slurm, SSH, SFTP, Amazon S3, Openstack Swift
- easy and powerful workflow creation environment

More docs at Read the Docs (https://yabi.readthedocs.org/).

Contact
-------

Hosted on GitHub:

https://github.com/muccg/yabi/

Documentation on Read the Docs: 

https://yabi.readthedocs.org/

Twitter:

http://twitter.com/#!/yabiproject

YouTube:

https://www.youtube.com/playlist?list=PLA7CAEC7DAA530AFA

Email:

yabi@ccg.murdoch.edu.au

For developers
--------------

We do our development using Docker containers. See: https://www.docker.com/.
You will have to set up Docker on your development machine.

Other development dependencies are Python 2 and virtualenv (https://virtualenv.pypa.io/en/latest/).

All the development tasks can be done by using the ``develop.sh`` shell script in this directory.
Please run it without any arguments for help on its usage.

Some typical usages are:

- ./develop.sh dev_rebuild
        You will need to run this the first time after you clone our repo to build the docker containers.
        You will also need to re-run it if you would like to rebuld the docker containers, for example when your python dependencies change.

- ./develop.sh dev
        To start up all the docker containers needed for dev. 
        You can access the Django Yabi application on http://localhost:8000
        (replace localhost with ``$ boot2docker ip`` if using boot2docker) after this.
        You can login with one of the default users *demo/demo* or *admin/admin*.

- ./develop.sh runtests
        Starts up all the docker containers and runs all our tests against them.

- ./develop.sh pylint
        Lint your python code.

- ./develop.sh jslint
        Lint your javascript code.

Note: Our docker containers are coordinated using docker-compose (https://docs.docker.com/compose/) but docker-compose will be installed into a virtualenv environment automatically by the ``./develop.sh`` script for you.

Contributing
------------

1. Fork next_release branch
2. Make changes on a feature branch
3. Submit pull request

Citation
--------

Hunter AA, Macgregor AB, Szabo TO, Wellington CA and Bellgard MI, Yabi: An online research environment for Grid, High Performance and Cloud computing, Source Code for Biology and Medicine 2012, 7:1 doi:10.1186/1751-0473-7-1 Published: 15 February 2012 Source Code for Biology and Medicine (http://www.scfbm.org/content/7/1/1/abstract)
",2023-07-07 18:52:30+00:00
yapp,yapp,picanumber/yapp,A parallel pipeline for stream processing,,False,49,2023-07-05 09:36:52+00:00,2022-06-20 11:28:32+00:00,3,5,1,3,gen2.2,2022-10-15 12:01:02+00:00,Apache License 2.0,166,gen2.2,3,2022-10-15 12:01:02+00:00,2023-07-05 09:36:46+00:00,2022-10-19 18:08:19+00:00,"# yapp
##### A parallel pipeline for stream processing

[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![](https://tokei.rs/b1/github/picanumber/yapp)](https://github.com/XAMPPRocky/tokei)
[![license](https://img.shields.io/hexpm/l/plug)](https://github.com/picanumber/task_timetable/blob/a7b8eb6eed728255221909583d9e757b4e345a5a/LICENSE)

[![Ubuntu](https://github.com/picanumber/yap_prelude/actions/workflows/ubuntu.yml/badge.svg)](https://github.com/picanumber/yap_prelude/actions/workflows/ubuntu.yml)
[![Threading](https://github.com/picanumber/yapp/actions/workflows/tsan.yml/badge.svg)](https://github.com/picanumber/yapp/actions/workflows/tsan.yml)
[![Memory](https://github.com/picanumber/yapp/actions/workflows/asan.yml/badge.svg)](https://github.com/picanumber/yapp/actions/workflows/asan.yml)
[![Codacy Security Scan](https://github.com/picanumber/yapp/actions/workflows/codacy.yml/badge.svg)](https://github.com/picanumber/yapp/actions/workflows/codacy.yml)
[![Style](https://github.com/picanumber/yap_prelude/actions/workflows/style.yml/badge.svg)](https://github.com/picanumber/yap_prelude/actions/workflows/style.yml)

![pipeline design](assets/pipeline_diagram2.png)

## Contents
- [Design](#Design)
- [Motivation](#Motivation)
- [Construction](#Construction)
  - [Strongly typed](#Strongly-typed)
  - [Polymorphic](#Polymorphic)
  - [Side notes](#Side-notes)
- [Operations](#Operations)
  - [Run](#Run)
  - [Stop](#Stop)
  - [Pause](#Pause)
  - [Consume](#Consume)
- [Topology](#Topology)
  - [Filter](#Filter)
  - [Farm](#Farm)
  - [Hatch](#Hatch)
- [Utilities](#Utilities)
  - [Consumer](#Consumer)
- [Examples](#Examples)
  - [Basic examples](#Basic-examples)
  - [Top k words](#Top-k-words)
  - [2D data operations](#2D-data-operations)
- [Installation](#Installation)

## Design

yapp is __yet another parallel pipeline__. It is a zero dependency, header only library providing a __multi-threaded implementation of the pipeline pattern__. It enables users to define a series of stages where:

1. Each stage runs in its own thread.
2. Buffering exists between stages to regulate stages of different latency.
3. The first stage is considered a __generator__, meaning it's the entity that feeds data into the pipeline.
4. Intermediate stages can have any `input`/`output` type, provided that the resulting chain is feasible, e.g. output of stage 2 can be an input to stage 3. Correctness in this respect is checked during compilation.
5. The final stage is considered a __sink__, meaning it's the entity that extracts data from the pipeline.

Using a pipeline can be as simple as:

```cpp
auto pln = yap::Pipeline{} | dataReader | stage1 | stage2 | storeOutput;
pln.run();  // Non-blocking call.
```

## Motivation

yapp is provided as an alternative to pipelines in large general purpose libraries for multi-threading. If you want to:

* Smoothly collaborate with code using standard thread facilities.
* Avoid the (bigger) learning curve.
* Easily get acquainted with the parallel pipeline pattern.
* Just use the ""pipeline"" part.

you can try-out yapp, which sports:

* Zero dependencies.
* Vanilla c++20.
* Exclusive use of C++ standard facilities like `<thread>` and friends.
* __Metaprogrammed stitching of user provided callables into pipeline stages__.

For a short introduction to the parallel pipeline pattern you can [check this post](https://ngathanasiou.wordpress.com/2022/06/30/pipelines-to-the-rescue/).

## Construction

This section outlines how to create a pipeline. To help drive our points, assume the existence of:

```cpp
auto generator = [val = 0] () mutable { return val++; };
auto transform = [](int val) { return std::to_string(val); };
auto sink = [](std::string const& s) { std::cout << s << std::endl; };
```

The example above shows the following data-flow from generator to sink:

```
[void, int] -> [int, string] -> [string, void]
```

infeasible type transformations result in compilation errors.

### Strongly typed

To construct a pipeline with a type mandated by the input/output types of each stage, simply pipe the required stages into `yap::Pipeline{}`:

```cpp
auto ps = yap::Pipeline{} | generator | transform | sink;
```

The `pipeLine` object above will be of type `yap::Pipeline<void,int, int,string, string,void>`. A user does not have to specify the type that results from the specified transformations, since [CTAD](https://en.cppreference.com/w/cpp/language/class_template_argument_deduction) handles this process. A strongly typed pipeline can be moved into a another pipeline to be chained with additional stages at a later point, provided that a sink stage has not yet been attached.

### Polymorphic

There are cases where having a strong type is cumbersome or plainly of little benefit, e.g. when no special treatment is planned for pipeline objects of different types or when a pipeline member object needs to create little noise to the containing class. In such cases a user can use a pipeline through its polymorphic base class:

```cpp
auto pp = yap::make_pipeline(generator, transform, sink);
```

The object returned from the `make_pipeline` function, is a `unique_ptr<yap::pipeline>`. This lower-case `pipeline` base class, is the abstract definition of a pipeline and even though information on type transformations is lost, all operations are carried out in way consistent to its construction properties. A polymorphic pipeline cannot be chained further, since information on how to relay types is lost.

### Side notes

* __Data flowing through pipeline stages can be move-only__, as shown in a [related example](https://github.com/picanumber/yap/blob/main/examples/basic/use_non_copyable_type.cpp).
* Similarly to how `std::thread` treats its callable argument, __callables provided as operations are move constructed__ in their respective stage. If an l-value is provided instead of a temporary object, the callable must be copy-constructible.

## Operations

This section describes the operations available to a pipeline. Bear in mind that depending on the construction method, you'd be calling an operation on a value or a pointer:

```cpp
ps.operation();   // Strongly typed pipeline.
pp->operation();  // Polymorphic pipeline.
```

### Run

The `run` method fires up the task processing threads on each stage. Note that since there's buffering between stages, unstable latency of stages is accounted for and data is pushed as forward as possible to be available for processing as soon as possible.

```cpp
ps.run();  // Non blocking-call. Fires up worker threads and continues.
           // A running pipeline will stop on its destructor.
```

No preconditions are imposed to the `run` method apart from having a properly constructed object.

### Stop

The `stop` method only has effect on a running or paused pipeline. It ceases all processing threads, meaning after its call no invocation of the user provided operations is possible. Additionally, it clears the intermediate buffers, meaning non-processed data left in the pipeline will be discarded.

```cpp
auto res = pp.stop();

if (yap::ReturnValue::NoOp == res)
{
    std::cout << ""The pipeline was not running"" << std::endl;
}

pp.stop();  // No effect, we just stopped above.
```

### Pause

The `pause` method only has effect on running pipelines. It ceases all processing threads but unlike `stop`, it does NOT clear the intermediate buffers, meaning a subsequent call to `run` will resume processing.

```cpp
auto res = pp.pause();
// ...
// Other task, e.g. non threadsafe modification of a stage.
// ...
pp.run(); // Non-processed data will resume processing.
```

### Consume

A use case might be that of processing a fixed amount of data. When this need arises, the generator stage can inform the pipeline on the end of the data stream by throwing a `GeneratorExit` exception. To run the pipeline until all data up to that point is processed, the `consume` method exists:

```cpp
auto gen = [val = 0] () mutable {
  if (val > 1'000'000) throw yap::GeneratorExit{};
  return val++;
};

auto pp = yap::make_pipeline(gen, stage1, stage2, stage3, sink);

// Run the pipeline until all data is processed. Blocking call.
pp->consume();
```

Consuming a pipeline leaves it in an idle state, with no threads running. `run` can be called once again, assuming the generator can produce more data, but `stop` or `pause` have no effect. A pipeline whose generator throws `yap::GeneratorExit` will anyways cease when all input is processed. The `consume` method is a way to explicitly wait for data to be processed and make the pipeline ""runable"" again.

## Topology

This section describes the tools to modify a pipeline's topology. Such a modification alters the linear flow of information from one stage to its subsequent, to provide properties that are attractive to specific computational patterns.

### [Filter](https://github.com/picanumber/yapp/blob/main/examples/basic/use_filtered.cpp)

A _filtering stage_ is one that can discard part of its input. As depicted below, `S2` can control the input items to pass to subsequent stages, while being free to perform any type of transformation:

![filtering_stage](assets/filter_pattern.png)

__A callable returning `yap::Filtered` is considered a filtering stage__. The filtered object is just a wrapper around `std::optional<T>`:

```cpp
template <class T>
struct Filtered
{
    std::optional<T> data;

    Filtered() = default;
    explicit Filtered(T &&data) : data(std::move(data)) {}
    explicit Filtered(std::optional<T> &&data) : data(std::move(data)) {}
};
```

The `std::optional` type was not used directly, since explicit use cases exist for `nullopt`, for example a stage handling ""empty"" or ""filler"" inputs. To avoid propagating data further down the pipeline, simply place an empty optional in the `Filtered<T>` return value of your stage. Conversely, filling the `data` member with a value means passing the data to the next stage.  

To __provide explicit syntax to your pipeline declaration__, a helper `Filter` caller can be used. This is a ""call forwarding"" wrapper that can use either  `std::optional` or `yap::Filtered` return types:

```cpp
auto oddPrinter = yap::Pipeline{}
    | gen
    | yap::Filter(s2)  // Explicitly declared filtering stage.
    | intPrinter{};
```

__A stage following a filter__ should accept a `yap::Filtered<T>` input. It can safely assume that the `data` member of the input is not `nullopt`.

### Farm

![farmed_stage](assets/farm_pattern.png)

### [Hatch](https://github.com/picanumber/yapp/blob/main/examples/basic/use_hatched.cpp)

A _hatching stage_ is one that can produce more than one outputs for a single input. In the diagram below, `S2` is such a stage:  

![hatching_stage](assets/hatch_pattern.png)

Note that this process is __fundamentally different from producing a collection of objects__. __A typical example where you might want to hatch your input is when processing text files__, say line by line. If the stage that produced the lines was to scan the whole text file and output a vector of text lines (strings) then you'd face the following deficiencies:

1. Extraneous memory used to hold the entirety of the text file. The program only needs a single line ""in-flight"" to do its processing.
2. The next stage has to wait until the whole file has been read. A ""vector of lines"" implies that text processing can only begin __after__ reading all of the text file.

Such a situation can be greatly improved if the ""text reader"" stage produces its output in a piece wise fashion: Each line that is ready, gets immediately pushed to the next stage for processing.

__To create a hatching stage__ use a callable that accepts `yap::Hatchable` objects as input, a class with logic similar to `yap::Filtered` that conveys how the stage does its processing:

1. The `yap::Hatchable` is convertible to `bool`. `true` means new input while `false` (empty optional) means you're still processing the last input.
2. The hatching stage outputs an object that is convertible to `bool`, e.g. an `std::optional` or again `yap::Hatchable`. __The pipeline stops processing the same input when the output is `false`__, alternatively it keeps invoking the stage with an empty `yap::Hatchable` to produce more output from the last input.

```cpp
auto exampleHatchingStage = [](yap::Hatchable<int> input)
{
     std::optional<char> ret;

     if (val)
     {  
         // New Input from previous stage. Input data is non empty.
         std::optional<int> &curInput = input.data;
         assert(curInput);
     }
     else
     {  
         // Keep processing the last input from previous stage. Input data is empty.
         assert(!input.data);
     }

     return ret; // Returning a contextually ""false"" object, here empty
                 // optional, means the input won't be hatched any more and
                 // the stage can process new values produced from the
                 // previous stage.
};
```

To __provide explicit syntax to your pipeline declaration__, a helper `OutputHatchable` caller can be used to denote the stage producing input for the hatching stage. This is a ""call forwarding"" wrapper that wraps the output of a stage into `yap::Hatchable` return types:

```cpp
auto hp = yap::Pipeline{}
    | yap::OutputHatchable(generator) // The previous stage can be annotated.
    | exampleHatchingStage
    | sinkStage{};
```

## Utilities

Utilities that accompany the library are described here. Creating a huge suite of accompanying tools is a non-goal for this library, however there should be provision for patterns that are often encountered. In that spirit, the following tools are made.

### Consumer

A consumer is a generator that can use a standard iterable container. It handles:

* Going through the elements of a container.
* Quitting the pipeline on ""end of input"".

Usage on a container `c` is pretty straightforward:

```cpp
// Input values are copied into the pipeline. Container is left untouched.
auto p1 = yap::Pipeline{} | yap::Consume(c.begin(), c.end()) | ...

// Input values are moved into the pipeline. Container has ""moved-from"" objects.
auto p2 = yap::Pipeline{} |
  yap::Consume(std::make_move_iterator(c.begin()), std::make_move_iterator(c.end())) | ...
```

## Examples

Examples can be found in the respective [folder](https://github.com/picanumber/yap/tree/main/examples). Each example folder is accompanied by a `README.md` file that documents it. In summary, the contents are:

### [Basic examples](https://github.com/picanumber/yap/tree/main/examples/basic)

These examples showcase simple usages of the library, and how it successfully manages:

* Non copyable data.
* Stages that return futures, e.g. because of an internal thread-pool.
* Filtering stages.

### [Top k words](https://github.com/picanumber/yap/tree/main/examples/top_k_words)

We analyze an input file and output the `k` most frequent words in the text.

### [2D data operations](https://github.com/picanumber/yap/tree/main/examples/2d_data_operations)

Usage of the framework with stages of substantial computational effort, while passing 2d data from one stage to another. Serves as a profiling experiment to deduce next steps and drive optimization efforts.

## Installation

This is a header only library. You can download and build the test suite which is using Google Test, but all you need to do to use the library is to point your build system to the `include` folder.
",2023-07-07 18:52:33+00:00
yawl,yawl,yawlfoundation/yawl,Yet Another Workflow Language,http://www.yawlfoundation.org,False,78,2023-03-30 21:13:49+00:00,2015-07-28 05:15:08+00:00,32,11,4,9,v5.0,2022-12-20 00:55:26+00:00,GNU Lesser General Public License v3.0,2408,v5.0,9,2022-12-20 00:55:26+00:00,2023-05-20 18:16:42+00:00,2023-02-09 06:36:13+00:00,"# YAWL (Yet Another Workflow Language)

[YAWL](https://yawlfoundation.github.io) is a BPM/Workflow system, based on a concise and powerful modelling language, that handles complex data transformations, and full integration with organizational resources and external Web Services. 

### Major Features
YAWL offers these distinctive features:

* the most powerful process specification language for capturing control-flow dependencies and resourcing requirements.
* native data handling using XML Schema, XPath and XQuery.
* a formal foundation that makes its specifications unambiguous and allows automated verification.
* a service-oriented architecture that provides an environment that can easily be tuned to specific needs.
* YAWL has been developed independent from any commercial interests. It simply aims to be the most powerful language for process specification.
* For its expressiveness, YAWL offers relatively few constructs (compare this e.g. to BPMN!).
* YAWL offers unique support for exception handling, both those that were and those that were not anticipated at design time.
* YAWL offers unique support for dynamic workflow through the Worklets approach. Workflows can thus evolve over time to meet new and changing requirements.
* YAWL aims to be straightforward to deploy. It offers a number of automatic installers and an intuitive graphical design environment.
* YAWL's architecture is Service-oriented and hence one can replace existing components with one's own or extend the environment with newly developed components.
* The YAWL environments supports the automated generation of forms. This is particularly useful for rapid prototyping purposes.
* Tasks in YAWL can be mapped to human participants, Web Services, external applications or to Java classes.
* Through the C-YAWL approach a theory has been developed for the configuration of YAWL models. For more information on process configuration visit [www.processconfiguration.com]
* Simulation support is offered through a link with the [ProM](https://www.processmining.org) environment. Through this environment it is also possible to conduct post-execution analysis of YAWL processes (e.g. in order to identify bottlenecks).

### Other Features
* new: completely rewritten Process Editor
* new: Auto Update + Install/Uninstall of selected components
* delayed case starting
* support for passing files as data
* support for non-human resources
* support for interprocess communication
* calendar service and scheduling capabilities
* task documentation facility
* revised logging format and exporting to OpenXES
* integration with external applications
* custom forms
* sophisticated verification support
* Web service communication
* Highly configurable and extensible
",2023-07-07 18:52:37+00:00
yesworkflow,yw-prototypes,yesworkflow-org/yw-prototypes,Research prototype with tutorial.  Start here to learn about and try YesWorkflow.,http://yesworkflow.org/wiki,False,31,2022-07-19 06:27:27+00:00,2014-12-12 21:16:01+00:00,13,19,7,5,v0.2.1.1,2016-09-10 04:35:36+00:00,Other,648,v0.2.1.2,6,2017-03-10 20:25:35+00:00,,2017-07-11 01:21:16+00:00,"YesWorkflow Prototypes
======================

The yw-prototypes repository contains early implementations of YesWorkflow, an approach to modeling conventional scripts and programs as scientific workflows.  The software is described in these two publications:

* T. McPhillips, T. Song, T. Kolisnik, S. Aulenbach, K. Belhajjame, R.K. Bocinsky, Y. Cao, J. Cheney, F. Chirigati, S. Dey, J. Freire, C. Jones, J. Hanken, K.W. Kintigh, T.A. Kohler, D. Koop, J.A. Macklin, P. Missier, M. Schildhauer, C. Schwalm, Y. Wei, M. Bieda, B. Ludäscher (2015). **[YesWorkflow: A User-Oriented, Language-Independent Tool for Recovering Workflow Information from Scripts](http://ijdc.net/index.php/ijdc/article/view/10.1.298)**. *International Journal of Digital Curation* **10**, 298-313. [[PDF](http://ijdc.net/index.php/ijdc/article/download/10.1.298/401)]

* T. McPhillips, S. Bowers, K. Belhajjame, B. Ludäscher (2015). **[Retrospective Provenance Without a Runtime Provenance Recorder](https://www.usenix.org/conference/tapp15/workshop-program/presentation/mcphillips)**. *7th USENIX Workshop on the Theory and Practice of Provenance (TaPP'15)*. [[PDF](https://www.usenix.org/system/files/tapp15-mcphillips.pdf)]

Overview
--------

YesWorkflow aims to provide a number of the benefits of using a scientific workflow management system without having to rewrite scripts and other scientific software.  Rather than reimplement code so that it can be executed and managed by a workflow engine, a YesWorkflow user simply adds special YesWorkflow (YW) comments to existing scripts.  These comments declare how data is used and results produced, step by step, by the script.  The YesWorkflow tools interpret the YW comments and produce graphical output that reveals the stages of computation and the flow of data in the script.

### Example YesWorkflow output

The image below was produced by YesWorkflow using the YW comments added to a conventional (non-dataflow oriented) python script ([example.py](https://github.com/yesworkflow-org/yw-prototypes/blob/master/src/main/resources/example.py ""example.py"")):

![example](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/example_process.png)

The green blocks represent stages in the computation performed by the script. The labels on arrows name the input, intermediate, and final data products of the script.

#### Introduction to YesWorkflow comments

The [example.py](https://github.com/yesworkflow-org/yw-prototypes/blob/master/src/main/resources/example.py ""example.py"") script includes YesWorkflow comments that precede the `main` function and declare the inputs and outputs of the script as a whole:

    # @BEGIN main
    # @PARAM db_pth
    # @PARAM fmodel
    # @IN input_mask_file  @URI file:{db_pth}/land_water_mask/LandWaterMask_Global_CRUNCEP.nc
    # @IN input_data_file  @URI file:{db_pth}/NEE_first_year.nc
    # @OUT result_NEE_pdf  @URI file:result_NEE.pdf

Each YesWorkflow (YW) comment is identified by a keyword that begins with the '`@`' symbol.  A `@BEGIN` comment declares the beginning of the script or of a block of computation within the script. (Because YW keywords are case-insensitive, `@BEGIN`, `@begin` and `@Begin` all work equally well.)  Each `@BEGIN` tag is paired with an `@END` later in the script, and together these tags delimit the code annotated by other YW comments found in that block.  The script `example.py` ends with this YW comment:

    # @END main

The script inputs (`input_data_file` and `input_mask_file`) and outputs (`result_NEE_pdf`) appear in the diagram produced by YesWorkflow because they are declared using the `@IN` and `@OUT` comments shown above.  The text following the first two `@URI` keywords indicate that the inputs are read from files at the indicated locations; the `{db_pth}` portion of these file paths indicate that the locations of these files are configurable, with the value of the `db_pth` (a parameter to the script) forming part of the path to the files.


Between the `@BEGIN` and `@END` comments for the main block, `example.py` includes four blocks of code also annotated with YW comments.  The block of code performing the `fetch_mask` operation (represented as a green box in the diagram above) is:

    # @BEGIN fetch_mask
    # @PARAM db_pth
    # @IN g @AS input_mask_file @URI file:{db_pth}/land_water_mask/LandWaterMask_Global_CRUNCEP.nc
    # @OUT mask @AS land_water_mask
    g = netCDF4.Dataset(db_pth+'/land_water_mask/LandWaterMask_Global_CRUNCEP.nc', 'r')
    mask = g.variables['land_water_mask']
    mask = mask[:].swapaxes(0,1)
    # @END fetch_mask

The text following the (optional) `@AS` keyword in an `@IN` or `@OUT` comment provides an *alias* for the actual value or variable (the term immediately following the `@IN` or `@OUT` keyword) that represents that input or output in the script.  It is the alias that is displayed in YesWorkflow results and that is used to infer how data flows through the script.  Note that in the diagram the arrow labeled `input_mask_file` is connected to the `fetch_mask` block because the alias for the `@IN` comment for `fetch_mask` matches the `@IN` comment on the encompassing `main` block.  

Note as well that the `@OUT` comment for `fetch_mask` declares the  name of the variable (`mask`) used to store the mask in the code.  It also provides an alias (`land_water_mask`) that is displayed in the graphical output of YesWorkflow. This alias matches the alias on an `@IN` comment on the downstream `standardize_with_mask` block, and YesWorkflow draws an arrow in the diagram accordingly.

YesWorkflow comments of the kind discussed here can be added to any script to highlight how data is processed by that script.  YesWorkflow tools discover these comments in the script and produce graphical representations of the script that highlight its workflow-like structure. YesWorkflow can render a number of different views of the workflow structure of a script, including a *process* view (shown above), a *data* view, and a *combined* (*data* + *process*) view.  The data view of the example script is shown below.

![example](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/example_data.png)

And the combined view is:

![example](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/example_combined.png)

Try YesWorkflow in a web browser
--------------------------------

The easiest way to experiment with YesWorkflow's basic capabilities is to use the online [YesWorkflow Editor](http://try.yesworkflow.org) at [try.yesworkflow.org](http://try.yesworkflow.org). This web-based application displays YesWorkflow graphical representations of scripts entered into the editor, and updates the graphics in real time as code is edited. Samples of scripts marked up with YesWorkflow annotations are available for experimentation within the editor.

Install YesWorkflow on your own computer
----------------------------------------

To take advantage of all of the features of YesWorkflow you will need to install the software on your own computer.

### 1. Check installed version of Java

YesWorkflow requires Java (JRE) version 1.7 or higher. To determine the version of java installed on your computer use the -version option to the java command. For example,


    $ java -version
    java version ""1.7.0_67""
    Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
    Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)
    $

 Instructions for installing Java may be found at [http://docs.oracle.com/javase/7/docs/webnotes/install/](http://docs.oracle.com/javase/7/docs/webnotes/install/).  If you plan to develop with YesWorkflow be sure that you install the JDK.

### 2.  Install Graphviz visualization software

YesWorkflow produces graphical representations that are rendered using Graphviz or other software capable of processing DOT graph files.  You can find instructions for installing Graphviz at [http://graphviz.org/Download.php](http://graphviz.org/Download.php ""http://graphviz.org/Download.php"").  Make sure that the `dot` command is in your path following installation.

### 3. Download the YesWorkflow jar file

The YesWorkflow prototype is distributed as a jar (Java archive) file that can be executed using the `java -jar` command. Download [yesworkflow-0.2.0-jar-with-dependencies.jar](https://github.com/yesworkflow-org/yw-prototypes/releases/download/v0.2.0/yesworkflow-0.2.0-jar-with-dependencies.jar) from the [YesWorkflow 0.2.0 pre-release](https://github.com/yesworkflow-org/yw-prototypes/releases/tag/v0.2.0/) and save the file in a convenient location. YesWorkflow now can be run using the `java -jar` command.  Test that the jar works correctly using the `--help` option to display usage information and command line options for YesWorkflow:

    $ java -jar yesworkflow-0.2.0-jar-with-dependencies.jar --help

    usage: yw <command> [source file(s)] [-c <name=value>]...

    Command                    Function
    -------                    --------
    extract                    Identify YW comments in script source file(s)
    model                      Build workflow model from identified YW comments
    graph                      Graphically render workflow model of script

    Option                     Description
    ------                     -----------
    -c, --config <name=value>  Assign value to configuration option
    -h, --help                 Display this help

    Configuration Name         Value
    ------------------         -----
    extract.comment            Single-line comment delimiter in source files
    extract.factsfile          File for storing prolog facts about scripts
    extract.language           Language used in source files
    extract.listfile           File for storing list of extracted comments
    extract.skeletonfile       File for storing YW-markup skeleton of source files
    extract.sources            List of source files to analyze

    model.factsfile            File for storing prolog facts describing model
    model.workflow             Name of top-level workflow in model

    graph.datalabel            Info to display in data nodes: NAME, URI, or BOTH
    graph.dotcomments          Include comments in dot file (ON or OFF)
    graph.dotfile              Name of GraphViz DOT file to write graph to
    graph.edgelabels           SHOW or HIDE labels on edges in process and data views
    graph.layout               Direction of graph layout: TB, LR, RL, or BT
    graph.params               SHOW, HIDE, or REDUCE visibility of parameters
    graph.portlayout           Layout mode for workflow ports: HIDE, RELAX or GROUP
    graph.subworkflow          Qualified name of (sub)workflow to render
    graph.title                Graph title (defaults to workflow name)
    graph.titleposition        Where to place graph title: TOP, BOTTOM, or HIDE
    graph.view                 Workflow view to render: PROCESS, DATA or COMBINED
    graph.workflowbox          SHOW or HIDE box around nodes internal to workflow

    Examples
    --------
    $ yw extract myscript -c extract.comment='#' -c extract.listfile=comments.txt
    $ yw graph myscript.py -config graph.view=combined -config graph.datalabel=uri
    $ yw graph scriptA.py scriptB.py > wf.gv; dot -Tpdf wf.gv -o wf.pdf; open wf.pdf

### 4.  Define a short command for running YesWorkflow at the prompt

If you are running YesWorkflow on an Apple OSX or Linux system (or use Git Bash or Cygwin on Windows), you may define a bash alias to simplify running YesWorkflow at the command line.  On Windows platforms you similarly may define a macro for running YesWorkflow at the prompt.

For example, if you have saved  `yesworkflow-0.2.0-jar-with-dependencies.jar` to the bin subdirectory of your home directory, the following command will create a bash alias for running YesWorkflow simply by typing `yw`:

    alias yw='java -jar ~/bin/yesworkflow-0.2.0-jar-with-dependencies.jar'

On Windows the command to create the `yw` macro is:

    doskey yw=java -jar %USERPROFILE%\bin\yesworkflow-0.2.0-jar-with-dependencies.jar $*

The command to display YesWorkflow command line options is now simply:

    $ yw --help


### 5. Run YesWorkflow on the example python script

The [`example.py`](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/example.py ""example.py"") script  is useful for demonstrating YesWorkflow capabilities. You can download it to your computer [here](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/example.py ""example.py"").  In the examples below it is assumed that `example.py` is in your current working directory.

#### Extracting YW comment lines

First, use the YesWorkflow `extract` command and `-c extract.listfile` option to list the YW commands found in the script:

    $ yw extract example.py -c extract.listfile
    @BEGIN main
    @PARAM db_pth
    @PARAM fmodel
    @IN input_mask_file  @URI file:{db_pth}/land_water_mask/LandWaterMask_Global_CRUNCEP.nc
    @IN input_data_file  @URI file:{db_pth}/NEE_first_year.nc
    @OUT result_NEE_pdf  @URI file:result_NEE.pdf
    @BEGIN fetch_mask
    @PARAM db_pth
    @IN g  @AS input_mask_file  @URI file:{db_pth}/land_water_mask/LandWaterMask_Global_CRUNCEP.nc
    @OUT mask  @AS land_water_mask
    @END fetch_mask
    @BEGIN load_data
    @PARAM db_pth
    @IN input_data_file  @URI file:{db_pth}/NEE_first_year.nc
    @OUT data  @AS NEE_data
    @END load_data
    @BEGIN standardize_with_mask
    @IN data @AS NEE_data
    @IN mask @AS land_water_mask
    @OUT data @AS standardized_NEE_data
    @END standardize_with_mask
    @BEGIN simple_diagnose
    @PARAM fmodel
    @IN data @AS standardized_NEE_data
    @OUT pp  @AS result_NEE_pdf  @URI file:result_NEE.pdf
    @END simple_diagnose
    @END main
    $

This command is useful for confirming that YesWorkflow is finding the comments that you have added to a script and is not confused by other comments and code in the script.

#### Creating a workflow graph for a script

Next, use the `graph` command to produce a graphical representations of the script based on the YW comments it contains.  YesWorkflow natively outputs GraphViz's DOT format (file extension `.gv`).  If you don't provide a file name for storing the DOT output it will be sent to the terminal:

    $ yw graph example.py
    digraph Workflow {
    rankdir=LR
    fontname=Courier; fontsize=18; labelloc=t
    label=main
    subgraph cluster_workflow_box_outer { label=""""; color=black; penwidth=2
    subgraph cluster_workflow_box_inner { label=""""; color=white
    node[shape=box style=filled fillcolor=""#CCFFCC"" peripheries=1 fontname=Courier]
    fetch_mask
    load_data
    standardize_with_mask
    simple_diagnose
    edge[fontname=Helvetica]
    load_data -> standardize_with_mask [label=NEE_data]
    fetch_mask -> standardize_with_mask [label=land_water_mask]
     -> simple_diagnose [label=standardized_NEE_data]
    }}
    subgraph cluster_input_ports_group_outer { label=""""; color=white
    subgraph cluster_input_ports_group_inner { label=""""; color=white
    node[shape=circle style=filled fillcolor=""#FFFFFF"" peripheries=1 fontname=Courier width=0.2]
    input_mask_file_input_port [label=""""]
    input_data_file_input_port [label=""""]
    }}
    subgraph cluster_output_ports_group_outer { label=""""; color=white
    subgraph cluster_output_ports_group_inner { label=""""; color=white
    node[shape=circle style=filled fillcolor=""#FFFFFF"" peripheries=1 fontname=Courier width=0.2]
    result_NEE_pdf_output_port [label=""""]
    }}
    edge[fontname=Helvetica]
    input_mask_file_input_port -> fetch_mask [label=input_mask_file]
    input_data_file_input_port -> load_data [label=input_data_file]
    edge[fontname=Helvetica]
    simple_diagnose -> result_NEE_pdf_output_port [label=result_NEE_pdf]
    }

You can save the DOT output to a file, render it as PDF file using Graphviz's `dot` command, then open the PDF file to view the diagram:

    $ yw graph example.py > example.gv
    $ dot -Tpdf example.gv -o example.pdf
    $ open example.pdf

On Windows platforms the last line above should be replaced with:

    $ start example.pdf

Alternatively, you can pipe `yw` into `dot` and open the graphics file immediately (here using png rather than PDF).  In the case of bash running on Unix platforms:

    $ yw graph example.py | dot -Tpng -o example.png && open example.png

And on Windows platforms:

    $ yw graph example.py | dot -Tpng -o example.png && start example.png

### 6. Mark up and analyze your own script

You should now be able to add YW comments to your own data processing script and analyze your script using the YesWorkflow prototype.

#### Delimit your script with `@begin` and `@end` comments

The YesWorkflow prototype assumes that the code for the entire script to be analyzed is bracketed by a pair of `@begin` and `@end` comments.  The YW comments may appear anywhere comments are allowed by the scripting language you are using.   For example, a script written in a language that uses the # character to start comments might look like the following

    # @begin MyScript    
    script statement
    script statement  
    script statement
    # a non-YW comment
    script statement
    script statement  
    script statement
    # @end MyScript
    
Note that comments that do not contain YW keywords are ignored by YesWorkflow.

`@begin` and `@end` keywords both should be followed by the name of the block of code they bracket (in this case, the script as a whole), and these names should match for each `@begin` and `@end` pair.  This convention makes it easier to identify incorrectly paired `@begin` and `@end` keywords in the script.

#### Use `@in` and `@out` comments to declare the data consumed and produced in the script

The next step in marking up a script with YW comments is to declare the inputs and outputs of the script.  These do not need to be actual command-line options to your script or files read from or output to disk by the script.  The comments you add simply declare that the script accepts these inputs in some way, and produces the indicated outputs somehow.  

This is done by adding `@in` and `@out` comments following the `@begin` comment for your script. For example:

    # @begin MyScript
    # @in x
    # @in y
    # @out d
    script statement
    script statement
    script statement
    # a non-YW comment
    script statement
    script statement
    script statement
    # @end MyScript

The `@in` and `@out` comments above indicate that the script takes two inputs, `x` and `y`, and produces output `d`.  The names of these inputs and outputs (multiple inputs and outputs are allowed) are expected to correspond to the names of variables that store these input and output values at some point in the script (although this is not enforced by the prototype).  Declaring the names of the relevant variables is meant to make it easier for others to find the actual input and output operations in your script.

Multiple YW comments can be placed on the same line.  For example, the example below is equivalent to the one above:

    # @begin MyScript @in x @in y @out d
    script statement
    script statement
    script statement
    # a non-YW comment
    script statement
    script statement
    script statement
    # @end MyScript

Because variable names are often kept relatively short in scripts, YesWorkflow allows you to associate a more verbose alias for each input and output using the `@as` keyword.  For example:

    # @begin MyScript
    # @in x @as XCoordinate
    # @in y @as YCoordinate
    # @out d @as DistanceFromOrigin
    script statement
    script statement
    script statement
    # a non-YW comment
    script statement
    script statement
    script statement
    # @end MyScript

Analysis performed by YesWorkflow and the outputs it produces use these aliases if present, and the unaliased names otherwise.

#### Declare computational code blocks within your script

The YesWorkflow prototype assumes that a script has a single, top-level block of code delimited by the `@begin` and `@end` statements described above, and additionally one or more marked up computational blocks nested within this top-level block.  You can use these nested blocks to describe the computational steps in your script in dataflow terms.  For example, we can declare two computational code blocks within MyScript:

    # @begin MyScript
    # @in  x @as XCoordinate
    # @in  y @as YCoordinate
    # @out d @as DistanceFromOrigin

      # get input x somehow
      # get input y somehow

      # @begin SquareCoordinates
      # @in  x  @as XCoordinate
      # @in  y  @as YCoordinate
      # @out xx @as XSquared
      # @out yy @as YSquared
      script statement
      script statement
      # @end SquareCoordinates

      # @begin SumSquares
      # @in  xx @as XSquared
      # @in  yy @as YSquared
      # @out s  @as SumOfSquares
      script statement
      script statement
      # @end SumSquares

      # @begin TakeSquareRoot
      # @in  s @as SumOfSquares
      # @out d @as DistanceFromOrigin
      script statement
      script statement
      # @end TakeSquareRoot

      # output d somehow

    # @end MyScript

The `@begin`, `@end`, `@in`, `@out`, and `@as` keywords have the same meaning for computational blocks within the script as for the script as a whole.

#### Analyze your script with YesWorkflow tool

At this point you may analyze your script and render it graphically, just as we did above for `example.py`.  If your script is called `MyScript.py` then the command (for Unix platforms):

    $ yw graph MyScript.py | dot -Tpng -o MyScript.png && open MyScript.png

or (for Windows platforms):

    $ yw graph MyScript.py | dot -Tpng -o MyScript.png && start MyScript.png

will render your script as a dataflow program and illustrate how data flows from script inputs, into successive computational blocks, and finally to script outputs.  For the example above, YesWorkflow produces this:

![](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/MyScript.png)


Notice that the `@in` and `@out` comments for MyScript (the script as a whole) correspond to the small, empty circles at the left and right sides of the figure, respectively.  The circles on the left are connected by arrows  to the SquareCoordinates block.  These arrows indicate dataflow into the script and are labeled with the aliases for the script `@in` comments, which in turn match the `@in` aliases for the SquareCoordinates block.

Similarly, the circle on the far right corresponds to the script `@out` comment, and is connected by an incoming arrow from the TakeSquareRoot block because the MyScript `@out` comment and TakeSquareRoot `@out` comment have matching aliases. This right-most arrow represents flow of data out of the script.

The remaining arrows are drawn between blocks and represent flow of data between computational blocks. They result from matching aliases from `@out` comments on upstream blocks with aliases for `@in` comments on downstream blocks.

#### Override the comment character used in your script

YesWorkflow infers the programming language employed in a script by inspecting the source file extension.  Currently the following file extensions and associated comment syntaxes are recognized.  Block comments may span multiple consecutive lines.

Language  | Extension | Single-line comments | Block comments
----------|----------|----------------------|--------------------
bash      | .sh      | `# a comment`        |
C         | .c, .h   | `// a comment`       | `/* a comment */`
C++       | .cpp     | `// a comment`       | `/* a comment */`
Java      | .java    | `// a comment`       | `/* a comment */`
MATLAB    | .m       | `% a comment`        | `%{ a comment  %}` or `... a comment ...`
python    | .py      | `# a comment`        | `''' a comment '''` or `"""""" a comment """"""`
R         | .R       | `# a comment`        |
SAS       | .sas     |                      | `* a comment ;` or `/* a comment */`

Support for single-line comments started with a `#` character is assumed if the extension is not one of the above, if the file name has no extension, or if the script code is piped to YesWorkflow via the standard input stream. To manually specify a single-line comment character use the `-c extract.comment=` configuration option to provide the comment character in quotes.  For example, to pipe a MATLAB program to YesWorkflow and use the correct comment character you may use the following command on Unix platforms:

    cat myprogram.m | yw graph -c extract.comment='%' > myprogram.gv

And on Windows:

    yw graph -c extract.comment='%' < myprogram.m > myprogram.gv

#### Store command-line options in a yw.properties file

Configutration options for YesWorkflow may be stored in a file named `yw.properties` in the directory in which you run `yw`.  Specify one option per line using a `name = value` syntax similar that used on the command line (in configuration files, spaces and tabs are allowed on either side of the `=` sign).  To try this out, create a `yw.properties` file with the following content in the directory containing `MyScript.py`:

    # extract configuration
    extract.sources     = MyScript.py
    extract.listfile    = listing.txt
    extract.comment     = #

    # graph configuration
    graph.view          = combined
    graph.layout        = tb
    graph.dotfile       = combined.gv
    graph.workflowbox   = show
    graph.portlayout    = relax

You may now create the graph rendering of your script using the above options simply by typing:

    $ yw graph

From top to bottom, the options specified in this `yw.properties` file cause YesWorkflow to (1) extract YW comments from `MyScript.py`, (2) leave the list of extracted comments in `listing.txt`, (3) interpret the `#` as the source code comment delimiter (this causes YesWorkflow to ignore comments in Python docstrings in this case), (4) render a view of the workflow graph that combines the process and data views in single graph (see figure below), (5) arrange the nodes in the graph from top to bottom, (6) write the DOT output to `combined.gv`, (7) draw a box around the nodes in the workflow, and (8) allow Graphviz to place the input and output nodes in locations that minimize the complexity of the graph.  The graph resulting from running YesWorkflow with these options looks like this:

![](https://raw.githubusercontent.com/yesworkflow-org/yw-prototypes/master/src/main/resources/MyScript_combined.png)

Type `yw --help` to see available options and valid values for each.

Instructions for developers
---------------------------

#### JDK and Maven configuration

The Java prototype is built using Maven 3. Before building YesWorkflow confirm that the `mvn` command is in your path, that your version of Maven is at least 3.0.5, and that a JDK version 1.7 (or higher) is found by Maven:
    
    $ mvn --version
    Apache Maven 3.2.3 (33f8c3e1027c3ddde99d3cdebad2656a31e8fdf4; 2014-08-11T13:58:10-07:00)
    Maven home: c:\Program Files\apache-maven-3.2.3
    Java version: 1.7.0_67, vendor: Oracle Corporation
    Java home: c:\Program Files\Java\jdk1.7.0_67\jre
    Default locale: en_US, platform encoding: Cp1252
    OS name: ""windows 7"", version: ""6.1"", arch: ""amd64"", family: ""windows""
    $

JDK 7 and Maven 3 downloads and detailed installation instructions can be found at the following links:

- [Instructions for installing and configuring JDK 1.7](http://docs.oracle.com/javase/7/docs/webnotes/install/) (Oracle Java Documentation)
- [Instructions for installing and configuring Maven 3](http://maven.apache.org/download.cgi) (Apache Maven Project)


#### Project directory layout

YesWorkflow adopts the default organization of source code, resources, and tests as defined by Maven.  See [maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html](http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html) for more information.  The most important directories are listed below:

Directory            | Description
---------------------|-----------
src/main/java        | Source code to be built and packaged for distribution.
src/main/resources   | Resource files to be packaged with production code.
src/test/java        | Source code for unit and functional tests. Not included in packaged distributions.
src/test/resources   | Resource files available to tests. Not included in packaged distributions.
target               | Destination directory for packaged distributions (jar files) built by maven.
target/classes       | Compiled java classes for source code found under src/main/java.
target/test-classes  | Compiled java classes for test code found under src/test/java.
target/dependency    | Automatically resolved and downloaded dependencies (jars) that will be included in the standalone distribution.
target/site/apidocs/ | Local build of Javadoc documentation.


#### Building and testing with maven

YesWorkflow can be built and tested from the command line using the following commands:

Maven command | Description
--------------|------------
mvn clean     | Delete the target directory including all compiled classes.
mvn compile   | Download required dependencies and compile source code in src/main/java.  Only those source files changes since the last compilation or clean are built.
mvn test      | Compile the classes in src/test/java and run all tests found therein. Peforms *mvn compile* first.
mvn package   | Package the compiled classes in target/classes and files found in src/main/resources in two jar files, **yesworkflow-0.2-SNAPSHOT.jar** and **yesworkflow-0.2-SNAPSHOT-jar-with-dependencies.jar**.  The latter also contains all jar dependencies. Performs *mvn compile* and *mvn test* first, and will not perform packaging step if any tests fail. Use the `-DskipTests` option to bypass tests.
mvn javadoc:javadoc | Build Javadoc documentation. The `mvn package` command also builds Javadoc.

#### Continuous integration with Bamboo

All code is built and tests run automatically on a build server at NCSA whenever changes are committed to directories used by maven.  Please confirm that the automated build and tests succeed after committing changes to code or resource files (it may take up to two minutes for a commit-triggered build to start).  Functional tests depend on the scripts in src/main/resources and are likely to fail if not updated following changes to these scripts.

Site                  | Url
----------------------| ---
Build history         | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-YW
Last build            | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-YW/latest
Last successful build | https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-YW/latestSuccessful

The link to the latest successful build is useful for obtaining the most recently built jar file without building it yourself.  Follow the link to the [last successful build](https://opensource.ncsa.illinois.edu/bamboo/browse/KURATOR-YW/latestSuccessful ""last successful build""), click the Artifacts tab, then download the executable jar.
",2023-07-07 18:52:42+00:00
zeebe,zeebe,camunda/zeebe,Distributed Workflow Engine for Microservices Orchestration,https://zeebe.io,False,2773,2023-07-07 09:40:54+00:00,2016-03-20 03:38:04+00:00,501,111,92,202,8.2.7,2023-06-13 12:15:39+00:00,,20296,clients/go/v8.3.0-alpha3,372,2023-07-05 08:58:48+00:00,2023-07-07 17:01:54+00:00,2023-07-07 16:51:44+00:00,"# Zeebe - Workflow Engine for Microservices Orchestration

[![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.camunda.zeebe/camunda-zeebe/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.camunda.zeebe/camunda-zeebe)

Zeebe provides visibility into and control over business processes that span multiple microservices. It is the engine that powers [Camunda Platform 8](https://camunda.com/platform/zeebe/).

**Why Zeebe?**

* Define processes visually in [BPMN 2.0](https://www.omg.org/spec/BPMN/2.0.2/)
* Choose your programming language
* Deploy with [Docker](https://www.docker.com/) and [Kubernetes](https://kubernetes.io/)
* Build processes that react to messages from [Kafka](https://kafka.apache.org/) and other message queues
* Scale horizontally to handle very high throughput
* Fault tolerance (no relational database required)
* Export process data for monitoring and analysis
* Engage with an active community

[Learn more at camunda.com](https://camunda.com/platform/zeebe/)

## Release Lifecycle

Our release cadence within major releases is a minor release every six months, with an alpha release on each of the five months between minor releases. Releases happen on the second Tuesday of the month, Berlin time (CET).

Minor releases are supported with patches for eighteen months after their release.

Here is a diagram illustrating the lifecycle of minor releases over a 27-month period:

```
2022                       2023                                2024
Ap Ma Ju Ju Au Se Oc No De Ja Fe Ma Ap Ma Ju Ju Au Se Oc No De Ja Fe Ma Ap Ma Ju
8.0--------------------------------------------------|
                  8.1--------------------------------------------------|
                                    8.2-----------------------------------------
                                                      8.3-----------------------
                                                                        8.4-----
1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27
```

Here is a diagram illustrating the release schedule of the five alpha releases prior to an upcoming minor release over a 7-month period:

```
2022                                2023
Oct       Nov          Dec          Jan          Feb          Mar          Apr
8.1-----------------------------------------------------------------------------
          8.2-alpha1   8.2-alpha2   8.2-alpha3   8.2-alpha4   8.2-alpha5   8.2-- 
1         2            3            4            5            6            7
```

## Status

To learn more about what we're currently working on, check the [GitHub issues](https://github.com/camunda/zeebe/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc) and the [latest commits](https://github.com/camunda/zeebe/commits/main).

## Helpful Links

* [Releases](https://github.com/camunda/zeebe/releases)
* [Pre-built Docker images](https://hub.docker.com/r/camunda/zeebe/tags?page=1&ordering=last_updated)
* [Building Docker images for other platforms](/docs/building_docker_images.md)
* [Blog](https://camunda.com/blog/category/process-automation-as-a-service/)
* [Documentation Home](https://docs.camunda.io)
* [Issue Tracker](https://github.com/camunda/zeebe/issues)
* [User Forum](https://forum.camunda.io)
* [Slack Channel](https://www.camunda.com/slack)
* [Contribution Guidelines](/CONTRIBUTING.md)

## Recommended Docs Entries for New Users

* [What is Camunda Platform 8?](https://docs.camunda.io/docs/components/concepts/what-is-camunda-platform-8/)
* [Getting Started Tutorial](https://docs.camunda.io/docs/guides/)
* [Technical Concepts](https://docs.camunda.io/docs/components/zeebe/technical-concepts/)
* [BPMN Processes](https://docs.camunda.io/docs/components/modeler/bpmn/bpmn-primer/)
* [Installation and Configuration](https://docs.camunda.io/docs/self-managed/zeebe-deployment/)
* [Java Client](https://docs.camunda.io/docs/apis-clients/java-client/)
* [Go Client](https://docs.camunda.io/docs/apis-clients/go-client/)
* [Spring Integration](https://github.com/camunda-community-hub/spring-zeebe/)

## Contributing

Read the [Contributions Guide](/CONTRIBUTING.md).

## Code of Conduct

This project adheres to the [Camunda Code of Conduct](https://camunda.com/events/code-conduct/).
By participating, you are expected to uphold this code. Please [report](https://camunda.com/events/code-conduct/reporting-violations/)
unacceptable behavior as soon as possible.

## License

Zeebe source files are made available under the [Zeebe Community License
Version 1.1](/licenses/ZEEBE-COMMUNITY-LICENSE-1.1.txt) except for the parts listed
below, which are made available under the [Apache License, Version
2.0](/licenses/APACHE-2.0.txt).  See individual source files for details.

Available under the [Apache License, Version 2.0](/licenses/APACHE-2.0.txt):
- Java Client ([clients/java](/clients/java))
- Go Client ([clients/go](/clients/go))
- Exporter API ([exporter-api](/exporter-api))
- Protocol ([protocol](/protocol))
- Gateway Protocol Implementation ([gateway-protocol-impl](/gateway-protocol-impl))
- BPMN Model API ([bpmn-model](/bpmn-model))

### Clarification on gRPC Code Generation

The Zeebe Gateway Protocol (API) as published in the
[gateway-protocol](/gateway-protocol/src/main/proto/gateway.proto) is licensed
under the [Zeebe Community License 1.1](/licenses/ZEEBE-COMMUNITY-LICENSE-1.1.txt). Using gRPC tooling to generate stubs for
the protocol does not constitute creating a derivative work under the Zeebe Community License 1.1 and no licensing restrictions are imposed on the
resulting stub code by the Zeebe Community License 1.1.
",2023-07-07 18:52:46+00:00
zenml,zenml,zenml-io/zenml,"ZenML 🙏: Build portable, production-ready MLOps pipelines. https://zenml.io.",,False,2943,2023-07-05 22:18:58+00:00,2020-11-19 09:25:46+00:00,310,41,68,71,0.41.0,2023-07-03 17:42:05+00:00,Apache License 2.0,5772,0.41.0,97,2023-07-03 17:42:05+00:00,2023-07-07 14:58:02+00:00,2023-07-03 17:42:05+00:00,"<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown ""reference style"" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->

[![PyPi][pypi-shield]][pypi-url]
[![PyPi][pypiversion-shield]][pypi-url]
[![PyPi][downloads-shield]][downloads-url]
[![Contributors][contributors-shield]][contributors-url]
[![License][license-shield]][license-url]
<!-- [![Build][build-shield]][build-url] -->
<!-- [![CodeCov][codecov-shield]][codecov-url] -->

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->

[pypi-shield]: https://img.shields.io/pypi/pyversions/zenml?style=for-the-badge

[pypi-url]: https://pypi.org/project/zenml/

[pypiversion-shield]: https://img.shields.io/pypi/v/zenml?style=for-the-badge

[downloads-shield]: https://img.shields.io/pypi/dm/zenml?style=for-the-badge

[downloads-url]: https://pypi.org/project/zenml/

[codecov-shield]: https://img.shields.io/codecov/c/gh/zenml-io/zenml?style=for-the-badge

[codecov-url]: https://codecov.io/gh/zenml-io/zenml

[contributors-shield]: https://img.shields.io/github/contributors/zenml-io/zenml?style=for-the-badge

[contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors

[license-shield]: https://img.shields.io/github/license/zenml-io/zenml?style=for-the-badge

[license-url]: https://github.com/zenml-io/zenml/blob/main/LICENSE

[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[linkedin-url]: https://www.linkedin.com/company/zenml/

[twitter-shield]: https://img.shields.io/twitter/follow/zenml_io?style=for-the-badge

[twitter-url]: https://twitter.com/zenml_io

[slack-shield]: https://img.shields.io/badge/-Slack-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[slack-url]: https://zenml.io/slack-invite

[build-shield]: https://img.shields.io/github/workflow/status/zenml-io/zenml/Build,%20Lint,%20Unit%20&%20Integration%20Test/develop?logo=github&style=for-the-badge

[build-url]: https://github.com/zenml-io/zenml/actions/workflows/ci.yml


<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <a href=""https://zenml.io"">
    <img alt=""ZenML Logo"" src=""https://user-images.githubusercontent.com/3348134/223112746-345126ff-a0e8-479f-8ac0-670d78f71712.png"" alt=""Logo"" width=""400"">
  </a>

<h3 align=""center"">Build portable, production-ready MLOps pipelines.</h3>

  <p align=""center"">
    A simple yet powerful open-source framework that integrates all your ML tools.
    <br />
    <a href=""https://docs.zenml.io/""><strong>Explore the docs »</strong></a>
    <br />
    <div align=""center"">
      Join our <a href=""https://zenml.io/slack-invite"" target=""_blank"">
      <img width=""25"" src=""https://cdn3.iconfinder.com/data/icons/logos-and-brands-adobe/512/306_Slack-512.png"" alt=""Slack""/>
    <b>Slack Community</b> </a> and be part of the ZenML family.
    </div>
    <br />
    <a href=""https://zenml.io/features"">Features</a>
    ·
    <a href=""https://zenml.io/roadmap"">Roadmap</a>
    ·
    <a href=""https://github.com/zenml-io/zenml/issues"">Report Bug</a>
    ·
    <a href=""https://zenml.io/discussion"">Vote New Features</a>
    ·
    <a href=""https://blog.zenml.io/"">Read Blog</a>
    ·
    <a href=""#-meet-the-team"">Meet the Team</a>
    <br />
    🎉 Version 0.41.0 is out. Check out the release notes
    <a href=""https://github.com/zenml-io/zenml/releases"">here</a>.
    <br />
    <br />
    <a href=""https://www.linkedin.com/company/zenml/"">
    <img src=""https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555"" alt=""Logo"">
    </a>
    <a href=""https://twitter.com/zenml_io"">
    <img src=""https://img.shields.io/badge/-Twitter-black.svg?style=for-the-badge&logo=twitter&colorB=555"" alt=""Logo"">
    </a>
    <a href=""https://www.youtube.com/c/ZenML"">
    <img src=""https://img.shields.io/badge/-YouTube-black.svg?style=for-the-badge&logo=youtube&colorB=555"" alt=""Logo"">
    </a>
  </p>
</div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>🏁 Table of Contents</summary>
  <ol>
    <li><a href=""#-introduction"">Introduction</a></li>
    <li><a href=""#-quickstart"">Quickstart</a></li>
    <li>
      <a href=""#-create-your-own-mlops-platform"">Create your own MLOps Platform</a>
      <ul>
        <li><a href=""##-1-deploy-zenml"">Deploy ZenML</a></li>
        <li><a href=""#-2-deploy-stack-components"">Deploy Stack Components</a></li>
        <li><a href=""#-3-create-a-pipeline"">Create a Pipeline</a></li>
        <li><a href=""#-4-start-the-dashboard"">Start the Dashboard</a></li>
      </ul>
    </li>
    <li><a href=""#-roadmap"">Roadmap</a></li>
    <li><a href=""#-contributing-and-community"">Contributing and Community</a></li>
    <li><a href=""#-getting-help"">Getting Help</a></li>
    <li><a href=""#-license"">License</a></li>
  </ol>
</details>

<br />

# 🤖 Introduction

🤹 ZenML is an extensible, open-source MLOps framework for creating portable,
production-ready machine learning pipelines. By decoupling infrastructure from
code, ZenML enables developers across your organization to collaborate more
effectively as they develop to production.

- 💼 ZenML gives data scientists the freedom to fully focus on modeling and
experimentation while writing code that is production-ready from the get-go.

- 👨‍💻 ZenML empowers ML engineers to take ownership of the entire ML lifecycle
  end-to-end. Adopting ZenML means fewer handover points and more visibility on
  what is happening in your organization.

- 🛫 ZenML enables MLOps infrastructure experts to define, deploy, and manage
sophisticated production environments that are easy to use for colleagues.

![The long journey from experimentation to production.](/docs/book/.gitbook/assets/intro-zenml-overview.png)

ZenML provides a user-friendly syntax designed for ML workflows, compatible with
any cloud or tool. It enables centralized pipeline management, enabling
developers to write code once and effortlessly deploy it to various
infrastructures.

<div align=""center"">
    <img src=""docs/book/.gitbook/assets/stack.gif"">
</div>

# 🤸 Quickstart

[Install ZenML](https://docs.zenml.io/getting-started/installation) via
[PyPI](https://pypi.org/project/zenml/). Python 3.7 - 3.10 is required:

```bash
pip install ""zenml[server]""
```

Take a tour with the guided quickstart by running:

```bash
zenml go
```

# 🖼️ Create your own MLOps Platform

ZenML allows you to create and manage your own MLOps platform using 
best-in-class open-source and cloud-based technologies. Here is an example of 
how you could set this up for your team:

## 🔋 1. Deploy ZenML

For full functionality ZenML should be deployed on the cloud to
enable collaborative features as the central MLOps interface for teams.

![ZenML Architecture Diagram.](docs/book/.gitbook/assets/Scenario3.png)

In case your machine is authenticated with one of the big three cloud 
providers, this command will do the full deployment for you.

```bash
zenml deploy --provider aws  # aws, gcp and azure are supported providers
```

You can also choose to deploy with docker or helm with full control over
the configuration and deployment. Check out the
[docs](https://docs.zenml.io/platform-guide/set-up-your-mlops-platform/deploy-zenml)
to find out how.

## 👨‍🍳 2. Deploy Stack Components

ZenML boasts a ton of [integrations](https://zenml.io/integrations) into 
popular MLOps tools. The [ZenML Stack](https://docs.zenml.io/user-guide/starter-guide/understand-stacks) 
concept ensures that these tools work nicely together, therefore bringing
structure and standardization into the MLOps workflow.

Deploying and configuring this is super easy with ZenML. For **AWS**, this might 
look a bit like this

```bash
# Deploy and register an orchestrator and an artifact store
zenml orchestrator deploy kubernetes_orchestrator --flavor kubernetes --cloud aws
zenml artifact-store deploy s3_artifact_store --flavor s3

# Register this combination of components as a stack
zenml stack register production_stack --orchestrator kubernetes_orchestrator --artifact-store s3_artifact_store --set # Register your production environment
```

When you run a pipeline with this stack set, it will be running on your deployed
Kubernetes cluster.

You can also [deploy your own tooling manually](https://docs.zenml.io/platform-guide/set-up-your-mlops-platform/deploy-and-set-up-a-cloud-stack)
or [**create your own MLOps Platform Sandbox**](https://docs.zenml.io/user-guide/advanced-guide/sandbox), 
a one-click deployment platform for an ephemeral MLOps stack that you can use 
to run production-ready MLOps pipelines in the cloud.

## 🏇 3. Create a Pipeline

Here's an example of a hello world ZenML pipeline in code:

```python
# run.py
from zenml import pipeline, step


@step
def step_1() -> str:
    """"""Returns the `world` substring.""""""
    return ""world""


@step
def step_2(input_one: str, input_two: str) -> None:
    """"""Combines the two strings at its input and prints them.""""""
    combined_str = input_one + ' ' + input_two
    print(combined_str)


@pipeline
def my_pipeline():
    output_step_one = step_1()
    step_2(input_one=""hello"", input_two=output_step_one)


if __name__ == ""__main__"":
    my_pipeline()
```

```bash
python run.py
```

## 👭 4. Start the Dashboard

Open up the ZenML dashboard using this command.

```bash
zenml show
```

![ZenML Dashboard](docs/book/.gitbook/assets/landingpage.png)

# 🗺 Roadmap

ZenML is being built in public. The [roadmap](https://zenml.io/roadmap) is a
regularly updated source of truth for the ZenML community to understand where
the product is going in the short, medium, and long term.

ZenML is managed by a [core team](https://zenml.io/company#CompanyTeam) of
developers that are responsible for making key decisions and incorporating
feedback from the community. The team oversees feedback via various channels,
and you can directly influence the roadmap as follows:

- Vote on your most wanted feature on our [Discussion
  board](https://zenml.io/discussion).
- Start a thread in our [Slack channel](https://zenml.io/slack-invite).
- [Create an issue](https://github.com/zenml-io/zenml/issues/new/choose) on our
  Github repo.

# 🙌 Contributing and Community

We would love to develop ZenML together with our community! Best way to get
started is to select any issue from the [`good-first-issue`
label](https://github.com/zenml-io/zenml/labels/good%20first%20issue). If you
would like to contribute, please review our [Contributing
Guide](CONTRIBUTING.md) for all relevant details.

# 🆘 Getting Help

The first point of call should
be [our Slack group](https://zenml.io/slack-invite/).
Ask your questions about bugs or specific use cases, and someone from
the [core team](https://zenml.io/company#CompanyTeam) will respond.
Or, if you
prefer, [open an issue](https://github.com/zenml-io/zenml/issues/new/choose) on
our GitHub repo.

# 📜 License

ZenML is distributed under the terms of the Apache License Version 2.0.
A complete version of the license is available in the [LICENSE](LICENSE) file in
this repository. Any contribution made to this project will be licensed under
the Apache License Version 2.0.
",2023-07-07 18:52:50+00:00
znflow,ZnFlow,zincware/ZnFlow,A general purpose framework for building and running computational graphs.,,False,4,2023-04-11 16:05:36+00:00,2023-01-18 09:06:45+00:00,0,1,2,10,v0.1.11,2023-04-08 18:25:38+00:00,Apache License 2.0,63,v0.1.11,10,2023-04-08 18:25:38+00:00,2023-07-04 03:38:57+00:00,2023-06-29 09:14:55+00:00,"[![zincware](https://img.shields.io/badge/Powered%20by-zincware-darkcyan)](https://github.com/zincware)
[![Coverage Status](https://coveralls.io/repos/github/zincware/ZnFlow/badge.svg?branch=main)](https://coveralls.io/github/zincware/ZnFlow?branch=main)
[![PyPI version](https://badge.fury.io/py/znflow.svg)](https://badge.fury.io/py/znflow)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/zincware/ZnFlow/HEAD)

# ZnFlow

The `ZnFlow` package provides a basic structure for building computational
graphs based on functions or classes. It is designed as a lightweight
abstraction layer to

- learn graph computing.
- build your own packages on top of it.

## Installation

```shell
pip install znflow
```

## Usage

### Connecting Functions

With ZnFlow you can connect functions to each other by using the `@nodify`
decorator. Inside the `znflow.DiGraph` the decorator will return a
`FunctionFuture` object that can be used to connect the function to other nodes.
The `FunctionFuture` object will also be used to retrieve the result of the
function. Outside the `znflow.DiGraph` the function behaves as a normal
function.

```python
import znflow

@znflow.nodify
def compute_mean(x, y):
    return (x + y) / 2

print(compute_mean(2, 8))
# >>> 5

with znflow.DiGraph() as graph:
    mean = compute_mean(2, 8)

graph.run()
print(mean.result)
# >>> 5

with znflow.DiGraph() as graph:
    n1 = compute_mean(2, 8)
    n2 = compute_mean(13, 7)
    n3 = compute_mean(n1, n2)

graph.run()
print(n3.result)
# >>> 7.5
```

### Connecting Classes

It is also possible to connect classes. They can be connected either directly or
via class attributes. This is possible by returning `znflow.Connections` inside
the `znflow.DiGraph` context manager. Outside the `znflow.DiGraph` the class
behaves as a normal class.

In the following example we use a dataclass, but it works with all Python
classes that inherit from `znflow.Node`.

```python
import znflow
import dataclasses

@znflow.nodify
def compute_mean(x, y):
    return (x + y) / 2

@dataclasses.dataclass
class ComputeMean(znflow.Node):
    x: float
    y: float

    results: float = None

    def run(self):
        self.results = (self.x + self.y) / 2

with znflow.DiGraph() as graph:
    n1 = ComputeMean(2, 8)
    n2 = compute_mean(13, 7)
    # connecting classes and functions to a Node
    n3 = ComputeMean(n1.results, n2)

graph.run()
print(n3.results)
# >>> 7.5
```

## Dask Support

ZnFlow comes with support for [Dask](https://www.dask.org/) to run your graph:

- in parallel.
- through e.g. SLURM (see https://jobqueue.dask.org/en/latest/api.html).
- with a nice GUI to track progress.

All you need to do is install ZnFlow with Dask `pip install znflow[dask]`. We
can then extend the example from above. This will run `n1` and `n2` in parallel.
You can investigate the graph on the Dask dashboard (typically
http://127.0.0.1:8787/graph or via the client object in Jupyter.)

```python
import znflow
import dataclasses
from dask.distributed import Client

@znflow.nodify
def compute_mean(x, y):
    return (x + y) / 2

@dataclasses.dataclass
class ComputeMean(znflow.Node):
    x: float
    y: float

    results: float = None

    def run(self):
        self.results = (self.x + self.y) / 2

with znflow.DiGraph() as graph:
    n1 = ComputeMean(2, 8)
    n2 = compute_mean(13, 7)
    # connecting classes and functions to a Node
    n3 = ComputeMean(n1.results, n2)

client = Client()
deployment = znflow.deployment.Deployment(graph=graph, client=client)
deployment.submit_graph()

n3 = deployment.get_results(n3)
print(n3)
# >>> ComputeMean(x=5.0, y=10.0, results=7.5)
```

We need to get the updated instance from the Dask worker via
`Deployment.get_results`. Due to the way Dask works, an inplace update is not
possible. To retrieve the full graph, you can use
`Deployment.get_results(graph.nodes)` instead.

### Working with lists

ZnFlow supports some special features for working with lists. In the following
example we want to `combine` two lists.

```python
import znflow

@znflow.nodify
def arange(size: int) -> list:
    return list(range(size))

print(arange(2) + arange(3))
>>> [0, 1, 0, 1, 2]

with znflow.DiGraph() as graph:
    lst = arange(2) + arange(3)

graph.run()
print(lst.result)
>>> [0, 1, 0, 1, 2]
```

This functionality is restricted to lists. There are some further features that
allow combining `data: list[list]` by either using
`data: list = znflow.combine(data)` which has an optional `attribute=None`
argument to be used in the case of classes or you can simply use
`data: list = sum(data, [])`.

### Attributes Access

Inside the `with znflow.DiGraph()` context manager, accessing class attributes
yields `znflow.Connector` objects. Sometimes, it may be required to obtain the
actual attribute value instead of a `znflow.Connector` object. It is not
recommended to run class methods inside the `with znflow.DiGraph()` context
manager since it should be exclusively used for building the graph and not for
actual computation.

In the case of properties or other descriptor-based attributes, it might be
necessary to access the actual attribute value. This can be achieved using the
`znflow.get_attribute` method, which supports all features from `getattr` and
can be imported as such:

```python
from znflow import get_attribute as getattr
```

Here's an example of how to use `znflow.get_attribute`:

```python
import znflow

class POW2(znflow.Node):
    """"""Compute the square of x.""""""
    x_factor: float = 0.5
    results: float = None
    _x: float = None

    @property
    def x(self):
        return self._x

    @x.setter
    def x(self, value):
        # using ""self._x = value * self.x_factor"" inside ""znflow.DiGraph()"" would run
        # ""value * Connector(self, ""x_factor"")"" which is not possible (TypeError)
        # therefore we use znflow.get_attribute.
        self._x = value * znflow.get_attribute(self, ""x_factor"")

    def run(self):
        self.results = self.x**2

with znflow.DiGraph() as graph:
    n1 = POW2()
    n1.x = 4.0

graph.run()
assert n1.results == 4.0

```

Instead, you can also use the `znflow.disable_graph` decorator / context manager
to disable the graph for a specific block of code or the `znflow.Property` as a
drop-in replacement for `property`.

# Supported Frameworks

ZnFlow includes tests to ensure compatibility with:

- ""Plain classes""
- `dataclasses`
- `ZnInit`
- `attrs`

It is currently **not** compatible with pydantic. I don't know what pydantic
does internally and wasn't able to find a workaround.
",2023-07-07 18:52:54+00:00
zymake,zymake-mirror,samuell/zymake-mirror,Mirror of the zymake tool. (See http://www-personal.umich.edu/~ebreck/code/zymake ),,False,4,2019-03-04 09:32:02+00:00,2015-03-03 13:42:44+00:00,0,3,1,0,,,,11,,0,,,2015-03-03 14:21:57+00:00,"# Mirror of the source code of zymake

- See [main site](http://www-personal.umich.edu/~ebreck/code/zymake/) for more information.

----
# A copy of the original webpage follows below, for documentation purposes
----

`zymake` is a high-level language for running complex sets of
experiments. The user writes a zymakefile, mostly consisting of
parameterized shell commands, and zymake determines the dependency
structure and executes the commands in the appropriate order.

Highlights
----------

-   make-like semantics, shell-like syntax
-   execution order follows dependencies, files are rebuilt only if
    older than files on which they depend, etc.
-   all filenames are inferred by the system
-   a file is determined by a set of key-value pairs, such as
    ""method=svm"" or ""number-of-hidden-units=10""
-   simple interpolation syntax: everything apart from whitespace and
    `$(...)` is passed untouched to the shell for execution.
-   parallel execution

Links
-----

-   [A slideshow (PDF) introducing
    `zymake`](/~ebreck/data/zymake/talk.pdf)
-   [Another slideshow (based on the paper
    below)](/~ebreck/data/zymake/AclSofteng.pdf)
-   [Download a Windows version of
    zymake](/~ebreck/data/zymake/win/zymake.exe)
-   [Download a Linux-x86 version of
    zymake](/~ebreck/data/zymake/linux/zymake)
-   [Download a Mac OSX-PPC version of
    zymake](/~ebreck/data/zymake/ppc/zymake)
-   [Download the zymake source
    code](/~ebreck/data/zymake/zymake-0.3.0-src.zip). Requires an
    installation of [Objective Caml](http://caml.inria.fr) to compile.
-   [Read a paper](http://www.aclweb.org/anthology/W/W08/W08-0503.pdf)
    -- `2008` Eric Breck. **zymake: a computational workflow
    system for machine learning and natural language processing.** In
    *Proceedings of the Workshop on 2008 ACL workshop on Software
    Engineering, Testing, and Quality Assurance for Natural Language
    Processing*.
-   A complete real-world [zymakefile](/~ebreck/data/zymake/ijcai07-run)
    used for the experiments reported in (Breck, Choi & Cardie, '07).
-   [An announcement-only mailing list for users interested in
    `zymake`.](http://tech.groups.yahoo.com/group/zymake-announce/)

Try it! - download zymake for your platform and the zymakefile above,
and run

```bash
zymake -d ijcai07-run
```

to see what it would execute (nothing will actually be run, that's what
`-d` means)

Some introductory examples
--------------------------

-   **Compilation**

    The make rule

    ```make
	%.exe: %.c
			cc -o $@ $^
	```

    would be written in zymake as

	```bash
	cc -o $(>).exe $().c
	```

    Note that rather than separating the specification of dependencies
    from the shell command, zymake integrates the two. In this case, the
    output file (the .exe) needs to be specified with the `>` character.
    The semantics of this rule are almost identical to that of make.

-   **Cross-validation**

    Suppose we have two commands. `run` takes an argument (the
    cross-validation fold) and produces an output (the result of running
    on that fold). `average-folds` takes a list of
    `run`-outputs and averages them, producing a LaTeX table. We
    can run this for 10 folds like this.

        run $(fold) > $().eval

        average_folds $(fold=*(range 1 10)) $().eval > $().table

Usage
-----

A `zymake` file consists of a series of *definitions* and
*rules*. A definition defines an immutable global variable. A rule
specifies a shell command to run, which takes certain kinds of files as
input and certain files as output. Rules with no outputs are called
*queries*, and the goal is to be able to execute all of the queries.
What `zymake` does is to determine what commands are necessary to
be able to execute the queries, and in what order to execute them.

For those who care, this involves constructing a directed acyclic graph
(in which each node is an interpolated rule) and executing each node in
topological order.

Parallel execution
------------------

This part is not fully tested. It seems to work, but I haven't done
anything large-scale with it yet. The basic idea is that as you're
proceeding through the dag in topological order, at any point where
multiple rules could be executed next, you execute them all in parallel.

There are, at the moment, two methods of parallel execution provided.
First, the user provide a list of compute nodes in a special global
variable called `machines`, and the system works out which nodes
are least loaded, and runs the processes on those using `ssh`.
Second, the user can provide a script `start` which will run a
given command on another machine (presumably through some sort of
job-submission). `zymake` requires that this script wait until
the job completes beore returning.

I'm open to other sorts of interfaces here, e.g. with the machine
learning cluster's queueing mechanisms, I just don't know what they are.

Files (key-value sets)
----------------------

One of the things that differentiates `zymake` from standard
`make` is how it understands files. Essentially, `make`
treats each filename as a string. For `zymake`, a file is a set
of key-value pairs. For example, a file might be defined by
`model=svm fold=2 C=0.5`. Each file also has a distinguished key,
the file suffix (such as `.svm`, `.eval`, or
`.output`). Each file does not have to define a value for each
key.

Matching
--------

A rule need only specify the information about a file that is relevant
for that rule. Other keys will be inferred and added as necessary. For
example, the rule for an evaluation script might specify

```bash
eval $(metric) $().predictions > $().eval
```

The `.eval` file must have the `metric` key, but it may
have many other keys as well, which will be passed along to the
`.predictions` file if they are needed.

Starting with the queries, the matcher tries to figure out how to build
each file needed. It matches the files it needs against all the rules,
trying to find a rule which produces an output all of whose keys are
present in the needed file. This must match exactly one rule; matching
zero or more than one rule is an error.

Syntax
------

As much as possible, the goal of `zymake`'s syntax is to avoid
having unnecessary escaping. Therefore, the rules that you write in the
zymakefile correspond to the strings that are passed to the shell for
execution, with two exceptions: any sequence of whitespace (including
newlines) are collapsed into a single space, and interpolations -
anything beginning with the characters `$(` and ending with a matching `)`
- are replaced by their value. Different rules are separated by blank
lines.

The syntax for global variable definitions is like that of rules, except
that a definition begins with `identifier =`. Definitions can
also appear on adjacent lines without an intervening blank line.

### Comments

Any line beginning with `#` is a comment. Any comments
immediately preceding a rule (with no intervening blank lines) are
associated with the rule. During execution, the comment can optionally
be displayed when the rule is executed, providing the user with a
description of what's going on that may be more comprehensible than the
command string.

A real example:

    grep -v ""fold $(fold)"" $().nz-svm | perl -ane '$eos = /#endsent/; 
    $class = m{#in'$(class)' }?""pos"":""neg""; s/#.*//; @F=split; 
    print ""@F[1..$#F] $class\n""; print ""\n"" if $eos' >$(both=""false"").mallet-train

What does this do? Well, if I precede it with the comment:

    # Make training file

then at runtime, 'Make training file' can be printed in addition to (or
instead of) the command-string above.

Interpolations
--------------

There are two kinds of interpolations: expression interpolations, and
file interpolations. An expression interpolation computes some
expression whose value is interpolated into the command to be executed.
A file interpolation represents an input or output file created or
needed by the rule. The file's name is inferred, and the filename is
interpolated into the command.

Syntax:

```bnf
File-interpolation ::= $( key1 = value1 key2 = value2 ... ).suffix

Expression-interpolation ::= $( value )

Expression-interpolation ::= $( value0 value1 value2 ... )
```

The latter syntax is shorthand for

`$( ( value0 value1 value2 .... ) )`, i.e. it saves you a level
of parentheses.

```bnf
Value ::= integer-literal | string-literal | identifier | ( value0 value1 value2 .... )
```

Identifiers evaluate to the value of the corresponding key, or the value
of the corresponding global variable if no key exists.

Lists evaluate by evaluating the first value. If this is a special form,
it is directly applied to the later values; otherwise, the other values
are evaluated, and the function is applied to them.

Interpolations are always introduced by the characters `$(`. To
interpolate the literal characters `$(`, write `$(()`.

File interpolations are currently not allowed in global variable
definitions. It's not clear what it would mean; if you can come up with
a compelling semantics and use case for this, let me know and I'll think
about including it.

### Input and output files

A file interpolation is an input unless otherwise specified. If the
interpolation begins with the `>` character, it is an output, or
if the most recent character before the interpolation was a `>`
(to cover the common case of creating a file by output redirection).
This latter case can be overridden by beginning the interpolation with
`<`.

### ""splats""

There are often cases where you'd like to write a single expression but
have it represent a list of objects. For example, I might want to write
that a final table depends on evaluation outputs from 10
cross-validation folds. I could write this using a 'splat':

```bash
make-table $(fold=*(1 2 3 4 5 6 7 8 9 10)).eval > $().table 
```

The asterisk indicates that the file interpolation should be replicated
once for each value in the list following the asterisk.

More than one key can be 'splatted' in a given file interpolation, in
which case the cross-product of all values will be created (i.e.
splatting a 3-value list and a 4-value list will result in 12 files).

You can also splat expression interpolations by writing
`$( * value)`. This allows you to join existing lists together.

### Functions

A small set of functions and special forms is provided for use in
interpolations.

-   `quote` Like Lisp or Scheme, this prevents evaluation of the
    following expression. This can also be written by preceding the
    expression with a single quote '
-   `list` creates a list composed of the following expressions.
-   `flatten` creates a list from a list of lists
-   `range` creates a list from a start value to a finish value,
    either of integers or of characters
-   `split` takes a string and returns a list of strings,
    splitting the initial string by whitespace
-   `concat` takes a list of strings or integers and returns the
    result of joining them all together (with no intervening spaces)
-   `shell` takes a string argument, and returns the standard
    output of executing that command. This is just like the shell
    function in GNU make or backticks in shells or Perl.
-   ... there are some others

Filenames
---------

To zymake, a file is uniquely determined by its set of key-value pairs
(including the suffix). The filesystem, however, requires that a file
have a string name. Therefore, zymake has a way of creating a mapping
between filenames and key-value sets. One of the basic principles of
zymake is that the user shouldn't depend on what these filenames look
like (apart from the suffix). But since you probably will want to look
at the files individually, here's a guide to how zymake does the
mapping.

First, zymake creates a mapping from 'labels' to key, value pairs. If
the value only occurs with one key in your zymakefile, then the label
will often just be the value (with some modification to make it a
filesystem-friendly string). If the value is a variable interpolation,
the label will often be the name of the variable. Additional characters
may be added to the end of the label to make this mapping unique. This
mapping is written out to the file `o/o.zymakefilename._dict`.

Next, zymake creates a name for a file by concatenating the labels for
all its key, value pairs (in a fixed order), followed by the suffix,
separated by periods. Finally, zymake prefixes a string unique to the
zymakefile, typically `o/o.zymakefilename.`

Key set inference
-----------------

`FIXME: replace this with declarative description`

The goal is that a file should contain all and only the keys it needs.
The algorithm to determine which keys a file has is this:

-   start at the queries (commands with no output), with empty keys.
-   execute (command, keys):
-   propagate all keys to inputs, adding any explicit input keys.
-   for each input execute (input,keys); replace input with this return
    val.
-   output gets keys filtered by: keys used by inputs, keys mentioned in
    interpolations in this command, keys mentioned in output.
-   return output

Query mode
----------

`zymake.byt -q QUERY zymakefile` prints to standard output the
result of parsing QUERY as if it were a command in a zymakefile. You can
use this, among other things, to print out the filename that zymake
would create for a given file interpolation: e.g.
`zymake.byt -q '$(a=1 b=""foo"").bar' zymakefile` would print out
something like ""o/zymakefile/o.1.foo.bar"".

Future features
---------------

-   Additional facilities for running jobs in parallel
-   Run-time variance of the dag
-   ...

Reporting bugs
--------------

Send me an e-mail if something breaks. The more info the better. In
particular, you can run:

```bash
zymake -vvv zymakefile
```

and send me everything that gets spewed out. If it's so much spew that
it's taking forever, delete some of the -vs.

Contributors
------------

`zymake` was written by [Eric
Breck](http://www-personal.umich.edu/~ebreck/).

",2023-07-07 18:52:59+00:00
